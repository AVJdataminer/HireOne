{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HiringApp_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNB/bccQcsNlQf19lGl3sDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AVJdataminer/HireOne/blob/master/HiringApp_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94PhLbg4tV_r",
        "colab_type": "text"
      },
      "source": [
        "1. Extract text from Resumes\n",
        "2. Job listings data from example Indeed API saved datasets\n",
        "3. Using Cosine similarity we will match resumes to job posting\n",
        "4. Review the scoring for matches\n",
        "5. Connect to streamlit app "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqYFf-x3w8NB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.summarization import keywords\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxf5mc9Jw8OA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "df7cbc67-e698-4ed3-ef81-9fc4990a34fe"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/AVJdataminer/HireOne/master/data/job_descriptions.csv', encoding = 'unicode_escape')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jobOrResumeDescription</th>\n",
              "      <th>role</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>: Artificial Intelligence / Machine Learning D...</td>\n",
              "      <td>Developer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>: Data Scientist/Architect\\n: 6+ months + Hig...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>: Data Analyst\\n: Davidson, NC\\n: 04+ Months\\...</td>\n",
              "      <td>Data Analyst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>: Big Data Architect or Data Scientist\\n: New...</td>\n",
              "      <td>Data Scientist</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>: Data Engineer\\n: Woonsocket, RI\\n: 6+ Months...</td>\n",
              "      <td>Data Engineer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              jobOrResumeDescription            role\n",
              "0  : Artificial Intelligence / Machine Learning D...       Developer\n",
              "1   : Data Scientist/Architect\\n: 6+ months + Hig...  Data Scientist\n",
              "2   : Data Analyst\\n: Davidson, NC\\n: 04+ Months\\...    Data Analyst\n",
              "3   : Big Data Architect or Data Scientist\\n: New...  Data Scientist\n",
              "4  : Data Engineer\\n: Woonsocket, RI\\n: 6+ Months...   Data Engineer"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzYIKHX8HMXC",
        "colab_type": "text"
      },
      "source": [
        "Clean up job description column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UK0cxBZw8NJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.replace('\\n', ' ')                # remove newline\n",
        "    text = text.replace(':', ' ')\n",
        "    return text\n",
        "df['description'] = df.apply(lambda x: clean_text(x['jobOrResumeDescription']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3zxu0UuHqG4",
        "colab_type": "text"
      },
      "source": [
        "Print first job desc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6NG3nbDKvOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "ef5156d4-b420-451e-95f4-2cbb94b31ccb"
      },
      "source": [
        "df['description'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"  Artificial Intelligence / Machine Learning Developer     Irving TX  Terms  Contract   Details             Bachelor's degree or 7-10 or more years of relevant  experience.     7+ years of server app development (design/develop/deploy).     3+ years of Python 3.x, experience in ML algorithms/data analytics.     5+ years of advanced SQL development (ER modeling, SQL scripts, stored procedures, functions, s) with RDBMS such as PostgreSQL/MS SQL Server.     3+ years on AWS S3, EC2, Serverless computing (Lambda).     3+ years of experience/familiarity with DevOps using Stash/Jenkins/Chef and Puppet.     Excellent communication  in interfacing with different cross-functional teams.         5+ years of experience in designing, building applications using .NET platform using C#, .NET Core, ORM, SQL, MS SQL Server, Visual Studio.     1+ years' experience in developing containerized Docker .net core apps.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yom1LRGaIc9J",
        "colab_type": "text"
      },
      "source": [
        "Create a list from the cleaned job description column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YE6II7vhIhxg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jd = df['description'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Q8puCuIQEu",
        "colab_type": "text"
      },
      "source": [
        "Build model to tag each job description as a seperate document.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhMYG7L6Md3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "323ffc87-3162-4390-895f-d9f8afe3912f"
      },
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim import models\n",
        "# Create the tagged document needed for Doc2Vec\n",
        "def create_tagged_document(list_of_list_of_words):\n",
        "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
        "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
        "\n",
        "train_data = list(create_tagged_document(jd))\n",
        "\n",
        "print(train_data[:1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TaggedDocument(words=\"  Artificial Intelligence / Machine Learning Developer     Irving TX  Terms  Contract   Details             Bachelor's degree or 7-10 or more years of relevant  experience.     7+ years of server app development (design/develop/deploy).     3+ years of Python 3.x, experience in ML algorithms/data analytics.     5+ years of advanced SQL development (ER modeling, SQL scripts, stored procedures, functions, s) with RDBMS such as PostgreSQL/MS SQL Server.     3+ years on AWS S3, EC2, Serverless computing (Lambda).     3+ years of experience/familiarity with DevOps using Stash/Jenkins/Chef and Puppet.     Excellent communication  in interfacing with different cross-functional teams.         5+ years of experience in designing, building applications using .NET platform using C#, .NET Core, ORM, SQL, MS SQL Server, Visual Studio.     1+ years' experience in developing containerized Docker .net core apps.\", tags=[0])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsndaLppIhJv",
        "colab_type": "text"
      },
      "source": [
        "Train the model on the job descriptions for matching later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIri67T3IlaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Init the Doc2Vec model\n",
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
        "\n",
        "# Build the Volabulary\n",
        "model.build_vocab(train_data)\n",
        "\n",
        "# Train the Doc2Vec model\n",
        "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXtAilyI07r",
        "colab_type": "text"
      },
      "source": [
        "Let's look at an example of how it converts a list of words to a vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lbisgLtIyIX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "19ccabb8-f32f-4d92-bcb9-cda5b020a70f"
      },
      "source": [
        "print(model.infer_vector(['data', 'science','python']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-4.1420567e-03 -8.9440355e-03  2.6590624e-03  6.8354174e-03\n",
            "  3.2979506e-03  6.3462456e-04  1.5134492e-03 -1.3752561e-03\n",
            "  3.1064297e-03 -8.6344322e-03  9.1372561e-03  1.5266376e-03\n",
            " -8.9762127e-03 -5.0899806e-03  8.4859366e-03  9.4511155e-03\n",
            "  3.6477793e-03 -9.6783286e-04 -7.2281910e-03 -2.0835691e-03\n",
            "  5.8390694e-03 -3.7875264e-03 -8.2854629e-03  6.2385662e-03\n",
            " -9.2807328e-03 -1.2103679e-03  4.2513972e-03  6.5911789e-03\n",
            "  6.5410248e-05  2.0612173e-03 -5.1063364e-03 -7.6137753e-03\n",
            " -1.4367555e-03  1.8819019e-03  1.9426921e-03  2.0372469e-03\n",
            "  7.4612787e-03 -1.4206677e-03  9.9517778e-03  2.7751981e-04\n",
            "  7.3231524e-04 -7.2414693e-03 -4.6009589e-03 -8.4810983e-03\n",
            "  2.3353449e-03  7.6348139e-03  5.6486344e-03  5.3576315e-03\n",
            " -2.8136016e-03 -3.1035359e-04]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FBp0Z2fMmd-",
        "colab_type": "text"
      },
      "source": [
        "Here we apply the model to each job description in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83wjxiCoMl46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "for i in range(len(jd)):\n",
        "    data.append(model.docvecs[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BeFuMMLJAB6",
        "colab_type": "text"
      },
      "source": [
        "## Now let's load the text from our resume"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vD2ko0AKL-q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d84a4cc5-2569-4baa-d7ae-319d4c96ca49"
      },
      "source": [
        "resume = pd.read_csv('https://raw.githubusercontent.com/AVJdataminer/HireOne/master/data/resumes.csv', encoding = 'unicode_escape')\n",
        "resume.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>jobOrResumeDescription</th>\n",
              "      <th>role</th>\n",
              "      <th>sourceType</th>\n",
              "      <th>description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Â  with around 5 years of experience in all p...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>resume</td>\n",
              "      <td>Â  with around 5 years of experience in all p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n \\nData scientist with a strong math backgro...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>resume</td>\n",
              "      <td>Data scientist with a strong math backgroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n\\n\\n* Around 4+  years of experience in Data...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>resume</td>\n",
              "      <td>* Around 4+  years of experience in Data An...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n\\nExpert in logical and problem-solving  wit...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>resume</td>\n",
              "      <td>Expert in logical and problem-solving  with ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Experienced  with 2+ years of hands-on experie...</td>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>resume</td>\n",
              "      <td>Experienced  with 2+ years of hands-on experie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              jobOrResumeDescription  ...                                        description\n",
              "0  Â  with around 5 years of experience in all p...  ...  Â  with around 5 years of experience in all p...\n",
              "1  \\n \\nData scientist with a strong math backgro...  ...     Data scientist with a strong math backgroun...\n",
              "2  \\n\\n\\n* Around 4+  years of experience in Data...  ...     * Around 4+  years of experience in Data An...\n",
              "3  \\n\\nExpert in logical and problem-solving  wit...  ...    Expert in logical and problem-solving  with ...\n",
              "4  Experienced  with 2+ years of hands-on experie...  ...  Experienced  with 2+ years of hands-on experie...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzdqFARmLUW0",
        "colab_type": "text"
      },
      "source": [
        "We only need one resume to start with so let's select the first one and split into words to push into our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeZUSh18P_Ye",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r1 = resume['description'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blLQ9AqBLblu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resume = resume['description'].iloc[0].split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXdIQ6qzMNxv",
        "colab_type": "text"
      },
      "source": [
        "Review the resulting vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ciaV5xMH-n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d314c6af-4fb6-4715-f0b3-b991e0b136a6"
      },
      "source": [
        "print(model.infer_vector(resume))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.2703119   0.46140584  1.3335586   0.194047    0.3511251  -0.05135229\n",
            "  1.2376658   1.104154    0.6905708  -0.66995645  0.00775168  1.7607471\n",
            " -0.56709665  0.71579945  1.4739642  -0.98076457 -0.70780426  0.93041635\n",
            " -0.38668808  0.03639724 -0.569789    0.30209452  1.1538218  -0.1179663\n",
            " -0.64267266 -0.57093453  0.14116059 -0.3806373   0.542782   -0.4670417\n",
            " -0.9182177  -0.00489526  2.1500022  -0.5747738   0.09500379 -0.12688264\n",
            "  0.07573358  0.37014386 -1.56851     0.15511167 -0.06379291 -0.63691014\n",
            "  0.957231    1.1769577  -1.1278763   0.05250629  0.93851274  0.62922686\n",
            " -0.03870421 -0.44639188]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdG-iOPhMTJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resume_vect = model.infer_vector(resume)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYvrFWw9M4dh",
        "colab_type": "text"
      },
      "source": [
        "## Compare our resume to the job descriptions using PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY7SGrOwMbgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "50405b4c-ee60-48e2-db15-27d0d7ce4540"
      },
      "source": [
        "def plot_pca(data):\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=2) #, whiten=True\n",
        "    X = pca.fit_transform(data)\n",
        "    xs,ys =X[:,0], X[:,1]\n",
        "    plt.scatter(X[:,0], X[:,1])\n",
        "    plt.scatter(xs[-1], ys[-1], c='Red', marker='+')\n",
        "    plt.text(xs[-1], ys[-1],'resume')\n",
        "    plt.grid()\n",
        "    plt.suptitle('PCA')\n",
        "    #plt.savefig('distance_PCA_improved.png')\n",
        "    plt.show()\n",
        "plot_pca(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyMDvR4-NG3d",
        "colab_type": "text"
      },
      "source": [
        "## Calculate the cosine distances between our resume and each of the job descriptions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZYbYjokM-KO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "cos_dist =[]\n",
        "for i in range(len(data)):\n",
        "    cos_dist.append(float(cosine_distances(resume_vect[0:].reshape(1,-1),data[i].reshape(1,-1))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf_bGHdyNja6",
        "colab_type": "text"
      },
      "source": [
        "create a key words list for each job description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVYzteudNgqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "key_list =[]\n",
        "\n",
        "for j in jd:\n",
        "    key =''\n",
        "    for word in keywords(j).split('\\n'):\n",
        "        key += '{} '.format(word)\n",
        "    key_list.append(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whc0xp1xNa4y",
        "colab_type": "text"
      },
      "source": [
        "Create a nice data frame to put the scores and keywords together. Print out the first 10 lowest scores. Those jobs will the most similar to the resume."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM85b1XbNW_o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "4b0136da-7d70-474c-885d-0e5cece648ab"
      },
      "source": [
        "role = df['role'].tolist()\n",
        "summary = pd.DataFrame({\n",
        "        'Role Title': role,\n",
        "        'Cosine Distances': cos_dist,\n",
        "        'Keywords': key_list,\n",
        "        'Job Description': jd\n",
        "    })\n",
        "z = summary.sort_values(by ='Cosine Distances', ascending=True)\n",
        "z.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Role Title</th>\n",
              "      <th>Cosine Distances</th>\n",
              "      <th>Keywords</th>\n",
              "      <th>Job Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>0.408452</td>\n",
              "      <td>knowledge engineer experience learning data th...</td>\n",
              "      <td>- Data Scientist / Data Engineer    - Chica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>0.493039</td>\n",
              "      <td>knowledge experience learning scientist theano</td>\n",
              "      <td>Data Scientist    Chicago, IL    Contract &amp;...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>0.605699</td>\n",
              "      <td>experience data lead leading time bdm field di...</td>\n",
              "      <td>Big Data Lead    Raritan- NJ    -12 months  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>0.616657</td>\n",
              "      <td>experience experiments data models model model...</td>\n",
              "      <td>Data Scientist     Richardson, TX    Full T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>Data Engineer</td>\n",
              "      <td>0.632020</td>\n",
              "      <td>computational high computing numerical methods...</td>\n",
              "      <td>Applied Computational Mathematician / Engineer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>0.661276</td>\n",
              "      <td>solutions like perform alternative solution pr...</td>\n",
              "      <td>Sr. Business Analyst UC Innovation - Irvine, C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Data Analyst</td>\n",
              "      <td>0.680710</td>\n",
              "      <td>data experience including providing business m...</td>\n",
              "      <td>Senior Healthcare Data Analyst Advantmed98 re ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>0.688905</td>\n",
              "      <td>experience data business solutions strong team...</td>\n",
              "      <td>-Lead BI    - Cheektowaga, NY   -Full Time ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Developer</td>\n",
              "      <td>0.692190</td>\n",
              "      <td>years developer development developing sql ser...</td>\n",
              "      <td>Artificial Intelligence / Machine Learning D...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Data Engineer</td>\n",
              "      <td>0.719788</td>\n",
              "      <td>data processing process models good solutions ...</td>\n",
              "      <td>Big Data Architect DE, PA, NJ, NYC, MA s (50-6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Role Title  ...                                    Job Description\n",
              "34  Data Scientist  ...     - Data Scientist / Data Engineer    - Chica...\n",
              "35  Data Scientist  ...     Data Scientist    Chicago, IL    Contract &...\n",
              "20  Data Scientist  ...    Big Data Lead    Raritan- NJ    -12 months  ...\n",
              "30  Data Scientist  ...     Data Scientist     Richardson, TX    Full T...\n",
              "98   Data Engineer  ...  Applied Computational Mathematician / Engineer...\n",
              "76    Data Analyst  ...  Sr. Business Analyst UC Innovation - Irvine, C...\n",
              "72    Data Analyst  ...  Senior Healthcare Data Analyst Advantmed98 re ...\n",
              "24  Data Scientist  ...     -Lead BI    - Cheektowaga, NY   -Full Time ...\n",
              "0        Developer  ...    Artificial Intelligence / Machine Learning D...\n",
              "37   Data Engineer  ...  Big Data Architect DE, PA, NJ, NYC, MA s (50-6...\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y2zTt1XPpFE",
        "colab_type": "text"
      },
      "source": [
        "Let's print the first job description and our resume text to visually compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGRBAfmPPnZl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "6207ec44-3a5e-4414-ef5a-fbacf1de7f1d"
      },
      "source": [
        "z['Job Description'].iloc[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'   - Data Scientist / Data Engineer    - Chicago, IL    - Long Term  Exp Req  - 8+ Years        -      Senior data scientist / engineer      Financial Domain Knowledge & experience     Strong Experience in AI related      Knowledge & exposure in rendering ML functionality     Understanding  AI/Deep Learning algorithm such as CNN, RNN, LSTM     Experience in building AI based NLP and OCR solution using Keras, Google Tensorflow, Theano, Caffe 2 etc? '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M8mMowlP5OB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "d13b3aff-57a3-47e9-9944-697a9be86afc"
      },
      "source": [
        "r1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"Â\\x95  with around 5 years of experience in all phases of diverse   specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.   Â\\x95 ed on analyzing large datasets on distributed databases and developing Machine Learning algorithms to gain operational insights and present them to the leadership.   Â\\x95 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervised and unsupervised modeling.   Â\\x95 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.   Â\\x95 Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX)   Â\\x95 Adapted statistical programming languages like R and Python   Â\\x95 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.   Â\\x95 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.   Â\\x95 Created dashboards as part of Data Visualization using Tableau.   Â\\x95 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.   Â\\x95 Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.   Â\\x95 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.   Â\\x95 Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.   Â\\x95 Performed multiple Data Mining techniques and derive new insights from the data.   Â\\x95 Skilled in Machine Learning, Statistical Modeling, and Big Data.   Â\\x95 Creative problem-solver with strong analytical, leadership, and communication    Â\\x95 Proficient in Python, R, Scala, Java, SQL, and C   Â\\x95 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling   Â\\x95 Data Science Specialties include: Machine Learning, Sequential Modeling, Natural Language Processing (NLP)   Â\\x95 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics   Â\\x95 Experienced in stochastic optimization and regression with machine learning algorithms   Â\\x95 Experienced in formulating and solving discrete and continuous optimization problems   Â\\x95 Able to research statistical machine learning, supervised learning, and classification methods   Â\\x95 Strong mathematical and statistical modeling and computer programming  in an innovative manner   Â\\x95 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, NaÃ¯ve Bayes, Support Vector Machines   Â\\x95 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets   Â\\x95 Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets, TensorFlow, Keras   Â\\x95 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights   Â\\x95 Development of clear analytical reports which directly address strategic goals   Â\\x95 Identified and learn applicable new techniques independently as needed   Â\\x95 Able to  comfortably and effectively within an interdisciplinary research environment   Â\\x95 Experienced with validation of machine learning ensemble classifiers   Â\\x95 Utilized the online datasets to implement machine learning models using Spark ML for building prototypes  Willing to relocate: Anywhere  Authorized to  in the US for any employer   Experience  Data Science Engineer  Jefferies LLC - Jersey City, NJ  February 2019 to Present  Responsibilities:   Â\\x95 Coded in Python with selenium and automated website data scraping   Â\\x95 Scripted using R for cleaning, merging and extraction of relevant data   Â\\x95 Created interactive visualization using Tableau and performed data analysis to report findings and trends   Â\\x95 Analyzed massive data models with tables having over 100s of millions of records to draw insights and useful information.   Â\\x95 Architect complex database systems on Hadoop to scale out data development processes.   Â\\x95 Explore  and gather insights from Amfam's operational data stores and data warehouses (stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau).   Â\\x95 Designed and developed scripts to test data and find data defects.   Â\\x95 Determined the quality of data, verify accuracy of information and ensure that the data is fit for modeling purposes.   Â\\x95 Transformed data elements and attributes into usable form based on business requirements.   Â\\x95 Blend data sets at different granularity levels using analytical queries, window functions and SQL joins.   Â\\x95 Identified data duplicates and develop means to remove them.   Â\\x95 Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries, MapReduce or python.   Â\\x95 Designed and develop data pipelines to preprocess modeling data such as handle null values and clean up defective data attributes.   Â\\x95 Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and CSVs into hive tables or data frames.   Â\\x95 Explored and determined ways to organize data in Hive tables for fast read and writes through hive table partitions and buckets for optimized performance.   Â\\x95 Developed programs to store data in appropriate file formats and logical grouping in tables.   Â\\x95 Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed data marts on Hadoop   Â\\x95 Maintained development activities in version control and create updated documentation.      Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL  Data Scientist  Anthem - Atlanta, GA  June 2018 to February 2019  Responsibilities:   Â\\x95 Translated business questions into research s, design and conduct analyses, develop findings and synthesize recommendations to deliver valuable, relevant, and actionable insights   Â\\x95 Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)   Â\\x95 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developing various machine learning models such Random forest   Â\\x95 The missing data in the dataset is handled using Imputer method in SkLearn library   Â\\x95 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encoder methods in sklearn library   Â\\x95 Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for modeling   Â\\x95 Defined a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores   Â\\x95 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standard machine learning datasets   Â\\x95 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenue for future   Â\\x95 ed with applied statistics and applied mathematics  for performance optimization   Â\\x95 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores   Â\\x95 Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms   Â\\x95 Used cross-validation to test the models with different batches of data to optimize the models and prevent over fitting   Â\\x95 Analyzed the SQL scripts and designed the solution to implement using PySpark and developed scripts as per the requirement   Â\\x95 ed with Tableau in order to represent the data in visual format and better describe the problem with solutions.      Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL  Data Scientist  US Bank - Brookfield, WI  September 2017 to June 2018  Responsibilities:   Â\\x95 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to develop operational and visual reports for KPI monitoring   Â\\x95 Data analysis and visualization (Python, R)   Â\\x95 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data   Â\\x95 Increased pace & confidence of learning algorithm by combining state of the art  and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes   Â\\x95 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format   Â\\x95 Implemented Topic Modelling, linear classifier models   Â\\x95 Collaborate with UI engineers, project managers, and designers to develop web portal that aggregates reports from various sources and    Â\\x95 Member of Data Science team tasked with helping clients turn data into a strategic asset   Â\\x95 Focused on front end features, browser manipulation, and cross-browser compatibility.   Â\\x95 Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and DOM to develop reporting portal.   Â\\x95 Used Agile Scrum for BI  across different clients, which allowed for production prototyping, rapid deployment and transparency.      Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.  Junior Data Scientist  Ordnance Factory Board - IN  January 2016 to July 2017  Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.      Responsibilities:   Â\\x95 ed on various phases of data mining- data collection, data cleaning, developing models, validation, visualization.   Â\\x95 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.   Â\\x95 Performed Data Manipulation and Aggregation from various sources including HDFS and created various Predictive and Descriptive analytics using R and Tableau.   Â\\x95 Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.   Â\\x95 Designed Predictive analysis algorithms using Historical Data.   Â\\x95 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.   Â\\x95 ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in Hadoop on AWS.   Â\\x95 ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports.      Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.  Data Science Intern  iPrism Technologies - Hyderabad, Telangana  April 2015 to December 2015  Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate.   Responsibilities:   Â\\x95 Collected data from end client, performed ETL and defined the uniform standard format   Â\\x95 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basic fields   Â\\x95 Performed string formatting on the dataset converting hours from date format to a numerical integer   Â\\x95 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the dataset such as day of week, age, hour and number of screens.   Â\\x95 Developed and implemented predictive models like Logistic Regression, Decision Tree, Support Vector Machine (SVM) to predict the probability of enrollment   Â\\x95 Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the final model based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment   Â\\x95 Tuned the hyper parameters of the above models using Grid Search to find the optimum models   Â\\x95 Designed and implemented K-Fold Cross-validation to test and verify the model's significance   Â\\x95 Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.      Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop  Education  Master's in Computer Engineering in Data Science and Analytics  Arizona State University - Tempe, AZ    Apache spark, Hadoop, Hive, Javascript, D3.js, Mapreduce, Natural, Pig, Python, Mapreduce, Data science, Hadoop, Machine learning, Natural language processing, Nosql, Ms sql server, Sql server, Mysql, Pl/sql, Sql  Additional Information   :   Programming languages Python, Java, R. C   Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio   Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception   Operating systems Windows, Linux.   Databases MySQL, MS SQL Server, NoSQL   Web and Cloud Technologies AWS, HTML5   Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiuCjeStOYUB",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Let's test it out on another resume or dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1EG7ZhrOJyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = pd.read_csv('https://raw.githubusercontent.com/JimKing100/techsearch/master/data/techsearch_p1.csv')\n",
        "df1 = df1.drop(df1.columns[0], axis=1)\n",
        "df2 = pd.read_csv('https://raw.githubusercontent.com/JimKing100/techsearch/master/data/techsearch_p2.csv')\n",
        "df2 = df2.drop(df2.columns[0], axis=1)\n",
        "both_df = pd.concat([df1, df2], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMVm-XbrQZjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bb92079c-21b7-4afe-b9b5-09d18f046034"
      },
      "source": [
        "both_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_title</th>\n",
              "      <th>company</th>\n",
              "      <th>location</th>\n",
              "      <th>description</th>\n",
              "      <th>counts</th>\n",
              "      <th>city</th>\n",
              "      <th>job</th>\n",
              "      <th>low_salary</th>\n",
              "      <th>high_salary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Data Scientist (All Levels) - Santa Clara</td>\n",
              "      <td>LeanTaaS</td>\n",
              "      <td>Santa Clara, CA 95050</td>\n",
              "      <td>Help build technology that saves lives!\\n\\nWe'...</td>\n",
              "      <td>1259</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>data scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Data Scientist (Intern) - United States</td>\n",
              "      <td>Cisco Careers</td>\n",
              "      <td>San Jose, CA</td>\n",
              "      <td>What You‚Äôll DoAcquire, clean and structure d...</td>\n",
              "      <td>1259</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>data scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Stanford University</td>\n",
              "      <td>Stanford, CA</td>\n",
              "      <td>Data Scientist (Data Analyst 2)\\nJob Family: I...</td>\n",
              "      <td>1259</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>data scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Data Scientist in Santa Clara, CA (corp-corp c...</td>\n",
              "      <td>Advantine Technologies</td>\n",
              "      <td>Santa Clara, CA</td>\n",
              "      <td>Job Description\\n\\nTitle : Data Scientist\\nLoc...</td>\n",
              "      <td>1259</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>data scientist</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Data Scientist</td>\n",
              "      <td>Palo Verde Consulting</td>\n",
              "      <td>Campbell, CA 95008</td>\n",
              "      <td>Job Title: Data ScientistLocation: Campbell, C...</td>\n",
              "      <td>1259</td>\n",
              "      <td>San Jose</td>\n",
              "      <td>data scientist</td>\n",
              "      <td>150000.0</td>\n",
              "      <td>210000.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           job_title  ... high_salary\n",
              "0          Data Scientist (All Levels) - Santa Clara  ...         NaN\n",
              "1            Data Scientist (Intern) - United States  ...         NaN\n",
              "2                                     Data Scientist  ...         NaN\n",
              "3  Data Scientist in Santa Clara, CA (corp-corp c...  ...         NaN\n",
              "4                                     Data Scientist  ...    210000.0\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLcPJxjoQ8pL",
        "colab_type": "text"
      },
      "source": [
        "If you feel like you've got it, go ahead apply the steps we used up above on the job descriptions in this new data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_IwTIVUQbtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}