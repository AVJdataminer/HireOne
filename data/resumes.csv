jobOrResumeDescription,role,sourceType,description
"  with around 5 years of experience in all phases of diverse   specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.  
 ed on analyzing large datasets on distributed databases and developing Machine Learning algorithms to gain operational insights and present them to the leadership.  
 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervised and unsupervised modeling.  
 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.  
 Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX)  
 Adapted statistical programming languages like R and Python  
 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.  
 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.  
 Created dashboards as part of Data Visualization using Tableau.  
 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  
 Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.  
 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  
 Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.  
 Performed multiple Data Mining techniques and derive new insights from the data.  
 Skilled in Machine Learning, Statistical Modeling, and Big Data.  
 Creative problem-solver with strong analytical, leadership, and communication   
 Proficient in Python, R, Scala, Java, SQL, and C  
 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling  
 Data Science Specialties include: Machine Learning, Sequential Modeling, Natural Language Processing (NLP)  
 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics  
 Experienced in stochastic optimization and regression with machine learning algorithms  
 Experienced in formulating and solving discrete and continuous optimization problems  
 Able to research statistical machine learning, supervised learning, and classification methods  
 Strong mathematical and statistical modeling and computer programming  in an innovative manner  
 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, Naïve Bayes, Support Vector Machines  
 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets  
 Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets, TensorFlow, Keras  
 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights  
 Development of clear analytical reports which directly address strategic goals  
 Identified and learn applicable new techniques independently as needed  
 Able to  comfortably and effectively within an interdisciplinary research environment  
 Experienced with validation of machine learning ensemble classifiers  
 Utilized the online datasets to implement machine learning models using Spark ML for building prototypes 
Willing to relocate: Anywhere 
Authorized to  in the US for any employer 
 Experience 
Data Science Engineer 
Jefferies LLC - Jersey City, NJ 
February 2019 to Present 
Responsibilities:  
 Coded in Python with selenium and automated website data scraping  
 Scripted using R for cleaning, merging and extraction of relevant data  
 Created interactive visualization using Tableau and performed data analysis to report findings and trends  
 Analyzed massive data models with tables having over 100s of millions of records to draw insights and useful information.  
 Architect complex database systems on Hadoop to scale out data development processes.  
 Explore  and gather insights from Amfam's operational data stores and data warehouses (stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau).  
 Designed and developed scripts to test data and find data defects.  
 Determined the quality of data, verify accuracy of information and ensure that the data is fit for modeling purposes.  
 Transformed data elements and attributes into usable form based on business requirements.  
 Blend data sets at different granularity levels using analytical queries, window functions and SQL joins.  
 Identified data duplicates and develop means to remove them.  
 Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries, MapReduce or python.  
 Designed and develop data pipelines to preprocess modeling data such as handle null values and clean up defective data attributes.  
 Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and CSVs into hive tables or data frames.  
 Explored and determined ways to organize data in Hive tables for fast read and writes through hive table partitions and buckets for optimized performance.  
 Developed programs to store data in appropriate file formats and logical grouping in tables.  
 Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed data marts on Hadoop  
 Maintained development activities in version control and create updated documentation.  
  
Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL 
Data Scientist 
Anthem - Atlanta, GA 
June 2018 to February 2019 
Responsibilities:  
 Translated business questions into research s, design and conduct analyses, develop findings and synthesize recommendations to deliver valuable, relevant, and actionable insights  
 Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)  
 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developing various machine learning models such Random forest  
 The missing data in the dataset is handled using Imputer method in SkLearn library  
 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encoder methods in sklearn library  
 Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for modeling  
 Defined a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores  
 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standard machine learning datasets  
 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenue for future  
 ed with applied statistics and applied mathematics  for performance optimization  
 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores  
 Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms  
 Used cross-validation to test the models with different batches of data to optimize the models and prevent over fitting  
 Analyzed the SQL scripts and designed the solution to implement using PySpark and developed scripts as per the requirement  
 ed with Tableau in order to represent the data in visual format and better describe the problem with solutions.  
  
Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL 
Data Scientist 
US Bank - Brookfield, WI 
September 2017 to June 2018 
Responsibilities:  
 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to develop operational and visual reports for KPI monitoring  
 Data analysis and visualization (Python, R)  
 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data  
 Increased pace & confidence of learning algorithm by combining state of the art  and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes  
 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format  
 Implemented Topic Modelling, linear classifier models  
 Collaborate with UI engineers, project managers, and designers to develop web portal that aggregates reports from various sources and   
 Member of Data Science team tasked with helping clients turn data into a strategic asset  
 Focused on front end features, browser manipulation, and cross-browser compatibility.  
 Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and DOM to develop reporting portal.  
 Used Agile Scrum for BI  across different clients, which allowed for production prototyping, rapid deployment and transparency.  
  
Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop. 
Junior Data Scientist 
Ordnance Factory Board - IN 
January 2016 to July 2017 
Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.  
  
Responsibilities:  
 ed on various phases of data mining- data collection, data cleaning, developing models, validation, visualization.  
 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.  
 Performed Data Manipulation and Aggregation from various sources including HDFS and created various Predictive and Descriptive analytics using R and Tableau.  
 Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.  
 Designed Predictive analysis algorithms using Historical Data.  
 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.  
 ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in Hadoop on AWS.  
 ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports.  
  
Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc. 
Data Science Intern 
iPrism Technologies - Hyderabad, Telangana 
April 2015 to December 2015 
Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate.  
Responsibilities:  
 Collected data from end client, performed ETL and defined the uniform standard format  
 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basic fields  
 Performed string formatting on the dataset converting hours from date format to a numerical integer  
 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the dataset such as day of week, age, hour and number of screens.  
 Developed and implemented predictive models like Logistic Regression, Decision Tree, Support Vector Machine (SVM) to predict the probability of enrollment  
 Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the final model based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment  
 Tuned the hyper parameters of the above models using Grid Search to find the optimum models  
 Designed and implemented K-Fold Cross-validation to test and verify the model's significance  
 Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.  
  
Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop 
Education 
Master's in Computer Engineering in Data Science and Analytics 
Arizona State University - Tempe, AZ 
 
Apache spark, Hadoop, Hive, Javascript, D3.js, Mapreduce, Natural, Pig, Python, Mapreduce, Data science, Hadoop, Machine learning, Natural language processing, Nosql, Ms sql server, Sql server, Mysql, Pl/sql, Sql 
Additional Information 
 :  
Programming languages Python, Java, R. C  
Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio  
Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception  
Operating systems Windows, Linux.  
Databases MySQL, MS SQL Server, NoSQL  
Web and Cloud Technologies AWS, HTML5  
Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL",Data Scientist,resume,"  with around 5 years of experience in all phases of diverse   specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.    ed on analyzing large datasets on distributed databases and developing Machine Learning algorithms to gain operational insights and present them to the leadership.    Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervised and unsupervised modeling.    Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.    Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX)    Adapted statistical programming languages like R and Python    Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.    Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.    Created dashboards as part of Data Visualization using Tableau.    Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.    Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.    Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.    Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.    Performed multiple Data Mining techniques and derive new insights from the data.    Skilled in Machine Learning, Statistical Modeling, and Big Data.    Creative problem-solver with strong analytical, leadership, and communication     Proficient in Python, R, Scala, Java, SQL, and C    Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling    Data Science Specialties include: Machine Learning, Sequential Modeling, Natural Language Processing (NLP)    Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics    Experienced in stochastic optimization and regression with machine learning algorithms    Experienced in formulating and solving discrete and continuous optimization problems    Able to research statistical machine learning, supervised learning, and classification methods    Strong mathematical and statistical modeling and computer programming  in an innovative manner    Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, Naïve Bayes, Support Vector Machines    Experienced in AWS cloud computing, Spark, and capable of ing with large datasets    Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets, TensorFlow, Keras    Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights    Development of clear analytical reports which directly address strategic goals    Identified and learn applicable new techniques independently as needed    Able to  comfortably and effectively within an interdisciplinary research environment    Experienced with validation of machine learning ensemble classifiers    Utilized the online datasets to implement machine learning models using Spark ML for building prototypes  Willing to relocate: Anywhere  Authorized to  in the US for any employer   Experience  Data Science Engineer  Jefferies LLC - Jersey City, NJ  February 2019 to Present  Responsibilities:    Coded in Python with selenium and automated website data scraping    Scripted using R for cleaning, merging and extraction of relevant data    Created interactive visualization using Tableau and performed data analysis to report findings and trends    Analyzed massive data models with tables having over 100s of millions of records to draw insights and useful information.    Architect complex database systems on Hadoop to scale out data development processes.    Explore  and gather insights from Amfam's operational data stores and data warehouses (stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau).    Designed and developed scripts to test data and find data defects.    Determined the quality of data, verify accuracy of information and ensure that the data is fit for modeling purposes.    Transformed data elements and attributes into usable form based on business requirements.    Blend data sets at different granularity levels using analytical queries, window functions and SQL joins.    Identified data duplicates and develop means to remove them.    Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries, MapReduce or python.    Designed and develop data pipelines to preprocess modeling data such as handle null values and clean up defective data attributes.    Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and CSVs into hive tables or data frames.    Explored and determined ways to organize data in Hive tables for fast read and writes through hive table partitions and buckets for optimized performance.    Developed programs to store data in appropriate file formats and logical grouping in tables.    Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed data marts on Hadoop    Maintained development activities in version control and create updated documentation.      Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL  Data Scientist  Anthem - Atlanta, GA  June 2018 to February 2019  Responsibilities:    Translated business questions into research s, design and conduct analyses, develop findings and synthesize recommendations to deliver valuable, relevant, and actionable insights    Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)    Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developing various machine learning models such Random forest    The missing data in the dataset is handled using Imputer method in SkLearn library    Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encoder methods in sklearn library    Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for modeling    Defined a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores    Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standard machine learning datasets    Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenue for future    ed with applied statistics and applied mathematics  for performance optimization    ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores    Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms    Used cross-validation to test the models with different batches of data to optimize the models and prevent over fitting    Analyzed the SQL scripts and designed the solution to implement using PySpark and developed scripts as per the requirement    ed with Tableau in order to represent the data in visual format and better describe the problem with solutions.      Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL  Data Scientist  US Bank - Brookfield, WI  September 2017 to June 2018  Responsibilities:    Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to develop operational and visual reports for KPI monitoring    Data analysis and visualization (Python, R)    Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data    Increased pace & confidence of learning algorithm by combining state of the art  and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes    Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format    Implemented Topic Modelling, linear classifier models    Collaborate with UI engineers, project managers, and designers to develop web portal that aggregates reports from various sources and     Member of Data Science team tasked with helping clients turn data into a strategic asset    Focused on front end features, browser manipulation, and cross-browser compatibility.    Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and DOM to develop reporting portal.    Used Agile Scrum for BI  across different clients, which allowed for production prototyping, rapid deployment and transparency.      Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.  Junior Data Scientist  Ordnance Factory Board - IN  January 2016 to July 2017  Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.      Responsibilities:    ed on various phases of data mining- data collection, data cleaning, developing models, validation, visualization.    Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.    Performed Data Manipulation and Aggregation from various sources including HDFS and created various Predictive and Descriptive analytics using R and Tableau.    Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.    Designed Predictive analysis algorithms using Historical Data.    Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.    ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in Hadoop on AWS.    ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports.      Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.  Data Science Intern  iPrism Technologies - Hyderabad, Telangana  April 2015 to December 2015  Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate.   Responsibilities:    Collected data from end client, performed ETL and defined the uniform standard format    Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basic fields    Performed string formatting on the dataset converting hours from date format to a numerical integer    Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the dataset such as day of week, age, hour and number of screens.    Developed and implemented predictive models like Logistic Regression, Decision Tree, Support Vector Machine (SVM) to predict the probability of enrollment    Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the final model based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment    Tuned the hyper parameters of the above models using Grid Search to find the optimum models    Designed and implemented K-Fold Cross-validation to test and verify the model's significance    Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.      Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop  Education  Master's in Computer Engineering in Data Science and Analytics  Arizona State University - Tempe, AZ    Apache spark, Hadoop, Hive, Javascript, D3.js, Mapreduce, Natural, Pig, Python, Mapreduce, Data science, Hadoop, Machine learning, Natural language processing, Nosql, Ms sql server, Sql server, Mysql, Pl/sql, Sql  Additional Information   :   Programming languages Python, Java, R. C   Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio   Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception   Operating systems Windows, Linux.   Databases MySQL, MS SQL Server, NoSQL   Web and Cloud Technologies AWS, HTML5   Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL"
"
 
Data scientist with a strong math background and 2+ years of experience in big data, machine learning, and statistics. Passionate about explaining data science to non- business audiences. 
 
 
        Programming: Python, R, SQL, CSS, HTML, Shell 
        Libraries: scikit-learn, pandas, cartopy, xarray, scipy, nltk, xgboost, hyperopt, matplotlib, seaborn 
        Machine Learning: Decision Trees, SVM, Neural Nets, Regression, Clustering, Boosting, Ensemble 
        Statistical Techniques: Time Series, Probability, Hypothesis Testing, Factor Analysis, ANOVA, PCA         : Git, AWS, Spark, Hadoop, Excel, Tableau, MS Office, Command Line, Jupyter Notebook 
 
 EXPERIENCE 
Data Scientist                                                                                                                                                                  Oct 2017  Present 
University of California, Irvine, CA 
 ing as a lead developer on NASA sponsored project to analyse climate change in Greenland and Antarctica 
 Identified winter-time melt events with around 90% accuracy using Random Forest and GBM algorithms 
 Discovered patterns, trends and indicators for global temperature change using Time-Series analysis 
 Translated findings to both  and non- audiences using Tableau dashboards 
 Developed open-source Python package JAWS, which translates data from idiosyncratic ASCII formats into homogenized netCDF format 
 Designed unit-testing frame to evaluate accuracy, robustness and fault-tolerance at each stage 
 Developed internal infrastructure, data pipelines and queries in SQL using a variety of data files 
 
Data Science Associate Instructor                                                                                                                         Sep 2016  Aug 2017 
Indiana University, Bloomington, IN 
 Assisted students in Python and R with special focus on machine learning models (Ensembling, XGBoost, etc.) 
 Delivered a guest lecture on Time Series Analysis and its implementation in R. 
 Identified the needs of learners and adapted course content and delivery style to meet their needs 
 
 
 Telstra Net Challenge: Developed a multi-class classification model to predict the severity of service disruptions on Telstras net. Built the model using Random Forest as well as XGBoost and used the Hyperopt library for tuning the parameters. (Kaggle Rank 55/974) 
 Predicting Correct Orientation of a given Picture: Implemented a fully-connected feed-forward net to classify image orientation, with backpropagation algorithm to train the net using gradient descent. Implemented K-Nearest Neighbor. (Neural Net, kNN) 
 Predicting Business Success Attributes: Developed a model using Yelp Dataset that helps businesses to identify what factors should they focus to make more profit or help new businesses to identify which location would be ideal for their business to thrive. (Python, Tableau) 
 Exploration & Exploitation of Darwins Reading: Discovered correlation between Darwins reading patterns and biographically important events. (RStudio, LDA, PCA, k-means) 
 Twitter Dataset Modelling and Analysis: Created MongoDB schemas for representing the twitter data, classified users based on their location and used Google Geocoding API for visualization. (MongoDB) 
 
  
Indiana University, Bloomington, IN                                                                                                                                 2015  2017  
Master of Science in Data Science 
 
Kurukshetra University, Kurukshetra, INDIA                                                                                                                   2009  2013 
Bachelor of  in Computer Science ",Data Scientist,resume,"   Data scientist with a strong math background and 2+ years of experience in big data, machine learning, and statistics. Passionate about explaining data science to non- business audiences.              Programming: Python, R, SQL, CSS, HTML, Shell          Libraries: scikit-learn, pandas, cartopy, xarray, scipy, nltk, xgboost, hyperopt, matplotlib, seaborn          Machine Learning: Decision Trees, SVM, Neural Nets, Regression, Clustering, Boosting, Ensemble          Statistical Techniques: Time Series, Probability, Hypothesis Testing, Factor Analysis, ANOVA, PCA         : Git, AWS, Spark, Hadoop, Excel, Tableau, MS Office, Command Line, Jupyter Notebook     EXPERIENCE  Data Scientist                                                                                                                                                                  Oct 2017  Present  University of California, Irvine, CA   ing as a lead developer on NASA sponsored project to analyse climate change in Greenland and Antarctica   Identified winter-time melt events with around 90% accuracy using Random Forest and GBM algorithms   Discovered patterns, trends and indicators for global temperature change using Time-Series analysis   Translated findings to both  and non- audiences using Tableau dashboards   Developed open-source Python package JAWS, which translates data from idiosyncratic ASCII formats into homogenized netCDF format   Designed unit-testing frame to evaluate accuracy, robustness and fault-tolerance at each stage   Developed internal infrastructure, data pipelines and queries in SQL using a variety of data files    Data Science Associate Instructor                                                                                                                         Sep 2016  Aug 2017  Indiana University, Bloomington, IN   Assisted students in Python and R with special focus on machine learning models (Ensembling, XGBoost, etc.)   Delivered a guest lecture on Time Series Analysis and its implementation in R.   Identified the needs of learners and adapted course content and delivery style to meet their needs       Telstra Net Challenge: Developed a multi-class classification model to predict the severity of service disruptions on Telstras net. Built the model using Random Forest as well as XGBoost and used the Hyperopt library for tuning the parameters. (Kaggle Rank 55/974)   Predicting Correct Orientation of a given Picture: Implemented a fully-connected feed-forward net to classify image orientation, with backpropagation algorithm to train the net using gradient descent. Implemented K-Nearest Neighbor. (Neural Net, kNN)   Predicting Business Success Attributes: Developed a model using Yelp Dataset that helps businesses to identify what factors should they focus to make more profit or help new businesses to identify which location would be ideal for their business to thrive. (Python, Tableau)   Exploration & Exploitation of Darwins Reading: Discovered correlation between Darwins reading patterns and biographically important events. (RStudio, LDA, PCA, k-means)   Twitter Dataset Modelling and Analysis: Created MongoDB schemas for representing the twitter data, classified users based on their location and used Google Geocoding API for visualization. (MongoDB)       Indiana University, Bloomington, IN                                                                                                                                 2015  2017   Master of Science in Data Science    Kurukshetra University, Kurukshetra, INDIA                                                                                                                   2009  2013  Bachelor of  in Computer Science "
"


* Around 4+  years of experience in Data Analysis, Data Conversion, Data Validation, Data Profiling, UAT Testing and Report Creation and ing experience in Tableau, Teradata, AWS Redshift, AWS S3, Python, Unix and Oracle. 
* Experience in Teradata, SQL and Utilities like Tpump, Multiload and Fast load. 
* Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ 
* Strong experience in using Excel and MS Access to dump the data and analyze based on business needs. 
* Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables. 
* Experience in data analysis using Python (Pandas, NumPy) 
* Experience in moving data to cloud platform like AWS (S3) and manipulate data using Redshift. 
* Strong  in statistical methodologies such as A/Btest, experimentdesign, hypothesistest, ANOVA.  
*  Extensively ed on R platform for the data analysis and predictive modeling.  
*  Experience in implementing data analysis with various analytic , such as Anaconda 4.0, R 3.0 (ggplot2, Caret, dplyr) and Excel.  
*  Solid ability to write and optimize diverse SQLqueries, ing knowledge of RDBMS like SQLServer2008  
*  Experience in BigData  like Spark1.6, Sparksql, pySpark, Hadoop2.X, HDFS, Hive1.X.  
*  Experience in visualization  likeTableau9.X, 10.X, DataBlendingfor creating dashboards.  
*  Proficient in PredictiveModeling, Datamining Methods, FactorAnalysis, ANOVA, Hypotheticaltesting, normal distribution and other advanced statistical and econometric techniques.  
*  Developed predictive models using DecisionTree, RandomForest, NaiveBayes, LogisticRegression, ClusterAnalysis, and NeuralNets.  
*  Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS , MATLAB, Relationaldatabases. Deep understanding & exposure of BigDataEco-system.  
*  Expert in creating PL/SQLSchemaobjects like Packages, Procedures, Functions, Subprograms, Triggers, Views, MaterializedViews, Indexes, Constraints, Sequences, ExceptionHandling, DynamicSQL/Cursors, NativeCompilation, CollectionTypes, RecordType, ObjectType using SQLDeveloper.  
*  Hands on Experience in implementing ModelViewControl (MVC) architecture using Spring, JDK, CoreJava (Collections, OOPSConcepts), JSP, Servlets, Struts, springs, Hibernate, JDBCand provided ServerAdministrator duties LogicalPosition.  
*  ed with complex applications such as R, SAS, Matlab,andSPSS to developeda neuralnet, cluster analysis.  Delivered in software development life cycle for various ETL and BI . Provided end-to-end solutions for requirement analysis, design, development, testing, and application maintenance.
* Identified organization process inefficiencies and gaps, implemented process improvement methodologies to streamline the process and to increase efficiencies.
* Maintained the application code quality at the organization level ensuring robust, stable applications
* Automated the processes requiring manual interventions using SSIS, SQL, SSRS. Power BI, and Tableau+ to reduce number of incidents, to maximize customer satisfaction, and to increase application usability.
* Tuned SQL queries to reduce cycle time, improving process execution time and ultimately stabilizing applications.
* Demonstrated attention to details, decision-making, problem solving, and critical thinking in the service delivery.
* Communicated findings using the most effective form of written and verbal communication to non- users.
* Experience in ing under Agile (Scrum) and waterfall models of software development life cycle.
* Ability to meet deadlines and handle multiple tasks, team player, motivated, able to grasp things quickly with analytics and problem-solving .


 :

Operating Systems    
Windows XP, Windows 7/10
Specialties                                                                     
   Data Visualization, Business Intelligence (BI),     Software Management, Data Collection.
Languages
 SQL, PL/SQL, XML, HTML, Java, C,Python, R language, Machine Learning 
BI 
Tableau Desktop/Public/Server, QlikView
Data Bases                   
Oracle 11g/10g, MS SQL Server 2008, Teradata, Snowflake, Amazon Web Serices (AWS)
Document Management
SharePoint
Modeling                    
Dimensional Data Modeling, Multi-dimensional modeling, Snowflake/Star Schema
Others                                    
MS Word, Excel, Visual Studio, Astah, Rational rose, Notepad++, JIRA                                                               Experience 
Data Scientist 
Dollar shave Club  Marina Del Rey, CA
November 2017 to Present
Responsibilities:

 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Impala, Sqoop, MySQL, Mem SQL, Grafana/Influx DB, and Kafka.  
 ed with statistical models for data analysis, predictive modeling, machine learning approaches and recommendation and optimization algorithms.  
 ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.  
 ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQLscripts for multiple purposes.  
 Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and Random Forest using R and Python packages.  
 ed with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions.  
 ed with Big Data  such Hadoop, Hive, Map Reduce  
 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.  
 Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.  
 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.  
 Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms.  
 Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS  
 Managed existing team members lead the recruiting and on boarding of a larger DataScience team that addresses analytical knowledge requirements.  
 ed directly with upper executives to define requirements of scoring models.  
 Developed a model for predicting repayment of debt owed to small and medium enterprise (SME) businesses.  
 Developed a generic model for predicting repayment of debt owed in the healthcare, large commercial, and government sectors.  
 Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscript mapping.  
 Developed a legal model for predicting which debtors respond to litigation only.  
 Created multiple dynamic scoring strategies for adjusting the score upon consumer behavior such as payment or right-party phone call.  
 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.  
 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors  
 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.  
 Identifying relevant key performing factors; testing their statistical significance  
 Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.  
  
Environment: R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib


Data Scientist
Citi-Bank  Irving, TX
January 2017 to September 2017

 Performed Data Profiling to learn about behavior with various features such as caller ID, traffic pattern, location, number validity.  
 Application of various machine learning algorithms like decision trees, regression models, neural nets, SVM, clustering to identify fraudulent profiles using scikit-learn package in python.  
 Used clustering technique K-Means to identify outliers and to classify unlabeled data.  
 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.  
 Analyze traffic patterns by calculating autocorrelation with different time lags.  
 Ensured that the model has low False Positive Rate.  
 Used Principal Component Analysis in feature engineering to analyze high dimensional data.  
 Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.  
 Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify Scammer, Telemarketer.  
 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.  
 Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.  
 Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in Python.  
 Developed MapReduce pipeline for feature extraction using Hive.  
 Created DataQualityScripts using SQL and Hive to validate successful data load and quality of the data. Created several types of data visualizations using Python and Tableau.  
 Communicated the results with operations team for taking best decisions.  
 Collected data needs and requirements by Interacting with the other departments within CitiBank.  
  
Environment:Linux, Hadoop, Python, Machine learning, MapReduce, HDFS, Python Libraries (NumPy/Pandas), Tableau, GIT, NLP, Neural Net, Hive, SQL


Data Analyst
Radiare software solutions-Ganga Nagar, Bangalore, India
October 2014 to November 2015

 Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle.  
 Involved in managing backup and restoring data in the live Cassandra Cluster.  
 Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.  
 Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.  
 Developed personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers.  
 Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net.  
 Evaluated parameters with K-Fold Cross Validation and optimized performance of models.  
 ed on benchmarking Cassandra Cluster using the Cassandra stress tool.  
 A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, UNIXCommands, Python programming, No SQL.  
 ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.  
 Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms.  
 Determined customer satisfaction and helped enhance customer using NLP.  
 Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.  
 Performed data visualization and Designed dashboards with Tableau and D3.js and provided complex reports, including charts, summaries, and graphs to interpret the findings to the team and stakeholder.  
 
Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.
SQL Developer
Radiare software solutions-Ganga Nagar, Bangalore, India
April 2014 to September 2014

 Gather data from different data sources like MySQL, Oracle and SQL Server
 Develop reports, dashboards using Tableau 9.3 for quick reviews to be presented to Business and IT users.
 Developed POCs by building reports and dashboards using Tableau to perform
 Statistical analysis of large data
 Developed Ad-hoc reports using Tableau Desktop, Excel
 Prototyped data visualizations using Charts, drill-down, parameterized controls using Tableau to highlight the value of analytics in Executive decision support control.
 Developed visualizations using sets, Parameters, Calculated Fields, Dynamic sorting, Filtering, Parameter driven analysis.
 Used SQL Server Reporting Services (SSRS) and SQL Server Management Studio 2008 -SSIS, SSAS, and SQL Profiler, including Crystal Reports to run scripts and maintain stored procedure and code functions across company's Databases
 Performed schema changes in the database per request and met with clientele and with various subgroups for requirement gathering in order to allow self-extracted reports and created critical field reporting  for Enterprise Data Management customer segments
 Validated data to correctly reflect reservations for passengers and carrying capacity of cargo.
 Created SQL stored procedures using SQL Query Analyzer and Transact-SQL as data sources for
 reports.
 Trained, mentored, supervised, and advised over a dozen analysts in the development of programs.
 Acted as a point of contact for answers to questions over two groups.
 Trained analysts in development standards; according to corporate models and paradigms.
 Developed databases to interface with SQL Server, Oracle, and MS Access databases.
 Developed and supported applications for the Revenue Management and Revenue Management

Environment: Tableau, SQL, UNIX, JSON, Jira, HADOOP (HDFS), Hive, Sqoop, Spark, JAVA, HBase, AWS, SSIS, Teradata, Oracle 10g.

Education:

* Bachelor of Technology, Electronic 	                                                      2010-2014
  Auroras Technological and Research Institute, Hyderabad, India.

* Masters in Information System                                                                        2016                             The University of Mary-Hardin Baylor, Belton, TX


",Data Scientist,resume,"   * Around 4+  years of experience in Data Analysis, Data Conversion, Data Validation, Data Profiling, UAT Testing and Report Creation and ing experience in Tableau, Teradata, AWS Redshift, AWS S3, Python, Unix and Oracle.  * Experience in Teradata, SQL and Utilities like Tpump, Multiload and Fast load.  * Good experience in Developing Teradata SQL queries and using Utilities such As BTEQ  * Strong experience in using Excel and MS Access to dump the data and analyze based on business needs.  * Experience in Creating Teradata SQL scripts using OLAP functions like rank and rank () Over to improve the query performance while pulling the data from large tables.  * Experience in data analysis using Python (Pandas, NumPy)  * Experience in moving data to cloud platform like AWS (S3) and manipulate data using Redshift.  * Strong  in statistical methodologies such as A/Btest, experimentdesign, hypothesistest, ANOVA.   *  Extensively ed on R platform for the data analysis and predictive modeling.   *  Experience in implementing data analysis with various analytic , such as Anaconda 4.0, R 3.0 (ggplot2, Caret, dplyr) and Excel.   *  Solid ability to write and optimize diverse SQLqueries, ing knowledge of RDBMS like SQLServer2008   *  Experience in BigData  like Spark1.6, Sparksql, pySpark, Hadoop2.X, HDFS, Hive1.X.   *  Experience in visualization  likeTableau9.X, 10.X, DataBlendingfor creating dashboards.   *  Proficient in PredictiveModeling, Datamining Methods, FactorAnalysis, ANOVA, Hypotheticaltesting, normal distribution and other advanced statistical and econometric techniques.   *  Developed predictive models using DecisionTree, RandomForest, NaiveBayes, LogisticRegression, ClusterAnalysis, and NeuralNets.   *  Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS , MATLAB, Relationaldatabases. Deep understanding & exposure of BigDataEco-system.   *  Expert in creating PL/SQLSchemaobjects like Packages, Procedures, Functions, Subprograms, Triggers, Views, MaterializedViews, Indexes, Constraints, Sequences, ExceptionHandling, DynamicSQL/Cursors, NativeCompilation, CollectionTypes, RecordType, ObjectType using SQLDeveloper.   *  Hands on Experience in implementing ModelViewControl (MVC) architecture using Spring, JDK, CoreJava (Collections, OOPSConcepts), JSP, Servlets, Struts, springs, Hibernate, JDBCand provided ServerAdministrator duties LogicalPosition.   *  ed with complex applications such as R, SAS, Matlab,andSPSS to developeda neuralnet, cluster analysis.  Delivered in software development life cycle for various ETL and BI . Provided end-to-end solutions for requirement analysis, design, development, testing, and application maintenance. * Identified organization process inefficiencies and gaps, implemented process improvement methodologies to streamline the process and to increase efficiencies. * Maintained the application code quality at the organization level ensuring robust, stable applications * Automated the processes requiring manual interventions using SSIS, SQL, SSRS. Power BI, and Tableau+ to reduce number of incidents, to maximize customer satisfaction, and to increase application usability. * Tuned SQL queries to reduce cycle time, improving process execution time and ultimately stabilizing applications. * Demonstrated attention to details, decision-making, problem solving, and critical thinking in the service delivery. * Communicated findings using the most effective form of written and verbal communication to non- users. * Experience in ing under Agile (Scrum) and waterfall models of software development life cycle. * Ability to meet deadlines and handle multiple tasks, team player, motivated, able to grasp things quickly with analytics and problem-solving .    :  Operating Systems     Windows XP, Windows 7/10 Specialties                                                                         Data Visualization, Business Intelligence (BI),     Software Management, Data Collection. Languages  SQL, PL/SQL, XML, HTML, Java, C,Python, R language, Machine Learning  BI  Tableau Desktop/Public/Server, QlikView Data Bases                    Oracle 11g/10g, MS SQL Server 2008, Teradata, Snowflake, Amazon Web Serices (AWS) Document Management SharePoint Modeling                     Dimensional Data Modeling, Multi-dimensional modeling, Snowflake/Star Schema Others                                     MS Word, Excel, Visual Studio, Astah, Rational rose, Notepad++, JIRA                                                               Experience  Data Scientist  Dollar shave Club  Marina Del Rey, CA November 2017 to Present Responsibilities:   Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Impala, Sqoop, MySQL, Mem SQL, Grafana/Influx DB, and Kafka.    ed with statistical models for data analysis, predictive modeling, machine learning approaches and recommendation and optimization algorithms.    ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.    ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQLscripts for multiple purposes.    Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XG Boost, SVM, and Random Forest using R and Python packages.    ed with data compliance teams, data governance team to maintain data models, Metadata, data Dictionaries, define source fields and its definitions.    ed with Big Data  such Hadoop, Hive, Map Reduce    Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.    Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.    A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.    Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms.    Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS    Managed existing team members lead the recruiting and on boarding of a larger DataScience team that addresses analytical knowledge requirements.    ed directly with upper executives to define requirements of scoring models.    Developed a model for predicting repayment of debt owed to small and medium enterprise (SME) businesses.    Developed a generic model for predicting repayment of debt owed in the healthcare, large commercial, and government sectors.    Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscript mapping.    Developed a legal model for predicting which debtors respond to litigation only.    Created multiple dynamic scoring strategies for adjusting the score upon consumer behavior such as payment or right-party phone call.    Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.    Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors    Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.    Identifying relevant key performing factors; testing their statistical significance    Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.      Environment: R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib   Data Scientist Citi-Bank  Irving, TX January 2017 to September 2017   Performed Data Profiling to learn about behavior with various features such as caller ID, traffic pattern, location, number validity.    Application of various machine learning algorithms like decision trees, regression models, neural nets, SVM, clustering to identify fraudulent profiles using scikit-learn package in python.    Used clustering technique K-Means to identify outliers and to classify unlabeled data.    Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for feature selection.    Analyze traffic patterns by calculating autocorrelation with different time lags.    Ensured that the model has low False Positive Rate.    Used Principal Component Analysis in feature engineering to analyze high dimensional data.    Created and designed reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.    Performed Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify Scammer, Telemarketer.    Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.    Implemented rule-based expertise system from the results of exploratory analysis and information gathered from the people from different departments.    Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in Python.    Developed MapReduce pipeline for feature extraction using Hive.    Created DataQualityScripts using SQL and Hive to validate successful data load and quality of the data. Created several types of data visualizations using Python and Tableau.    Communicated the results with operations team for taking best decisions.    Collected data needs and requirements by Interacting with the other departments within CitiBank.      Environment:Linux, Hadoop, Python, Machine learning, MapReduce, HDFS, Python Libraries (NumPy/Pandas), Tableau, GIT, NLP, Neural Net, Hive, SQL   Data Analyst Radiare software solutions-Ganga Nagar, Bangalore, India October 2014 to November 2015   Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle.    Involved in managing backup and restoring data in the live Cassandra Cluster.    Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.    Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.    Developed personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers.    Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net.    Evaluated parameters with K-Fold Cross Validation and optimized performance of models.    ed on benchmarking Cassandra Cluster using the Cassandra stress tool.    A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, UNIXCommands, Python programming, No SQL.    ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.    Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms.    Determined customer satisfaction and helped enhance customer using NLP.    Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.    Performed data visualization and Designed dashboards with Tableau and D3.js and provided complex reports, including charts, summaries, and graphs to interpret the findings to the team and stakeholder.     Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce. SQL Developer Radiare software solutions-Ganga Nagar, Bangalore, India April 2014 to September 2014   Gather data from different data sources like MySQL, Oracle and SQL Server  Develop reports, dashboards using Tableau 9.3 for quick reviews to be presented to Business and IT users.  Developed POCs by building reports and dashboards using Tableau to perform  Statistical analysis of large data  Developed Ad-hoc reports using Tableau Desktop, Excel  Prototyped data visualizations using Charts, drill-down, parameterized controls using Tableau to highlight the value of analytics in Executive decision support control.  Developed visualizations using sets, Parameters, Calculated Fields, Dynamic sorting, Filtering, Parameter driven analysis.  Used SQL Server Reporting Services (SSRS) and SQL Server Management Studio 2008 -SSIS, SSAS, and SQL Profiler, including Crystal Reports to run scripts and maintain stored procedure and code functions across company's Databases  Performed schema changes in the database per request and met with clientele and with various subgroups for requirement gathering in order to allow self-extracted reports and created critical field reporting  for Enterprise Data Management customer segments  Validated data to correctly reflect reservations for passengers and carrying capacity of cargo.  Created SQL stored procedures using SQL Query Analyzer and Transact-SQL as data sources for  reports.  Trained, mentored, supervised, and advised over a dozen analysts in the development of programs.  Acted as a point of contact for answers to questions over two groups.  Trained analysts in development standards; according to corporate models and paradigms.  Developed databases to interface with SQL Server, Oracle, and MS Access databases.  Developed and supported applications for the Revenue Management and Revenue Management  Environment: Tableau, SQL, UNIX, JSON, Jira, HADOOP (HDFS), Hive, Sqoop, Spark, JAVA, HBase, AWS, SSIS, Teradata, Oracle 10g.  Education:  * Bachelor of Technology, Electronic 	                                                      2010-2014   Auroras Technological and Research Institute, Hyderabad, India.  * Masters in Information System                                                                        2016                             The University of Mary-Hardin Baylor, Belton, TX   "
"

Expert in logical and problem-solving  with strong knowledge in math and statistical concepts. Ability to take strong business judgement for ambiguous problems and solve them in a structured, hypothesis-driven and data-supported way. Data geek with strong programming background, experience in building and deploying machine learning and predictive models.

 :
> 4+ years of  experience as a Data Scientist/ Data Analyst, including deep expertise and experience with Statistical Analysis, Data Mining and Machine Learning  using R, Python and SQL.
> Extensive programming  in analytical and statistical programming languages such as python, R, SAS, and SQL.
> Data Driven and highly analytical with ing knowledge and statistical model approaches and methodologies like Clustering, Regression analysis, Hypothesis testing, Decision trees, Machine learning rules and ever evolving regulatory environment.
>  ing experience in Machine Learning algorithms such as Linear Regression, Logistic Regression, Random Forests, Decision Trees, K-Means Clustering and Association Rules.
> Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data.
> Extensive ing experience in writing SQL queries.
> Knowledge and experience in various sectors like Banking services, Healthcare and Industries.
> Involved in entire lifecycle of software development (SDLC) and Data Science Project lifecycle.
> Knowledge about Auditing the data to assess its quality or profitable for a specific purpose
> Proficient in statistical programming language for analyzing data using R and SAS
> Experience ing on Python libraries such as Numpy, Pandas, scikit learn, NLTK, Keras and Tensor flow.
> Sound knowledge on interpreting data analysis using multivariate data  longitudinal and mixed models.
> Knowledge on supervised/unsupervised learning, models classification, parametric/non- parametric and Neural Net algorithms.
> Knowledge about Database management system i.e. systematic way to create, retrieve, update and manage data.
> Experience in Machine Learning, Deep Learning, and Data mining with large datasets of Structured/Unstructured Data, Data Acquisition, Data Validation, Predictive Modeling and Data Visualization.
> Proficient in Tableau, Qlik and R-Shiny data visualization  to analyze and obtain insights into large datasets.
> Effective communication , experienced in progressing problem statement to documentation, have knowledge of deploying code on GitHub, perceptive to details and has great commitment to deadlines.

 :

Languages		: 	Python, R, SAS, Java, ASP.NET
Operating Systems	: 	Windows, Linux, Unix, and Mac OS
Database		:	Oracle, MySQL, MS SQL, Sqlite
Big Data Eco Systems	: 	HDFS, Spark, Flume, Pig, Hive, HBase
Regression Techniques: 	Linear regression, Logistic Regression, Gradient Descent
Data 		: 	NumPy, Pandas, Scikit-learn, Tensorflow, NLTK, ggplot2, 
				Advanced Excel
Reporting 	: 	Tableau, Power BI
Statistical Methods	: 	Time series, Regression models, hypothesis testing,
				Supervised/unsupervised Algorithms, Bayesian statistics, SVM 
				And deep learning.
: 

Infuzion Solutions, Delaware						June 2018 -Present
Data Scientist 

Responsibilities:
> Using SQL to analyze the companys Dataset and create the reports using SSRS.
> Developed the SAS models to import the Excel dataset into SQL.
> Using performance point dashboard to deploy the report to SharePoint.
> Performed statistical analysis to determine key factors for planning and conducting experiments to prove Total fraud loss using prescriptive and predictive analytics by application of appropriate machine learning algorithms.
> Designed and   built real-time   contextual behavior personalization system using econometric and machine learning to predict customer's behavior and help them to navigate through products of their choice. 
> ed in low latency models to learn and predict online, enabling it to respond constantly to changes in card member's payment behavior at various stages, with accuracy over 90%.
> ed  with  credit card  Profit  and  risk  analysis  team  to  analyze  the  current  customer  base  to  model  better offers and schemes to maximize the profits.
> Developed statistical models and applied them to assign a risk score to credit applications and existing credit accounts.
> Hands on experience with pivot tables and Lookups.
> Improved the accuracy of the developed statistical models and monitored the effect that score-based decisions have on key business performance indicators.
Norfolk Southern, Atlanta GA        					         Nov 2017  May 2018
Data Analyst

Responsibilities:
> Analyzed and processed complex data sets using advanced querying, visualization and analytics .
> Create complex SQL stored procedures, Triggers, Functions, Views, Indexes in Microsoft SQL Server
> Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification.
> Developed subject segmentation algorithm using R.
> Involved in the process of load, transform, and analyze data from various sources into HDFS (Hadoop Distributed File System) using Hive, Pig and Sqoop.
> Used Python 3.0 (numpy, scipy, pandas, scikit-learn, seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, ML) to develop variety of models and algorithms for analytic purposes.
> Developed an algorithm that can identify bad assessments that are expected to Fail under central review.
> Used statistical methods to analyze the performance of each clinical site across 27 countries on 30 studies, predicting number of days to reach the target number of sites for a clinical study.
> Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends.
> Developed pipelines for test data.
> Enhanced statistical models (linear mixed models) for predicting the best products for commercialization using Machine Learning Linear regression models, KNN and K-means clustering algorithms.
> Builds machine learning models on independent AWS EC2 server to enhance data quality.
> Handle Unstructured Data to derive some information from which helps in development of the company.
> Finding the sentiment about the organization using Text Mining and NLP techniques.

Deutsche Bank, NYC 							June 2015  Oct 2017
Jr Analysts 

Responsibilities:
> Predicted sales of a store given the store attributes to plan budgeting, staffing, and marketing.
> Built an ensemble of machine learning models to forecast sales and perform model comparison using MAPE and segmented the customers based on demographics using K-means Clustering.
> Generated insights from the clusters to perform target marketing based on the customer  such as age, , income levels and median rooms.
> Developed data pre-processing modules and rule extraction engines in Python using Random Forest and Decision Trees for an analytics product and reduced the execution time by 30%.
> Performed data discovery by generating compliance trend reports to show the calculated value for certain compliance rules over time for significant KPI's.
> Developed adhoc reports on top performing products on a weekly basis using tableau.
> Develop campaign insights from a combination of proprietary client data, syndicated industry information, with geographically relevant consumer and vendor demographics.
> Audited and validated the customer selection results, segmented based on behavior, predictive models, and other criteria.
> Supported new products, communication methods, marketing campaigns, and brands.
> Created visualizations, reports and dashboards to present clients and stakeholders.

Academic  and Related Course:
* Predicting house prices with relevant features after feature scaling.
* Email Spam classifier using logistic regression and gradient descent
* Photo OCR technique
* PageRank algorithm visualization
* Web scraped using JSON
* Generated word cloud from webpages

Online Courses
Machine Learning-Stanford University
https://www.coursera.org/account/accomplishments/verify/759VDG99SK3H
Python Specialization-University of Michigan 
https://www.coursera.org/account/accomplishments/specialization/MNVVDDY5D86T


Bachelors Degree in Computer Science and Engineering
Hindustan College of Engineering
Chennai, India
",Data Scientist,resume,"  Expert in logical and problem-solving  with strong knowledge in math and statistical concepts. Ability to take strong business judgement for ambiguous problems and solve them in a structured, hypothesis-driven and data-supported way. Data geek with strong programming background, experience in building and deploying machine learning and predictive models.   : > 4+ years of  experience as a Data Scientist/ Data Analyst, including deep expertise and experience with Statistical Analysis, Data Mining and Machine Learning  using R, Python and SQL. > Extensive programming  in analytical and statistical programming languages such as python, R, SAS, and SQL. > Data Driven and highly analytical with ing knowledge and statistical model approaches and methodologies like Clustering, Regression analysis, Hypothesis testing, Decision trees, Machine learning rules and ever evolving regulatory environment. >  ing experience in Machine Learning algorithms such as Linear Regression, Logistic Regression, Random Forests, Decision Trees, K-Means Clustering and Association Rules. > Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data. > Extensive ing experience in writing SQL queries. > Knowledge and experience in various sectors like Banking services, Healthcare and Industries. > Involved in entire lifecycle of software development (SDLC) and Data Science Project lifecycle. > Knowledge about Auditing the data to assess its quality or profitable for a specific purpose > Proficient in statistical programming language for analyzing data using R and SAS > Experience ing on Python libraries such as Numpy, Pandas, scikit learn, NLTK, Keras and Tensor flow. > Sound knowledge on interpreting data analysis using multivariate data  longitudinal and mixed models. > Knowledge on supervised/unsupervised learning, models classification, parametric/non- parametric and Neural Net algorithms. > Knowledge about Database management system i.e. systematic way to create, retrieve, update and manage data. > Experience in Machine Learning, Deep Learning, and Data mining with large datasets of Structured/Unstructured Data, Data Acquisition, Data Validation, Predictive Modeling and Data Visualization. > Proficient in Tableau, Qlik and R-Shiny data visualization  to analyze and obtain insights into large datasets. > Effective communication , experienced in progressing problem statement to documentation, have knowledge of deploying code on GitHub, perceptive to details and has great commitment to deadlines.   :  Languages		: 	Python, R, SAS, Java, ASP.NET Operating Systems	: 	Windows, Linux, Unix, and Mac OS Database		:	Oracle, MySQL, MS SQL, Sqlite Big Data Eco Systems	: 	HDFS, Spark, Flume, Pig, Hive, HBase Regression Techniques: 	Linear regression, Logistic Regression, Gradient Descent Data 		: 	NumPy, Pandas, Scikit-learn, Tensorflow, NLTK, ggplot2,  				Advanced Excel Reporting 	: 	Tableau, Power BI Statistical Methods	: 	Time series, Regression models, hypothesis testing, 				Supervised/unsupervised Algorithms, Bayesian statistics, SVM  				And deep learning. :   Infuzion Solutions, Delaware						June 2018 -Present Data Scientist   Responsibilities: > Using SQL to analyze the companys Dataset and create the reports using SSRS. > Developed the SAS models to import the Excel dataset into SQL. > Using performance point dashboard to deploy the report to SharePoint. > Performed statistical analysis to determine key factors for planning and conducting experiments to prove Total fraud loss using prescriptive and predictive analytics by application of appropriate machine learning algorithms. > Designed and   built real-time   contextual behavior personalization system using econometric and machine learning to predict customer's behavior and help them to navigate through products of their choice.  > ed in low latency models to learn and predict online, enabling it to respond constantly to changes in card member's payment behavior at various stages, with accuracy over 90%. > ed  with  credit card  Profit  and  risk  analysis  team  to  analyze  the  current  customer  base  to  model  better offers and schemes to maximize the profits. > Developed statistical models and applied them to assign a risk score to credit applications and existing credit accounts. > Hands on experience with pivot tables and Lookups. > Improved the accuracy of the developed statistical models and monitored the effect that score-based decisions have on key business performance indicators. Norfolk Southern, Atlanta GA        					         Nov 2017  May 2018 Data Analyst  Responsibilities: > Analyzed and processed complex data sets using advanced querying, visualization and analytics . > Create complex SQL stored procedures, Triggers, Functions, Views, Indexes in Microsoft SQL Server > Scheduled jobs to automate different database related activities such as backups, maintaining index, and monitoring disk space and backup verification. > Developed subject segmentation algorithm using R. > Involved in the process of load, transform, and analyze data from various sources into HDFS (Hadoop Distributed File System) using Hive, Pig and Sqoop. > Used Python 3.0 (numpy, scipy, pandas, scikit-learn, seaborn, NLTK) and Spark 1.6 / 2.0 (PySpark, ML) to develop variety of models and algorithms for analytic purposes. > Developed an algorithm that can identify bad assessments that are expected to Fail under central review. > Used statistical methods to analyze the performance of each clinical site across 27 countries on 30 studies, predicting number of days to reach the target number of sites for a clinical study. > Processed huge datasets (over billion data points, over 1 TB of datasets) for data association pairing and provided insights into meaningful data association and trends. > Developed pipelines for test data. > Enhanced statistical models (linear mixed models) for predicting the best products for commercialization using Machine Learning Linear regression models, KNN and K-means clustering algorithms. > Builds machine learning models on independent AWS EC2 server to enhance data quality. > Handle Unstructured Data to derive some information from which helps in development of the company. > Finding the sentiment about the organization using Text Mining and NLP techniques.  Deutsche Bank, NYC 							June 2015  Oct 2017 Jr Analysts   Responsibilities: > Predicted sales of a store given the store attributes to plan budgeting, staffing, and marketing. > Built an ensemble of machine learning models to forecast sales and perform model comparison using MAPE and segmented the customers based on demographics using K-means Clustering. > Generated insights from the clusters to perform target marketing based on the customer  such as age, , income levels and median rooms. > Developed data pre-processing modules and rule extraction engines in Python using Random Forest and Decision Trees for an analytics product and reduced the execution time by 30%. > Performed data discovery by generating compliance trend reports to show the calculated value for certain compliance rules over time for significant KPI's. > Developed adhoc reports on top performing products on a weekly basis using tableau. > Develop campaign insights from a combination of proprietary client data, syndicated industry information, with geographically relevant consumer and vendor demographics. > Audited and validated the customer selection results, segmented based on behavior, predictive models, and other criteria. > Supported new products, communication methods, marketing campaigns, and brands. > Created visualizations, reports and dashboards to present clients and stakeholders.  Academic  and Related Course: * Predicting house prices with relevant features after feature scaling. * Email Spam classifier using logistic regression and gradient descent * Photo OCR technique * PageRank algorithm visualization * Web scraped using JSON * Generated word cloud from webpages  Online Courses Machine Learning-Stanford University https://www.coursera.org/account/accomplishments/verify/759VDG99SK3H Python Specialization-University of Michigan  https://www.coursera.org/account/accomplishments/specialization/MNVVDDY5D86T   Bachelors Degree in Computer Science and Engineering Hindustan College of Engineering Chennai, India "
"Experienced  with 2+ years of hands-on experience in statistical analysis using Python/R/SQL/SAS/Tableau/Excel interested in leveraging analytics to develop strategies. 
 
 
University of Connecticut School of Business  	 	 	 	 	 	                                                                                            Hartford, CT 
Master of Science in Business Analytics and Project Management, GPA: 3.74/4.00                                                                                                                                          Dec 2019 (Expected)  
Relevant Course: Statistics with R, Predictive Modeling, Data Mining and Business Intelligence, Python, Survival Analysis with SAS, Big Data/Hadoop 
 
Guru Gobind Singh Indraprastha University 	 	 	 	 	                                                                                                                             New Delhi, India 
Bachelor of Engineering in Electronics and Communication                                	                                                                                                                                                            May 2017 
 
  
 Statistical Analysis: Linear Regression, Logistic Regression, Dimension Reduction, Decision Trees, Random Forests, Boosted trees, Feature Selection, Bagging, Neural Nets, Clustering, Machine Learning, Time Series, Recommender Systems, Forecasting, Sentiment Analysis, Data Mining, Statistical Process Control, matplotlib, scikit-learn, scipy, pandas, numpy, ggplot2, dplyr, gvlma, statsr, caret,  
 : Python, R, Hadoop (HDFS, Sqoop, Pig, Hive, Spark), MapReduce, Scala, SQL (Oracle/Postgre), SAS, AWS (S3, Glue, IAM), JMP, Linux, Tableau, Visio, Advanced 
Excel, JAVA 
 
Experience 
Keysight , Santa Rosa, CA 
Data Analyst Intern (Python, R, JMP, SQL)                                                                                                                                                                                                                  May 2019- present 
 Devised a mathematical model to reduce the measurement locations on a production wafer using Neural Nets, Feature Selection, Clustering and Combinations by text parsing more than 11000 files (.txt) on Python (sklearn, scipy, matplotlib, iter, pandas, numpy). 
 Automated Gage R&R study between new and old profilometers to measure performance and variance using Hypothesis Testing (One-Sample, Two Sample t-tests) resulting in a 14% increase in manufacturing technician productivity on Python, JMP and R. 
 Developed a GUI platform to predict the IC Test Wafer time and email the operator (on real time basis) with a comprehensive list of all the wafers with their test times and estimated completion dates using Time Series and Advanced SPC on Python (pandas, tkinter). 
 
Deep Play for Kids, LLC (UConn Consulting Group), Hartford, CT 
Analytics Consultant (Python, R, SQL)                                                                                                                                                                                                                      Oct 2018  Apr 2019 
 Recommended solutions to increase revenue and maximize operational efficiency by 35%. 
 Performed A/B Testing and ETL on websites to identify ways to improve content which led to higher traffic using Python, R, and SQL. 
 Created predictive statistical models to identify target customers using data from engagement surveys, and behavioral measures. 
 
Accenture, Gurugram, India 
Data Analytics Associate (SAS, SQL, Excel)                                                                                                                                                                                                               Sep 2017  Jul 2018 
 Reduced organizational problems by 18% for the BMT (Business Manufacture Transform) owners through regression analysis, hypothesis testing and k-means clustering in order to determine new possibilities for owners using SAS. 
 Implemented data migration, cleansing and profiling by analyzing large volumes of banking data to identify useful trends and patterns for Budgeting/Forecasting using SAS, SQL and Advanced Excel. 
 Performed ETL on banking data by executing complex SQL queries to generate analysis on datasets with over millions of records to derive customer insights. 
 
Quality Council of India, New Delhi, India 
Data Analyst (R, Python, Tableau, Excel)                                                            	 	 	 	                                                                             Apr 2017  Aug 2017 
 Conducted regression analysis (decision trees and random forests) and hypothesis testing with 76% accuracy using R (ggplot2, gvlma, caret) and Python, to bridge the gaps and suggest possible policy solutions to NABET, India. 
 Created visually impactful dashboards in Tableau and explored missing values, outliers, incorrect data using R (dplyr, tidyr, ggplot2), Python (matplotlib/numpy) and Advanced Excel (VLOOKUP, pivot analysis) for 107 NGOs pan India. 
 Successfully developed strategies for curtailing the rake turnaround time by building regression models (linear/logistic) to reduce the platform congestion at New Delhi Railway Station, piloted in June 2017. 
 
Analytics  
Text mining and Sentiment Analysis -Amazon Reviews Dataset (Python/R/Tableau) White Paper                                                                                                                                 April 2019 
 Created word clouds, text clusters followed by sentiment analysis of Amazon unlocked phones reviews using bag of words model, tfidf and ngram with an accuracy of 
97.8% using Python (matplotlib, sklearn, countvectorizer, tfidf, ngram), SAS EM, R (ggplot2) and Tableau. 
 
Black Friday Sales Prediction (R/Python/SAS JMP/Excel) White Paper                                                                                                                                                                                   Oct 2018 
 Estimated the amount of purchase with 83% accuracy, by performing regression analysis (decision trees, neural nets and LASSO) on Black Friday sales data, using 
R (ggplot2, caret), Python (matplotlib, sklearn) and SAS JMP 
 
Modeling and Prediction of Movies (R/Excel) Certificate                                                                                                                                                                                                        Sep  2016 
 Predicted what attributes make a movie popular by completing stepwise regression analysis with 72% accuracy on data from Rotten Tomatoes and IMDB for a random sample of movies, using R (dplyr, statsr, ggplot2) and MS Excel. 
 
Leadership/Certifications 
 Data Scientist at LIMRA, Health Insurance, building predictive models to review customer s that are likely to lapse - Aug 2019-present 
 Advanced Statistical Process Control- Keysight-Mic Quality - Jul 2019 Certificate 
 Python for Data Science-IBM-Jan 2019 Certificate 
 Programming in R for Data Science-Microsoft (edX)- Nov 2016 Certificate ",Data Scientist,resume,"Experienced  with 2+ years of hands-on experience in statistical analysis using Python/R/SQL/SAS/Tableau/Excel interested in leveraging analytics to develop strategies.      University of Connecticut School of Business  	 	 	 	 	 	                                                                                            Hartford, CT  Master of Science in Business Analytics and Project Management, GPA: 3.74/4.00                                                                                                                                          Dec 2019 (Expected)   Relevant Course: Statistics with R, Predictive Modeling, Data Mining and Business Intelligence, Python, Survival Analysis with SAS, Big Data/Hadoop    Guru Gobind Singh Indraprastha University 	 	 	 	 	                                                                                                                             New Delhi, India  Bachelor of Engineering in Electronics and Communication                                	                                                                                                                                                            May 2017        Statistical Analysis: Linear Regression, Logistic Regression, Dimension Reduction, Decision Trees, Random Forests, Boosted trees, Feature Selection, Bagging, Neural Nets, Clustering, Machine Learning, Time Series, Recommender Systems, Forecasting, Sentiment Analysis, Data Mining, Statistical Process Control, matplotlib, scikit-learn, scipy, pandas, numpy, ggplot2, dplyr, gvlma, statsr, caret,    : Python, R, Hadoop (HDFS, Sqoop, Pig, Hive, Spark), MapReduce, Scala, SQL (Oracle/Postgre), SAS, AWS (S3, Glue, IAM), JMP, Linux, Tableau, Visio, Advanced  Excel, JAVA    Experience  Keysight , Santa Rosa, CA  Data Analyst Intern (Python, R, JMP, SQL)                                                                                                                                                                                                                  May 2019- present   Devised a mathematical model to reduce the measurement locations on a production wafer using Neural Nets, Feature Selection, Clustering and Combinations by text parsing more than 11000 files (.txt) on Python (sklearn, scipy, matplotlib, iter, pandas, numpy).   Automated Gage R&R study between new and old profilometers to measure performance and variance using Hypothesis Testing (One-Sample, Two Sample t-tests) resulting in a 14% increase in manufacturing technician productivity on Python, JMP and R.   Developed a GUI platform to predict the IC Test Wafer time and email the operator (on real time basis) with a comprehensive list of all the wafers with their test times and estimated completion dates using Time Series and Advanced SPC on Python (pandas, tkinter).    Deep Play for Kids, LLC (UConn Consulting Group), Hartford, CT  Analytics Consultant (Python, R, SQL)                                                                                                                                                                                                                      Oct 2018  Apr 2019   Recommended solutions to increase revenue and maximize operational efficiency by 35%.   Performed A/B Testing and ETL on websites to identify ways to improve content which led to higher traffic using Python, R, and SQL.   Created predictive statistical models to identify target customers using data from engagement surveys, and behavioral measures.    Accenture, Gurugram, India  Data Analytics Associate (SAS, SQL, Excel)                                                                                                                                                                                                               Sep 2017  Jul 2018   Reduced organizational problems by 18% for the BMT (Business Manufacture Transform) owners through regression analysis, hypothesis testing and k-means clustering in order to determine new possibilities for owners using SAS.   Implemented data migration, cleansing and profiling by analyzing large volumes of banking data to identify useful trends and patterns for Budgeting/Forecasting using SAS, SQL and Advanced Excel.   Performed ETL on banking data by executing complex SQL queries to generate analysis on datasets with over millions of records to derive customer insights.    Quality Council of India, New Delhi, India  Data Analyst (R, Python, Tableau, Excel)                                                            	 	 	 	                                                                             Apr 2017  Aug 2017   Conducted regression analysis (decision trees and random forests) and hypothesis testing with 76% accuracy using R (ggplot2, gvlma, caret) and Python, to bridge the gaps and suggest possible policy solutions to NABET, India.   Created visually impactful dashboards in Tableau and explored missing values, outliers, incorrect data using R (dplyr, tidyr, ggplot2), Python (matplotlib/numpy) and Advanced Excel (VLOOKUP, pivot analysis) for 107 NGOs pan India.   Successfully developed strategies for curtailing the rake turnaround time by building regression models (linear/logistic) to reduce the platform congestion at New Delhi Railway Station, piloted in June 2017.    Analytics   Text mining and Sentiment Analysis -Amazon Reviews Dataset (Python/R/Tableau) White Paper                                                                                                                                 April 2019   Created word clouds, text clusters followed by sentiment analysis of Amazon unlocked phones reviews using bag of words model, tfidf and ngram with an accuracy of  97.8% using Python (matplotlib, sklearn, countvectorizer, tfidf, ngram), SAS EM, R (ggplot2) and Tableau.    Black Friday Sales Prediction (R/Python/SAS JMP/Excel) White Paper                                                                                                                                                                                   Oct 2018   Estimated the amount of purchase with 83% accuracy, by performing regression analysis (decision trees, neural nets and LASSO) on Black Friday sales data, using  R (ggplot2, caret), Python (matplotlib, sklearn) and SAS JMP    Modeling and Prediction of Movies (R/Excel) Certificate                                                                                                                                                                                                        Sep  2016   Predicted what attributes make a movie popular by completing stepwise regression analysis with 72% accuracy on data from Rotten Tomatoes and IMDB for a random sample of movies, using R (dplyr, statsr, ggplot2) and MS Excel.    Leadership/Certifications   Data Scientist at LIMRA, Health Insurance, building predictive models to review customer s that are likely to lapse - Aug 2019-present   Advanced Statistical Process Control- Keysight-Mic Quality - Jul 2019 Certificate   Python for Data Science-IBM-Jan 2019 Certificate   Programming in R for Data Science-Microsoft (edX)- Nov 2016 Certificate "
"
 
Self-directed  with over ten years of analytics experience.   functions have included project coordination, consulting, training, programming, data mining, reporting, training, and teaching.  Seeking role with an organization that would allow me to showcase my mastery of data exploration/visualization/modeling, statistics, and machine learning.

 AND EXPERIENCE Analytics
 Devised an XGBoost regression model in Python to predict revenue as a function of marketing spend for a dental support organization. Utilized the predictive model to optimize marketing spend resulting in an average cost reduction of 70%.
 Modeled dental patient no-shows with a logistic regression in R to determine key driving factors.
 Designed a direct-mail marketing campaign and advised client on necessary statistical testing.
 Developed a CART model in SAS Enterprise Miner to find primary drivers of life annuity surrenders.  Performed exploratory data analysis, feature engineering, and feature selection utilizing R.  Presented the teams results to a panel of insurance experts.
 Performed market segmentation through latent-class analysis in R to discover relationships between spending habits and hobbies/interests among people aged 15 to 30.
 Analyzed $50B of leads data for a wealth-management firm through SQL, Tableau and R to improve sales activity.  Created a linearregression model to predict net new assets by branch and a decision-tree model to understand sales representatives behavior.
 Created an XGBoost classifier in Python to estimate insurance policy renewal probabilities.  Optimized incentives to insurance agents to maximize net revenue.  (McKinsey Analytics Online Hackathon, July 2018, ranked in the top 1% of 5,052 participants.)
 Forecasted the disappearance of Arctic sea ice through an ARIMA model in Base SAS.  Predictions for key dates in sea ice measurement compared favorably with those by meteorological and oceanographic authorities.
 Performed survival analysis (parametric and Cox regressions) to identify factors driving employee attrition at a pharmaceutical company.
 Applied net and integer optimization techniques for social net analysis using Excel, VBA, and Risk Solver Platform.
 Crafted a Tableau storyboard to assess gender inequality globally.  Visualizations covered the areas of employment, health, and .
 Directed the Fraud, Waste, and Abuse team's transformation to a data-driven operation through the introduction of Access-Excel integration for financial and operational reporting. This led to the creation and automation of 20 recurring reports (daily metrics, production metrics, measuring, managing, accelerating  in progress) for investigators, managers, and commissioners of insurance.
Databases and Programming
 Created a prototype Oracle database to model Evercare CTs skilled-nursing-services process and address appeals brought forth against them.
 Automated the collection and cleansing of real-estate data from the Internet using VBA (90% time saving over manual process).
 Developed a document-sharing website prototype utilizing HTML, CSS, PHP, JavaScript, and MySQL.
 Redesigned the Transfer Alternative Protocol process (a hospitalization tracking system used by health services and medical directors) through VBA by devising a user-friendly, centralized storage and reporting Access database.
Presentation, Teaching, and Training
 Instructed and provided support to 40 graduate students in the courses Analytics in the Organizational Context and Storytelling with Data.
 Taught a class of 45 graduate students to use SAS software (Base, Enterprise Guide, and Enterprise Miner) for time series and text mining.
 Presented to a group of 70 students the merits of R for visualizations.  Packages shown included ggplot2, Plotly, Shiny, and googleVis.
 Provided training and  support for the Excel/Access/VBA reporting and automation  that I developed (audiences of up to 100 trainees ranging from analysts to C-level employees).
 HISTORY
Adjunct Faculty Associate, Columbia University School of  Studies, New York, NY
2018  Present
Principal, Data Science, Xaxu Consulting, Bristol, CT
2014  Present
Consultant, Evangelista Consulting, Bristol, CT
2014  Present
Data Mining Teaching Assistant, University of Connecticut School of Business, Hartford, CT
2017
Senior Reporting Analyst, United Payment Integrity  Fraud, Waste, and Abuse, UnitedHealth Group, Rocky Hill, CT	2011  2013
Senior Business Process Analyst, Evercare Corporate Finance, UnitedHealth Group, Rocky Hill, CT	2009  2011
Operations Analyst, Evercare Connecticut, UnitedHealth Group, Hartford, CT	2005  2009
 AND  DEVELOPMENT
MS, Business Analytics and Project Management, University of Connecticut School of Business, Hartford, CT
Relevant course:  Predictive Modeling, Business Decision Modeling, Visual Analytics, Data Analytics Using R BS, Physics, University of Connecticut, Storrs, CT
Honors:  Graduated Summa Cum Laude.  Member of Sigma Pi Sigma Society
Certificates, Coursera:  Neural Nets and Deep Learning, Structuring Machine Learning 
Open Badges, Cognitive Class (IBM):  Hadoop Foundations, Hadoop Programming, Scala Programming for Data Science",Data Scientist,resume,"   Self-directed  with over ten years of analytics experience.   functions have included project coordination, consulting, training, programming, data mining, reporting, training, and teaching.  Seeking role with an organization that would allow me to showcase my mastery of data exploration/visualization/modeling, statistics, and machine learning.   AND EXPERIENCE Analytics  Devised an XGBoost regression model in Python to predict revenue as a function of marketing spend for a dental support organization. Utilized the predictive model to optimize marketing spend resulting in an average cost reduction of 70%.  Modeled dental patient no-shows with a logistic regression in R to determine key driving factors.  Designed a direct-mail marketing campaign and advised client on necessary statistical testing.  Developed a CART model in SAS Enterprise Miner to find primary drivers of life annuity surrenders.  Performed exploratory data analysis, feature engineering, and feature selection utilizing R.  Presented the teams results to a panel of insurance experts.  Performed market segmentation through latent-class analysis in R to discover relationships between spending habits and hobbies/interests among people aged 15 to 30.  Analyzed $50B of leads data for a wealth-management firm through SQL, Tableau and R to improve sales activity.  Created a linearregression model to predict net new assets by branch and a decision-tree model to understand sales representatives behavior.  Created an XGBoost classifier in Python to estimate insurance policy renewal probabilities.  Optimized incentives to insurance agents to maximize net revenue.  (McKinsey Analytics Online Hackathon, July 2018, ranked in the top 1% of 5,052 participants.)  Forecasted the disappearance of Arctic sea ice through an ARIMA model in Base SAS.  Predictions for key dates in sea ice measurement compared favorably with those by meteorological and oceanographic authorities.  Performed survival analysis (parametric and Cox regressions) to identify factors driving employee attrition at a pharmaceutical company.  Applied net and integer optimization techniques for social net analysis using Excel, VBA, and Risk Solver Platform.  Crafted a Tableau storyboard to assess gender inequality globally.  Visualizations covered the areas of employment, health, and .  Directed the Fraud, Waste, and Abuse team's transformation to a data-driven operation through the introduction of Access-Excel integration for financial and operational reporting. This led to the creation and automation of 20 recurring reports (daily metrics, production metrics, measuring, managing, accelerating  in progress) for investigators, managers, and commissioners of insurance. Databases and Programming  Created a prototype Oracle database to model Evercare CTs skilled-nursing-services process and address appeals brought forth against them.  Automated the collection and cleansing of real-estate data from the Internet using VBA (90% time saving over manual process).  Developed a document-sharing website prototype utilizing HTML, CSS, PHP, JavaScript, and MySQL.  Redesigned the Transfer Alternative Protocol process (a hospitalization tracking system used by health services and medical directors) through VBA by devising a user-friendly, centralized storage and reporting Access database. Presentation, Teaching, and Training  Instructed and provided support to 40 graduate students in the courses Analytics in the Organizational Context and Storytelling with Data.  Taught a class of 45 graduate students to use SAS software (Base, Enterprise Guide, and Enterprise Miner) for time series and text mining.  Presented to a group of 70 students the merits of R for visualizations.  Packages shown included ggplot2, Plotly, Shiny, and googleVis.  Provided training and  support for the Excel/Access/VBA reporting and automation  that I developed (audiences of up to 100 trainees ranging from analysts to C-level employees).  HISTORY Adjunct Faculty Associate, Columbia University School of  Studies, New York, NY 2018  Present Principal, Data Science, Xaxu Consulting, Bristol, CT 2014  Present Consultant, Evangelista Consulting, Bristol, CT 2014  Present Data Mining Teaching Assistant, University of Connecticut School of Business, Hartford, CT 2017 Senior Reporting Analyst, United Payment Integrity  Fraud, Waste, and Abuse, UnitedHealth Group, Rocky Hill, CT	2011  2013 Senior Business Process Analyst, Evercare Corporate Finance, UnitedHealth Group, Rocky Hill, CT	2009  2011 Operations Analyst, Evercare Connecticut, UnitedHealth Group, Hartford, CT	2005  2009  AND  DEVELOPMENT MS, Business Analytics and Project Management, University of Connecticut School of Business, Hartford, CT Relevant course:  Predictive Modeling, Business Decision Modeling, Visual Analytics, Data Analytics Using R BS, Physics, University of Connecticut, Storrs, CT Honors:  Graduated Summa Cum Laude.  Member of Sigma Pi Sigma Society Certificates, Coursera:  Neural Nets and Deep Learning, Structuring Machine Learning  Open Badges, Cognitive Class (IBM):  Hadoop Foundations, Hadoop Programming, Scala Programming for Data Science"
"Senior Data Scientist 
Unisys  San Diego, CA 	   	Apr. 2017 to Present 
 Time series and anomaly detection for forecasting demand and identifying events (e.g. ARIMA, LSTM). Automated pipeline to notify events to the team. 
 Created custom semi-supervised text-mining algorithms to extract info and categorize documents using topic modeling, sentiment analysis, NER, word embedding, text summarization using SpaCy, GENSIM, Stanford CoreNLP, NLTK etc.  
 End to end ML models for consumer segmentation, lift analysis and lifetime value estimation to optimize marketing and revenue prediction (TensorFlow, Keras, Scikit Learn etc.)  
 ed closely with product managers and engineers to design and evaluate solutions in cloud environment (AWS, 
AZURE). Lead migration of data and machine learning pipelines to the cloud environment (Spark, Spark ML, Scala) 
 Applied statistical analysis, dimensionality reduction and visualization techniques (e.g. PCA, T-SNE) to discover and communicate insights from data.  
 Conducted research, developed data driven prototype for business use cases and presented to stakeholders and clients. 
 Extraction, cleaning and manipulating of data in Linux environment (Python, Scala, Perl, SQL, Hive, Pandas) Data Scientist II 
Speedwell Holdings - Las Vegas, NV 	 	Dec. 2015 to Mar. 2017 
 Created data pipelines and ML models for content marketing, ping tree and consumer segmentation, fraud detection and evaluating credit worthiness of loan applications. 
 Designed and evaluated experiments for hypothesis testing (AB testing, cohort analysis, Bayesian Bandits). 
 Automated summarization reports and visualization  to discover underlying trends and time series forecasting.  
 Production using machine learning APIs, JavaScript, PHP, Python, SAS, PostgreSQL and MySQL. 
Data Science Research Fellow 
Galvanize  San Francisco, CA 	 	Jun. 2015 to Nov. 2015 
 Geospatial image processing and classification using Convolutional Neural Nets (CNN).  
Post-Doctoral Researcher  
University of California - Irvine, CA   	 	Oct. 2014 to Mar. 2015 
 Monte Carlo simulations on stochastic fracture nets, revealing effects of micro-scale net properties on largescale transport. 
 	 
 	   
 
Research Assistant 
University of California - Irvine, CA   	 	Jan. 2010 to Sep. 2014 
 Studied stochastic transport in fracture nets. Proposed and verified mathematical schema for large-scale anomalous transport. Developed distributed computing models using MPI, Fortran, Matlab, R, Python and C++ to simulate flow and transport in fractured rocks and porous media. 
 Simulated streamline-scale flow in nets of fractures using Finite Element Analysis, and reported algorithms with enhanced computational efficiency as alternative to direct solution of convective-diffusive PDEs (NavierStokes Eq.) 
 
Ph.D., University of California Irvine, Irvine, CA 	 	2014 
Computational Geosciences 
 Thesis: High-Resolution Analyses of Anomalous Transport in Large-Scale Variable-Aperture Discrete Fracture Nets 
M.Sc., Temple University, Philadelphia, PA 	 	2009 
Civil Engineering   	 	 
B.Sc., Sharif University of , Tehran, IR 	 	2007 
Structural Engineering 
 
Data Analysis, Statistics, Machine learning, Time series, NLP, Transfer Learning, Entity recognition, Text 
Summarization, Python, Scala, R, SAS, Matlab, Pandas, Scikit-Learn, TensorFlow, Spark, Web Services, Neural 
Nets and Deep Learning, Linear regression, Decision Trees, Boosting, SVM, PostgreSQL, MySQL, ArcGIS, 
Microsoft Office, AWS, Business Intelligence, Hydrogeology, Numerical Analysis 
 
 ",Data Scientist,resume,"Senior Data Scientist  Unisys  San Diego, CA 	   	Apr. 2017 to Present   Time series and anomaly detection for forecasting demand and identifying events (e.g. ARIMA, LSTM). Automated pipeline to notify events to the team.   Created custom semi-supervised text-mining algorithms to extract info and categorize documents using topic modeling, sentiment analysis, NER, word embedding, text summarization using SpaCy, GENSIM, Stanford CoreNLP, NLTK etc.    End to end ML models for consumer segmentation, lift analysis and lifetime value estimation to optimize marketing and revenue prediction (TensorFlow, Keras, Scikit Learn etc.)    ed closely with product managers and engineers to design and evaluate solutions in cloud environment (AWS,  AZURE). Lead migration of data and machine learning pipelines to the cloud environment (Spark, Spark ML, Scala)   Applied statistical analysis, dimensionality reduction and visualization techniques (e.g. PCA, T-SNE) to discover and communicate insights from data.    Conducted research, developed data driven prototype for business use cases and presented to stakeholders and clients.   Extraction, cleaning and manipulating of data in Linux environment (Python, Scala, Perl, SQL, Hive, Pandas) Data Scientist II  Speedwell Holdings - Las Vegas, NV 	 	Dec. 2015 to Mar. 2017   Created data pipelines and ML models for content marketing, ping tree and consumer segmentation, fraud detection and evaluating credit worthiness of loan applications.   Designed and evaluated experiments for hypothesis testing (AB testing, cohort analysis, Bayesian Bandits).   Automated summarization reports and visualization  to discover underlying trends and time series forecasting.    Production using machine learning APIs, JavaScript, PHP, Python, SAS, PostgreSQL and MySQL.  Data Science Research Fellow  Galvanize  San Francisco, CA 	 	Jun. 2015 to Nov. 2015   Geospatial image processing and classification using Convolutional Neural Nets (CNN).   Post-Doctoral Researcher   University of California - Irvine, CA   	 	Oct. 2014 to Mar. 2015   Monte Carlo simulations on stochastic fracture nets, revealing effects of micro-scale net properties on largescale transport.   	   	      Research Assistant  University of California - Irvine, CA   	 	Jan. 2010 to Sep. 2014   Studied stochastic transport in fracture nets. Proposed and verified mathematical schema for large-scale anomalous transport. Developed distributed computing models using MPI, Fortran, Matlab, R, Python and C++ to simulate flow and transport in fractured rocks and porous media.   Simulated streamline-scale flow in nets of fractures using Finite Element Analysis, and reported algorithms with enhanced computational efficiency as alternative to direct solution of convective-diffusive PDEs (NavierStokes Eq.)    Ph.D., University of California Irvine, Irvine, CA 	 	2014  Computational Geosciences   Thesis: High-Resolution Analyses of Anomalous Transport in Large-Scale Variable-Aperture Discrete Fracture Nets  M.Sc., Temple University, Philadelphia, PA 	 	2009  Civil Engineering   	 	  B.Sc., Sharif University of , Tehran, IR 	 	2007  Structural Engineering    Data Analysis, Statistics, Machine learning, Time series, NLP, Transfer Learning, Entity recognition, Text  Summarization, Python, Scala, R, SAS, Matlab, Pandas, Scikit-Learn, TensorFlow, Spark, Web Services, Neural  Nets and Deep Learning, Linear regression, Decision Trees, Boosting, SVM, PostgreSQL, MySQL, ArcGIS,  Microsoft Office, AWS, Business Intelligence, Hydrogeology, Numerical Analysis     "
" 
Data Scientist with 4+ years experience executing data-driven solutions to increase effectively, accuracy, and utility of data processing. Experienced at creating data regression and classification models, using data mining and machine learning models to deliver insights and implement action-oriented solutions to complex research and business problems. Hunger for impact, be creative, and win as a team.  
Computer Software:  R (neuralnet; randomForest; rpart, gam)  SAS  JMP  SPSS  Python (netx; numpy; pandas; seaborn; math; sklearn)  HTML  ArcMap GIS  AutoCAD  Microsoft Office  Excel Visual Basic  Microsoft Project  Microsoft SQL Server  MySQL 
Machine Learning (ML) & Artificial Intelligence (AI):  Multiple Linear Regression (MLR)  Generalized Additive Models (GAM)  Logistic Regression  Neural Nets  Decision Tree Learning  Random Forests  Support Vector Machines (SVM)  Cross-Validation  Bootstrap  Data Cleaning  Data Imputation  Model Selection  Data Visualization 
Others:  Leadership  Collaboration  Problem Solving  Detail Oriented 
Experience 
Postdoctoral Researcher; Louisiana State University 	 	 	 	 	2019?Present 
 Develop data scientist programs in python to analyze research data related to natural hazard mitigation 
 Supervise graduate PhD students 
 Develop and conduct an online research survey 
 Write research papers 
Research Assistant; Louisiana State University  	 	 	 	 	2014?2019 
 Develop an advanced statistical model to estimate flood mitigation project cost in R program by examining several prediction models such as multiple regression, random forest, and neural nets 
 Impute missing data for a project with missing data on 60% of properties by developing a method with using generalized additive models in R program 
 Develop a methodology for probabilistic flood loss modeling by implementing Monte Carlo simulation in Excel 
 Leading a collaborative  with two other graduate students for developing a program in Python for hurricane wind loss modeling with generalizing the loss functions in HAZUS-MH Hurricane Model repository with 103 MB data 
Teaching Assistant; Louisiana State University 	 	 	 	 	 	Spring 2014 
 Prepare course materials and help students in three online courses 
al Background 
Ph.D. in Engineering Science; Louisiana State University  	 	 	 	Jan. 2019 
Title of dissertation: Costs and Benefits of Flood Mitigation in Louisiana 
Ph.D. Minor in Applied Statistics 
Related course: Statistical techniques I & II (SAS programming)  Regression analysis (SAS programming)  Statistical methods for reliability and survival data (JMP)  Principles and theory of statistics  Statistical data mining (R programming) 
 
M.Sc.  in Engineering Science; Louisiana State University 	 	 	 	Dec. 2017 
Title of project: Prediction of Flood Mitigation Elevation Project Cost in Louisiana by Statistical Modeling of Current Mitigation Practices 
 
M.Sc. in Construction Management; University  Malaysia 	 	 	Mar. 2013 
 
 	1 
 
Arash Taghinezhad 
Bachelor of Science in Civil Engineering; University of Mashhad, Iran 	 	 
Training Certificates 
May 2008 
SQL Essential Training; Lynda.com 	 	 	 	 	 	 
July 2019 
Python for Data Science Essential Training; Lynda.com  	 	 	 
Fun Data Scientist  
May 2019 
 Analyze Lending Club dataset to make a prediction model to see if the loan applicants able to pay off their loans based on their credit history 
 Develop a machine learning model to predict concrete strength 
 Presentations 
Presenter in Construction Management GSA research poster competition (3rd place) 	 	2019 Performing an R-Programming shop for LSU Construction Management graduate students 2018  
Lecture to Episcopal High School students about engineering careers 	 	 	 	2018 
Presenter in three-minute thesis (3MT®) competition at LSU 	 	 	 
Sociaety Memberships 
 	2017 
The Data Science Society @ LSU 	 	 	 	 	 	 
2019-present 
Construction Student Association (CSA)  	 	 	 	 	 
2018-2019 
LSU Construction Management Graduate Student Association 	 	 	 
2018-2019 
LSU International Ambassadors  	 	 	 	 	 	 
2017-2018 
American Society of Civil Engineers (ASCE) 	 	 	 	 	 
2016-present 
Association of State Floodplain Managers (ASFPM) 	 	 	 	 
2016-2017 
Iranian Student Association at LSU (ISA)  	 	 	 	 	 
 Funded Grant Research Experience 
2013-present
Multi-Scale Spatiotemporal Evaluation of Mitigation Effectiveness in Reducing Natural Hazard Damage 
and Loss 	 	 	 	 	 	 	 	 	 	 
FEMA (sub-grantee through Louisiana GOHSEP), collaborative project with Arizona State University and East Tennessee State University, March 2015  February 2018, Budget: $475,673 + $104,974 internal/external budgeted match = $580,647 
Role: Lead graduate research assistant for building-level mitigation effectiveness 
Volunteer s 
 LSU representative at American Institute of Architects and the initials (AIA) exhibition (2016)  Student helper for ABC's National Construction Management Competition (2017)  Presentor at STEM Exhibition 
Day at St. Jude the Apostle School (2017)  Poster competition judge at LSU Discover Day (2019)  Publications 
Taghinezhad, A., Friedland, C. J., Robert, R. V., and Marx, B. D. (2020). ""Data imputation for avoided loss analysis in natural disaster mitigation ."" Under Consideration by Journal of Performance of Constructed Facilities. 
Taghinezhad, A., Friedland, C. J., Rohli, R. V., Marx, B. D., and Giering, J. (2020). ""Predictive statistical cost estimation model for existing single-family home elevation ."" Under Consideration by Journal of Flood Risk Management. 
Taghinezhad, A., Jafari, A., and Kermanshachi, S. (2020). ""Best practices for state DOTs to determine project delivery time, project management, and ratio of consultant to in-house design."" TBD. 
Taghinezhad, A., Jafari, A., and Kermanshachi, S. (2020). ""Exploring project management best practices for project success in U.S.transportation agencies "" Under Consideration by International Journal of Project Management. 
 
 
 	2 ",Data Scientist,resume,"  Data Scientist with 4+ years experience executing data-driven solutions to increase effectively, accuracy, and utility of data processing. Experienced at creating data regression and classification models, using data mining and machine learning models to deliver insights and implement action-oriented solutions to complex research and business problems. Hunger for impact, be creative, and win as a team.   Computer Software:  R (neuralnet; randomForest; rpart, gam)  SAS  JMP  SPSS  Python (netx; numpy; pandas; seaborn; math; sklearn)  HTML  ArcMap GIS  AutoCAD  Microsoft Office  Excel Visual Basic  Microsoft Project  Microsoft SQL Server  MySQL  Machine Learning (ML) & Artificial Intelligence (AI):  Multiple Linear Regression (MLR)  Generalized Additive Models (GAM)  Logistic Regression  Neural Nets  Decision Tree Learning  Random Forests  Support Vector Machines (SVM)  Cross-Validation  Bootstrap  Data Cleaning  Data Imputation  Model Selection  Data Visualization  Others:  Leadership  Collaboration  Problem Solving  Detail Oriented  Experience  Postdoctoral Researcher; Louisiana State University 	 	 	 	 	2019?Present   Develop data scientist programs in python to analyze research data related to natural hazard mitigation   Supervise graduate PhD students   Develop and conduct an online research survey   Write research papers  Research Assistant; Louisiana State University  	 	 	 	 	2014?2019   Develop an advanced statistical model to estimate flood mitigation project cost in R program by examining several prediction models such as multiple regression, random forest, and neural nets   Impute missing data for a project with missing data on 60% of properties by developing a method with using generalized additive models in R program   Develop a methodology for probabilistic flood loss modeling by implementing Monte Carlo simulation in Excel   Leading a collaborative  with two other graduate students for developing a program in Python for hurricane wind loss modeling with generalizing the loss functions in HAZUS-MH Hurricane Model repository with 103 MB data  Teaching Assistant; Louisiana State University 	 	 	 	 	 	Spring 2014   Prepare course materials and help students in three online courses  al Background  Ph.D. in Engineering Science; Louisiana State University  	 	 	 	Jan. 2019  Title of dissertation: Costs and Benefits of Flood Mitigation in Louisiana  Ph.D. Minor in Applied Statistics  Related course: Statistical techniques I & II (SAS programming)  Regression analysis (SAS programming)  Statistical methods for reliability and survival data (JMP)  Principles and theory of statistics  Statistical data mining (R programming)    M.Sc.  in Engineering Science; Louisiana State University 	 	 	 	Dec. 2017  Title of project: Prediction of Flood Mitigation Elevation Project Cost in Louisiana by Statistical Modeling of Current Mitigation Practices    M.Sc. in Construction Management; University  Malaysia 	 	 	Mar. 2013     	1    Arash Taghinezhad  Bachelor of Science in Civil Engineering; University of Mashhad, Iran 	 	  Training Certificates  May 2008  SQL Essential Training; Lynda.com 	 	 	 	 	 	  July 2019  Python for Data Science Essential Training; Lynda.com  	 	 	  Fun Data Scientist   May 2019   Analyze Lending Club dataset to make a prediction model to see if the loan applicants able to pay off their loans based on their credit history   Develop a machine learning model to predict concrete strength   Presentations  Presenter in Construction Management GSA research poster competition (3rd place) 	 	2019 Performing an R-Programming shop for LSU Construction Management graduate students 2018   Lecture to Episcopal High School students about engineering careers 	 	 	 	2018  Presenter in three-minute thesis (3MT®) competition at LSU 	 	 	  Sociaety Memberships   	2017  The Data Science Society @ LSU 	 	 	 	 	 	  2019-present  Construction Student Association (CSA)  	 	 	 	 	  2018-2019  LSU Construction Management Graduate Student Association 	 	 	  2018-2019  LSU International Ambassadors  	 	 	 	 	 	  2017-2018  American Society of Civil Engineers (ASCE) 	 	 	 	 	  2016-present  Association of State Floodplain Managers (ASFPM) 	 	 	 	  2016-2017  Iranian Student Association at LSU (ISA)  	 	 	 	 	   Funded Grant Research Experience  2013-present Multi-Scale Spatiotemporal Evaluation of Mitigation Effectiveness in Reducing Natural Hazard Damage  and Loss 	 	 	 	 	 	 	 	 	 	  FEMA (sub-grantee through Louisiana GOHSEP), collaborative project with Arizona State University and East Tennessee State University, March 2015  February 2018, Budget: $475,673 + $104,974 internal/external budgeted match = $580,647  Role: Lead graduate research assistant for building-level mitigation effectiveness  Volunteer s   LSU representative at American Institute of Architects and the initials (AIA) exhibition (2016)  Student helper for ABC's National Construction Management Competition (2017)  Presentor at STEM Exhibition  Day at St. Jude the Apostle School (2017)  Poster competition judge at LSU Discover Day (2019)  Publications  Taghinezhad, A., Friedland, C. J., Robert, R. V., and Marx, B. D. (2020). ""Data imputation for avoided loss analysis in natural disaster mitigation ."" Under Consideration by Journal of Performance of Constructed Facilities.  Taghinezhad, A., Friedland, C. J., Rohli, R. V., Marx, B. D., and Giering, J. (2020). ""Predictive statistical cost estimation model for existing single-family home elevation ."" Under Consideration by Journal of Flood Risk Management.  Taghinezhad, A., Jafari, A., and Kermanshachi, S. (2020). ""Best practices for state DOTs to determine project delivery time, project management, and ratio of consultant to in-house design."" TBD.  Taghinezhad, A., Jafari, A., and Kermanshachi, S. (2020). ""Exploring project management best practices for project success in U.S.transportation agencies "" Under Consideration by International Journal of Project Management.       	2 "
" Over 5+ years of  as a Quantitative Analyst /Data Scientist ing across
Finance, Telecom and Retail industries with a master's degree in Science and Specialization in
Mathematics and Quantitative Finance. 
 Proven ability to translate high-level  into practical analysis and deliver actionablerecommendations. Record of managing complex  and creating solutions that . Selfdirected innovator searching for challenges. 
 Proficient in Machine Learning Techniques, R, Python, SAS, Tableau, SQL & Advanced Excel.  Developed predictive models to identify the most significant behavioral patterns that lead to a member conversion which eventually increased the ROI of 0.96 million$ per quarter 
 Re-built the existing model and increased its accuracy from 68% to 89% using Advanced StatisticalAlgorithms 
 Proficient in managing entire phases of CRISP-DM project life cycle including data acquisition, datacleaning, data engineering, features scaling, features engineering, statistical modeling (Decision trees, regression models, neural nets, SVM, KMeans Clustering), dimensionality reduction using Principal Component Analysis and Factor Analysis, testing and validation using ROC plot, K- fold cross validation and data visualization. 
 Good practical knowledge in performing Data Analysis process using Python like Importing datasets,
Data wrangling, Exploratory Data Analysis, Model development and Model Evaluation. 
 Hands on Expertise on Classification, Regression, Time Series Data, Churn Prediction, Home Pricevaluation, Exit Strategies using various packages in R and Python 
 Adept and deep understanding of Statistical modeling, Multivariate Analysis, model testing, problemanalysis, model comparison and validation. 
 Skilled in performing data parsing, data manipulation and data preparation with methods includingdescribe data contents, compute descriptive statistics of data, regex, split and combine, Remap, merge, subset, re index, melt and reshape. 
 Experience in using various packages in R and libraries in Python. 
 ing knowledge in Hadoop, Hive and NOSQL databases like Cassandra and HBase. 
 Ability to handle multiple tasks simultaneously. 
 Proven leader with outstanding relationship building  and strong communication abilities 
 Highly motivated team player with ability to  cross-organizationally and manage strict deadline.
 
 Extensive ing experience in developing mappings in Informatica, tuning them to achieve optimalperformance and migrate objects in all environments including DEV, QA testing and PROD.
 Experience

Quantitative Analyst/Data Scientist
Tourmalet, CT
June 2017 to Present
Project: Conversion Efficacy - Review and assess the Exit strategy and performance of the targeting system for converting NPL to payoff 
 Created a dataset with data in 7-8 different repositories using SQL Teradata involving complexquerying. 
 Analyzed dataset of about 1.34M records and identified trends and effective factors for datamodelling. 
 Preprocessed the data and developed various visualizations using packages ggplot/choroplethr/caretin R. 
 Rebuilt the propensity model with increased accuracy from 68%-89% using Advanced ML Algorithms
 
 The model estimates to increase the ROI and least estimated standard error 2.05% 
 Developed dashboards using Tableau and automated the process using SSIS for daily update  Documented the phase2 Analysis for the project and also designed roadmap and conducted KT sessions 
 Built multiple models using various Machine Learning Algorithms like Multinomial Regression,
Decision Tree, SVM, Random Forest, Neural Nets etc., and finalized with the better model using ROC Curve 
 Used various Parameter Tuning Techniques to get better results from the model. 
 Used different methods like Univariate approach, Boxplots, Cook's distance to find the outliers  Developed time series forecasting model (ARIMA) & provided strategic analysis for the business demands and supply, in which the model predicted actual demand with 92% accuracy. 
 Created Exploratory Data Analysis to identify trend, seasonality and outliers etc. 
 Managed team processes and deliverables for Ramp-up and Ramp down demand forecasts 
 Responsible for providing reports, analysis and insightful recommendations to business leaders onkey performance metrics pertaining to employee performance 
 Built predictive models to identify the most significant behavioral patterns that lead to employeechurn 
 Created Propensity model to identify the most influential attributes contributing to Indent/Demandcancellation
DATA SCIENTIST
Idea Cellular Ltd
December 2013 to August 2016
Project: Customer Churn Model 
 Understanding business context and strategic plans and develop a data-driven business plan tosupport the attainment of business goals. 
 Created a dataset using SQL Teradata complex queries along with identification to factors tocustomers calling to customer care from customer care text repository. 
 Data manipulation/treatment based on nature of data (for example missing value imputation,Information Value (IV), Weight of Evidence (WOE), Data profiling, correlation matrix, relative importance between predictors, variable clustering, univariate and bivariate plots, etc.) 
 Building predictive models from start-to-finish following CRISP-DM life cycle (i.e. extract data,manipulate data, Data Profiling, build and validate model) and then deployed model using flask.  Preparation of final project presentation documents for overall significance of the project in a welldefined manner 
 Identify the most significant behavioral patterns that lead to customer churn and build an attritionmodel using Random Forest, SVM with Precision-Recall curve as evaluation metric for the model.  Data exploration to make the data useable for building data insights and initial hypothesis. 
 Used the analysis of behavior and characteristics of terminated employees to drew the importantfactor for attrition. 
 Build a model to check the factors affecting the termination of employees and their reasons fortermination using various dimension reduction techniques. 
 Predict the employees who are high risk to maintain the talent pool and to effectively retain talent atevery level (High Potential and High performer) 
 Responsible for ing with stakeholders to troubleshoot issues, communicate to team members,leadership and stakeholders on findings to ensure models are well understood and optimized.
DATA ANALYST
Axis Bank - IN
August 2011 to December 2013
 Creating output to explain data analysis, data visualization, and statistical modeling results tomanagers. 
 Modeling survey data responses with ordinal logistic regression. 
 Experience with ing on clickstream activities, Customer Journey activities, Fraud Detection,
Sales and managing Store items. 
 Analyzing and visualizing user behavior migration. 
 Created mappings to load data from source and target to staging, staging to reporting tables byapplying business requirements using Informatica Power Center. 
 Applying machine learning concepts to capture insights. 
 Handled importing data from various data sources, performed transformations using Hive,
MapReduce, and loaded data into HDFS. 
 Providing timely, relevant, accurate reports and analysis of the organization's performance tofacilitate decision-making towards achievement of the budget and strategic plan. 
 Documented all phases of project implementation for future reference and conducting KT sessions
Emerging Market Analyst
IndusInd Bank - IN
May 2010 to August 2011
 Gathered requirements from onsite coordinators, performed Requirement Gap analysis and finalizeddesign documents using Erwin Data Modeler and Microsoft Visio. 
 Designed complex SQL queries to input at the beginning of mappings to filter the data as perrequirements. 
 Created reporting tables for comparing source and target data and report data discrepancies
(mismatch, missing scenarios) found in the data. 
 Performed validations not received in the requirement document from the customer end and learntthe SQL queries which helped to attend defect triage calls. 
 Results obtained from report mappings were displayed using MicroStrategy which is a better UserInterface tool. 
 Extensive hands on experience of HP Quality Center tool used for performing production supportactivities. 
 Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model,
Sequence diagrams, and Activity diagrams for SDLC process of the application. 
 Performed debugging of the code as per inputs given by IST (Integrated System Testing) team anddeployed code into PROD environment after receiving approval from IST team. 
 Performed performance improvement of the existing Data warehouse applications to increaseefficiency of the existing system. 
 Finalized the factors required for predictive model development by gathering inputs from MCC(Customer service) team, performed feature engineering to identify factors best suited for model development and gathered data from various repositories as per required factors using SQL complex querying n Teradata.
FSM/Data Analyst
ICICI Bank - IN
May 2006 to April 2009
 Gathered requirements from onsite coordinators, performed Requirement Gap analysis and finalizeddesign documents using Erwin Data Modeler and Microsoft Visio. 
 Understanding the requirements and develop various packages in SSIS. 
 Gathered requirements from JAD/JAR sections with developers and business clients. 
 Designed the business requirement collection approach based on the project scope and SDLCmethodology. 
 Designs and develops the logical and physical data models to support the Data Marts and the DataWarehouse 
 Create SQL queries for product components to update FACETS backend tables and create productprefixes. 
 Involved in formatting data stores and generate UML diagrams of logical and physical data. 
 Developed project plans and managed project scope. 
 Performed user acceptance and parallel testing for coding, pricing, and benefit builds in Facets. 
 Prepared a handbook of standards and Documented standards for Informatica code development.
Education

Master of Science in Mathematics and Quantitative Finance
Sacred Heart University December 2017
MBA
Institute for Technology and Management May 2006
B.com
Lucknow University - Lucknow, Uttar Pradesh May 2003


Sql server, Sql server 2005, Mysql, Oracle, Sql, Cassandra, Clustering, Hadoop, Informatica, Machine learning, Nosql, Teradata, Microstrategy, Sas, Tableau, Decision trees, Deep learning, Logistic regression, Random forest, Support vector machines
Additional Information

TECHNICAL 
 
Expertise 
Caret, Tidyverse, MASS, GGPLOT2, Scikit-Learn, NumPy, SciPy, Deep learning, RNN, CNN, Tensor flow, Keras, matplotlib, Microsoft Visual Studio, Microsoft Office 
 
Machine Learning Algorithms: 
Multinomial Regression, Logistic Regression, Decision Trees, Random Forest, K Means Clustering, Support Vector Machines, Gradient Boost Machines & XGBoost, 
 
RDBMS SQL Server 2005/2008/2012, MySQL, Teradata 
NoSQL DB Cassandra 
Frames Hadoop Ecosystem, Apache Spark 
Programming Languages R, Python 
Tools/Platforms 
RStudio, Tableau, Informatica, MicroStrategy, Toad, SAS, Eclipse, Windows, SQL developer, Toad for
Oracle, Microsoft SQL, Teradata, Hadoop",Data Scientist,resume," Over 5+ years of  as a Quantitative Analyst /Data Scientist ing across Finance, Telecom and Retail industries with a master's degree in Science and Specialization in Mathematics and Quantitative Finance.   Proven ability to translate high-level  into practical analysis and deliver actionablerecommendations. Record of managing complex  and creating solutions that . Selfdirected innovator searching for challenges.   Proficient in Machine Learning Techniques, R, Python, SAS, Tableau, SQL & Advanced Excel.  Developed predictive models to identify the most significant behavioral patterns that lead to a member conversion which eventually increased the ROI of 0.96 million$ per quarter   Re-built the existing model and increased its accuracy from 68% to 89% using Advanced StatisticalAlgorithms   Proficient in managing entire phases of CRISP-DM project life cycle including data acquisition, datacleaning, data engineering, features scaling, features engineering, statistical modeling (Decision trees, regression models, neural nets, SVM, KMeans Clustering), dimensionality reduction using Principal Component Analysis and Factor Analysis, testing and validation using ROC plot, K- fold cross validation and data visualization.   Good practical knowledge in performing Data Analysis process using Python like Importing datasets, Data wrangling, Exploratory Data Analysis, Model development and Model Evaluation.   Hands on Expertise on Classification, Regression, Time Series Data, Churn Prediction, Home Pricevaluation, Exit Strategies using various packages in R and Python   Adept and deep understanding of Statistical modeling, Multivariate Analysis, model testing, problemanalysis, model comparison and validation.   Skilled in performing data parsing, data manipulation and data preparation with methods includingdescribe data contents, compute descriptive statistics of data, regex, split and combine, Remap, merge, subset, re index, melt and reshape.   Experience in using various packages in R and libraries in Python.   ing knowledge in Hadoop, Hive and NOSQL databases like Cassandra and HBase.   Ability to handle multiple tasks simultaneously.   Proven leader with outstanding relationship building  and strong communication abilities   Highly motivated team player with ability to  cross-organizationally and manage strict deadline.    Extensive ing experience in developing mappings in Informatica, tuning them to achieve optimalperformance and migrate objects in all environments including DEV, QA testing and PROD.  Experience  Quantitative Analyst/Data Scientist Tourmalet, CT June 2017 to Present Project: Conversion Efficacy - Review and assess the Exit strategy and performance of the targeting system for converting NPL to payoff   Created a dataset with data in 7-8 different repositories using SQL Teradata involving complexquerying.   Analyzed dataset of about 1.34M records and identified trends and effective factors for datamodelling.   Preprocessed the data and developed various visualizations using packages ggplot/choroplethr/caretin R.   Rebuilt the propensity model with increased accuracy from 68%-89% using Advanced ML Algorithms    The model estimates to increase the ROI and least estimated standard error 2.05%   Developed dashboards using Tableau and automated the process using SSIS for daily update  Documented the phase2 Analysis for the project and also designed roadmap and conducted KT sessions   Built multiple models using various Machine Learning Algorithms like Multinomial Regression, Decision Tree, SVM, Random Forest, Neural Nets etc., and finalized with the better model using ROC Curve   Used various Parameter Tuning Techniques to get better results from the model.   Used different methods like Univariate approach, Boxplots, Cook's distance to find the outliers  Developed time series forecasting model (ARIMA) & provided strategic analysis for the business demands and supply, in which the model predicted actual demand with 92% accuracy.   Created Exploratory Data Analysis to identify trend, seasonality and outliers etc.   Managed team processes and deliverables for Ramp-up and Ramp down demand forecasts   Responsible for providing reports, analysis and insightful recommendations to business leaders onkey performance metrics pertaining to employee performance   Built predictive models to identify the most significant behavioral patterns that lead to employeechurn   Created Propensity model to identify the most influential attributes contributing to Indent/Demandcancellation DATA SCIENTIST Idea Cellular Ltd December 2013 to August 2016 Project: Customer Churn Model   Understanding business context and strategic plans and develop a data-driven business plan tosupport the attainment of business goals.   Created a dataset using SQL Teradata complex queries along with identification to factors tocustomers calling to customer care from customer care text repository.   Data manipulation/treatment based on nature of data (for example missing value imputation,Information Value (IV), Weight of Evidence (WOE), Data profiling, correlation matrix, relative importance between predictors, variable clustering, univariate and bivariate plots, etc.)   Building predictive models from start-to-finish following CRISP-DM life cycle (i.e. extract data,manipulate data, Data Profiling, build and validate model) and then deployed model using flask.  Preparation of final project presentation documents for overall significance of the project in a welldefined manner   Identify the most significant behavioral patterns that lead to customer churn and build an attritionmodel using Random Forest, SVM with Precision-Recall curve as evaluation metric for the model.  Data exploration to make the data useable for building data insights and initial hypothesis.   Used the analysis of behavior and characteristics of terminated employees to drew the importantfactor for attrition.   Build a model to check the factors affecting the termination of employees and their reasons fortermination using various dimension reduction techniques.   Predict the employees who are high risk to maintain the talent pool and to effectively retain talent atevery level (High Potential and High performer)   Responsible for ing with stakeholders to troubleshoot issues, communicate to team members,leadership and stakeholders on findings to ensure models are well understood and optimized. DATA ANALYST Axis Bank - IN August 2011 to December 2013  Creating output to explain data analysis, data visualization, and statistical modeling results tomanagers.   Modeling survey data responses with ordinal logistic regression.   Experience with ing on clickstream activities, Customer Journey activities, Fraud Detection, Sales and managing Store items.   Analyzing and visualizing user behavior migration.   Created mappings to load data from source and target to staging, staging to reporting tables byapplying business requirements using Informatica Power Center.   Applying machine learning concepts to capture insights.   Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.   Providing timely, relevant, accurate reports and analysis of the organization's performance tofacilitate decision-making towards achievement of the budget and strategic plan.   Documented all phases of project implementation for future reference and conducting KT sessions Emerging Market Analyst IndusInd Bank - IN May 2010 to August 2011  Gathered requirements from onsite coordinators, performed Requirement Gap analysis and finalizeddesign documents using Erwin Data Modeler and Microsoft Visio.   Designed complex SQL queries to input at the beginning of mappings to filter the data as perrequirements.   Created reporting tables for comparing source and target data and report data discrepancies (mismatch, missing scenarios) found in the data.   Performed validations not received in the requirement document from the customer end and learntthe SQL queries which helped to attend defect triage calls.   Results obtained from report mappings were displayed using MicroStrategy which is a better UserInterface tool.   Extensive hands on experience of HP Quality Center tool used for performing production supportactivities.   Implemented Microsoft Visio and Rational Rose for designing the Use Case Diagrams, Class model, Sequence diagrams, and Activity diagrams for SDLC process of the application.   Performed debugging of the code as per inputs given by IST (Integrated System Testing) team anddeployed code into PROD environment after receiving approval from IST team.   Performed performance improvement of the existing Data warehouse applications to increaseefficiency of the existing system.   Finalized the factors required for predictive model development by gathering inputs from MCC(Customer service) team, performed feature engineering to identify factors best suited for model development and gathered data from various repositories as per required factors using SQL complex querying n Teradata. FSM/Data Analyst ICICI Bank - IN May 2006 to April 2009  Gathered requirements from onsite coordinators, performed Requirement Gap analysis and finalizeddesign documents using Erwin Data Modeler and Microsoft Visio.   Understanding the requirements and develop various packages in SSIS.   Gathered requirements from JAD/JAR sections with developers and business clients.   Designed the business requirement collection approach based on the project scope and SDLCmethodology.   Designs and develops the logical and physical data models to support the Data Marts and the DataWarehouse   Create SQL queries for product components to update FACETS backend tables and create productprefixes.   Involved in formatting data stores and generate UML diagrams of logical and physical data.   Developed project plans and managed project scope.   Performed user acceptance and parallel testing for coding, pricing, and benefit builds in Facets.   Prepared a handbook of standards and Documented standards for Informatica code development. Education  Master of Science in Mathematics and Quantitative Finance Sacred Heart University December 2017 MBA Institute for Technology and Management May 2006 B.com Lucknow University - Lucknow, Uttar Pradesh May 2003   Sql server, Sql server 2005, Mysql, Oracle, Sql, Cassandra, Clustering, Hadoop, Informatica, Machine learning, Nosql, Teradata, Microstrategy, Sas, Tableau, Decision trees, Deep learning, Logistic regression, Random forest, Support vector machines Additional Information  TECHNICAL    Expertise  Caret, Tidyverse, MASS, GGPLOT2, Scikit-Learn, NumPy, SciPy, Deep learning, RNN, CNN, Tensor flow, Keras, matplotlib, Microsoft Visual Studio, Microsoft Office    Machine Learning Algorithms:  Multinomial Regression, Logistic Regression, Decision Trees, Random Forest, K Means Clustering, Support Vector Machines, Gradient Boost Machines & XGBoost,    RDBMS SQL Server 2005/2008/2012, MySQL, Teradata  NoSQL DB Cassandra  Frames Hadoop Ecosystem, Apache Spark  Programming Languages R, Python  Tools/Platforms  RStudio, Tableau, Informatica, MicroStrategy, Toad, SAS, Eclipse, Windows, SQL developer, Toad for Oracle, Microsoft SQL, Teradata, Hadoop"
"A Computer Science graduate from Purdue University with programming experience in scraping, combining and managing data from diverse sources and a data scientist with expertise in deriving insights from the raw & messy data, with a strong interest in Machine Learning, data mining, and information visualization.  
I am passionate about Data Science and its applications in real-world and would love to apply it to challenging business problems to bring value to the company. 
 
My career is a one-part strategy and two-parts  expertise. 
 
 Areas of Expertise:  
 Data Science: Machine Learning, Data Mining, Predictive Analytics  
 Data Engineering: C/Java, SQL, Python (NumPy, Pandas, SciPy, Scikit-learn, Seaborn, Matplotlib)  
 Hadoop: MapReduce, Hive, Pig, HDFS 
 Open Source Relational Databases: MySQL, PostgreSQL, Microsoft SQL Server 
 Big Data Analytics and Data Warehousing: Amazon Web Services (S3, EC2, EMR, Machine Learning,Redshift) 
 Data Visualization: Tableau, D3.js 
 Machine Learning: Supervised (Classification: KNN, Neural Net, SVM, GBM, XGBoost),
Unsupervised(Clustering: K-means, Graph, Dimensionality Reduction: PCA, SVD, Recommender
Systems- Frequent Itemset Mining: Apriori, FP Growth) 
 Deep Learning: TensorFlow, Keras 
 Statistical Modeling: R, MATLAB, SAS, SPSS 
 Business Intelligence: Qlikview, Cloud Analytics 
 Database Administration: Microsoft SQL Server 
 Database Development 
 Data Architecture, Data Modeling 
Most part of my  has involved in understanding the Linear Algebra, Probability, and Statistics involved in Machine Learning and implementing those Machine Learning algorithms from scratch. 
 
I like designing, developing, deploying, and managing Machine Learning models in the AWS environment. 
 
I would love to  as a Data Scientist, Machine Learning Engineer, Data Analyst roles. 
 

Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Database Developer and Data Analyst
IU SCHOOL OF MEDICINE - Indianapolis, IN December 2017 to December 2018
Database Developer & Administrator: phpMyAdmin, MySQL 
 Built a web application for IU School of Nursing to provide services for nursing centers and patients across Indiana State. 
 Designed, Developed, Administered, and Maintained a database for storing, retrieving, updating the data at the back-end. (Prototype won grant money of $30,000).
Business Data Analyst Intern
ProGen Business Solutions
April 2016 to October 2016
Business Intelligence Developer: Tableau, R, Python, SQL 
 Assisted in getting the insights from sales data and recognized the patterns associated with the customer transactions. 
 Prepared Financial and Business Sales Reports 
 Created sales data visualizations using Tableau Dashboards. 
 Improved product sales from 70% to 85%.
Data Analyst Intern
TakenMind
May 2019
· Performed EDA, Data Cleaning, and Statistical Analysis in Python. 
· Delivered a prediction analysis using supervised machine learning algorithms (Decision Tree, Random Forest, SVM). 


Master of Science in Computer & Information Science (Data Science Track)
Purdue University - Indianapolis, IN January 2017 to December 2018
Bachelor's in Computer Science and Engineering
Jawaharlal Nehru Technological University
June 2011 to June 2015


AWS (3 years), Data Analytics (3 years), Detail Oriented, SQL, Excel, Business Intelligence, Machine
Learning (2 years), Python (3 years), R (3 years), MySQL (4 years), Database Management (4 years),
Data Mining (2 years), Data Modeling (4 years), Linear Algebra (5 years), Statistical Analysis (3 years),
Tableau (3 years), Microsoft SQL Server (3 years), Object Oriented Programming (3 years), Data
Science (3 years)
Links

Certifications/Licenses

Artificial Intelligence Foundations: Machine Learning
January 2019 to Present
Learning Cloud Data Storage
January 2019 to Present
SQL for Data Science
August 2017 to Present
Database Modeling and Design
August 2017 to Present
Kaggle Python Tutorial on Machine Learning
February 2019 to Present
Groups

Data Science Indy
January 2017 to Present
Student Group Leader for Data Science Indy Meetup.
Additional Information

Available Immediately. Open for Relocation. Authorized to  for any employer in the US.",Data Scientist,resume,"A Computer Science graduate from Purdue University with programming experience in scraping, combining and managing data from diverse sources and a data scientist with expertise in deriving insights from the raw & messy data, with a strong interest in Machine Learning, data mining, and information visualization.   I am passionate about Data Science and its applications in real-world and would love to apply it to challenging business problems to bring value to the company.    My career is a one-part strategy and two-parts  expertise.     Areas of Expertise:    Data Science: Machine Learning, Data Mining, Predictive Analytics    Data Engineering: C/Java, SQL, Python (NumPy, Pandas, SciPy, Scikit-learn, Seaborn, Matplotlib)    Hadoop: MapReduce, Hive, Pig, HDFS   Open Source Relational Databases: MySQL, PostgreSQL, Microsoft SQL Server   Big Data Analytics and Data Warehousing: Amazon Web Services (S3, EC2, EMR, Machine Learning,Redshift)   Data Visualization: Tableau, D3.js   Machine Learning: Supervised (Classification: KNN, Neural Net, SVM, GBM, XGBoost), Unsupervised(Clustering: K-means, Graph, Dimensionality Reduction: PCA, SVD, Recommender Systems- Frequent Itemset Mining: Apriori, FP Growth)   Deep Learning: TensorFlow, Keras   Statistical Modeling: R, MATLAB, SAS, SPSS   Business Intelligence: Qlikview, Cloud Analytics   Database Administration: Microsoft SQL Server   Database Development   Data Architecture, Data Modeling  Most part of my  has involved in understanding the Linear Algebra, Probability, and Statistics involved in Machine Learning and implementing those Machine Learning algorithms from scratch.    I like designing, developing, deploying, and managing Machine Learning models in the AWS environment.    I would love to  as a Data Scientist, Machine Learning Engineer, Data Analyst roles.     Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Database Developer and Data Analyst IU SCHOOL OF MEDICINE - Indianapolis, IN December 2017 to December 2018 Database Developer & Administrator: phpMyAdmin, MySQL   Built a web application for IU School of Nursing to provide services for nursing centers and patients across Indiana State.   Designed, Developed, Administered, and Maintained a database for storing, retrieving, updating the data at the back-end. (Prototype won grant money of $30,000). Business Data Analyst Intern ProGen Business Solutions April 2016 to October 2016 Business Intelligence Developer: Tableau, R, Python, SQL   Assisted in getting the insights from sales data and recognized the patterns associated with the customer transactions.   Prepared Financial and Business Sales Reports   Created sales data visualizations using Tableau Dashboards.   Improved product sales from 70% to 85%. Data Analyst Intern TakenMind May 2019 · Performed EDA, Data Cleaning, and Statistical Analysis in Python.  · Delivered a prediction analysis using supervised machine learning algorithms (Decision Tree, Random Forest, SVM).    Master of Science in Computer & Information Science (Data Science Track) Purdue University - Indianapolis, IN January 2017 to December 2018 Bachelor's in Computer Science and Engineering Jawaharlal Nehru Technological University June 2011 to June 2015   AWS (3 years), Data Analytics (3 years), Detail Oriented, SQL, Excel, Business Intelligence, Machine Learning (2 years), Python (3 years), R (3 years), MySQL (4 years), Database Management (4 years), Data Mining (2 years), Data Modeling (4 years), Linear Algebra (5 years), Statistical Analysis (3 years), Tableau (3 years), Microsoft SQL Server (3 years), Object Oriented Programming (3 years), Data Science (3 years) Links  Certifications/Licenses  Artificial Intelligence Foundations: Machine Learning January 2019 to Present Learning Cloud Data Storage January 2019 to Present SQL for Data Science August 2017 to Present Database Modeling and Design August 2017 to Present Kaggle Python Tutorial on Machine Learning February 2019 to Present Groups  Data Science Indy January 2017 to Present Student Group Leader for Data Science Indy Meetup. Additional Information  Available Immediately. Open for Relocation. Authorized to  for any employer in the US."
"
Experienced data analyst and machine learning engineer in manufacturing and customer services. Looking for Intern/Full-time Data Scientist/NLP specialist positions. Proficient in Machine learning, NLP, Python, R, SQL, TensorFlow and Tableau. 
 
 

Programming: Python (Pandas, Numpy), R (dplyr, ggplot2), SQL (MySQL, PostgreSQL), MongoDB, C++, Java 
Frame: pySpark, Tensorflow, Keras, Sk-learn, Tableau, PowerPoint, Excel, Kafka, Ariflow 
Modeling: Regression Analysis, Hypothesis Test, Clustering Analysis, Predictive Modeling, Time Serie Analysis Platform: AWS (EC2, S3, Redshift, EMR), Linux, Git, Hadoop, Spark (SparkSQL, MLlib, Hive), Docker 
 
 

Georgetown University                                                                                                                        Washington, D.C.    M.S. in Mathematics and Statistics                                                                                                   Aug. 2017 - May 2019 
Shanghai Jiaotong University                                                                                                                  Shanghai, China       
M.S. in Material Science and Engineering                                                                                       Sep. 2013 - Mar. 2016 
Southeast University                                                                                                                                   Nanjing, China        
B.S. in Material Science and Engineering                                                                                        Aug. 2009 - Jun. 2013 
Relevant Courses: Reinforcement Learning, C++, Advanced Statistical Computing, Numeric Optimization, Time Series Analysis, Matrix Theory, Social Net Analysis, Probability Theory 
 
 EXPERIENCE 

Georgetown University, McDonough School of Business                                                                  Washington, D.C.   
Business Research Assistant                                                                                                           Nov. 2018  May 2019             
 Led a team of 5 constructing business risk database providing augmented analysis for thousands of companies 
 Implemented automated ETL pipelines with Python and MongoDB, saving 65% time spent on data processing 
 Formulated test-driven international business risk management using text mining and machine learning toolkits  
 
Pingan                                                                                                                                   Shenzhen, China 
Machine Learning Intern-NLP                                                                                                       May 2018 - Sep. 2018         
 Structured business use cases and redesigned databases that increased system recall rate by 36% 
 Designed feedback analysis system using unsupervised learning, leading to significant reduction in  capital 
 Reduced over 30% operating cost by designing actionable metrics and personalized solutions 
 Enhanced customer satisfaction rate by solving 92% of requests applying deep learning conversation system  
 
Ford Motor Co., Ltd                                                                                                                                   Nanjing, China             
Marketing Data Analyst Intern                                                                                                      May 2015 - Aug. 2015             
 Managed research on 4 Ford C-type cars, discovering 2 leading factors that contributed to 34% negative reviews 
 Constructed dashboards using Excel and Tableau to provide insights for 12 production problems 
 Presented reports on sales performance, leading improvement in 3 key components 
 
Shanghai Jiaotong University, State Key Laboratory                                                                           Shanghai, China 
Material Science Research Associate                                                                                              Jun. 2014  Sep. 2016 
 Designed computation driven experiments that motivated 2 manufacturing innovations 
 Overcame 3 key techniques based on the test-driven analysis, process simulations, and data visualizations 
 Co-authored 2 top research papers and 2 patents on theoretical and quantitative analysis of new techniques 
 
 	 

Credit Card Fraud Detection                                                                                                            July. 2019  Present 
 Process 50 million transaction records in Spark and Cassandra distributed environment  
 Constructed real-time fraud monitoring infrastructures with Spark SQL, MLlib and Kafka  
 
Algorithmic Trading Bot                                                                                                                Mar. 2019 - May 2019 
 Created self-trainable trading agents using deep reinforcement learning architectures with TensorFlow 
 Achieved 5% net revenue over a 10-day trading period in a simulated stock market ",Data Scientist,resume," Experienced data analyst and machine learning engineer in manufacturing and customer services. Looking for Intern/Full-time Data Scientist/NLP specialist positions. Proficient in Machine learning, NLP, Python, R, SQL, TensorFlow and Tableau.       Programming: Python (Pandas, Numpy), R (dplyr, ggplot2), SQL (MySQL, PostgreSQL), MongoDB, C++, Java  Frame: pySpark, Tensorflow, Keras, Sk-learn, Tableau, PowerPoint, Excel, Kafka, Ariflow  Modeling: Regression Analysis, Hypothesis Test, Clustering Analysis, Predictive Modeling, Time Serie Analysis Platform: AWS (EC2, S3, Redshift, EMR), Linux, Git, Hadoop, Spark (SparkSQL, MLlib, Hive), Docker       Georgetown University                                                                                                                        Washington, D.C.    M.S. in Mathematics and Statistics                                                                                                   Aug. 2017 - May 2019  Shanghai Jiaotong University                                                                                                                  Shanghai, China        M.S. in Material Science and Engineering                                                                                       Sep. 2013 - Mar. 2016  Southeast University                                                                                                                                   Nanjing, China         B.S. in Material Science and Engineering                                                                                        Aug. 2009 - Jun. 2013  Relevant Courses: Reinforcement Learning, C++, Advanced Statistical Computing, Numeric Optimization, Time Series Analysis, Matrix Theory, Social Net Analysis, Probability Theory     EXPERIENCE   Georgetown University, McDonough School of Business                                                                  Washington, D.C.    Business Research Assistant                                                                                                           Nov. 2018  May 2019               Led a team of 5 constructing business risk database providing augmented analysis for thousands of companies   Implemented automated ETL pipelines with Python and MongoDB, saving 65% time spent on data processing   Formulated test-driven international business risk management using text mining and machine learning toolkits     Pingan                                                                                                                                   Shenzhen, China  Machine Learning Intern-NLP                                                                                                       May 2018 - Sep. 2018           Structured business use cases and redesigned databases that increased system recall rate by 36%   Designed feedback analysis system using unsupervised learning, leading to significant reduction in  capital   Reduced over 30% operating cost by designing actionable metrics and personalized solutions   Enhanced customer satisfaction rate by solving 92% of requests applying deep learning conversation system     Ford Motor Co., Ltd                                                                                                                                   Nanjing, China              Marketing Data Analyst Intern                                                                                                      May 2015 - Aug. 2015               Managed research on 4 Ford C-type cars, discovering 2 leading factors that contributed to 34% negative reviews   Constructed dashboards using Excel and Tableau to provide insights for 12 production problems   Presented reports on sales performance, leading improvement in 3 key components    Shanghai Jiaotong University, State Key Laboratory                                                                           Shanghai, China  Material Science Research Associate                                                                                              Jun. 2014  Sep. 2016   Designed computation driven experiments that motivated 2 manufacturing innovations   Overcame 3 key techniques based on the test-driven analysis, process simulations, and data visualizations   Co-authored 2 top research papers and 2 patents on theoretical and quantitative analysis of new techniques     	   Credit Card Fraud Detection                                                                                                            July. 2019  Present   Process 50 million transaction records in Spark and Cassandra distributed environment    Constructed real-time fraud monitoring infrastructures with Spark SQL, MLlib and Kafka     Algorithmic Trading Bot                                                                                                                Mar. 2019 - May 2019   Created self-trainable trading agents using deep reinforcement learning architectures with TensorFlow   Achieved 5% net revenue over a 10-day trading period in a simulated stock market "
" Proficient in statistical or mathematical programming languages: Fortran, SAS, SQL, R, MATLAB,
GUASS, Python and Excel, Word, PowerPoint. 
 Experienced in economic/financial modeling and/or forecasting. 
 6+ years of experience performing advanced quantitative analyses. 
? Manipulating, analyzing and interpreting big data. (Data Mining & Analysis). 
? Defining problems, collecting data, establishing facts, and drawing valid conclusions. 
? Managing statistical . 
 3+ years experience of marketing analysis. 
 Able to  well under pressure and with limited resources 
 Able to effectively present information to top management, public groups, and/or boards of directors.
 
 Willing to  in cross-disciplinary teams, in a collaborative and transparent way.
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Statistician
Kelley Blue Book
May 2016 to January 2017
 Tracked, analyzed and interpreted trends in traffic and marketing data in order to provide relevantconclusions and recommendations to management and core teams. 
 Developed Upfront statistical models utilizing typical methodologies based on data available fromdifferent sources. 
 Updated data and models for statistical modeling purposes. 
 Developed KPI tracker report and driver analysis to track results against forecasts and re-specifymodels when required.
Research Assistant
Economic & Policy Analysis Research Center - Columbia, MO
August 2010 to March 2016
Doctoral Research: initiate, design models and study teachers' retirement decision under different retirement policies. Innovate novel efficient and robust regression methodologies in the intersection of applied Math and Econometrics. 
 Cleaned, maintained and organized Missouri public schools dataset with over millions of records inboth Windows and Unix systems using SAS and SQL. 
 Analyzed and reported dataset to center director. 
 Collected and categorized retirement policies for different States. 
 Built quantitative models independently including Multivariate Probit model, Dynamic model,Bayesian statistics, Hazard model, Monte Carlo simulation and Structural model using Fortran, R and SAS.
Teaching Assistant
University of Missouri-Columbia - Columbia, MO
August 2009 to June 2010
 Led discussion sessions, hold office hour and coached students course materials 
 Assisted instructors with preparing and proctoring exams 
 Graded home and exams
Financial Data Analyst
Thomson Reuters Ltd, Co - Beijing, CN
August 2008 to August 2009
 Collected corporate latest actions data of all listed companies and funds in Greater China region. 
 Took charge of training all new employees and document ing procedures. 
 Managed the resolution process for all external customer related issues via CRM system. 
 Led various , including meeting events investigation, China automation investigation and
China share buyback study etc., collaborating with Corporate Actions team. 
 Solved internal queries and communicate with various regional project managers.
Management Trainee & Market Executive
Double A Public Ltd, Co
July 2006 to April 2008
 Designed market, conducted surveys, and analyzed raw data and report. 
 Managed Media Chosen and TVC campaign in China Market and follow up WIP & Strategic meetings. 
 Controlled global ATL budget, expenses and making comparison.
Research Executive
TNS Market Research Ltd, Co - Beijing, CN
April 2006 to August 2006
 Conducted qualitative research  and analyzed raw data: 
? Service Satisfaction Survey for British Consultant 
 Conducted quantitative research  and analyzed raw data: 
? New products market research for Nokia 
? Market share research for HomeWorld Supermarket
Project Assistant
AC Nielsen Ltd - Beijing, CN
November 2004 to December 2004
Collected and analyzed relevant information of the research project on Chinese tobacco industry


Ph.D. in Economic
University of Missouri - Columbia, MO
2019
M.A. in Mathematics?ABD)
University of Missouri-Columbia - Columbia, MO 2019
M.A. in Economics
Renmin University of China - Beijing, CN June 2006
B.A. in Economics
Renmin University of China - Beijing, CN June 2004


Economics, Python, R, Machine Learning",Data Scientist,resume," Proficient in statistical or mathematical programming languages: Fortran, SAS, SQL, R, MATLAB, GUASS, Python and Excel, Word, PowerPoint.   Experienced in economic/financial modeling and/or forecasting.   6+ years of experience performing advanced quantitative analyses.  ? Manipulating, analyzing and interpreting big data. (Data Mining & Analysis).  ? Defining problems, collecting data, establishing facts, and drawing valid conclusions.  ? Managing statistical .   3+ years experience of marketing analysis.   Able to  well under pressure and with limited resources   Able to effectively present information to top management, public groups, and/or boards of directors.    Willing to  in cross-disciplinary teams, in a collaborative and transparent way. Willing to relocate: Anywhere Sponsorship required to  in the US  Experience  Statistician Kelley Blue Book May 2016 to January 2017  Tracked, analyzed and interpreted trends in traffic and marketing data in order to provide relevantconclusions and recommendations to management and core teams.   Developed Upfront statistical models utilizing typical methodologies based on data available fromdifferent sources.   Updated data and models for statistical modeling purposes.   Developed KPI tracker report and driver analysis to track results against forecasts and re-specifymodels when required. Research Assistant Economic & Policy Analysis Research Center - Columbia, MO August 2010 to March 2016 Doctoral Research: initiate, design models and study teachers' retirement decision under different retirement policies. Innovate novel efficient and robust regression methodologies in the intersection of applied Math and Econometrics.   Cleaned, maintained and organized Missouri public schools dataset with over millions of records inboth Windows and Unix systems using SAS and SQL.   Analyzed and reported dataset to center director.   Collected and categorized retirement policies for different States.   Built quantitative models independently including Multivariate Probit model, Dynamic model,Bayesian statistics, Hazard model, Monte Carlo simulation and Structural model using Fortran, R and SAS. Teaching Assistant University of Missouri-Columbia - Columbia, MO August 2009 to June 2010  Led discussion sessions, hold office hour and coached students course materials   Assisted instructors with preparing and proctoring exams   Graded home and exams Financial Data Analyst Thomson Reuters Ltd, Co - Beijing, CN August 2008 to August 2009  Collected corporate latest actions data of all listed companies and funds in Greater China region.   Took charge of training all new employees and document ing procedures.   Managed the resolution process for all external customer related issues via CRM system.   Led various , including meeting events investigation, China automation investigation and China share buyback study etc., collaborating with Corporate Actions team.   Solved internal queries and communicate with various regional project managers. Management Trainee & Market Executive Double A Public Ltd, Co July 2006 to April 2008  Designed market, conducted surveys, and analyzed raw data and report.   Managed Media Chosen and TVC campaign in China Market and follow up WIP & Strategic meetings.   Controlled global ATL budget, expenses and making comparison. Research Executive TNS Market Research Ltd, Co - Beijing, CN April 2006 to August 2006  Conducted qualitative research  and analyzed raw data:  ? Service Satisfaction Survey for British Consultant   Conducted quantitative research  and analyzed raw data:  ? New products market research for Nokia  ? Market share research for HomeWorld Supermarket Project Assistant AC Nielsen Ltd - Beijing, CN November 2004 to December 2004 Collected and analyzed relevant information of the research project on Chinese tobacco industry   Ph.D. in Economic University of Missouri - Columbia, MO 2019 M.A. in Mathematics?ABD) University of Missouri-Columbia - Columbia, MO 2019 M.A. in Economics Renmin University of China - Beijing, CN June 2006 B.A. in Economics Renmin University of China - Beijing, CN June 2004   Economics, Python, R, Machine Learning"
" 
 ?	 	 
	University of California, Berkeley 	 	                             8/2017-?	?8/2019 
Bachelor of Arts in Mathematics 
       Relevant Course: Linear Algebra, Ordinary Differential Equations, Real Analysis, Complex Analysis, Number  	Theory, Numerical Analysis, and Classical Geometries. 
	Santa Barbara City College 	                                                                                             ?	8/2013-5/2016?	 
Associate in Mathematics 
 
  Experience?	 	 
	Lecturer 	 ? Santa Barbara City College?	? ?Santa Barbara, CA                                               ?	7/2017-8/2017?	 
? Communicated complex ideas to a group of 30+ students. 
? Organized and planned lectures that required on-spot adjustments to accommodate the pace of student understanding.  
? Lectures included home questions, teaching new material, and engaging students in group . 
? Strong understanding of material in order to explain concepts clearly, effectively, and in different perspectives.  
 
	Math Tutor 	 ? Santa Barbara City College?	? ?Santa Barbara, CA                                          ?	1/2016-7/2017?	 
? Tutored 50+ diverse students in one-on-one, small group, and classroom sized settings. 
? Increased test scores by 20% 
? Sessions included home help, textbook or lecture note clarifications, a series of guided questions, encouragement of independent problem solving, and critical thinking. ? Subjects ranged from intermediate algebra to differential equations. 
 
	Sales Associate 	 ? In-N-Out Burger?	? ?Santa Barbara, CA                                                    ?	8/2012-8/2015?	 
? Provided exceptional customer service in a fast-paced  setting.  
? Required effective and clear communication between employees in a high demand environment. 
 
  Leadership?	 	    
	Vice President of Math Club 	 ? Santa Barbara City College?	? ?Santa Barbara, CA             ?	1/2016-7/2017?	  
? Coordinated a lecture series with STEM field s and math professors. 
? Talks entailed how math is related to different STEM fields, and math beyond the scope of community college classes. 
? Provided math tutoring to all members.  
 

  
?
?
?
?
MatLab 
Python 
Java 
MapleLab 
 
?
?
?
?
PowerPoint 
Excel 
Customer Service  
Public Speaking 
 
 ",Data Scientist,resume,"   ?	 	  	University of California, Berkeley 	 	                             8/2017-?	?8/2019  Bachelor of Arts in Mathematics         Relevant Course: Linear Algebra, Ordinary Differential Equations, Real Analysis, Complex Analysis, Number  	Theory, Numerical Analysis, and Classical Geometries.  	Santa Barbara City College 	                                                                                             ?	8/2013-5/2016?	  Associate in Mathematics      Experience?	 	  	Lecturer 	 ? Santa Barbara City College?	? ?Santa Barbara, CA                                               ?	7/2017-8/2017?	  ? Communicated complex ideas to a group of 30+ students.  ? Organized and planned lectures that required on-spot adjustments to accommodate the pace of student understanding.   ? Lectures included home questions, teaching new material, and engaging students in group .  ? Strong understanding of material in order to explain concepts clearly, effectively, and in different perspectives.     	Math Tutor 	 ? Santa Barbara City College?	? ?Santa Barbara, CA                                          ?	1/2016-7/2017?	  ? Tutored 50+ diverse students in one-on-one, small group, and classroom sized settings.  ? Increased test scores by 20%  ? Sessions included home help, textbook or lecture note clarifications, a series of guided questions, encouragement of independent problem solving, and critical thinking. ? Subjects ranged from intermediate algebra to differential equations.    	Sales Associate 	 ? In-N-Out Burger?	? ?Santa Barbara, CA                                                    ?	8/2012-8/2015?	  ? Provided exceptional customer service in a fast-paced  setting.   ? Required effective and clear communication between employees in a high demand environment.      Leadership?	 	     	Vice President of Math Club 	 ? Santa Barbara City College?	? ?Santa Barbara, CA             ?	1/2016-7/2017?	   ? Coordinated a lecture series with STEM field s and math professors.  ? Talks entailed how math is related to different STEM fields, and math beyond the scope of community college classes.  ? Provided math tutoring to all members.         ? ? ? ? MatLab  Python  Java  MapleLab    ? ? ? ? PowerPoint  Excel  Customer Service   Public Speaking     "
"	 
 
California State University - San 
Bernardino  
San Bernardino, CA    2019 
Bachelor of Science: Computer 
Engineering  
Achievements: 
 Deans list for 6 quarters 
  
Programming Languages:  
 Python, C++, C, Java 
 HTML, CSS, JavaScript, PHP, SQL 
 Parallel programming 
 Object oriented languages 
 Assembly language  
 Verilog 
 
Experience using:  
 LabVIEW  
 VIVADO  
 MATLAB  
 Visual Studios  
 Spyder  
 Linux and Windows OS  
 NetBeans 
  
 
 
  
 Bachelor's degree in Computer Engineering 
 One year of lab instructor and teacher assistant for computer architecture course at CSUSB 
 Bilingual: Spanish and English 
 Efficient with algorithm analysis, software testing and debugging  
 Exceptional at leading and ing in teams 
 Self-starter with great time management and organizational  
 Excellent oral and written communication  
 Strong  ethic and thrives on challenges 
 Proficient on Microsoft Word, Excel, and PowerPoint 
 
SCHOOL RELATED EXPERIENCE 
Senior Engineering Team  - Team Leader 
     CSUSB                                                          09/2018 to 06/2019  
Lead a team of six to complete two  on smart home appliances using the Arduino, Raspberry Pi, different types of sensors, 3D-printing and mobile applications.  consisted of a smart home door lock and smart blinds. Conducted research, analyzed data, created a proposal, and wrote the requirements and system specification papers. Completed the final products with a PowerPoint presentation and a live demo presented to the professor and classmates. 
 
 EXPERIENCE 
Home Depot Inc - Sales Associate  
Hemet, CA                                                       05/2014 - Current  
Management rewarded with three promotions and maximum raises during annual evaluations. Established recognitions for Employee of the month and Homer Awards for exceeding ing expectations. Able to quickly learn Home Depot's multiservice computer applications. Took on responsibilities such as delivering orders to appropriate locations, managed multiple customer's orders and able to meet customer's requests.  
 
CSUSB Computer Engineering and Science - Teacher 
Assistant/Lab Instructor  
San Bernardino, CA                                          09/2018 - 06/2019  Supported classroom activities: tutoring, reviewing for exams, and guiding students on their lab assignments. Effectively completed grading home assignments,  and labs on their respectful due dates. Assisted over 150 students, and 100% of students completed their labs and  successfully. ",Data Scientist,resume,"	    California State University - San  Bernardino   San Bernardino, CA    2019  Bachelor of Science: Computer  Engineering   Achievements:   Deans list for 6 quarters     Programming Languages:    Python, C++, C, Java   HTML, CSS, JavaScript, PHP, SQL   Parallel programming   Object oriented languages   Assembly language    Verilog    Experience using:    LabVIEW    VIVADO    MATLAB    Visual Studios    Spyder    Linux and Windows OS    NetBeans             Bachelor's degree in Computer Engineering   One year of lab instructor and teacher assistant for computer architecture course at CSUSB   Bilingual: Spanish and English   Efficient with algorithm analysis, software testing and debugging    Exceptional at leading and ing in teams   Self-starter with great time management and organizational    Excellent oral and written communication    Strong  ethic and thrives on challenges   Proficient on Microsoft Word, Excel, and PowerPoint    SCHOOL RELATED EXPERIENCE  Senior Engineering Team  - Team Leader       CSUSB                                                          09/2018 to 06/2019   Lead a team of six to complete two  on smart home appliances using the Arduino, Raspberry Pi, different types of sensors, 3D-printing and mobile applications.  consisted of a smart home door lock and smart blinds. Conducted research, analyzed data, created a proposal, and wrote the requirements and system specification papers. Completed the final products with a PowerPoint presentation and a live demo presented to the professor and classmates.     EXPERIENCE  Home Depot Inc - Sales Associate   Hemet, CA                                                       05/2014 - Current   Management rewarded with three promotions and maximum raises during annual evaluations. Established recognitions for Employee of the month and Homer Awards for exceeding ing expectations. Able to quickly learn Home Depot's multiservice computer applications. Took on responsibilities such as delivering orders to appropriate locations, managed multiple customer's orders and able to meet customer's requests.     CSUSB Computer Engineering and Science - Teacher  Assistant/Lab Instructor   San Bernardino, CA                                          09/2018 - 06/2019  Supported classroom activities: tutoring, reviewing for exams, and guiding students on their lab assignments. Effectively completed grading home assignments,  and labs on their respectful due dates. Assisted over 150 students, and 100% of students completed their labs and  successfully. "
" Strong expertise on Hadoop big data, SQL and HQL. ed on Kafka p.o.c for end-to-endimplementation to receive real time data flow from AWS server. 
 Experience in data visualization, data analytics, data integration, data quality using Python. Used
MATPLOT, DJANGO libraries in Python to show data visualization and trends. 
 Used NLTK libraries to detect the customer satisfaction through their verbatim comments.
(Tokenization and standardization are part of this process). 
 Querying on large data sets to process and refine the data for down streams. Automated the auditsystem to recognize any errors on fly (Null percentage and validity). 
 Expertise in RDBMS, Agile, Scrumand waterfall methodologies. 
 CSS, HTML and JavaScript along with good knowledge of backend  including Python.
Tableau expertise with 12 months ofexperience. 
 Strong understanding of project life cycle and Software Development Life Cycle (SDLC). WordPress,
MS office , CMS and multiple platforms Windows andMac. Strong command on MS Excelwith
VLOOKUP, MATCH and INDEXfunctions. 
 Good industry knowledge, analyticaland problem solving  and ability to  well with in ateam as well as an individual. Expertise in transforming business requirements into analytical models, designingalgorithms, buildingmodels, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data. 
 Hands on experience with big data  like Hadoop, Spark, Hive, Pig, Impala, Pyspark, SparkSql.  Good Knowledge in Proof of Concepts (PoC's), gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging. 
 Highly creative, innovative, committed, intellectually curious, business savvy with goodcommunication and interpersonal . 
 Experience in using various packages in Rand python like ggplot2, caret, dplyr, Rweka, gmodels,RCurl, tm, C50, twitteR, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplotlib, scikitlearn, Beautiful Soup, Rpy2. 
 Extensive experience in Data Visualization including producing tables, graphs, listings using variousprocedures and  such as Tableau.
Willing to relocate to: CA - WA - AZ,NV,OR
 Experience

Data Engineer (Telemetry)
Cisco - San Jose, CA
June 2017 to Present
Description: Identifying, gathering and analyzing complex multi-dimensional Telemetry datasets utilizing a variety of  while parallelly implementing them using SCRUM frame. 
 
Responsibilities: 
 As a Data engineer ed on huge raw data sets (Telemetry data). 
 Data classification and text analysis using NLTK libraries in Python. Collecting and compiling thevarious datasets from various data generating sources (AWS, MYSQL servers, Oracle Database).  Ingesting the data by creating tables in Hive and achieving high-level optimization of the ingested data by altering and refining through HQL. 
 Resolved compatibility issues by ing on different areas of data architecture including dataingestion and pipeline design. ed on Machine learning and advanced data processing to see the customer trends (scrutinize) in cumulative. 
 Developed and implemented scripts using machine-learning algorithms like Naïve Bayes Classifier
Algorithm, k means clustering, Random Forest classifier for regression, clustering etc. 
 Created many plots, bar charts, graphs, histograms of the complex data by visualizing the data usingmatplotlib in python. Maintain source code repository in subversion and handled branching, tagging & merging process. 
 Research, design and develop computer software systems, applications which require use ofadvanced computational and quantitative methodologies
Data Scientist
West Corporation - Omaha, NE
October 2015 to May 2017
Description: Lead in database maintenance and data visualization. Conducted end-to-end statistical data analysis and modeling, including querying, model building, forecasting, visualization, and implementation. 
 
Responsibilities: 
 Performed Data Profiling to learn about behavior with various features such as traffic pattern,location, time, Date and Time etc. Evaluated models using Cross Validation, Logloss function, ROC curves and used AUC for feature selection. 
 Detected the near-duplicated news by applying NLP methods(word2vec) and developing machinelearning models like label spreading, clustering. 
 Collected data needs and requirements by Interacting with the other departments. 
 Used Principal Component Analysis in feature engineering to analyze high dimensional data. 
 Used clustering technique K-Means to identify outliers and to classify unlabeled data. 
 Ensured that the model has low False Positive Rate. 
 Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behavior. 
 Used MLlib, Spark's Machine learning library to build and evaluate different models. 
 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Created various types of data visualizations using Python and Tableau. 
 Developed Map Reduce pipeline for feature extraction using Hive. 
 Application of various machine learning algorithms and statistical modeling like decision trees,regression models, neural nets, SVM, clustering to identify Volume using scikit-learn package in python, Matlab. 
 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python. 
 Implemented rule based expertise system from the results of exploratory analysis and informationgathered from the people from different departments. 
 Analyze traffic patterns by calculating auto correlation with different time lags. 
 Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1.  Performed Multi nomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route. 
 Communicated the results with operations team for taking best decisions. 
 
Environment: Impala, Linux, Spark, Tableau Desktop, Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, SQL Server 2012, Microsoft Excel, NLP
Data Scientist/Data Analyst
Assurant Specialty Property - Santa Ana, CA December 2014 to September 2015
Description: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property. Used python data structures for data sampling and validation. 
 
Responsibilities: 
 Developed Python scripts to automate data sampling process. Ensured the data integrity by checkingfor completeness, duplication, accuracy, and consistency 
 ed on model selection based on confusion matrices, minimized the TypeII error 
 Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented theresults for C-level decision makers 
 ed on data cleaning and reshaping, generatedsegmented subsets using Numpy and Pandas inPython 
 Continuously collected business requirements during the whole projectlifecycle. 
 Generated cost-benefit analysis to quantify the model implementation comparing with the formersituation. 
 Identified the variables that significantly affect the target 
 Applied various machine learning algorithms and statistical modeling like decision tree, logisticregression, GradientBoostingMachine to build predictive model using scikit-learn package in Python  Conducted model optimization and comparison using stepwise function based on AIC value 
 Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functionsto perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database 
 
Environment: Numpy, Pandas, Tableau 7, Python 2.6.8, Matplotlib, Oracle 10g, SQL,Scikit-Learn, MongoDB,
Data Architect/Data Modeler
Pitney Bowes Inc - Stamford, CT
November 2013 to November 2014
Description: PITNEY BOWES (PB) is the manufacturer of copiers, faxes and other office automation.
Pitney Bowes's operations are aligned under three lines of business, Global Mailing Systems (GMS) - It is the company's core mail automation and shipping business. Extracting data and making them refined for transfer and load while managing big data are some of my tasks. 
 
Responsibilities: 
 Develop Integrations jobs to transfer data from source system to Hadoop. 
 Installation of Talend Studio. 
 Technical design documents for Transformation processes. 
 Application of business rules on the data being transferred. 
 Task allocation for the ETL and Reporting team. 
 Communicate effectively with client and their internal development team to deliver productfunctionality requirements. 
 Architecting and design of data warehouse ETL processes. 
 Demo of POC built for the prospective customer and provide guidance and gather the feedback toback end ETL testing on SQL Server 2008 using SSIS. 
 Create Integration Jobs to backup a copy of data in net file system. 
 Design and implement the ETL Data model and create staging, source and Target tables in SQL  server database. 
 Gathering and analysis requirements definition meetings with business users and document meetingoutcomes. 
 
Environment: Hadoop, MS Office,Talend Studio, ETL, ODS, OLAP ,SQL Server 2008.
Data Analyst/Data Modeler
Nestle - IN
February 2011 to October 2013
Description: The Nestle is a Swiss transnational food and drink company. Building and maintaining the company website while parallelly visualizing the data using Tableau. 
 
Responsibilities: 
 Implemented a job which leads an electronic medical record, extract data into OracleDatabase andgenerate an output. Analyze the data and provide the insights about the customers using Tableau.  Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data. 
 Created dynamic linear models to perform trend analysis on customer transactional data in Python.  Increased pace & confidence of learning algorithm by combining state of the art technology and statistical methods. 
 Parseddata, producing concise conclusions from rawdata in a clean, well-structured and easilymaintainable format. Developed clustering models for customer segmentation using Python. 
 Developed entire frontend and backend modules using Python on Django Web Frame. 
 Implemented the presentation layer with HTML, CSS and JavaScript. 
 Involved in writing stored procedures using Oracle. 
 Optimized the database queries to improve the performance. 
 Designed and developed data management system using Oracle.. 
 
Environment: Python 2.x, Tableau, Oracle, MySQL 5.x, ORACLE, HTML5, CSS3, JavaScript, Shell, Linux & Windows, Django.
Data Analyst
Syntel - Pune, Maharashtra
September 2010 to January 2011
Description: My main role is to analyze data, check their functionalities and build the requirements with in the given time frame. 
 
Responsibilities: 
 Applied Business Objects best practices during development with a strong focus on reusability andbetter performance. 
 Developed and executed load scripts using Tera data client utilities MULTI LOAD, FAST LOAD andBTEQ. 
 Responsible for development and testing of conversion programs for importing Data from text filesinto map Oracle Database utilizing PERL shell scripts &SQL*Loader. 
 Developed Tableau visualizations and dashboards using Tableau Desktop. 
 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,graphical interface. 
 Formatting the data sets read into SAS by using Format statement in the data step as well as ProcFormat. 
 ed with the ETL team to document the Transformation Rules for Data Migration from OLTP to
Warehouse Environment for reporting purposes. 
 Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,graphical interface. 
 Co-ordinate with various business users, stakeholders and SME to get Functional expertise, designand business test scenarios review, UAT participation and validation of financial data. 
 Responsible for development and testing of conversion programs for importing Data from text filesinto map Oracle Database,utilizing,PERL,shell scripts&SQL*Loader. 
 
Environment: Business Objects, Oracle SQL Developer, PL/SQL, MS SQL Server, TOAD, Tableau, Informatica, SQL*PLUS, SQL*LOADER, XML.
Education

Bachelor's",Data Scientist,resume," Strong expertise on Hadoop big data, SQL and HQL. ed on Kafka p.o.c for end-to-endimplementation to receive real time data flow from AWS server.   Experience in data visualization, data analytics, data integration, data quality using Python. Used MATPLOT, DJANGO libraries in Python to show data visualization and trends.   Used NLTK libraries to detect the customer satisfaction through their verbatim comments. (Tokenization and standardization are part of this process).   Querying on large data sets to process and refine the data for down streams. Automated the auditsystem to recognize any errors on fly (Null percentage and validity).   Expertise in RDBMS, Agile, Scrumand waterfall methodologies.   CSS, HTML and JavaScript along with good knowledge of backend  including Python. Tableau expertise with 12 months ofexperience.   Strong understanding of project life cycle and Software Development Life Cycle (SDLC). WordPress, MS office , CMS and multiple platforms Windows andMac. Strong command on MS Excelwith VLOOKUP, MATCH and INDEXfunctions.   Good industry knowledge, analyticaland problem solving  and ability to  well with in ateam as well as an individual. Expertise in transforming business requirements into analytical models, designingalgorithms, buildingmodels, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.   Hands on experience with big data  like Hadoop, Spark, Hive, Pig, Impala, Pyspark, SparkSql.  Good Knowledge in Proof of Concepts (PoC's), gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.   Highly creative, innovative, committed, intellectually curious, business savvy with goodcommunication and interpersonal .   Experience in using various packages in Rand python like ggplot2, caret, dplyr, Rweka, gmodels,RCurl, tm, C50, twitteR, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplotlib, scikitlearn, Beautiful Soup, Rpy2.   Extensive experience in Data Visualization including producing tables, graphs, listings using variousprocedures and  such as Tableau. Willing to relocate to: CA - WA - AZ,NV,OR  Experience  Data Engineer (Telemetry) Cisco - San Jose, CA June 2017 to Present Description: Identifying, gathering and analyzing complex multi-dimensional Telemetry datasets utilizing a variety of  while parallelly implementing them using SCRUM frame.    Responsibilities:   As a Data engineer ed on huge raw data sets (Telemetry data).   Data classification and text analysis using NLTK libraries in Python. Collecting and compiling thevarious datasets from various data generating sources (AWS, MYSQL servers, Oracle Database).  Ingesting the data by creating tables in Hive and achieving high-level optimization of the ingested data by altering and refining through HQL.   Resolved compatibility issues by ing on different areas of data architecture including dataingestion and pipeline design. ed on Machine learning and advanced data processing to see the customer trends (scrutinize) in cumulative.   Developed and implemented scripts using machine-learning algorithms like Naïve Bayes Classifier Algorithm, k means clustering, Random Forest classifier for regression, clustering etc.   Created many plots, bar charts, graphs, histograms of the complex data by visualizing the data usingmatplotlib in python. Maintain source code repository in subversion and handled branching, tagging & merging process.   Research, design and develop computer software systems, applications which require use ofadvanced computational and quantitative methodologies Data Scientist West Corporation - Omaha, NE October 2015 to May 2017 Description: Lead in database maintenance and data visualization. Conducted end-to-end statistical data analysis and modeling, including querying, model building, forecasting, visualization, and implementation.    Responsibilities:   Performed Data Profiling to learn about behavior with various features such as traffic pattern,location, time, Date and Time etc. Evaluated models using Cross Validation, Logloss function, ROC curves and used AUC for feature selection.   Detected the near-duplicated news by applying NLP methods(word2vec) and developing machinelearning models like label spreading, clustering.   Collected data needs and requirements by Interacting with the other departments.   Used Principal Component Analysis in feature engineering to analyze high dimensional data.   Used clustering technique K-Means to identify outliers and to classify unlabeled data.   Ensured that the model has low False Positive Rate.   Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behavior.   Used MLlib, Spark's Machine learning library to build and evaluate different models.   Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Created various types of data visualizations using Python and Tableau.   Developed Map Reduce pipeline for feature extraction using Hive.   Application of various machine learning algorithms and statistical modeling like decision trees,regression models, neural nets, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.   Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database.   Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python.   Implemented rule based expertise system from the results of exploratory analysis and informationgathered from the people from different departments.   Analyze traffic patterns by calculating auto correlation with different time lags.   Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1.  Performed Multi nomial Logistic Regression, Random forest, Decision Tree, SVM to classify package is going to deliver on time for the new route.   Communicated the results with operations team for taking best decisions.    Environment: Impala, Linux, Spark, Tableau Desktop, Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, SQL Server 2012, Microsoft Excel, NLP Data Scientist/Data Analyst Assurant Specialty Property - Santa Ana, CA December 2014 to September 2015 Description: Assurant partners with leaders in mortgage lending, manufactured housing, multifamily housing and other industries to protect client and consumer property. Used python data structures for data sampling and validation.    Responsibilities:   Developed Python scripts to automate data sampling process. Ensured the data integrity by checkingfor completeness, duplication, accuracy, and consistency   ed on model selection based on confusion matrices, minimized the TypeII error   Generated data analysis reports using Matplotlib, Tableau, successfully delivered and presented theresults for C-level decision makers   ed on data cleaning and reshaping, generatedsegmented subsets using Numpy and Pandas inPython   Continuously collected business requirements during the whole projectlifecycle.   Generated cost-benefit analysis to quantify the model implementation comparing with the formersituation.   Identified the variables that significantly affect the target   Applied various machine learning algorithms and statistical modeling like decision tree, logisticregression, GradientBoostingMachine to build predictive model using scikit-learn package in Python  Conducted model optimization and comparison using stepwise function based on AIC value   Wrote and optimized complex SQL queries involving multiple joins and advanced analytical functionsto perform data extraction and merging from large volumes of historical data stored in Oracle 11g, validating the ETL processed data in target database    Environment: Numpy, Pandas, Tableau 7, Python 2.6.8, Matplotlib, Oracle 10g, SQL,Scikit-Learn, MongoDB, Data Architect/Data Modeler Pitney Bowes Inc - Stamford, CT November 2013 to November 2014 Description: PITNEY BOWES (PB) is the manufacturer of copiers, faxes and other office automation. Pitney Bowes's operations are aligned under three lines of business, Global Mailing Systems (GMS) - It is the company's core mail automation and shipping business. Extracting data and making them refined for transfer and load while managing big data are some of my tasks.    Responsibilities:   Develop Integrations jobs to transfer data from source system to Hadoop.   Installation of Talend Studio.   Technical design documents for Transformation processes.   Application of business rules on the data being transferred.   Task allocation for the ETL and Reporting team.   Communicate effectively with client and their internal development team to deliver productfunctionality requirements.   Architecting and design of data warehouse ETL processes.   Demo of POC built for the prospective customer and provide guidance and gather the feedback toback end ETL testing on SQL Server 2008 using SSIS.   Create Integration Jobs to backup a copy of data in net file system.   Design and implement the ETL Data model and create staging, source and Target tables in SQL  server database.   Gathering and analysis requirements definition meetings with business users and document meetingoutcomes.    Environment: Hadoop, MS Office,Talend Studio, ETL, ODS, OLAP ,SQL Server 2008. Data Analyst/Data Modeler Nestle - IN February 2011 to October 2013 Description: The Nestle is a Swiss transnational food and drink company. Building and maintaining the company website while parallelly visualizing the data using Tableau.    Responsibilities:   Implemented a job which leads an electronic medical record, extract data into OracleDatabase andgenerate an output. Analyze the data and provide the insights about the customers using Tableau.  Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data.   Created dynamic linear models to perform trend analysis on customer transactional data in Python.  Increased pace & confidence of learning algorithm by combining state of the art technology and statistical methods.   Parseddata, producing concise conclusions from rawdata in a clean, well-structured and easilymaintainable format. Developed clustering models for customer segmentation using Python.   Developed entire frontend and backend modules using Python on Django Web Frame.   Implemented the presentation layer with HTML, CSS and JavaScript.   Involved in writing stored procedures using Oracle.   Optimized the database queries to improve the performance.   Designed and developed data management system using Oracle..    Environment: Python 2.x, Tableau, Oracle, MySQL 5.x, ORACLE, HTML5, CSS3, JavaScript, Shell, Linux & Windows, Django. Data Analyst Syntel - Pune, Maharashtra September 2010 to January 2011 Description: My main role is to analyze data, check their functionalities and build the requirements with in the given time frame.    Responsibilities:   Applied Business Objects best practices during development with a strong focus on reusability andbetter performance.   Developed and executed load scripts using Tera data client utilities MULTI LOAD, FAST LOAD andBTEQ.   Responsible for development and testing of conversion programs for importing Data from text filesinto map Oracle Database utilizing PERL shell scripts &SQL*Loader.   Developed Tableau visualizations and dashboards using Tableau Desktop.   Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,graphical interface.   Formatting the data sets read into SAS by using Format statement in the data step as well as ProcFormat.   ed with the ETL team to document the Transformation Rules for Data Migration from OLTP to Warehouse Environment for reporting purposes.   Used Graphical Entity-Relationship Diagramming to create new database design via easy to use,graphical interface.   Co-ordinate with various business users, stakeholders and SME to get Functional expertise, designand business test scenarios review, UAT participation and validation of financial data.   Responsible for development and testing of conversion programs for importing Data from text filesinto map Oracle Database,utilizing,PERL,shell scripts&SQL*Loader.    Environment: Business Objects, Oracle SQL Developer, PL/SQL, MS SQL Server, TOAD, Tableau, Informatica, SQL*PLUS, SQL*LOADER, XML. Education  Bachelor's"
"
* 8+ years of neuroscience research experience in which I lead and collaborated in the creative design and execution of stroke research aims, resulting in 30+ publications, oral presentations, and $1 million+ in NIH grant funding
* Passionate about the critical thinking, problem solving, and data analysis aspects of my academic experience and am transitioning into a data science industry career in which those attributes are more predominantly utilized
* Self-taught R and Python programming languages, IDEs such as RStudio, Jupyter, and Spyder, and database queries through Hadoop and SQL. Created my own website in which I independently analyzed publicly available datasets and posted results. Achieving industry data science credentials, including the Dell EMC Proven   Data Science Associate certification

 and Certifications
Dell EMC Proven 
Data Science Associate	June 2019

Loma Linda University School of Medicine; Loma Linda, CA
Doctor of Philosophy in Physiology	July 2012  June 2016
Dissertation: Mechanisms of Post-hemorrhagic Hydrocephalus Development after Germinal Matrix Hemorrhage

University of California Berkeley; Berkeley, CA
Bachelor of Arts Double Major: Molecular and Cell Biology; Economics	August 2006  May 2010



R, RStudio, Microsoft R
Python, Jupyter, Spyder, Komodo, Visual Studio Code
SQL, Microsoft Azure
Hadoop, Hive, Impala
Machine Learning
Data Analytics
Project management
Time management
Written communication
Oral communication
Critical thinking, decision-making, and problem solving
Grant writing
Business acumen
Leadership 
Mentorship and training
Gathering and interpreting data 
Rodent models of stroke
Aseptic rodent surgeries
Rodent neurobehavioral assessments
Histology
Immunohistochemistry
Diolistic staining
Golgi staining
Brightfield microscopy
Fluorescence microscopy
Confocal microscopy 
Stereology
Spectrophotometric assays
ELISA
Western blot
RT-qPCR
Zymography
Cell culturing
Flow cytometry
ImageJ
SigmaPlot / SigmaStat
GraphPad
Neurolucida 360
NeuronStudio
Microsoft Office  Word, Excel, PowerPoint
Bioinformatics
Statistics
Predictive Modeling
Website Design, WordPress

Memberships and Associations
* 
Cheeky Scientist Association
Society for Neuroscience
Golden Key International Honors Society
American Physiological Society
Tau Kappa Epsilon International Fraternity
California Golden Bears Alumni Association
* 


Albert Einstein College of Medicine, Bronx, NY
Postdoctoral Research Fellow							February 2017  June 2018
* Managed two separate  that investigated hippocampal neurogenesis and dendrite spine development in premature rabbits (delivered via C-section) as well as premature rabbits with germinal matrix hemorrhage
* Quantified hippocampal dendrite spines from 430+ histological z-stack images of Golgi-stained coronal brain sections, taken by brightfield microscopy and analyzed using Neurolucida 360. Also quantified hippocampal dendrite spines from 540+ histological z-stack images of diolistically stained coronal brain sections, taken by confocal microscopy and analyzed using NeuronStudio
* Stereologically quantified hippocampal dendrite branching and dendritic spines in 36+ histological brain sections. Also stereologically quantified immuno-stained markers of neurogenesis in 240+ histological coronal brain sections
* Performed BLAST searches and other bioinformatics search techniques to find antibodies specific for target sequences that were significantly homologous with rabbits 
MilliporeSigma / Randstad, Temecula, CA
Data Associate								November 2016  February 2017
* Data mined and calculated product formulations for 3000+ products with fellow EHSM team members
* Intensively searched databases, SOPs, manufacturing records, and other relevant resources to compile formulation data
Loma Linda University School of Medicine, Loma Linda, CA
Laboratory Manager, Pre-doctoral Student, and Visiting Student Researcher	July 2011  June 2016
* Lead the germinal matrix hemorrhage / intracerebral hemorrhage research groups and executed the specific research aims in our NIH R01 grant for germinal matrix hemorrhage, resulting in 5 publications related to the project. Communicated our findings in annual progress reports for the NIH
* Wrote, edited, and submitted two NIH R01 grants and one R21 grant. Wrote, edited, and submitted 30+ research / review manuscripts for publication. Peer-reviewed and evaluated 10+ grants and 30+ manuscripts
* Mentored and trained 10+ new lab members (a mix of students, post-docs, and neurosurgeons) in our hemorrhagic stroke models and basic laboratory techniques. Assisted with the design and implementation of their independent studies
* Performed over 500 stereotaxic rodent surgeries using aseptic technique to model glioblastoma multiforme, intracerebral hemorrhage, and germinal matrix hemorrhage, then assessed rodent neurofunctional deficits utilizing a battery of evaluations
Uber | Lyft | DoorDash | Postmates 
Independent Contractor	July 2016  November 2016 | July 2018 - Present
* Using the company smartphone app, picked up customers and/or their food orders and dropped off at their given delivery address

",Data Scientist,resume," * 8+ years of neuroscience research experience in which I lead and collaborated in the creative design and execution of stroke research aims, resulting in 30+ publications, oral presentations, and $1 million+ in NIH grant funding * Passionate about the critical thinking, problem solving, and data analysis aspects of my academic experience and am transitioning into a data science industry career in which those attributes are more predominantly utilized * Self-taught R and Python programming languages, IDEs such as RStudio, Jupyter, and Spyder, and database queries through Hadoop and SQL. Created my own website in which I independently analyzed publicly available datasets and posted results. Achieving industry data science credentials, including the Dell EMC Proven   Data Science Associate certification   and Certifications Dell EMC Proven  Data Science Associate	June 2019  Loma Linda University School of Medicine; Loma Linda, CA Doctor of Philosophy in Physiology	July 2012  June 2016 Dissertation: Mechanisms of Post-hemorrhagic Hydrocephalus Development after Germinal Matrix Hemorrhage  University of California Berkeley; Berkeley, CA Bachelor of Arts Double Major: Molecular and Cell Biology; Economics	August 2006  May 2010    R, RStudio, Microsoft R Python, Jupyter, Spyder, Komodo, Visual Studio Code SQL, Microsoft Azure Hadoop, Hive, Impala Machine Learning Data Analytics Project management Time management Written communication Oral communication Critical thinking, decision-making, and problem solving Grant writing Business acumen Leadership  Mentorship and training Gathering and interpreting data  Rodent models of stroke Aseptic rodent surgeries Rodent neurobehavioral assessments Histology Immunohistochemistry Diolistic staining Golgi staining Brightfield microscopy Fluorescence microscopy Confocal microscopy  Stereology Spectrophotometric assays ELISA Western blot RT-qPCR Zymography Cell culturing Flow cytometry ImageJ SigmaPlot / SigmaStat GraphPad Neurolucida 360 NeuronStudio Microsoft Office  Word, Excel, PowerPoint Bioinformatics Statistics Predictive Modeling Website Design, WordPress  Memberships and Associations *  Cheeky Scientist Association Society for Neuroscience Golden Key International Honors Society American Physiological Society Tau Kappa Epsilon International Fraternity California Golden Bears Alumni Association *    Albert Einstein College of Medicine, Bronx, NY Postdoctoral Research Fellow							February 2017  June 2018 * Managed two separate  that investigated hippocampal neurogenesis and dendrite spine development in premature rabbits (delivered via C-section) as well as premature rabbits with germinal matrix hemorrhage * Quantified hippocampal dendrite spines from 430+ histological z-stack images of Golgi-stained coronal brain sections, taken by brightfield microscopy and analyzed using Neurolucida 360. Also quantified hippocampal dendrite spines from 540+ histological z-stack images of diolistically stained coronal brain sections, taken by confocal microscopy and analyzed using NeuronStudio * Stereologically quantified hippocampal dendrite branching and dendritic spines in 36+ histological brain sections. Also stereologically quantified immuno-stained markers of neurogenesis in 240+ histological coronal brain sections * Performed BLAST searches and other bioinformatics search techniques to find antibodies specific for target sequences that were significantly homologous with rabbits  MilliporeSigma / Randstad, Temecula, CA Data Associate								November 2016  February 2017 * Data mined and calculated product formulations for 3000+ products with fellow EHSM team members * Intensively searched databases, SOPs, manufacturing records, and other relevant resources to compile formulation data Loma Linda University School of Medicine, Loma Linda, CA Laboratory Manager, Pre-doctoral Student, and Visiting Student Researcher	July 2011  June 2016 * Lead the germinal matrix hemorrhage / intracerebral hemorrhage research groups and executed the specific research aims in our NIH R01 grant for germinal matrix hemorrhage, resulting in 5 publications related to the project. Communicated our findings in annual progress reports for the NIH * Wrote, edited, and submitted two NIH R01 grants and one R21 grant. Wrote, edited, and submitted 30+ research / review manuscripts for publication. Peer-reviewed and evaluated 10+ grants and 30+ manuscripts * Mentored and trained 10+ new lab members (a mix of students, post-docs, and neurosurgeons) in our hemorrhagic stroke models and basic laboratory techniques. Assisted with the design and implementation of their independent studies * Performed over 500 stereotaxic rodent surgeries using aseptic technique to model glioblastoma multiforme, intracerebral hemorrhage, and germinal matrix hemorrhage, then assessed rodent neurofunctional deficits utilizing a battery of evaluations Uber | Lyft | DoorDash | Postmates  Independent Contractor	July 2016  November 2016 | July 2018 - Present * Using the company smartphone app, picked up customers and/or their food orders and dropped off at their given delivery address  "
" 
Full Stack LOCATION 	Los Angeles, CA 
Most used programming languages, frames, 
: 
   Amazon.com  	                        LOS ANGELES, CA                                                     
   Full-stack Developer                                                    Jun.2018  Jun.2019 ES6-7 JavaScript, Python, jQuery, React JS, Angular JS, Node 
JS, Vue JS, Redux, Router, Babel, Gulp, Webpack 
 Developed the Amazon affiliated referral program using ES6, React,  Most used UX/ UI  & testing : 

 	Angular, Node JS, Express, Routers, running real-time analysis 
 Specified user interface, system-level interfaces, deployment and  	neting/ security architecture, and RDBMS data modeling 
 Designed and implemented Restful APIs and GraphQL APIs by  using Express, Apollo as the client side 
 Automated test and deployment to AWS with AWS Cloudformation, 
 	EC2, S3, RDS, DynamoDB, Jenkins, and Ansible 
	Bank of America HQ 	 LOS ANGELES, CA  
	Full-stack Developer 	                     Jun.2017  May.2018 
 Programmed automation analyzation of public firms financial statements, debt repayment leverage (DSO & FCC), interpreted sensitivity analysis and asset investment project forecasts by using DCF and statistics correlation analysis 
 Built risks adjusted investment  for individuals with net worth more than 500k focused on the fixed income market (specifically treasury bonds, corporate bonds, mutual funds, and ETFs) 
 Performed equity research and conducted industry outlook through Bloomberg Terminal, WSJ and Seeking Alpha 
 Developed 3 marketing modules for asset portfolio accrual,  targeting money in motion, for 3 million dollars 
	SALEM PARTNERS, LLC 	NEW YORK, NY 
	Full-stack Developer Intern 	Feb.2017  May.2017?
 Programmed a tool to identify prospects for M&A opportunities and  the evaluation of strategic alternatives in preparation for  introductory presentations with investors and CEOs 
 Learned to build full-integrated model system for projection model,  DCF, LBO model, accretion/ dilution merger models, including  ability to run operational and capital structure sensitives within  models and data tables 
 Participated in 3 live transactions and 7 prospective deals across  the consumer, industrial, and real estate industries; including due  diligence sessions the drafting of confidential information 
HTML5, CSS3, SASS, LESS, Material Design, Flexbox, Grid 
System, Bootstrap4, Vuetify, Unit Test, Integration Test, Enzyme, Jasmine, Jest, Mocha  
Most used APIs, database and backend: 
RESTFUL APIs, Axios, GraphQL APIs, Apollo, PostgressQL 
Docker, Express, Mongo DB, MySQL, PHP, Laravel, AWS EC2 
Methodologies: Agile, Scrum, Kanban 

University of Southern California  
Viterbi School of Engineer  Computer Science 
Major GPA in Computer Science: 3.6 
LOS ANGELES, CA                          Aug.2013  May.2017 
Completed full-time web development intensive course + received in-depth training and feedback from  developers + built  using full-stack development  in fast paced environment
Interests
, computer algorithms, visual design, machine learning, sociology, photography",Data Scientist,resume,"  Full Stack LOCATION 	Los Angeles, CA  Most used programming languages, frames,  :     Amazon.com  	                        LOS ANGELES, CA                                                         Full-stack Developer                                                    Jun.2018  Jun.2019 ES6-7 JavaScript, Python, jQuery, React JS, Angular JS, Node  JS, Vue JS, Redux, Router, Babel, Gulp, Webpack   Developed the Amazon affiliated referral program using ES6, React,  Most used UX/ UI  & testing :    	Angular, Node JS, Express, Routers, running real-time analysis   Specified user interface, system-level interfaces, deployment and  	neting/ security architecture, and RDBMS data modeling   Designed and implemented Restful APIs and GraphQL APIs by  using Express, Apollo as the client side   Automated test and deployment to AWS with AWS Cloudformation,   	EC2, S3, RDS, DynamoDB, Jenkins, and Ansible  	Bank of America HQ 	 LOS ANGELES, CA   	Full-stack Developer 	                     Jun.2017  May.2018   Programmed automation analyzation of public firms financial statements, debt repayment leverage (DSO & FCC), interpreted sensitivity analysis and asset investment project forecasts by using DCF and statistics correlation analysis   Built risks adjusted investment  for individuals with net worth more than 500k focused on the fixed income market (specifically treasury bonds, corporate bonds, mutual funds, and ETFs)   Performed equity research and conducted industry outlook through Bloomberg Terminal, WSJ and Seeking Alpha   Developed 3 marketing modules for asset portfolio accrual,  targeting money in motion, for 3 million dollars  	SALEM PARTNERS, LLC 	NEW YORK, NY  	Full-stack Developer Intern 	Feb.2017  May.2017?  Programmed a tool to identify prospects for M&A opportunities and  the evaluation of strategic alternatives in preparation for  introductory presentations with investors and CEOs   Learned to build full-integrated model system for projection model,  DCF, LBO model, accretion/ dilution merger models, including  ability to run operational and capital structure sensitives within  models and data tables   Participated in 3 live transactions and 7 prospective deals across  the consumer, industrial, and real estate industries; including due  diligence sessions the drafting of confidential information  HTML5, CSS3, SASS, LESS, Material Design, Flexbox, Grid  System, Bootstrap4, Vuetify, Unit Test, Integration Test, Enzyme, Jasmine, Jest, Mocha   Most used APIs, database and backend:  RESTFUL APIs, Axios, GraphQL APIs, Apollo, PostgressQL  Docker, Express, Mongo DB, MySQL, PHP, Laravel, AWS EC2  Methodologies: Agile, Scrum, Kanban   University of Southern California   Viterbi School of Engineer  Computer Science  Major GPA in Computer Science: 3.6  LOS ANGELES, CA                          Aug.2013  May.2017  Completed full-time web development intensive course + received in-depth training and feedback from  developers + built  using full-stack development  in fast paced environment Interests , computer algorithms, visual design, machine learning, sociology, photography"
"? Around 4 years of experience in IT and Computer Science as Data Scientist with strong expertise, business experience, and communication  to drive high-impact business outcomes through data-driven innovations and decisions. 
? Experience with Statistical Analysis, Data Mining and Machine Learning  using Python and SQL.
 
? Data Driven and highly analytical with ing knowledge and statistical model approaches andmethodologies (Clustering, Regression analysis, Hypothesis testing, Decision trees, Machine learning), rules and ever evolving regulatory environment. 
? Good familiarity in entire Data Science project life cycle, including Data Acquisition, Data Cleansing,
Data Manipulation, Feature Engineering, Modeling, Evaluation, Optimization, Testing and Deployment. 
? Experience in using Statistical procedures and Machine Learning algorithms such as ANOVA,
Clustering, Regression and Time Series Analysis to analyze data for further Model Building. ? Experience in problem solving, data science, Machine learning, statistical inference, predictive analytics, descriptive analytics, prescriptive analytics, graph analysis, natural language processing, and computational linguistics; with extensive experience in predictive analytics and recommendation. 
? Hands on experience on clustering algorithms like K-means, Medoids clustering and Predictive andDescriptive algorithms. 
? Proficient knowledge on Mathematical Matrix Operations, Statistics, Linear Algebra, Probability,
Differentiation, Integration and Geometry. 
? Expertise in Model Development, Data Mining, Predictive Modeling, Descriptive Modeling Data
Visualization, Data Clearing and Management, and Database Management. 
? Experience using machine learning models such as random forest, KNN, SVM, logistic regression andused packages such as ggplot, dplyr, lm, rpart, Random Forest, nnet, NumPy, sci-kit learn, pandas, etc., in python. 
? Logistic Regression, SVM, Clustering, neural nets, Principal Component Analysis and goodknowledge on Recommender Systems. 
? Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees,
Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics,
Segmentation methodologies, Regression-based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
? ed and extracted data from various database sources like SQL Server, BigQuery, and SQLite. ? Proficient in advising on the use of data for compiling personnel and statistical reports and preparing personnel action documents, patterns within data, analyzing data and interpreting results. 
? Good knowledge on Deep Learning concepts like Multi-Layer Perceptron, Deep Neural Nets,
Artificial Neural Nets, Convolutional Neural Nets, Recurrent Neural Nets. 
? Hands on experience on Deep Learning Techniques such as Back Propagation, Choosing Activation
Functions, Weight Initialization based on Optimizer, Avoiding Vanishing Gradient and Exploding
Gradient Problems, Using Dropout, Regularization and Batch Normalization, Gradient Monitoring and
Clipping Padding and Striding, Max pooling, LSTM. 
? Experience in using Optimization Techniques like Gradient Descent, Stochastic Gradient Descent,Adam, RMS prop. 
? Experience in building models with Deep Learning frames like Tensor Flow and Keras. 
? Actively involved in all phases of data science project life cycle including Data Extraction, Data
Cleaning, Data Visualization and building Models. 
? Extensive hands-on experience and high proficiency in writing complex SQL queries like storedprocedures, triggers, joins and subqueries along with that used MongoDB for extraction data. ? Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, MapReduce concepts, and ecosystems including Hive and Pig. 
? Experience with data visualization using  like GGplot, Matplotlib, Seaborn, Tableau and using
Tableau software to publish and presenting dashboards, storyline on web and desktop platforms. ? Well experienced in Normalization & De-Normalization techniques for optimum performance in relational and dimensional database environments. 
? Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
? Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing
RDBMS specific features. 
? Expertise in all aspects of Software Development Lifecycle (SDLC) from requirement analysis,Design, Development Coding, Testing, Implementation, and Maintenance. 
 
 AND : 
Languages Java, Python (NumPy, SciPy, Pandas, Genism, Keras, Seaborn) Scala, JavaScript, SQL, HTML, CSS, Kotlin, Perl. 
Databases MySQL, MongoDB, SQLite and big data. 
Data Mining and 
Dev  
 
MS Excel, Tableau, SQL Server, IDLE, Jupyter Notebook, Django, 
Flask, PyCharm, Visual Studio Code, Keras, Tensor Flow, Postman, Docker. 
 
Big Data  Hadoop, PySpark, Hive, HDFS, MapReduce, Pig, Kafka. 
 and Utilities Microsoft Office, Tableau, Microsoft Excel, Tensorboard, Matplotlib, NLTK, Numpy,
Pandas, scikit-learn, Node.js. 
Machine Learning 
Linear Regression, Logistic Regression, Gradient boosting, Random 
Forests, Maximum likelihood estimation, Clustering, Classification & 
Association Rules, K-Nearest Neighbors (KNN), K-Means Clustering, Decision 
Tree (CART & CHAID), Neural Nets, Principal Component Analysis, 
Weight of Evidence (WOE) and Information Value (IV), Factor Analysis, 
Sampling Design, Time Series Analysis, ARIMA, ARMA, GARCH, Market Basket Analysis, Text mining. 
 
Deep Learning 
Artificial Neural Nets, Convolutional Neural Nets, ResNet50, 
MobileNet, Recurrent Neural Nets, LSTM, Natural Language Processing, 
Transfer Learning, Embeddings, Residual Nets, Fine Tuning, GRU, 
SoftMax Classifier, Back Propagation, Choosing Activation Functions, 
Drop out, Optimization Algorithms, Vanishing and Exploding Gradient, 
Striding, Padding, Optimized weight Initializations, Gradient Monitoring and Clipping, Batch Normalization, Max Pooling. 
 
Cloud : Amazon Web Services (EC2, EBS, S3, VPC, RDS, SES), Google Cloud (App Engine,
Compute Engine, AI Platform). 
Methodologies and Version Control Agile, Scrum, Git.
 Experience

Data Scientist
Rosedale Federal - Rosedale, MD
September 2017 to Present
Description: Rosedale Federal Savings and Loan Association operates as a full-service bank. I  there as Machine Learning Engineer. I ed on a project to classify customer complaints and queries using Natural Language Processing and Deep Learning. I utilized various machine learning/statistical algorithms to identify the correlation between different features and utilized  like TFID and SHAP to visualize the relation between customer queries and responses. 
 
Responsibilities: 
? ed on huge data sets and developed predictive models with machine learning techniques 
? Used Natural Language Processing to perform complaint classification and labeling ? Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression,
Naive Bayes, Random Forests, Decision Trees, K-means, & KNN 
? Evaluated models using Cross validation, Log loss function used to measure the performance andused ROC curves and AUC for feature selection. 
? Used Spark's Machine learning library to build and evaluate different models. 
? Used LSTM and RNN models to detect phishing websites using Keras and TensorFlow. 
? Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by usingL1 and L2 Regularization 
? Trained machine learning models for fraud detection using customer transaction data. 
? Visualized data using different visualization libraries such as bokeh, seaborn and matplotlib 
 
Environment: Jupyter Notebook, Postman, Flask, Python, AWS, Visual Studio, Gitlab, Anaconda.
Data Scientist
InSure - Towson, MD
January 2016 to August 2017
Description: The INSuRE (Information Security Research and ) project aims to build research  and experience for graduate students through a research net between CAE-Rs (Centers of Academic Excellence in Research) in Information Assurance/Cyber Defense. Through the project, students engage in interdisciplinary, distributed-team research on tasks in the national information security domain. The students learn research by doing, building , expertise, and connections that will enable them to hit the ground running faster on information assurance research projects later in their careers. 
 
Responsibilities: 
? Developed Python modules, machine learning & predictive analytics for day to day businessactivities 
? Analyzed large data sets apply machine learning techniques and develop predictive models,statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques 
? Conducting studies, rapid plots and using advanced data mining and statistical modeling techniquesto build a solution that optimizes the quality and performance of data 
? ed with parameter tuning and model evaluation techniques Confusion Matrix, Cross validation,AUC-ROC etc. Customer Profiling models using K-means and K-means++ clustering algorithms to enable targeted marketing 
? Implemented dimensionality reduction using Principal Component Analysis and k-fold crossvalidation as part of Model Improvement 
? Used LSTM and RNN models to detect malicious websites using Pandas, Keras and TensorFlow. ? Perform Exploratory analysis, hypothesis testing, cluster analysis, correlation, ANOVA, ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms, Text Analytics & Time Series Forecasting 
? Deployed the trained and optimized model to AWS EC2 instance using Flask production server to beserved as an API for other services. 
? Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, metadata solution and data lifecycle management in both RDBMS, Big Data environments 
? Generated dashboards using Tableau to visualize the data and provide solutions to the existingproblems as well as insights to improve customer experience and organizational performance 
? ed with numerous data visualization  in python like matplotlib, seaborn, ggplot. 
 
Environments: Python, PyCharm, Jupyter Notebook, Spyder, Tableau, MySQL, Ubuntu server, Visual Studio Code.
Data Analyst and Machine Learning
BigScale Tech - Surat, Gujarat June 2014 to November 2014
Description: BigScale Tech is leading software company based in Surat, Gujarat, India. Specializing in custom software development in many different fields including Data Analysis, custom Software Development and Database Storage. I ed with the data analysis team at the company on several projects with the clients. 
 
Responsibilities: 
? Collected, Cleaned and Analyzed the data from different government websites and private fertilizercompanies to be modeled. 
? Designed, built, and implemented relational databases. 
? ed on huge data sets and developed predictive models with machine learning techniques. 
? ed with various regression algorithms like Random Forest Regression, Decision Tree regression,
XGBoost regression, etc. to forecast fertilizer requirement in rural farming. 
? Communicated effectively in both a verbal and written manner to client team. 
? Successfully interpreted data to draw conclusions for managerial action and strategy. 
? Completed documentation on all assigned systems and databases, including business rules, logic,and processes. 
? Maintained the data integrity during extraction, manipulation, processing, analysis and storage. 
? Dedicated time in keeping the code clean and organized while having the proper documentation inplace by logging all activity. 
? Constructed efficient data infrastructures that are easy to maintain and can be used effectively.Seamlessly spotting and resolving any issue within the infrastructure 
 
Environments: Python, Notepad ++, MySQL database, Ubuntu, Anaconda.",Data Scientist,resume,"? Around 4 years of experience in IT and Computer Science as Data Scientist with strong expertise, business experience, and communication  to drive high-impact business outcomes through data-driven innovations and decisions.  ? Experience with Statistical Analysis, Data Mining and Machine Learning  using Python and SQL.   ? Data Driven and highly analytical with ing knowledge and statistical model approaches andmethodologies (Clustering, Regression analysis, Hypothesis testing, Decision trees, Machine learning), rules and ever evolving regulatory environment.  ? Good familiarity in entire Data Science project life cycle, including Data Acquisition, Data Cleansing, Data Manipulation, Feature Engineering, Modeling, Evaluation, Optimization, Testing and Deployment.  ? Experience in using Statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regression and Time Series Analysis to analyze data for further Model Building. ? Experience in problem solving, data science, Machine learning, statistical inference, predictive analytics, descriptive analytics, prescriptive analytics, graph analysis, natural language processing, and computational linguistics; with extensive experience in predictive analytics and recommendation.  ? Hands on experience on clustering algorithms like K-means, Medoids clustering and Predictive andDescriptive algorithms.  ? Proficient knowledge on Mathematical Matrix Operations, Statistics, Linear Algebra, Probability, Differentiation, Integration and Geometry.  ? Expertise in Model Development, Data Mining, Predictive Modeling, Descriptive Modeling Data Visualization, Data Clearing and Management, and Database Management.  ? Experience using machine learning models such as random forest, KNN, SVM, logistic regression andused packages such as ggplot, dplyr, lm, rpart, Random Forest, nnet, NumPy, sci-kit learn, pandas, etc., in python.  ? Logistic Regression, SVM, Clustering, neural nets, Principal Component Analysis and goodknowledge on Recommender Systems.  ? Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.  ? ed and extracted data from various database sources like SQL Server, BigQuery, and SQLite. ? Proficient in advising on the use of data for compiling personnel and statistical reports and preparing personnel action documents, patterns within data, analyzing data and interpreting results.  ? Good knowledge on Deep Learning concepts like Multi-Layer Perceptron, Deep Neural Nets, Artificial Neural Nets, Convolutional Neural Nets, Recurrent Neural Nets.  ? Hands on experience on Deep Learning Techniques such as Back Propagation, Choosing Activation Functions, Weight Initialization based on Optimizer, Avoiding Vanishing Gradient and Exploding Gradient Problems, Using Dropout, Regularization and Batch Normalization, Gradient Monitoring and Clipping Padding and Striding, Max pooling, LSTM.  ? Experience in using Optimization Techniques like Gradient Descent, Stochastic Gradient Descent,Adam, RMS prop.  ? Experience in building models with Deep Learning frames like Tensor Flow and Keras.  ? Actively involved in all phases of data science project life cycle including Data Extraction, Data Cleaning, Data Visualization and building Models.  ? Extensive hands-on experience and high proficiency in writing complex SQL queries like storedprocedures, triggers, joins and subqueries along with that used MongoDB for extraction data. ? Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, MapReduce concepts, and ecosystems including Hive and Pig.  ? Experience with data visualization using  like GGplot, Matplotlib, Seaborn, Tableau and using Tableau software to publish and presenting dashboards, storyline on web and desktop platforms. ? Well experienced in Normalization & De-Normalization techniques for optimum performance in relational and dimensional database environments.  ? Regularly accessing JIRA tool and other internal issue trackers for the Project development.  ? Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features.  ? Expertise in all aspects of Software Development Lifecycle (SDLC) from requirement analysis,Design, Development Coding, Testing, Implementation, and Maintenance.     AND :  Languages Java, Python (NumPy, SciPy, Pandas, Genism, Keras, Seaborn) Scala, JavaScript, SQL, HTML, CSS, Kotlin, Perl.  Databases MySQL, MongoDB, SQLite and big data.  Data Mining and  Dev     MS Excel, Tableau, SQL Server, IDLE, Jupyter Notebook, Django,  Flask, PyCharm, Visual Studio Code, Keras, Tensor Flow, Postman, Docker.    Big Data  Hadoop, PySpark, Hive, HDFS, MapReduce, Pig, Kafka.   and Utilities Microsoft Office, Tableau, Microsoft Excel, Tensorboard, Matplotlib, NLTK, Numpy, Pandas, scikit-learn, Node.js.  Machine Learning  Linear Regression, Logistic Regression, Gradient boosting, Random  Forests, Maximum likelihood estimation, Clustering, Classification &  Association Rules, K-Nearest Neighbors (KNN), K-Means Clustering, Decision  Tree (CART & CHAID), Neural Nets, Principal Component Analysis,  Weight of Evidence (WOE) and Information Value (IV), Factor Analysis,  Sampling Design, Time Series Analysis, ARIMA, ARMA, GARCH, Market Basket Analysis, Text mining.    Deep Learning  Artificial Neural Nets, Convolutional Neural Nets, ResNet50,  MobileNet, Recurrent Neural Nets, LSTM, Natural Language Processing,  Transfer Learning, Embeddings, Residual Nets, Fine Tuning, GRU,  SoftMax Classifier, Back Propagation, Choosing Activation Functions,  Drop out, Optimization Algorithms, Vanishing and Exploding Gradient,  Striding, Padding, Optimized weight Initializations, Gradient Monitoring and Clipping, Batch Normalization, Max Pooling.    Cloud : Amazon Web Services (EC2, EBS, S3, VPC, RDS, SES), Google Cloud (App Engine, Compute Engine, AI Platform).  Methodologies and Version Control Agile, Scrum, Git.  Experience  Data Scientist Rosedale Federal - Rosedale, MD September 2017 to Present Description: Rosedale Federal Savings and Loan Association operates as a full-service bank. I  there as Machine Learning Engineer. I ed on a project to classify customer complaints and queries using Natural Language Processing and Deep Learning. I utilized various machine learning/statistical algorithms to identify the correlation between different features and utilized  like TFID and SHAP to visualize the relation between customer queries and responses.    Responsibilities:  ? ed on huge data sets and developed predictive models with machine learning techniques  ? Used Natural Language Processing to perform complaint classification and labeling ? Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN  ? Evaluated models using Cross validation, Log loss function used to measure the performance andused ROC curves and AUC for feature selection.  ? Used Spark's Machine learning library to build and evaluate different models.  ? Used LSTM and RNN models to detect phishing websites using Keras and TensorFlow.  ? Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by usingL1 and L2 Regularization  ? Trained machine learning models for fraud detection using customer transaction data.  ? Visualized data using different visualization libraries such as bokeh, seaborn and matplotlib    Environment: Jupyter Notebook, Postman, Flask, Python, AWS, Visual Studio, Gitlab, Anaconda. Data Scientist InSure - Towson, MD January 2016 to August 2017 Description: The INSuRE (Information Security Research and ) project aims to build research  and experience for graduate students through a research net between CAE-Rs (Centers of Academic Excellence in Research) in Information Assurance/Cyber Defense. Through the project, students engage in interdisciplinary, distributed-team research on tasks in the national information security domain. The students learn research by doing, building , expertise, and connections that will enable them to hit the ground running faster on information assurance research projects later in their careers.    Responsibilities:  ? Developed Python modules, machine learning & predictive analytics for day to day businessactivities  ? Analyzed large data sets apply machine learning techniques and develop predictive models,statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques  ? Conducting studies, rapid plots and using advanced data mining and statistical modeling techniquesto build a solution that optimizes the quality and performance of data  ? ed with parameter tuning and model evaluation techniques Confusion Matrix, Cross validation,AUC-ROC etc. Customer Profiling models using K-means and K-means++ clustering algorithms to enable targeted marketing  ? Implemented dimensionality reduction using Principal Component Analysis and k-fold crossvalidation as part of Model Improvement  ? Used LSTM and RNN models to detect malicious websites using Pandas, Keras and TensorFlow. ? Perform Exploratory analysis, hypothesis testing, cluster analysis, correlation, ANOVA, ROC Curve and build models in Supervised and Unsupervised Machine Learning algorithms, Text Analytics & Time Series Forecasting  ? Deployed the trained and optimized model to AWS EC2 instance using Flask production server to beserved as an API for other services.  ? Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, metadata solution and data lifecycle management in both RDBMS, Big Data environments  ? Generated dashboards using Tableau to visualize the data and provide solutions to the existingproblems as well as insights to improve customer experience and organizational performance  ? ed with numerous data visualization  in python like matplotlib, seaborn, ggplot.    Environments: Python, PyCharm, Jupyter Notebook, Spyder, Tableau, MySQL, Ubuntu server, Visual Studio Code. Data Analyst and Machine Learning BigScale Tech - Surat, Gujarat June 2014 to November 2014 Description: BigScale Tech is leading software company based in Surat, Gujarat, India. Specializing in custom software development in many different fields including Data Analysis, custom Software Development and Database Storage. I ed with the data analysis team at the company on several projects with the clients.    Responsibilities:  ? Collected, Cleaned and Analyzed the data from different government websites and private fertilizercompanies to be modeled.  ? Designed, built, and implemented relational databases.  ? ed on huge data sets and developed predictive models with machine learning techniques.  ? ed with various regression algorithms like Random Forest Regression, Decision Tree regression, XGBoost regression, etc. to forecast fertilizer requirement in rural farming.  ? Communicated effectively in both a verbal and written manner to client team.  ? Successfully interpreted data to draw conclusions for managerial action and strategy.  ? Completed documentation on all assigned systems and databases, including business rules, logic,and processes.  ? Maintained the data integrity during extraction, manipulation, processing, analysis and storage.  ? Dedicated time in keeping the code clean and organized while having the proper documentation inplace by logging all activity.  ? Constructed efficient data infrastructures that are easy to maintain and can be used effectively.Seamlessly spotting and resolving any issue within the infrastructure    Environments: Python, Notepad ++, MySQL database, Ubuntu, Anaconda."
"
Health Services Research and Evaluation Intern
IEHP, Inland Empire - Rancho Cucamonga, CA
November 2018 to Present
 Conducted literature review on health disparities, social determinants of health, evaluation of health programs 
 interpreted statistical results for Health Evaluation presentations at American Public Health
Association (APHA) 2018 conference 
 Investigation into maternal health and potential programs that may help disadvantaged, low-incomepregnant women 
 Took part in research meetings with Health Services Research Director, analysts, evaluators, andprofessors to talk about survey data usage, data analysis
Resident Conference Assistant
UCR- Housing - Riverside, CA
June 2014 to June 2016
* ed on organizational  for the planning of present and future conferences for over threethousand residents 
* Maintained accurate conference files and documents for accurate billing information, conducted keyinventory and file audits 
* Performed general office duties, distributed mail, maintained conference files and documents foraccurate billing information, conducted key inventory and file audits 
* Provided administrative support to Conference Group Directors, RSO Administrators, Dining Services,
Custodial, Maintenance and Residence Life Staff
Resident Services Assistant
UCR- Housing - Riverside, CA
August 2013 to June 2016
* Led university housing tours on the biggest UCR welcoming events and throughout the year. * Ensured that UCR Housing Protocol was being followed by residents, wrote out incident reports, identified and reported maintenance issues on property. 
* Conducted monthly audits, room inventories, inspection reports, answered phones, and created andposted critical notices. 
* Monitored and maintained the residential computer labs and aided with general inventory stocking.


MPH in Epidemiology
Loma Linda University - Loma Linda, CA
September 2017 to June 2019
B.S. in Biology
University of California - Riverside, CA June 2016


SAS (1 year), R (1 year), Receptionist (3 years), SPSS (Less than 1 year), Microsoft Access (Less than 1 year), Microsoft Excel (4 years), Microsoft Word (10+ years)
Awards

Dean's Honors List
December 2016
Dean's Honors List June 2016
Groups

Loma Linda School of Public Health Association
September 2017 to Present
Member
LLU Writing Center Pilot Study Group
August 2018 to Present
Assist in writing center services by critiquing podcasts, seminars, and other  that may be provided to all students
Additional Information

Typing speed: 71",Data Scientist,resume," Health Services Research and Evaluation Intern IEHP, Inland Empire - Rancho Cucamonga, CA November 2018 to Present  Conducted literature review on health disparities, social determinants of health, evaluation of health programs   interpreted statistical results for Health Evaluation presentations at American Public Health Association (APHA) 2018 conference   Investigation into maternal health and potential programs that may help disadvantaged, low-incomepregnant women   Took part in research meetings with Health Services Research Director, analysts, evaluators, andprofessors to talk about survey data usage, data analysis Resident Conference Assistant UCR- Housing - Riverside, CA June 2014 to June 2016 * ed on organizational  for the planning of present and future conferences for over threethousand residents  * Maintained accurate conference files and documents for accurate billing information, conducted keyinventory and file audits  * Performed general office duties, distributed mail, maintained conference files and documents foraccurate billing information, conducted key inventory and file audits  * Provided administrative support to Conference Group Directors, RSO Administrators, Dining Services, Custodial, Maintenance and Residence Life Staff Resident Services Assistant UCR- Housing - Riverside, CA August 2013 to June 2016 * Led university housing tours on the biggest UCR welcoming events and throughout the year. * Ensured that UCR Housing Protocol was being followed by residents, wrote out incident reports, identified and reported maintenance issues on property.  * Conducted monthly audits, room inventories, inspection reports, answered phones, and created andposted critical notices.  * Monitored and maintained the residential computer labs and aided with general inventory stocking.   MPH in Epidemiology Loma Linda University - Loma Linda, CA September 2017 to June 2019 B.S. in Biology University of California - Riverside, CA June 2016   SAS (1 year), R (1 year), Receptionist (3 years), SPSS (Less than 1 year), Microsoft Access (Less than 1 year), Microsoft Excel (4 years), Microsoft Word (10+ years) Awards  Dean's Honors List December 2016 Dean's Honors List June 2016 Groups  Loma Linda School of Public Health Association September 2017 to Present Member LLU Writing Center Pilot Study Group August 2018 to Present Assist in writing center services by critiquing podcasts, seminars, and other  that may be provided to all students Additional Information  Typing speed: 71"
"Medical and entrepreneurial services are what I can provide. Additional information upon request. Extensive credentials.
Willing to relocate to: Beaumont, CA - Apple Valley, CA - Fontana, CA
Authorized to  in the US for any employer
 Experience

Anesthesia Technician
Loma Linda University Medical Center
2019 to Present
Primary responsibility is to assist the licensed anesthesia provider in clinical settings where anesthesia is administered at all sites to include MRI/CT/GI Labs/Cath Labs/Interventional 
Radiology. Prepared appropriate instruments, equipment and supplies for the administration of each age-specific type of anesthetic procedure. Can demonstrate practical knowledge and expertise in all general areas of anesthesia (general anesthesia, procedure-related sedation, and regional techniques). Assists with operating room turnover and patient flow. Achieves and maintains a trusting and credible ing relationship with anesthesiologists, nurse anesthetists and operating room administration and staff. Performed other duties as needed.
Anesthesia Technician
Victor Valley Global Medical Center 2014 to 2018
Assisting the Anesthesiologist with all indicated procedures. Assist the circulating nurse with all tasks intra-op within my scope of practice. Complete daily anesthesia machine check-outs and calibration. Assist intra-op with non-critical and critical monitor and/or anesthesia machine errors or faults. Reconciled incoming supplies and materials. Set-up and turn over all anesthesia supply carts for incoming cases. Maintain expired and non-expired materials. Stock all Operating Rooms fully with anesthesia related items before, during and at the end of the  day. Properly sterilized and packaged all anesthesia equipment after every case. Recorded temperature logs for fluids and warm blankets. Help others to adhere to policies and procedures. Complete all assigned tasks efficiently and courteously.
Partner/General Manager
Black Rose Marketing and Design - Apple Valley, CA
January 2016 to January 2017
Partner and Sales Executive of Black Rose Marketing and Design. Facilitated in acquiring new clients, provide face-to-face or video call consulting and customer-service. Established online presence across all clientele via. Social Media advertising and marketing, SEO, Web Page Optimization, Google Analytics, etc.
Squad Leader, Platoon Guide
US Army National Guard
2010 to 2015
Combat Medic / Health Care Specialist 2010 - 2015 Completed Basic Training with honors and early promotion PV1-PV2. Became Guide-on Bearer, Squad Leader, Platoon Guide. Completed Advanced Individual Training (Health Care Specialist) with honors.
Director
Marketing and Advertising
2013 to 2013
Answered phone calls, handled invoices and bills. Market and Advertised newly listed 
Residential and Commercial real estate. Syndicated  marketing materials to various websites. Listed real estate in local newspaper advertising. Prepped office for meetings, classes and events.
Finish (Trim) Plumber, Construction Apprentice
Hitt Plumbing Co
2006 to 2013
Answered phone calls, handled invoices and bills, reconciled company equipment and incoming materials. Prepped jobs for plumbing, demolition, construction debris clean up, delivered supplies to jobs. Assisted foreman with the current tasks of the day. Installed finish (trim) plumbing for new construction and/or remodeled homes and facilities to include commercial and residential.


BSN in nursing
Los Angeles Pacific University
2017 to Present
High school or equivalent
Granite Hills High School - Apple Valley, CA 2009


Patient care, Ultrasound, Excellent written, Written and verbal, Problem-solving, Team player, Excel, Microsoft word, Word, Publisher (2 years), Adobe (2 years)
Military Service

Branch: United States National Guard
Rank: SPC / PV4
Additional Information

 
Patient advocate. 
Well versed with Sonosite, Echo Machine and Ultrasound Guided procedures, such as regional blocks and line placements. Arterial Line setup and assist. 
CVP Line setup and assist. 
Swan Ganz setup and assist. 
HotLine setup proficient. 
Spinal Block assist proficient. 
Epidural assist proficient. 
Difficult Intubation proficient. 
Strong Computer  (Microsoft Word, Publisher, Excel / All Adobe Products). Proficient and balanced with medical treatment and patient care. 
Proficient and adapts well to trauma settings. 
Excellent written and verbal . 
A strong eye for detail. 
Productive in a high stress environment. 
Strong problem-solving . 
Driven individually, and as a team player. 
Can recognize personal weaknesses and strengths. 
Understands limitations established by and for corporate well-being.",Data Scientist,resume,"Medical and entrepreneurial services are what I can provide. Additional information upon request. Extensive credentials. Willing to relocate to: Beaumont, CA - Apple Valley, CA - Fontana, CA Authorized to  in the US for any employer  Experience  Anesthesia Technician Loma Linda University Medical Center 2019 to Present Primary responsibility is to assist the licensed anesthesia provider in clinical settings where anesthesia is administered at all sites to include MRI/CT/GI Labs/Cath Labs/Interventional  Radiology. Prepared appropriate instruments, equipment and supplies for the administration of each age-specific type of anesthetic procedure. Can demonstrate practical knowledge and expertise in all general areas of anesthesia (general anesthesia, procedure-related sedation, and regional techniques). Assists with operating room turnover and patient flow. Achieves and maintains a trusting and credible ing relationship with anesthesiologists, nurse anesthetists and operating room administration and staff. Performed other duties as needed. Anesthesia Technician Victor Valley Global Medical Center 2014 to 2018 Assisting the Anesthesiologist with all indicated procedures. Assist the circulating nurse with all tasks intra-op within my scope of practice. Complete daily anesthesia machine check-outs and calibration. Assist intra-op with non-critical and critical monitor and/or anesthesia machine errors or faults. Reconciled incoming supplies and materials. Set-up and turn over all anesthesia supply carts for incoming cases. Maintain expired and non-expired materials. Stock all Operating Rooms fully with anesthesia related items before, during and at the end of the  day. Properly sterilized and packaged all anesthesia equipment after every case. Recorded temperature logs for fluids and warm blankets. Help others to adhere to policies and procedures. Complete all assigned tasks efficiently and courteously. Partner/General Manager Black Rose Marketing and Design - Apple Valley, CA January 2016 to January 2017 Partner and Sales Executive of Black Rose Marketing and Design. Facilitated in acquiring new clients, provide face-to-face or video call consulting and customer-service. Established online presence across all clientele via. Social Media advertising and marketing, SEO, Web Page Optimization, Google Analytics, etc. Squad Leader, Platoon Guide US Army National Guard 2010 to 2015 Combat Medic / Health Care Specialist 2010 - 2015 Completed Basic Training with honors and early promotion PV1-PV2. Became Guide-on Bearer, Squad Leader, Platoon Guide. Completed Advanced Individual Training (Health Care Specialist) with honors. Director Marketing and Advertising 2013 to 2013 Answered phone calls, handled invoices and bills. Market and Advertised newly listed  Residential and Commercial real estate. Syndicated  marketing materials to various websites. Listed real estate in local newspaper advertising. Prepped office for meetings, classes and events. Finish (Trim) Plumber, Construction Apprentice Hitt Plumbing Co 2006 to 2013 Answered phone calls, handled invoices and bills, reconciled company equipment and incoming materials. Prepped jobs for plumbing, demolition, construction debris clean up, delivered supplies to jobs. Assisted foreman with the current tasks of the day. Installed finish (trim) plumbing for new construction and/or remodeled homes and facilities to include commercial and residential.   BSN in nursing Los Angeles Pacific University 2017 to Present High school or equivalent Granite Hills High School - Apple Valley, CA 2009   Patient care, Ultrasound, Excellent written, Written and verbal, Problem-solving, Team player, Excel, Microsoft word, Word, Publisher (2 years), Adobe (2 years) Military Service  Branch: United States National Guard Rank: SPC / PV4 Additional Information    Patient advocate.  Well versed with Sonosite, Echo Machine and Ultrasound Guided procedures, such as regional blocks and line placements. Arterial Line setup and assist.  CVP Line setup and assist.  Swan Ganz setup and assist.  HotLine setup proficient.  Spinal Block assist proficient.  Epidural assist proficient.  Difficult Intubation proficient.  Strong Computer  (Microsoft Word, Publisher, Excel / All Adobe Products). Proficient and balanced with medical treatment and patient care.  Proficient and adapts well to trauma settings.  Excellent written and verbal .  A strong eye for detail.  Productive in a high stress environment.  Strong problem-solving .  Driven individually, and as a team player.  Can recognize personal weaknesses and strengths.  Understands limitations established by and for corporate well-being."
" 
 
Personable Data Analyst with experience in ing with large data sets to provide custom reports and analytical support. Passionate about leveraging current  and learning new  to identify and contribute to business solutions. Highly interested in a parttime Data Scientist position to continue to grow Data Science et. Motivated to learn the necessary  to succeed at The Massage Negotiator. Experienced in leveraging SQL, Python and Excel for Data Analysis, Text Processing, Reporting and ETL processes. Maintains a high  ethic and is adept to ing independently or within a team setting to meet strict deadlines. Possesses excellent customer service and oral/written communication . 
  
Python 
SQL 
Excel / VBA 
Reporting 
Tableau 
SAS 
Java 
Experience 
Data Analyst  
ERI Economic Research Institute | Feb 2019  Present 
 Research and collect compensation data, conduct statistical analysis/modeling and communicate findings to senior team members. 
 Leverage Python, SQL and Excel to perform analysis for recurring & ad-hoc requests, maintain/clean databases and enhance current data management processes. 
 Perform text processing and analysis with python using Natural Language Processing and String Matching libraries.  
 Develop internal python GUIs and programs for web scraping and automation of ETL processes. 
 
Business Management Analyst  
Northrop Grumman | July 2018  Feb 2019 
 Perform and present analysis and prepare reports using Project Management software (SAP & MPM) and Excel to ensure that contracts are within negotiated and agreed-upon parameters. 
 Extract large amounts of data from databases to clean and transform data into presentable charts and reports. 
 Create Excel Macros using VBA to automate routine reports and streamline processes for analysis and reporting. 
 
Operations Data Analyst 
Irvine Company | Aug 2017  May 2018 
 Extensively utilized SQL Server, MS Query functions, vlookup, pivot tables, Tableau Server and SAP Business Objects Web Intelligence to provide analytical support. 
 Prepared and presented various quarterly, monthly, weekly and ad-hoc reports to executive management regarding Irvine Company Apartment data. 
 Prepared documentation and new user guide to assist in implementing new automated reports. 
 
Data Analyst (Part-time, Remote) 
Colibri Group | Oct 2018  Present 
 Extract data from various sources (Google Ads, Bing Ads, Google Analytics), transform in Excel and load data to be read into PowerBI and Tableau. 
 
Bachelor of Arts, Business Administration 
California State University, Fullerton | 2013  2017 
Emphasis: Information Systems 
 
Certification, Applied Data Science with Python University of Michigan | 2018 - 2019 
 
Computer Science Course 
Foothill College | 2019 
Courses: Intro to Programming  Python, Object Oriented 
Programming  Java, Object Oriented Programming  Python, 
Intermediate Software Development  Java,  
 ",Data Scientist,resume,"    Personable Data Analyst with experience in ing with large data sets to provide custom reports and analytical support. Passionate about leveraging current  and learning new  to identify and contribute to business solutions. Highly interested in a parttime Data Scientist position to continue to grow Data Science et. Motivated to learn the necessary  to succeed at The Massage Negotiator. Experienced in leveraging SQL, Python and Excel for Data Analysis, Text Processing, Reporting and ETL processes. Maintains a high  ethic and is adept to ing independently or within a team setting to meet strict deadlines. Possesses excellent customer service and oral/written communication .     Python  SQL  Excel / VBA  Reporting  Tableau  SAS  Java  Experience  Data Analyst   ERI Economic Research Institute | Feb 2019  Present   Research and collect compensation data, conduct statistical analysis/modeling and communicate findings to senior team members.   Leverage Python, SQL and Excel to perform analysis for recurring & ad-hoc requests, maintain/clean databases and enhance current data management processes.   Perform text processing and analysis with python using Natural Language Processing and String Matching libraries.    Develop internal python GUIs and programs for web scraping and automation of ETL processes.    Business Management Analyst   Northrop Grumman | July 2018  Feb 2019   Perform and present analysis and prepare reports using Project Management software (SAP & MPM) and Excel to ensure that contracts are within negotiated and agreed-upon parameters.   Extract large amounts of data from databases to clean and transform data into presentable charts and reports.   Create Excel Macros using VBA to automate routine reports and streamline processes for analysis and reporting.    Operations Data Analyst  Irvine Company | Aug 2017  May 2018   Extensively utilized SQL Server, MS Query functions, vlookup, pivot tables, Tableau Server and SAP Business Objects Web Intelligence to provide analytical support.   Prepared and presented various quarterly, monthly, weekly and ad-hoc reports to executive management regarding Irvine Company Apartment data.   Prepared documentation and new user guide to assist in implementing new automated reports.    Data Analyst (Part-time, Remote)  Colibri Group | Oct 2018  Present   Extract data from various sources (Google Ads, Bing Ads, Google Analytics), transform in Excel and load data to be read into PowerBI and Tableau.    Bachelor of Arts, Business Administration  California State University, Fullerton | 2013  2017  Emphasis: Information Systems    Certification, Applied Data Science with Python University of Michigan | 2018 - 2019    Computer Science Course  Foothill College | 2019  Courses: Intro to Programming  Python, Object Oriented  Programming  Java, Object Oriented Programming  Python,  Intermediate Software Development  Java,    "
"Significant experience turning data into easily accessible information to support decision making. Skilled in the use of open source  to manage data, conduct descriptive and inferential statistics, and present relationships between data in an intuitive graphical format. Concise, clear writer adept at explaining complex concepts to a general audience. Readily learn and adopt new . Tactfully engage end users to formulate answerable research questions that are important to them. Help users to plan and conduct data collections. 
 
Analysis, Visualization, Reporting 
flow Design and Prototyping 
Advanced Statistics and Modelling 
Budget and Personnel Management
Willing to relocate: Anywhere
 Experience

Associate Director, Institutional Research and Effectiveness
Harvey Mudd College
July 2018 to June 2019
Supported the continuous improvement efforts of the college by promoting the informed use of institutional data in evidence-based decision making. Managed entire student evaluation of teaching process from instrument design and creation, through data collection, analysis, and reporting of results for 250 courses per semester for both formative and summative purposes. 
? Wrote R program that tabulated survey results by student major (allowing for double and joint majors) to allow departments to focus on just the students they teach and improve their learning.
Staff to the Academic Deans Committee
The Claremont Colleges Academic Deans Committee - Claremont, CA
July 2012 to June 2019
Negotiated a new process with central business staff for the submission of intercollegiate budgets for presidential consideration to allow for more considered use of consortial resources. Had signature authority over their $100K off-budget expenditures and partnered with central business staff to move  onto the budget when possible to improve financial planning. 
? Wrote SQL queries to produce consortium-level views of registration data to inform the planning ofintercollegiate programs to optimize the use of shared resources. 
? Built searchable online archive of Committee minutes and documents to enhance institutionalmemory and learning.
Director, Academic Operations
Harvey Mudd College
July 2007 to June 2018
Researched causes of disparate academic performance and wrote a report used to demonstrate commitment to a culture of evidence in the reaffirmation of accreditation process. 
Supervised machine and electronic shops to promote safety and the prudent use of resources. ? Built and piloted online system for Student Evaluation of Teaching using Qualtrics to email students at a class time of the instructor's choosing to ensure high response rates and collect meaningful information to allow instructors to optimize courses for student learning. 
? Queried Student Information System to get data to analyze in order to study the effect of sex, ethnicity, and SES on student performance and wrote report that served as an appendix to the 2010 WASC self-study and motivated faculty to make courses more welcoming.
Associate Dean for Planning, IR and Assessment
Harvey Mudd College
July 2004 to June 2007
Oversaw reporting to governmental and accrediting agencies. Chaired college assessment committee. Co-chaired college diversity task force. Co-authored planning documents with the president of the college. Developed pilot portfolio assessment of student learning. 
? Built and deployed a LAMP server to collect, store, and manipulate administrative data resulting indecreased administrative overhead. 
? Collaborated with College President to study the financial impact of cross registration on theClaremont Consortium to inform strategic planning and staffing.


Doctor of Philosophy in 
Claremont Graduate University - Claremont, CA
Master of Arts in Philosophy
The Claremont Graduate School - Claremont, CA
Bachelor of Arts in Economics
Pitzer College - Claremont, CA


DATABASE, DBASE, SQL (10+ years), C++, GGPLOT2 (3 years), VBA, COGNOS (10+ years), LINUX (10+ years), SPSS (10+ years), STATISTICS (10+ years), VISUALIZATION, EXCEL (10+ years), PARADOX, Managerial, Budget Management (10+ years), R (9 years)
Links


Military Service

Branch: Army
Service Country: United States
Rank: Cadet
September 1980 to May 1985
Assessments

Data Analysis  Expert
July 2019
Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/yiec2z6gigfkth8f
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Publications

Leibniz: His Philosophy and His Calculi https://pdfs.semanticscholar.org/4e92/1c48fa35049d40f50a5cb2fd626d29dd9779.pdf March 1999
Additional Information

COMPUTER  
 
? Operating systems: OSX, WIN, and LINUX 
? Programming language: C++ 
? Database programs: dBase, Filemaker, Paradox, and SQL 
? Database query : Cognos, Impromptu 8, SQL 
? Statistics and visualization software: ggmap, ggplot2, knitr, Lattice, R, SPSS, TeX 
? Spreadsheets: Excel with VBA, Google Sheets",Data Scientist,resume,"Significant experience turning data into easily accessible information to support decision making. Skilled in the use of open source  to manage data, conduct descriptive and inferential statistics, and present relationships between data in an intuitive graphical format. Concise, clear writer adept at explaining complex concepts to a general audience. Readily learn and adopt new . Tactfully engage end users to formulate answerable research questions that are important to them. Help users to plan and conduct data collections.    Analysis, Visualization, Reporting  flow Design and Prototyping  Advanced Statistics and Modelling  Budget and Personnel Management Willing to relocate: Anywhere  Experience  Associate Director, Institutional Research and Effectiveness Harvey Mudd College July 2018 to June 2019 Supported the continuous improvement efforts of the college by promoting the informed use of institutional data in evidence-based decision making. Managed entire student evaluation of teaching process from instrument design and creation, through data collection, analysis, and reporting of results for 250 courses per semester for both formative and summative purposes.  ? Wrote R program that tabulated survey results by student major (allowing for double and joint majors) to allow departments to focus on just the students they teach and improve their learning. Staff to the Academic Deans Committee The Claremont Colleges Academic Deans Committee - Claremont, CA July 2012 to June 2019 Negotiated a new process with central business staff for the submission of intercollegiate budgets for presidential consideration to allow for more considered use of consortial resources. Had signature authority over their $100K off-budget expenditures and partnered with central business staff to move  onto the budget when possible to improve financial planning.  ? Wrote SQL queries to produce consortium-level views of registration data to inform the planning ofintercollegiate programs to optimize the use of shared resources.  ? Built searchable online archive of Committee minutes and documents to enhance institutionalmemory and learning. Director, Academic Operations Harvey Mudd College July 2007 to June 2018 Researched causes of disparate academic performance and wrote a report used to demonstrate commitment to a culture of evidence in the reaffirmation of accreditation process.  Supervised machine and electronic shops to promote safety and the prudent use of resources. ? Built and piloted online system for Student Evaluation of Teaching using Qualtrics to email students at a class time of the instructor's choosing to ensure high response rates and collect meaningful information to allow instructors to optimize courses for student learning.  ? Queried Student Information System to get data to analyze in order to study the effect of sex, ethnicity, and SES on student performance and wrote report that served as an appendix to the 2010 WASC self-study and motivated faculty to make courses more welcoming. Associate Dean for Planning, IR and Assessment Harvey Mudd College July 2004 to June 2007 Oversaw reporting to governmental and accrediting agencies. Chaired college assessment committee. Co-chaired college diversity task force. Co-authored planning documents with the president of the college. Developed pilot portfolio assessment of student learning.  ? Built and deployed a LAMP server to collect, store, and manipulate administrative data resulting indecreased administrative overhead.  ? Collaborated with College President to study the financial impact of cross registration on theClaremont Consortium to inform strategic planning and staffing.   Doctor of Philosophy in  Claremont Graduate University - Claremont, CA Master of Arts in Philosophy The Claremont Graduate School - Claremont, CA Bachelor of Arts in Economics Pitzer College - Claremont, CA   DATABASE, DBASE, SQL (10+ years), C++, GGPLOT2 (3 years), VBA, COGNOS (10+ years), LINUX (10+ years), SPSS (10+ years), STATISTICS (10+ years), VISUALIZATION, EXCEL (10+ years), PARADOX, Managerial, Budget Management (10+ years), R (9 years) Links   Military Service  Branch: Army Service Country: United States Rank: Cadet September 1980 to May 1985 Assessments  Data Analysis  Expert July 2019 Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. Full results: https://share.indeedassessments.com/share_assignment/yiec2z6gigfkth8f Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field. Publications  Leibniz: His Philosophy and His Calculi https://pdfs.semanticscholar.org/4e92/1c48fa35049d40f50a5cb2fd626d29dd9779.pdf March 1999 Additional Information  COMPUTER     ? Operating systems: OSX, WIN, and LINUX  ? Programming language: C++  ? Database programs: dBase, Filemaker, Paradox, and SQL  ? Database query : Cognos, Impromptu 8, SQL  ? Statistics and visualization software: ggmap, ggplot2, knitr, Lattice, R, SPSS, TeX  ? Spreadsheets: Excel with VBA, Google Sheets"
" Experience

HotBox Pizza
September 2017 to Present
 Made pizzas for high-volume catering clients as well as individuals just wanting dinner 
 Responsible for taking orders both over the phone and in person 
 Communicated with entire crew to ensure that no mistakes would be made for high-volume orders 
 Balanced being a full-time student with ing a fast-paced job 25 hours per week 
 
Key : 
 : Microsoft Excel, Microsoft Word, R, Python 
 : Strong  Ethic, Team, Multitasking, Detail-Oriented 
 Academic: Pure and Applied Mathematics, Statistics, Finance, Independent Learning 
 
Relevant Course: 
Real Analysis Differential Equations Abstract Algebra 
Statistical Analysis Probability Linear Algebra 
Financial Mathematics Corporate Finance Accounting


Bachelor of Science in Mathematics in Mathematics
Purdue University - West Lafayette, IN
May 2021",Data Scientist,resume," Experience  HotBox Pizza September 2017 to Present  Made pizzas for high-volume catering clients as well as individuals just wanting dinner   Responsible for taking orders both over the phone and in person   Communicated with entire crew to ensure that no mistakes would be made for high-volume orders   Balanced being a full-time student with ing a fast-paced job 25 hours per week    Key :   : Microsoft Excel, Microsoft Word, R, Python   : Strong  Ethic, Team, Multitasking, Detail-Oriented   Academic: Pure and Applied Mathematics, Statistics, Finance, Independent Learning    Relevant Course:  Real Analysis Differential Equations Abstract Algebra  Statistical Analysis Probability Linear Algebra  Financial Mathematics Corporate Finance Accounting   Bachelor of Science in Mathematics in Mathematics Purdue University - West Lafayette, IN May 2021"
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Linux System Administrator/ Jr Devops engeneer
Data Service Group Inc - Delaware County, PA
June 2016 to Present
Experienced DevOps /Red hat certified Linux systems Administrator with over five years of experience in the IT field with excellent reputation for resolving problems, improving customer satisfaction, and driving 
overall operational improvements. Consistently saved costs while increasing profits. I am a motivated
IT  with hands on systems administration, System configuration, Enthusiastic team player , always looking for innovative and efficient engineering solutions, Energetic self-starter capable of learning quickly 
with minimal guidance. I am seeking to progress my career in the Information  sector where
I 
will use my  and experience in system maintenance and  troubleshooting to contribute to an active growth and productivity of the company. I am authorized to  in the United States for any employer.


PROBATOIRE
FULTANG


MYSQL, Javascript, CSS, Windows 2008, Automation, System Administrator, Bash Shell Scripting,,
Version Control Systems: Git, bitbucket, Continuous Integration: Jenkins, Nexus, Automation/ Deployment: Ansible,, Virtualization: Oracle Virtual Box,, Platforms: Linux Centos 6&7 RHEL 6&7, and Windows Server 2008, Storage: LVM, NAS, Ticketing: Jira/kanban board/Confluence/, Database: MySql, Apache, web Server, error.log,, Automation, Bash, Hardware, content,, Customer service,
Database Management, Firewalls, Help Desk, HTTP, inventory, LAMP, Managing, memory,, NAS,, NFS,
Net installation, net, Process management, Redhat Linux, SAMBA,, scrum, SSH, servers, Shell Scripting, shell scripts, SMTP, System Administrator, Tech Support, Troubleshoot, troubleshooting, upgrades,, Windows Server, Patching, Git",Data Scientist,resume,"Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Linux System Administrator/ Jr Devops engeneer Data Service Group Inc - Delaware County, PA June 2016 to Present Experienced DevOps /Red hat certified Linux systems Administrator with over five years of experience in the IT field with excellent reputation for resolving problems, improving customer satisfaction, and driving  overall operational improvements. Consistently saved costs while increasing profits. I am a motivated IT  with hands on systems administration, System configuration, Enthusiastic team player , always looking for innovative and efficient engineering solutions, Energetic self-starter capable of learning quickly  with minimal guidance. I am seeking to progress my career in the Information  sector where I  will use my  and experience in system maintenance and  troubleshooting to contribute to an active growth and productivity of the company. I am authorized to  in the United States for any employer.   PROBATOIRE FULTANG   MYSQL, Javascript, CSS, Windows 2008, Automation, System Administrator, Bash Shell Scripting,, Version Control Systems: Git, bitbucket, Continuous Integration: Jenkins, Nexus, Automation/ Deployment: Ansible,, Virtualization: Oracle Virtual Box,, Platforms: Linux Centos 6&7 RHEL 6&7, and Windows Server 2008, Storage: LVM, NAS, Ticketing: Jira/kanban board/Confluence/, Database: MySql, Apache, web Server, error.log,, Automation, Bash, Hardware, content,, Customer service, Database Management, Firewalls, Help Desk, HTTP, inventory, LAMP, Managing, memory,, NAS,, NFS, Net installation, net, Process management, Redhat Linux, SAMBA,, scrum, SSH, servers, Shell Scripting, shell scripts, SMTP, System Administrator, Tech Support, Troubleshoot, troubleshooting, upgrades,, Windows Server, Patching, Git"
" Being a Passionate Data enthusiast and have ing experience as  qualified DataScientist in Statistical modeling, Machine Learning, Data Visualization and Data mining with large sets of both Structured and Unstructured Data. 
 Experience in feature extraction, creating Regression models, Classification, Predictive data modelingand Cluster analysis. 
 Strong experience in implementing Supervised Machine Learning Algorithms like Linear Regression,
Logistic Regression, Linear Discriminant Analysis (LDA), Decision Tree, Random Forest, Support Vector
Machines (SVM), Naive Bayes, K-Nearest Neighbor. 
 Extensive experience in providing Machine Learning and Data Mining solutions to various businessproblems based on requirements using Python 
 Strong expertise in implementing Unsupervised Machine Learning Algorithms like Hierarchicalclustering, K-means clustering, Probability Clustering, Density-Based Clustering. 
 Proficient in using Python libraries like Pandas, NumPy, Scikit-learn, Seaborn, Scipy for developingvarious machine learning models. 
 Around 3 years of experience in developing Deep Learning models like Conventional Neural Net(CNN), Artificial Neural Net, Multilayer perception's (MLPs), Recurrent Neural Nets (RNN) for recommended systems. 
 As a Data scientist actively involved in all phases of project life cycle including Data Extraction, Data
Cleaning, Data Visualization and building Models. 
 Strong experience in Software Development Life Cycle (SDLC) including Requirement Analysis,
Design Specification and Testing in both Waterfall and Agile methodologies. 
 Implementation experiences in Machine Learning and deep learning including Regression,Classification, Neural net, object tracking and Natural Language Processing (NLP's) using packages like Tensor Flow, Keras, NLTK and Spacy. 
 Proficient Mathematical knowledge on Matrix Operations, Statistics, Probability, Linear Algebra,
Differentiation, Integration and Geometry 
 ed with various data visualization  of python like Matplotlib, Seaborn, ggplot, pygal andusing of Tableau. 
 Hands on Experience in using GIT Version Control System. 
 Proficient with excellent initiative and innovative thinking  and ability to guide teammates tobreakdown large and complex issues to simplified versions for easy execution.
 Experience

Data Scientist
Whole Foods Market, Austin
October 2018 to Present
 Performed Customer segmentation based on customers behavior, demographics, transactions byusing customer specific details like age, income and created multiple customer classes. 
 Analyzed the customers purchase data and product trends to recommend the types of products forcustomers based on their behavior tracked through customer accounts. 
 Explored and created different new data sets to  with and implement few data science  flowplatforms for future applications. 
 Constructed customer classes with historical, demographic and behavioral data as features usingRandom Forest Classifier and Logistic Regression to help marketing team understand purchase pattern of customers. 
 Predicted sales and profits using machine learning and deep learning strategies. 
 Assisted marketing team to devise business strategy to target customers with discount coupons,deals and offers to improve customer purchases and maintaining stock at stores. 
 Communicated with management to discuss insights obtained from data, assisted in makingbest business decisions and reduced customer churn by 15% in few months of implementation by extracting value from data. 
 Applying clustering algorithms like partitioning clustering, fuzzy clustering, density-based clusteringmethods to group the data on their similar behavior patterns. 
 Identified distinct patterns in which customers respond to offers and clustered their actions usingK-means, K-means++ Clustering, Hierarchical Clustering and segmented them into different groups, helped marketing team to further analyze behavioral patterns of customers. 
 Created Customer Lifetime Value (CLV) from the customers data by using Multi-Linear Regressionalgorithm, identified high and low value segments and helped organization to understand customers and improve customer service to retain customers. 
 Performed personal and food sales Predictive Modeling by using decision trees and regressions inorder to get the risk involved by giving individual scores to the customers. 
 Proposed marketing strategies to target potential customers using their first three months data andfrom regression model, we evaluated CLV for every new customer. 
 Investigated large datasets to handle missing values, cleaned messy datasets and applied featurescaling to standardize range of independent variables. 
 Researched predictive models including Logistic Regression, Support Vector Machine (SVC) and re-enforcement learning to prevent retail fraud. 
 Improved model performance by tuning hyper-parameters using optimization techniques like Gridsearch, Random search and Bayesian optimization and increased model efficiency by XG-Boosting  Validated models using Cross validation, loss function to measure model performance and created Confusion Matrix, Receiver Operating Characteristic (ROC) and Cumulative Accuracy  (CAP) curves. Addressed over-fitting and under-fitting by tuning hyper parameters using L1 and L2 Regularization 
 Applied dimensionality reduction technique like Principal Component Analysis (PCA) to extractrelevant optimal features from high dimensional data. 
 Visualized results using Matplotlib, Seaborn libraries of scikit-learn and used Tableau to presentresults on dashboards for team members, Management and other relevant departments in company.  Forecast the company's short-term and long-term growth in terms of revenue, number of customers, various costs, stock changes etc., using machine learning algorithms.
Data Scientist
Adidas, Oregon
July 2017 to September 2018
 Developed predictive solutions to support online shopping using machine learning algorithms suchas Linear Regression, Logistic Regression, Naive Bayes, Decision Trees, Random Forest, Support Vector
Machine in Python. 
 ed on data cleaning, data preparation and feature engineering with Python, including NumPy,
SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn. 
 Responsible for data identification, collection, exploration, cleaning for appropriate modeling.  ed on NLTK library in python for doing sentiment analysis on customer product reviews and other third-party websites using web scrapping. 
 Performed sentiment analysis of customer reviews and classified each review into good, bad andneutral class to understand pulse of customers about business. 
 Implemented Time Series analysis on sales data to consider what measures to be taken for improvethe Sales. 
 Used MySQL and created SQL tables and involved in data loading and writing SQL UDFs. 
 Conducted analysis in assessing customer behaviors with clustering algorithms such as K-Means
Clustering and Hierarchical Clustering. 
 Evaluated parameters with K-Fold Cross Validation, Grid search methods to optimize performance ofmodels. 
 Along with data analytics and Excel data extracts, Implemented Agile Methodologies, Scrum storiesand sprints in a Python based environment. 
 ed on .csv, .json, .excel different types of files for the data cleaning and data analysis.  Performed Time Series Analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values. 
 ed in Tableau environment to create weekly, monthly, daily reports using tableau desktop &publish them to server. 
 ed on Excel using pivots, conditional formatting, large record sets, data manipulation andcleaning. 
 Used GIT HUB as version control software to manage the source code and to keep track of changesto files which is fast and light weight system.
Data Scientist
First Data - GE
February 2016 to June 2017
 Analyzed the data using various machine learning algorithms to segregate all transactions made bycustomers depending on the amount and total transactions. 
 Extracted Tera bytes of both structured and unstructured data by using SQL queries and performeddata mining tasks including handling missing data, data wrangling, feature scaling. 
 Developed an easy to use documentation for the frames and  developed for adaption byother teams. 
 Implemented Porter Stemmer (Natural Language Tool Kit) with NLP bag of words model using Count
Vectorizer class to process text data. 
 Created predictive model using LSTM, Recurrent Neural Nets (RNNs) and studied reviews,obtained feedback on customer service to help employer reduce customer churn. 
 Experimented with other classification models like Random Forests, Logistic Regression and Naïve
Bayes to classify customers reviews. 
 Extracted data from web using Web Scraping, Text mining and processes data into tab separated fileto separate reviews by tab in data. 
 Cleaned dirty data and prepared data for feature extraction using Count Vectorizer of sci kit-learnfeature extraction library. 
 Automated customer service by creating chat box which responds to customer queries using deeplearning and text processing with nltk of NLP library. 
 Evaluated model performance by creating confusion matrix, classification report and accuracy score.Improved model performance by k-fold cross validation and XG-Boosting and achieved model accuracy of 92%. 
 Developed recommended systems using Apriori Principle Algorithm, for mining frequent item setsand relevant association rules to operate database containing a lot of transactions. 
 Built machine learning algorithms to forecast the company's short term and long term growth interms of revenue, number of customers, stock changes and other. 
 Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, meta data solution and data life-cycle management in both RDBMS, Big Data environments. 
 Presented simple visualization of results using seaborn visualization libraries of Python.  Used python for statistical operations on the data and seaborn, ggplot for visualizing the data regarding the sales and customers.
Data Analyst
Karvy capital Ltd - Hyderabad, Telangana May 2013 to December 2015
 Acquired data from primary or secondary data sources and maintain databases/data systems. 
 Established new client data preparing them for entry into new platform. 
 Loaded data by converting CSV file into corresponding database tables. 
 ed with management team to create prioritized list of needs for each business segment.  Monitored and resolved issues of data flow on daily basis. Also created views for reporting team to use data for marketing numbers on daily basis. 
 Collaborated with reporting team to resolve data discrepancies and logical data corrections which areoccurring throughout reports. 
 Generated Tableau ad-hoc reports using excel sheet, flat files, CSV files. 
 Designed, built, and implemented relational databases 
 Used data mining techniques for outlier detection and created algorithm to connect patternsbetween customer trends. 
 Created Software solutions in Software development lifecycle (SDLC) and Agile methodologiesenvironment. 
 Performed computational tasks on data by creating pig, hive and Map reduce scripts to access andtransform data in HDFS. 
 Developed and implemented metadata models for reporting functionalities and developedautomated process for data corrections. 
 Developed SQL, NoSQL and PL/SQL scripts to extract data from database and for testing Purposes.  Reviewed logical model with application developers, ETL team, DBAs, and testing team to provide information about data model and business requirements. 
 Identified and logged defects if/when test fail, using SQL to narrow down root cause of problem forefficient investigation by development team and log accordingly. 
 Used advanced Excel functions to generate spreadsheets and pivot tables.
Education

M.S.
Southern Arkansas University


Boosting, Decision trees, Hierarchical clustering, K-means, K-nearest neighbor, Linear discriminant analysis, Lda, Linear regression, Logistic regression, Machine learning, Principal component analysis,
Random forests, Support vector machines, Clustering, Python, Sql, Java, Algorithms, Statistics, Operations
Links


Additional Information

TECHNICAL : 
Languages Python, SQL, Java 
Mathematical  Statistics and Probability, Linear Algebra, Matrix Operations, Calculus 
Machine Learning Algorithms 
Linear Regression, Logistic Regression, Linear Discriminant Analysis (LDA), Decision Trees, Random
Forests with Adaboost and Gradient Descent Boosting, Support Vector Machines (SVM's), Naive Bayes,
K-Nearest Neighbor, Hierarchical clustering, K-means clustering, Probability Clustering, Density-Based Clustering. 
 
Machine Learning Techniques 
Principal Component Analysis, Data Standardization Techniques, L1 and L2 regularization, Hyperparameter tuning, Resampling Techniques like SMOTE, Cluster Centroid Methods, Feature selection and Feature Engineering, Cross Validation Methods(K-fold).",Data Scientist,resume," Being a Passionate Data enthusiast and have ing experience as  qualified DataScientist in Statistical modeling, Machine Learning, Data Visualization and Data mining with large sets of both Structured and Unstructured Data.   Experience in feature extraction, creating Regression models, Classification, Predictive data modelingand Cluster analysis.   Strong experience in implementing Supervised Machine Learning Algorithms like Linear Regression, Logistic Regression, Linear Discriminant Analysis (LDA), Decision Tree, Random Forest, Support Vector Machines (SVM), Naive Bayes, K-Nearest Neighbor.   Extensive experience in providing Machine Learning and Data Mining solutions to various businessproblems based on requirements using Python   Strong expertise in implementing Unsupervised Machine Learning Algorithms like Hierarchicalclustering, K-means clustering, Probability Clustering, Density-Based Clustering.   Proficient in using Python libraries like Pandas, NumPy, Scikit-learn, Seaborn, Scipy for developingvarious machine learning models.   Around 3 years of experience in developing Deep Learning models like Conventional Neural Net(CNN), Artificial Neural Net, Multilayer perception's (MLPs), Recurrent Neural Nets (RNN) for recommended systems.   As a Data scientist actively involved in all phases of project life cycle including Data Extraction, Data Cleaning, Data Visualization and building Models.   Strong experience in Software Development Life Cycle (SDLC) including Requirement Analysis, Design Specification and Testing in both Waterfall and Agile methodologies.   Implementation experiences in Machine Learning and deep learning including Regression,Classification, Neural net, object tracking and Natural Language Processing (NLP's) using packages like Tensor Flow, Keras, NLTK and Spacy.   Proficient Mathematical knowledge on Matrix Operations, Statistics, Probability, Linear Algebra, Differentiation, Integration and Geometry   ed with various data visualization  of python like Matplotlib, Seaborn, ggplot, pygal andusing of Tableau.   Hands on Experience in using GIT Version Control System.   Proficient with excellent initiative and innovative thinking  and ability to guide teammates tobreakdown large and complex issues to simplified versions for easy execution.  Experience  Data Scientist Whole Foods Market, Austin October 2018 to Present  Performed Customer segmentation based on customers behavior, demographics, transactions byusing customer specific details like age, income and created multiple customer classes.   Analyzed the customers purchase data and product trends to recommend the types of products forcustomers based on their behavior tracked through customer accounts.   Explored and created different new data sets to  with and implement few data science  flowplatforms for future applications.   Constructed customer classes with historical, demographic and behavioral data as features usingRandom Forest Classifier and Logistic Regression to help marketing team understand purchase pattern of customers.   Predicted sales and profits using machine learning and deep learning strategies.   Assisted marketing team to devise business strategy to target customers with discount coupons,deals and offers to improve customer purchases and maintaining stock at stores.   Communicated with management to discuss insights obtained from data, assisted in makingbest business decisions and reduced customer churn by 15% in few months of implementation by extracting value from data.   Applying clustering algorithms like partitioning clustering, fuzzy clustering, density-based clusteringmethods to group the data on their similar behavior patterns.   Identified distinct patterns in which customers respond to offers and clustered their actions usingK-means, K-means++ Clustering, Hierarchical Clustering and segmented them into different groups, helped marketing team to further analyze behavioral patterns of customers.   Created Customer Lifetime Value (CLV) from the customers data by using Multi-Linear Regressionalgorithm, identified high and low value segments and helped organization to understand customers and improve customer service to retain customers.   Performed personal and food sales Predictive Modeling by using decision trees and regressions inorder to get the risk involved by giving individual scores to the customers.   Proposed marketing strategies to target potential customers using their first three months data andfrom regression model, we evaluated CLV for every new customer.   Investigated large datasets to handle missing values, cleaned messy datasets and applied featurescaling to standardize range of independent variables.   Researched predictive models including Logistic Regression, Support Vector Machine (SVC) and re-enforcement learning to prevent retail fraud.   Improved model performance by tuning hyper-parameters using optimization techniques like Gridsearch, Random search and Bayesian optimization and increased model efficiency by XG-Boosting  Validated models using Cross validation, loss function to measure model performance and created Confusion Matrix, Receiver Operating Characteristic (ROC) and Cumulative Accuracy  (CAP) curves. Addressed over-fitting and under-fitting by tuning hyper parameters using L1 and L2 Regularization   Applied dimensionality reduction technique like Principal Component Analysis (PCA) to extractrelevant optimal features from high dimensional data.   Visualized results using Matplotlib, Seaborn libraries of scikit-learn and used Tableau to presentresults on dashboards for team members, Management and other relevant departments in company.  Forecast the company's short-term and long-term growth in terms of revenue, number of customers, various costs, stock changes etc., using machine learning algorithms. Data Scientist Adidas, Oregon July 2017 to September 2018  Developed predictive solutions to support online shopping using machine learning algorithms suchas Linear Regression, Logistic Regression, Naive Bayes, Decision Trees, Random Forest, Support Vector Machine in Python.   ed on data cleaning, data preparation and feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn.   Responsible for data identification, collection, exploration, cleaning for appropriate modeling.  ed on NLTK library in python for doing sentiment analysis on customer product reviews and other third-party websites using web scrapping.   Performed sentiment analysis of customer reviews and classified each review into good, bad andneutral class to understand pulse of customers about business.   Implemented Time Series analysis on sales data to consider what measures to be taken for improvethe Sales.   Used MySQL and created SQL tables and involved in data loading and writing SQL UDFs.   Conducted analysis in assessing customer behaviors with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.   Evaluated parameters with K-Fold Cross Validation, Grid search methods to optimize performance ofmodels.   Along with data analytics and Excel data extracts, Implemented Agile Methodologies, Scrum storiesand sprints in a Python based environment.   ed on .csv, .json, .excel different types of files for the data cleaning and data analysis.  Performed Time Series Analysis on animal medicine and vaccine product sales data in order to extract meaningful statistics and other characteristics of the data to predict future values based on previously observed values.   ed in Tableau environment to create weekly, monthly, daily reports using tableau desktop &publish them to server.   ed on Excel using pivots, conditional formatting, large record sets, data manipulation andcleaning.   Used GIT HUB as version control software to manage the source code and to keep track of changesto files which is fast and light weight system. Data Scientist First Data - GE February 2016 to June 2017  Analyzed the data using various machine learning algorithms to segregate all transactions made bycustomers depending on the amount and total transactions.   Extracted Tera bytes of both structured and unstructured data by using SQL queries and performeddata mining tasks including handling missing data, data wrangling, feature scaling.   Developed an easy to use documentation for the frames and  developed for adaption byother teams.   Implemented Porter Stemmer (Natural Language Tool Kit) with NLP bag of words model using Count Vectorizer class to process text data.   Created predictive model using LSTM, Recurrent Neural Nets (RNNs) and studied reviews,obtained feedback on customer service to help employer reduce customer churn.   Experimented with other classification models like Random Forests, Logistic Regression and Naïve Bayes to classify customers reviews.   Extracted data from web using Web Scraping, Text mining and processes data into tab separated fileto separate reviews by tab in data.   Cleaned dirty data and prepared data for feature extraction using Count Vectorizer of sci kit-learnfeature extraction library.   Automated customer service by creating chat box which responds to customer queries using deeplearning and text processing with nltk of NLP library.   Evaluated model performance by creating confusion matrix, classification report and accuracy score.Improved model performance by k-fold cross validation and XG-Boosting and achieved model accuracy of 92%.   Developed recommended systems using Apriori Principle Algorithm, for mining frequent item setsand relevant association rules to operate database containing a lot of transactions.   Built machine learning algorithms to forecast the company's short term and long term growth interms of revenue, number of customers, stock changes and other.   Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, meta data solution and data life-cycle management in both RDBMS, Big Data environments.   Presented simple visualization of results using seaborn visualization libraries of Python.  Used python for statistical operations on the data and seaborn, ggplot for visualizing the data regarding the sales and customers. Data Analyst Karvy capital Ltd - Hyderabad, Telangana May 2013 to December 2015  Acquired data from primary or secondary data sources and maintain databases/data systems.   Established new client data preparing them for entry into new platform.   Loaded data by converting CSV file into corresponding database tables.   ed with management team to create prioritized list of needs for each business segment.  Monitored and resolved issues of data flow on daily basis. Also created views for reporting team to use data for marketing numbers on daily basis.   Collaborated with reporting team to resolve data discrepancies and logical data corrections which areoccurring throughout reports.   Generated Tableau ad-hoc reports using excel sheet, flat files, CSV files.   Designed, built, and implemented relational databases   Used data mining techniques for outlier detection and created algorithm to connect patternsbetween customer trends.   Created Software solutions in Software development lifecycle (SDLC) and Agile methodologiesenvironment.   Performed computational tasks on data by creating pig, hive and Map reduce scripts to access andtransform data in HDFS.   Developed and implemented metadata models for reporting functionalities and developedautomated process for data corrections.   Developed SQL, NoSQL and PL/SQL scripts to extract data from database and for testing Purposes.  Reviewed logical model with application developers, ETL team, DBAs, and testing team to provide information about data model and business requirements.   Identified and logged defects if/when test fail, using SQL to narrow down root cause of problem forefficient investigation by development team and log accordingly.   Used advanced Excel functions to generate spreadsheets and pivot tables. Education  M.S. Southern Arkansas University   Boosting, Decision trees, Hierarchical clustering, K-means, K-nearest neighbor, Linear discriminant analysis, Lda, Linear regression, Logistic regression, Machine learning, Principal component analysis, Random forests, Support vector machines, Clustering, Python, Sql, Java, Algorithms, Statistics, Operations Links   Additional Information  TECHNICAL :  Languages Python, SQL, Java  Mathematical  Statistics and Probability, Linear Algebra, Matrix Operations, Calculus  Machine Learning Algorithms  Linear Regression, Logistic Regression, Linear Discriminant Analysis (LDA), Decision Trees, Random Forests with Adaboost and Gradient Descent Boosting, Support Vector Machines (SVM's), Naive Bayes, K-Nearest Neighbor, Hierarchical clustering, K-means clustering, Probability Clustering, Density-Based Clustering.    Machine Learning Techniques  Principal Component Analysis, Data Standardization Techniques, L1 and L2 regularization, Hyperparameter tuning, Resampling Techniques like SMOTE, Cluster Centroid Methods, Feature selection and Feature Engineering, Cross Validation Methods(K-fold)."
"
Experience 
 	Mustang Analytics LLC Data Scientist and co-founder   Sept 2018  Present 
 Provided and presented deliverables to company executives 
 Created a brand and established a company 
 Consulted with clients on a number of data analytics issues 
 	Risk Analyst Summer at Goldman Sachs  	 	June 2017 - Aug 2017 
 Evaluate and assessing daily risk in the market 
 Predicted changes in the market and when underperformance could occur  
 ed within a large team to analyze a shock situations effect on the market 
Data Analysis  
 Wrist Wearable data analysis to predict an individuals activity using only directional movements (January 2018  June 2018) 
 IPO Financial investment project using web scraping and machine learning to find the best long term and short term investments (May 2018 - Present) 
 Performed web scraping and topic modeling techniques on presidential candidate speeches to find differences between syntax (December 2017) 
 Building an NFL prediction model and 4th down analysis using a database of NFL plays with SAS software (January 2017) o http://support.sas.com/resources/papers/proceedings17/2023-2017.pdf 
	Data Scientist Internship at VISA  	 	 	June 2016 - Sept 2016 
 ed to find revenue leakage and find ways to maximize profits 
 Analyzed datasets containing over 160 million records & 120 variables 
 
California Polytechnic State University, San Luis Obispo 
 Bachelors: Statistics (Minor: Data Science & Computer Science)  Graduated June 2018 
 GPA: 3.2  
 Certified: SAS 9 Advanced Programming Certified 
 
Computer  
 Python, R, Spark, Map Reduce, SAS, C, Java, SQL, Pandas, Numpy, Scikit-learn 
Other Experience  
 NBA Hackathon Finalist  	 	 	 	Sept. 2017 
 Finalist: SAS Student Symposium  	 	 	April 2017 ",Data Scientist,resume," Experience   	Mustang Analytics LLC Data Scientist and co-founder   Sept 2018  Present   Provided and presented deliverables to company executives   Created a brand and established a company   Consulted with clients on a number of data analytics issues   	Risk Analyst Summer at Goldman Sachs  	 	June 2017 - Aug 2017   Evaluate and assessing daily risk in the market   Predicted changes in the market and when underperformance could occur    ed within a large team to analyze a shock situations effect on the market  Data Analysis    Wrist Wearable data analysis to predict an individuals activity using only directional movements (January 2018  June 2018)   IPO Financial investment project using web scraping and machine learning to find the best long term and short term investments (May 2018 - Present)   Performed web scraping and topic modeling techniques on presidential candidate speeches to find differences between syntax (December 2017)   Building an NFL prediction model and 4th down analysis using a database of NFL plays with SAS software (January 2017) o http://support.sas.com/resources/papers/proceedings17/2023-2017.pdf  	Data Scientist Internship at VISA  	 	 	June 2016 - Sept 2016   ed to find revenue leakage and find ways to maximize profits   Analyzed datasets containing over 160 million records & 120 variables    California Polytechnic State University, San Luis Obispo   Bachelors: Statistics (Minor: Data Science & Computer Science)  Graduated June 2018   GPA: 3.2    Certified: SAS 9 Advanced Programming Certified    Computer    Python, R, Spark, Map Reduce, SAS, C, Java, SQL, Pandas, Numpy, Scikit-learn  Other Experience    NBA Hackathon Finalist  	 	 	 	Sept. 2017   Finalist: SAS Student Symposium  	 	 	April 2017 "
" 
* Dedicated IT  with around7 years of experience in Data Analysis, DataProfiling, DataIntegration, Data Visualization, Predictive modeling, MetadataManagement,Business requirement analysis, Quality assurance, Design, Development, and Testing to further the success of various organizations business goals and s.
* Extensive experience in Machine Learning solutions to various business problems and generating data visualizations using Python and R.
* Hands on experience with in extracting and modeling datasets from variety of data sources like Hadoop using Big data  such asKafka, Pig, Hive, Spark,Sqoopfor ad­hoc analysis.
* Advanced knowledge of statistical techniques in Sampling, Probability, Correlation, Multivariatedataanalysis, PCA, Time-series analysis and application of Statistical Concepts using SAS and SPSS.
* Hands on experience in implementing Web Services  like RESTAPI.
* ing SQL knowledge and experience ing with relational databases and optimizing the queries with a variety of databases such as PostgreSQL, MySQL, Microsoft SQL Server and Oracle.
* Experience ing with NoSQL databases such as MongoDB.
* Expertise in programming languages like Python, Scala, R, SAS,Java and JavaScript.
* Experience with AWS  like Redshift, S3, EC2, SageMaker&EMR.
* Extensive experience in ing with Tableau 9.0 and 10.0 Desktop along with Tableau Server.
* Excellent Software Development Life Cycle (SDLC) with good ing knowledge of testing methodologies, disciplines, tasks, resources and scheduling.
* Experience in Cloud  such as Google Cloud Platform and SAP
* Extensive ing knowledge in Microsoft Excel for Data Analysis.

Domain
 Used
Machine Learning
Classification, Regression, Clustering, Association, Logistic Regression, Decision Trees, Random Forest, Naïve Bayes, K-Nearest Neighbors(K-NN), Kernel SVM.
Big Data
Pig, Hive, Spark, HBase, Kafka, Sqoop, Cassandra, MongoDB.
Databases
Oracle, MySQL, MS SQL Server, Postgres.
Analytics
Python, R, SAS, SPSS, Excel.
Visualization
Tableau and PowerBI
Operating Systems
Windows, Unix/Linux, Mac
Design Methodologies
Agile, Scrum, Waterfall
IDEs
IntelliJ IDEA, Pycharm, Jupyter, Spyder 
			
 EXPERIENCE
Credit Suisse, Raleigh - NC			
Data Scientist				Feb 2019 - Till date
* Developed a Rule based engine that helps the business in identifying the potential conflicts during the business initiation phase using Python.
* ed on configuring Apache Kafka to publish real-time business data to various topics.
* ed on Kafka messaging systems for extracting the user input data that feeds to the Rules engine.
* Created Restful Web Services that integrate with Apache Kafka and web interface using Flask.
* Develop a user interface for the client to display the algorithm results for easy access of data using React JS and Bootstrap.
* Involved in fixing and testing the python functions using unit testing.	
* Improved the ETL flow by building a python script that transfers data from MongoDB to Oracle database.
Environment: Python, Kafka, React, Flask, MongoDB, Oracleand Git.
MetLife, Charlotte - NC				
Data Scientist										June 2018 - Jan 2019
* Involved in tuning the Propensity models predicting the customer behavior to help Marketing and Finance team in analyzing various data sources to promote the growth of insurance products.
* Involved in extracting data from various sources and performed data cleansing, data integration, data transformation, data mapping and loading data into Hadoop with Apache Spark using PySpark and SparkSQL.
* Wrote SparkSQL scripts to Query multiple tables, performed joins and create tables to load data into Hive.
* Applied Text Mining techniques to clean the marketing survey data using NLP techniques such as lemmatization and porter stemmer.
* Wrote Hashing algorithm to secure PII data, and stored the data in the Hadoop.
* ed on SAP Cloud services and ingested data from Oracle and MySQL data bases using Sqoop.
* Prepared analysis reports and dashboards for the stakeholders using Tableau.
Environment: Python, Spark, NLP, Hive, HDFS, SSIS, Oracle, SAP cloud, MySQL, Tableau and Git.
JPMorgan Chase				
Data Scientist									Apr2017-Mar2018
* Provided recommendations by evaluating the customer life value to the business.
* ed on Data cleaning, Data preparation and Feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn. 
* Hands on experience in Dimensionality Reduction, Model selection and Model boosting methods using Principal Component Analysis (PCA), K-Fold Cross Validation and Gradient Tree Boosting.
* Advanced knowledge of statistical techniques in Sampling, Probability, Multivariatedataanalysis, PCA, and Time-seriesanalysis using SAS.
* Responsible for creating Hive tables, loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns.
* Performing statistical data analysis and data visualization using R and Python.
* Performed analysis of implementing Spark uses and wrote spark sample programs using PySpark. 
* Implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately. 
Environment: SQL, Informatica, SAS, Hive, Tableau, Python, GIT, Google cloud platform and Tableau.
Wells Fargo, India					
Data Analyst/Scientist									Feb 2015-Aug 2016
* Involved in evaluating customer credit data and financial statements in order to determine the degree of risk involved in lending money.
* Developed predictive solutions to support commercial banking team using machine learning algorithms such as Linear Regression, Logistic Regression, Naive Bayes, Decision Trees, Random Forest, Support Vector Machine in Python.
* Conducted analysis in assessing customer behaviors with clustering algorithms such as K-Means Clustering and Hierarchical Clustering.
* ed on data cleaning, data preparation and feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn.
* Implemented Agile Methodologies, Scrum stories and sprints in a Python based environment, along with data analytics and Excel data extracts. 
* Used Hive and created Hive tables and involved in data loading and writing HiveUDFs. 
* Experience designing and optimizing complex SQL queries involving table joins using MySQL.
* ed in Tableau environment to create weekly, monthly, daily reports using tableau desktop & publish them to server.
* ed on importing and exporting data from Oracle into HDFS using Sqoop.
* ed on Excel using VLOOKUP, pivots, conditional formatting, large record sets, data manipulation and cleaning.
* Used GIT as version control software to manage the source code and to keep track of changes to files which is fast and light weight system.
Environment: Python, MySQL, SAS, HDFS, Hive, Excel, Tableau and GIT.
Wells Fargo, India					
Data Analyst										May 2013-Jan 2015 
* ed for Risk management team in identifying the risk involved in the Mortgage process by evaluating the customer and property records.
* Involved in addressing a wide range of challenging problems using techniques from applied statistics, machine learning and data mining fields.
* Experience in descriptive statistics and hypothesis testing using Chi-square, T-test, Pearson correlation and Analysis of variance (ANOVA).
* Advanced knowledge of statistical techniques in Sampling, Probability, Multivariate data analysis, PCA, and Time-series analysis using SAS.
* Analyzed Relational & Non-relational data using MySQL.
* Contributed , Projectmanagement, and Business management functions to push the business forward with innovative solutions.
* Performed data acquisition and exploratory data analysis in R.
* Visualized team metrics and communicated to the higher management using PowerBI.
Environment: R, MySQL, PowerBI and GIT. 
Bankatlal Institute of Information , Hyderabad, India
Data Analyst											Aug 2011-Apr2013
* As part of a project, ed on HTML, CSS and JavaScript to develop a web application for online applications
* Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and
* Data Formats
* Involved in development of General Ledger module, which streamlined analysis, reporting and recording of
* accounting information
* Managed connectivity using JDBC for querying/inserting and data management including triggers and stored
* procedures
* Involved in maintaining student records in the database using Oracle
* Resolved the data related issues such as: assessing data quality, data consolidation, evaluating existing datasources using MS EXCEL
Environment: HTML, CSS, Oracle, MS Excel, Tableau
ACADEMIC PROJECTS
Big Data Modeling & Management
* As part of course analyzed streaming data in real-time from a weather station and created plots.
* Used Map-Reduce algorithms on Twitter data and Performed words count to study the presidential elections.
Big Data Integration & Processing
* As a final project, ed on Soccer world cup data to study the behavior of fans across the globe with the help of Twitter.
* Using Population census data performed statistical calculations using Splunk and accessed Postgres database tables and manipulated data frames using Spark.
* Involved in the retrieval of NoSQL data using MongoDB and performed data aggregation of Data Frames.
Statistical Thinking for Data Science
* ed with Machine Learning Algorithms: Neural Nets, Deep Learning, Net analysis, Supervised Learning, and Unsupervised Learning.
* Importing data into Python from files native to other software such as Excel spreadsheets, Stata, SAS and MATLAB files from relational databases such as SQLite&PostgreSQL.
* Reshaping and tidying data using techniques such as pivoting and melting.
* Performed Time-series analysis and A/B testing.
* Used Machine Learning components: Classification, Regression, and Clustering.
R Programming for Data Science
* Characterized relationships graphically, in the form of  statistics, and through simple linear regression models.
* Used Regression Trees, Decision Trees, Random Forest, Ensemble Methods and conducted performance metrics analysis to find the best model to forecast currency trend and generate detailed reports in RStudio.
* Used appropriate libraries to clean data using (tidyr, dplyr), visualize data (ggplot2), and finally tune and evaluate models. 
Database Management System
* Developed a car rental system using Java and MySQL as a part of Database Design course project to perform CRUD operations.
Project and Change Management
* As part of the team project, ed on doctors appointment website using Microsoft project 2013. Created business requirement documents, business use cases and system use cases. Conducted Cost-Benefit analysis, GAP Analysis, SWOT Analysis to validate the compatibility of the system infrastructure with the new business requirements.



",Data Scientist,resume,"  * Dedicated IT  with around7 years of experience in Data Analysis, DataProfiling, DataIntegration, Data Visualization, Predictive modeling, MetadataManagement,Business requirement analysis, Quality assurance, Design, Development, and Testing to further the success of various organizations business goals and s. * Extensive experience in Machine Learning solutions to various business problems and generating data visualizations using Python and R. * Hands on experience with in extracting and modeling datasets from variety of data sources like Hadoop using Big data  such asKafka, Pig, Hive, Spark,Sqoopfor ad­hoc analysis. * Advanced knowledge of statistical techniques in Sampling, Probability, Correlation, Multivariatedataanalysis, PCA, Time-series analysis and application of Statistical Concepts using SAS and SPSS. * Hands on experience in implementing Web Services  like RESTAPI. * ing SQL knowledge and experience ing with relational databases and optimizing the queries with a variety of databases such as PostgreSQL, MySQL, Microsoft SQL Server and Oracle. * Experience ing with NoSQL databases such as MongoDB. * Expertise in programming languages like Python, Scala, R, SAS,Java and JavaScript. * Experience with AWS  like Redshift, S3, EC2, SageMaker&EMR. * Extensive experience in ing with Tableau 9.0 and 10.0 Desktop along with Tableau Server. * Excellent Software Development Life Cycle (SDLC) with good ing knowledge of testing methodologies, disciplines, tasks, resources and scheduling. * Experience in Cloud  such as Google Cloud Platform and SAP * Extensive ing knowledge in Microsoft Excel for Data Analysis.  Domain  Used Machine Learning Classification, Regression, Clustering, Association, Logistic Regression, Decision Trees, Random Forest, Naïve Bayes, K-Nearest Neighbors(K-NN), Kernel SVM. Big Data Pig, Hive, Spark, HBase, Kafka, Sqoop, Cassandra, MongoDB. Databases Oracle, MySQL, MS SQL Server, Postgres. Analytics Python, R, SAS, SPSS, Excel. Visualization Tableau and PowerBI Operating Systems Windows, Unix/Linux, Mac Design Methodologies Agile, Scrum, Waterfall IDEs IntelliJ IDEA, Pycharm, Jupyter, Spyder  			  EXPERIENCE Credit Suisse, Raleigh - NC			 Data Scientist				Feb 2019 - Till date * Developed a Rule based engine that helps the business in identifying the potential conflicts during the business initiation phase using Python. * ed on configuring Apache Kafka to publish real-time business data to various topics. * ed on Kafka messaging systems for extracting the user input data that feeds to the Rules engine. * Created Restful Web Services that integrate with Apache Kafka and web interface using Flask. * Develop a user interface for the client to display the algorithm results for easy access of data using React JS and Bootstrap. * Involved in fixing and testing the python functions using unit testing.	 * Improved the ETL flow by building a python script that transfers data from MongoDB to Oracle database. Environment: Python, Kafka, React, Flask, MongoDB, Oracleand Git. MetLife, Charlotte - NC				 Data Scientist										June 2018 - Jan 2019 * Involved in tuning the Propensity models predicting the customer behavior to help Marketing and Finance team in analyzing various data sources to promote the growth of insurance products. * Involved in extracting data from various sources and performed data cleansing, data integration, data transformation, data mapping and loading data into Hadoop with Apache Spark using PySpark and SparkSQL. * Wrote SparkSQL scripts to Query multiple tables, performed joins and create tables to load data into Hive. * Applied Text Mining techniques to clean the marketing survey data using NLP techniques such as lemmatization and porter stemmer. * Wrote Hashing algorithm to secure PII data, and stored the data in the Hadoop. * ed on SAP Cloud services and ingested data from Oracle and MySQL data bases using Sqoop. * Prepared analysis reports and dashboards for the stakeholders using Tableau. Environment: Python, Spark, NLP, Hive, HDFS, SSIS, Oracle, SAP cloud, MySQL, Tableau and Git. JPMorgan Chase				 Data Scientist									Apr2017-Mar2018 * Provided recommendations by evaluating the customer life value to the business. * ed on Data cleaning, Data preparation and Feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn.  * Hands on experience in Dimensionality Reduction, Model selection and Model boosting methods using Principal Component Analysis (PCA), K-Fold Cross Validation and Gradient Tree Boosting. * Advanced knowledge of statistical techniques in Sampling, Probability, Multivariatedataanalysis, PCA, and Time-seriesanalysis using SAS. * Responsible for creating Hive tables, loading the structured data resulted from Map Reduce jobs into the tables and writing hive queries to further analyze the logs to identify issues and behavioral patterns. * Performing statistical data analysis and data visualization using R and Python. * Performed analysis of implementing Spark uses and wrote spark sample programs using PySpark.  * Implemented data refreshes on Tableau Server for biweekly and monthly increments based on business change to ensure that the views and dashboards were displaying the changed data accurately.  Environment: SQL, Informatica, SAS, Hive, Tableau, Python, GIT, Google cloud platform and Tableau. Wells Fargo, India					 Data Analyst/Scientist									Feb 2015-Aug 2016 * Involved in evaluating customer credit data and financial statements in order to determine the degree of risk involved in lending money. * Developed predictive solutions to support commercial banking team using machine learning algorithms such as Linear Regression, Logistic Regression, Naive Bayes, Decision Trees, Random Forest, Support Vector Machine in Python. * Conducted analysis in assessing customer behaviors with clustering algorithms such as K-Means Clustering and Hierarchical Clustering. * ed on data cleaning, data preparation and feature engineering with Python, including NumPy, SciPy, Matplotlib, Seaborn, Pandas, and Scikit-learn. * Implemented Agile Methodologies, Scrum stories and sprints in a Python based environment, along with data analytics and Excel data extracts.  * Used Hive and created Hive tables and involved in data loading and writing HiveUDFs.  * Experience designing and optimizing complex SQL queries involving table joins using MySQL. * ed in Tableau environment to create weekly, monthly, daily reports using tableau desktop & publish them to server. * ed on importing and exporting data from Oracle into HDFS using Sqoop. * ed on Excel using VLOOKUP, pivots, conditional formatting, large record sets, data manipulation and cleaning. * Used GIT as version control software to manage the source code and to keep track of changes to files which is fast and light weight system. Environment: Python, MySQL, SAS, HDFS, Hive, Excel, Tableau and GIT. Wells Fargo, India					 Data Analyst										May 2013-Jan 2015  * ed for Risk management team in identifying the risk involved in the Mortgage process by evaluating the customer and property records. * Involved in addressing a wide range of challenging problems using techniques from applied statistics, machine learning and data mining fields. * Experience in descriptive statistics and hypothesis testing using Chi-square, T-test, Pearson correlation and Analysis of variance (ANOVA). * Advanced knowledge of statistical techniques in Sampling, Probability, Multivariate data analysis, PCA, and Time-series analysis using SAS. * Analyzed Relational & Non-relational data using MySQL. * Contributed , Projectmanagement, and Business management functions to push the business forward with innovative solutions. * Performed data acquisition and exploratory data analysis in R. * Visualized team metrics and communicated to the higher management using PowerBI. Environment: R, MySQL, PowerBI and GIT.  Bankatlal Institute of Information , Hyderabad, India Data Analyst											Aug 2011-Apr2013 * As part of a project, ed on HTML, CSS and JavaScript to develop a web application for online applications * Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and * Data Formats * Involved in development of General Ledger module, which streamlined analysis, reporting and recording of * accounting information * Managed connectivity using JDBC for querying/inserting and data management including triggers and stored * procedures * Involved in maintaining student records in the database using Oracle * Resolved the data related issues such as: assessing data quality, data consolidation, evaluating existing datasources using MS EXCEL Environment: HTML, CSS, Oracle, MS Excel, Tableau ACADEMIC PROJECTS Big Data Modeling & Management * As part of course analyzed streaming data in real-time from a weather station and created plots. * Used Map-Reduce algorithms on Twitter data and Performed words count to study the presidential elections. Big Data Integration & Processing * As a final project, ed on Soccer world cup data to study the behavior of fans across the globe with the help of Twitter. * Using Population census data performed statistical calculations using Splunk and accessed Postgres database tables and manipulated data frames using Spark. * Involved in the retrieval of NoSQL data using MongoDB and performed data aggregation of Data Frames. Statistical Thinking for Data Science * ed with Machine Learning Algorithms: Neural Nets, Deep Learning, Net analysis, Supervised Learning, and Unsupervised Learning. * Importing data into Python from files native to other software such as Excel spreadsheets, Stata, SAS and MATLAB files from relational databases such as SQLite&PostgreSQL. * Reshaping and tidying data using techniques such as pivoting and melting. * Performed Time-series analysis and A/B testing. * Used Machine Learning components: Classification, Regression, and Clustering. R Programming for Data Science * Characterized relationships graphically, in the form of  statistics, and through simple linear regression models. * Used Regression Trees, Decision Trees, Random Forest, Ensemble Methods and conducted performance metrics analysis to find the best model to forecast currency trend and generate detailed reports in RStudio. * Used appropriate libraries to clean data using (tidyr, dplyr), visualize data (ggplot2), and finally tune and evaluate models.  Database Management System * Developed a car rental system using Java and MySQL as a part of Database Design course project to perform CRUD operations. Project and Change Management * As part of the team project, ed on doctors appointment website using Microsoft project 2013. Created business requirement documents, business use cases and system use cases. Conducted Cost-Benefit analysis, GAP Analysis, SWOT Analysis to validate the compatibility of the system infrastructure with the new business requirements.    "
"		 

Analyst with proven ability to succeed as part of team or individually, driven to providing actionable machine learning solutions along with excellent presentation .


Fowler College of Business, San Diego State University			                        Aug 2017 - May 2019           
Master of Science in Information Systems 						                             GPA: 3.55
Relevant Courses: Machine Learning, Business Analytics, Big Data, Relational Database Management, Statistics, Spark
Anna University					   	                  	             	           Aug 2006 - May 2010
Bachelor of Engineering in Computer Science						    	                  GPA: 3.1

 
* Programming Languages: Python, R, SQL, Matplotlib, NumPy, Pandas. Database: Sybase, SQL Server
* Big Data: Spark, PySpark, SparkML. Machine Learning Methodology: Supervised and Unsupervised Learning
* Analytical models: Linear Regression, Logistic Regression, PCA, SVM, Classification, Predictive Modeling
* : R Studio, Jupyter Notebook, Putty, AWS Spark, Microsoft Excel, Scikit-Learn, SciPy. 
* Others: Exploratory Data Analysis, Data visualization, NLP, Statistical Analysis, Regression Analysis, Correlation

 EXPERIENCE
Research Assistant (Data Scientist), San Diego State University, San Diego, US	          Feb 2019  May 2019
Churn rate prediction for Telecom Industry

* Cleansed data by applying various techniques like, missing value treatment, outlier treatment, data normalization.
* Identified group of customers who are likely to churn through exploratory analysis. Visualized the findings.
* Found out factors/variables that are responsible for churning by performing dataset slicing and dicing.
* Leveraged class imbalances technique from Python for dataset balancing and improved the accuracy in churn prediction by 50%.
* Increased revenue of the business by $6000 on monthly basis.
* Provided recommendations for customer engagement.  
Environment: Python 3.5, NumPy, Pandas, Matplotlib, Seaborn, Microsoft Excel, Microsoft PowerPoint.

Data Analyst/Software Engineer, HCL , Bangalore, India		            Jul 2010  Sep 2013        
Client: Deutsche Bank/State Street Bank
Project: Loan Default Prediction						          	           

* Coordinated and communicated with other departments in collecting client requirements.
* Conducted exploratory data analysis and discovered patterns on borrowers who are likely to default the loan using NumPy, Pandas and visualized the findings using Seaborn and Matplotlib. 
* Merged different data sets into a single data set and read data from various data sources like CSV, text, excel and json formats.
* Performed pre-processing steps like imputing missing values, removing highly correlated variables and converting categorical variables to dummy variables.
* Employed feature selection method to select best features to prevent the curse of dimensionality.
* Involved in model building process and incorporated cross-validation technique to avoid over-fitting and implemented classification algorithms like Logistic Regression, Decision Trees, Random Forest, Support Vector Machines.
* Validated the machine learning classifiers using AUC ROC curve and the accuracy. The accuracy of the best model is 82.7% using Decision Trees.
* Presented the findings to the  team.

Environment: Python, Pandas, NumPy, Scikit-Learn, Seaborn, PCA, Linear models, Non-linear models, Ensemble models.


Project: Portfolio Investment Banking System				             				             
* Involved in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance with timely delivery against aggressive deadlines.
* Developed stored procedures, written complex SQL queries using multiple table joins, sub-queries with knowledge of optimal software performance techniques and generated reports for business shareholders and customers.
* Migrated code from testing to production environment ensuring accuracy.
* Monitored systems post go live. Proactively delivered solutions for continuous improvement. 
* ed in scrum team setting as a developer with very good understanding of each of the scrum roles.
* As a scrum team player, liaised with product manager and product owner to understand client requirements and helped develop software that exceeds clients expectations.
* Collaborated with team members to eliminate unwanted dependencies of autosys batch jobs.
* Automated reports to end clients using TCL scripts and job scheduler that saved manual efforts of 72 hours/month.
* Improved the performance of stored procedure by using techniques like bulk insertion of data from the source to the database.	
Environment: Sybase, TCL scripting, SQL, python scripting, Putty, UNIX/Linux. 

		                                               
ACADEMIC 
1) Prediction of new user reservation using Airbnb: (4-person project).                                  Jan 2019  May 2019
 used: R, ggplot2, dplyr, PowerPoint, Neural Nets & Support Vector Machine (SVM), Boosting             
* Reduced dimension of the dataset from 10 million to 100,000s by using data wrangling technique.
2) Insights on donation made in 2016 presidential campaign						    Apr 2019
* Discovered valuable insights on total donation made to each campaign by all contributors and to that small contributors using Spark, Amazon Web Services cloud (AWS EMR and AWS S3).
3) Detection of insults in social commentary: (SciPy, Pandas, Naïve Bayes, SVM, Logistic Regression)    Jun 2018
* Handled end-to-end machine learning pipeline for sentiment classification.
4) Designed ER model for Bakery Management and implemented database solution using MS SQL.  Mar 2018






",Data Scientist,resume,"		   Analyst with proven ability to succeed as part of team or individually, driven to providing actionable machine learning solutions along with excellent presentation .   Fowler College of Business, San Diego State University			                        Aug 2017 - May 2019            Master of Science in Information Systems 						                             GPA: 3.55 Relevant Courses: Machine Learning, Business Analytics, Big Data, Relational Database Management, Statistics, Spark Anna University					   	                  	             	           Aug 2006 - May 2010 Bachelor of Engineering in Computer Science						    	                  GPA: 3.1    * Programming Languages: Python, R, SQL, Matplotlib, NumPy, Pandas. Database: Sybase, SQL Server * Big Data: Spark, PySpark, SparkML. Machine Learning Methodology: Supervised and Unsupervised Learning * Analytical models: Linear Regression, Logistic Regression, PCA, SVM, Classification, Predictive Modeling * : R Studio, Jupyter Notebook, Putty, AWS Spark, Microsoft Excel, Scikit-Learn, SciPy.  * Others: Exploratory Data Analysis, Data visualization, NLP, Statistical Analysis, Regression Analysis, Correlation   EXPERIENCE Research Assistant (Data Scientist), San Diego State University, San Diego, US	          Feb 2019  May 2019 Churn rate prediction for Telecom Industry  * Cleansed data by applying various techniques like, missing value treatment, outlier treatment, data normalization. * Identified group of customers who are likely to churn through exploratory analysis. Visualized the findings. * Found out factors/variables that are responsible for churning by performing dataset slicing and dicing. * Leveraged class imbalances technique from Python for dataset balancing and improved the accuracy in churn prediction by 50%. * Increased revenue of the business by $6000 on monthly basis. * Provided recommendations for customer engagement.   Environment: Python 3.5, NumPy, Pandas, Matplotlib, Seaborn, Microsoft Excel, Microsoft PowerPoint.  Data Analyst/Software Engineer, HCL , Bangalore, India		            Jul 2010  Sep 2013         Client: Deutsche Bank/State Street Bank Project: Loan Default Prediction						          	             * Coordinated and communicated with other departments in collecting client requirements. * Conducted exploratory data analysis and discovered patterns on borrowers who are likely to default the loan using NumPy, Pandas and visualized the findings using Seaborn and Matplotlib.  * Merged different data sets into a single data set and read data from various data sources like CSV, text, excel and json formats. * Performed pre-processing steps like imputing missing values, removing highly correlated variables and converting categorical variables to dummy variables. * Employed feature selection method to select best features to prevent the curse of dimensionality. * Involved in model building process and incorporated cross-validation technique to avoid over-fitting and implemented classification algorithms like Logistic Regression, Decision Trees, Random Forest, Support Vector Machines. * Validated the machine learning classifiers using AUC ROC curve and the accuracy. The accuracy of the best model is 82.7% using Decision Trees. * Presented the findings to the  team.  Environment: Python, Pandas, NumPy, Scikit-Learn, Seaborn, PCA, Linear models, Non-linear models, Ensemble models.   Project: Portfolio Investment Banking System				             				              * Involved in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance with timely delivery against aggressive deadlines. * Developed stored procedures, written complex SQL queries using multiple table joins, sub-queries with knowledge of optimal software performance techniques and generated reports for business shareholders and customers. * Migrated code from testing to production environment ensuring accuracy. * Monitored systems post go live. Proactively delivered solutions for continuous improvement.  * ed in scrum team setting as a developer with very good understanding of each of the scrum roles. * As a scrum team player, liaised with product manager and product owner to understand client requirements and helped develop software that exceeds clients expectations. * Collaborated with team members to eliminate unwanted dependencies of autosys batch jobs. * Automated reports to end clients using TCL scripts and job scheduler that saved manual efforts of 72 hours/month. * Improved the performance of stored procedure by using techniques like bulk insertion of data from the source to the database.	 Environment: Sybase, TCL scripting, SQL, python scripting, Putty, UNIX/Linux.   		                                                ACADEMIC  1) Prediction of new user reservation using Airbnb: (4-person project).                                  Jan 2019  May 2019  used: R, ggplot2, dplyr, PowerPoint, Neural Nets & Support Vector Machine (SVM), Boosting              * Reduced dimension of the dataset from 10 million to 100,000s by using data wrangling technique. 2) Insights on donation made in 2016 presidential campaign						    Apr 2019 * Discovered valuable insights on total donation made to each campaign by all contributors and to that small contributors using Spark, Amazon Web Services cloud (AWS EMR and AWS S3). 3) Detection of insults in social commentary: (SciPy, Pandas, Naïve Bayes, SVM, Logistic Regression)    Jun 2018 * Handled end-to-end machine learning pipeline for sentiment classification. 4) Designed ER model for Bakery Management and implemented database solution using MS SQL.  Mar 2018       "
"
 
* Around 8 years of hands on experience and comprehensive industry knowledge of Machine Learning, Statistical Modeling, Deep Learning, Data Analytics, Data Modeling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP), Artificial Intelligence algorithms, Business Intelligence.
* Having good experience in Analytics Models like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and PostgreSQL,Erwin.
* Strong knowledge in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance. 
* Experienced in Data Modeling techniques employing Data Warehousing concepts like star/snowflake schema and Extended Star.
* Expertise in applying Data Mining techniques and optimization techniques in B2B and B2C industries.
* Expertise in writing functional specifications, translating business requirements to  specifications, created/maintained/modified database design document with detailed description of logical entities and physical tables.
* Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system. 
* Expertise in Data Analysis, Data Migration, Data Profiling, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL  such as Informatica Power Center.
* Proficient in Machine Learning, Data/Text Mining, Statistical Analysis&Predictive Modeling. 
* Good Knowledge and experience in deep learning algorithms such as Artificial Neural net (ANN), Convolutional Neural Net (CNN) and Recurrent Neural Net (RNN), LSTM and RNN based speech recognition using Tensor Flow.  
* Excellent knowledge and experience in OLTP/OLAP System Study with focus on Oracle Hyperion Suite of , developing Database Schemas like Star schema and Snowflake schema (Fact Tables, Dimension Tables) used in relational, dimensional and multidimensional modeling, physical and logical Data Modeling using Erwin tool.
* Used Cognitive Science in Machine Learning for Neurofeedback training which is essential for intentional control of brain rhythms.
* Experienced in building data models using machine learning techniques for Classification, Regression, Clustering and Associative mining. 
* Good Knowledge on Natural Language Processing (NLP) and Time Series Analysis and Forecasting using ARIMA model in Python and R.
* Enabling rapid insights generation from adverse event Data via cognitive  to increase the translational research capabilities
* ing experience in Hadoop ecosystem and Apache Spark frame such as HDFS, MapReduce,HiveQL, SparkSQL, PySpark.
* Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2, S3, and EMR.
* Proficient in data visualization  such as Tableau, Python Matplotlib, R Shiny to create visually powerful and actionable interactive reports and dashboards.
* Expertise in building, publishing customized interactive reports and dashboards with customized parameters and user-filters using Tableau(9.x/10.x).
* Experienced in Agile methodology and SCRUM process.
* Strong business sense and abilities to communicate data insights to both  and non clients.
* Proficient in Python, experience building, and product ionizing end-to-end systems.
* Strong programming expertise (preferably in Python) and strong in Database SQL.
* Solid coding and engineering  preferably in Machine Learning.
* Exposure to python and python packages.
* Be a valued contributor in shaping the future of our products and services.




Bachelor of Computer Science 

 

Databases
MySQL, PostgreSQL, Oracle, HBase, Amazon Redshift, MS SQL Server 2016/2014/2012/2008 R2/2008, Teradata
Statistical Methods
Hypothetical Testing, ANOVA, Time Series,Confidence Intervals, Bayes Law, PrincipalComponent Analysis (PCA), Dimensionality Reduction, Cross-Validation, Auto-correlation
Machine Learning
Regression analysis, Bayesian Method, Decision Tree, Random Forests, SupportVector Machine, Neural Net, SentimentAnalysis, K-Means Clustering, KNN andEnsemble Method
Hadoop Ecosystem	
Hadoop 2.x, Spark 2.x, MapReduce, Hive, HDFS, Sqoop, Flume
Reporting 
Tableau Suite of  10.x, 9.x, 8.x which includes Desktop, Server and Online, Server Reporting Services(SSRS)
Languages
Python (2.x/3.x), R, SAS, SQL, T-SQL
Operating Systems
PowerShell, UNIX/UNIX Shell Scripting , Linux and Windows
Data Analytics 
Python (numpy, scipy, pandas, Gensim, Keras), R (Caret, Weka, ggplot).
Data Visualization
Tableau, Visualization packages, Matplotlib, Seaborn, ggplot2, Microsoft Office.
R Package
dplyr, sqldf, data table, Random Forest, gbm, caret, elastic net and all sort of Machine Learning Packages.
:


Role: Sr.Data scientist/Machine learning Engineer
Client: Best Buy, Minneapolis, MN  May 2018-Till Date

Description:Best Buy Co., Inc. is an American multinational consumer electronics retailer headquartered in Richfield, Minnesota. It was originally founded by Richard M. Schulze and James Wheeler in 1966 as an audio specialty store called Sound of Music. In 1983, it was re-branded under its current name with an emphasis placed on consumer electronics.Best Buy is the largest specialty retailer in the United States consumer electronics retail industry.

Responsibilities:
* Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, R a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories. 
* Participated in features engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learnpre-processing.
* Performing statistical analysis on textual data. Building Machine learning/ Deep Learning models in the domain of Natural Language
* Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.
* Application of various Machine Learning algorithms and statistical modeling like decision trees, regression models, neural nets, SVM, clustering to identify Volume using the scikit-learn package in python, Matlab.
* Create and build Dockers images for prototype deep learning models running on local GPU.
* Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau.
* Developed and implemented predictive models using machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, RandomForest, K-meansclustering, KNN, PCA and regularization for Data Analysis.
* Performed Data Collection, Data Cleaning, Data Visualization and developing Machine Learning  
Algorithms by using several packages: Numpy, Pandas, Scikit-learn and Matplotlib.
* Implemented various data pre-processing techniques to manipulate the unstructured, structured data  
and imbalanced data like SMOTE.  
* Clustered customers' actions data by using K-means clustering and Hierarchical clustering, then  
segmented them into different groups for further analyses.
* Built Support Vector Machine algorithms for detecting the fraud and dishonest behaviors of customers  
by using several packages: Scikit-learn, Numpy, Pandas in Python.
* Designed and developed NLP models for sentiment analysis. 
* Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in BusinessIntelligence and Data Visualization : Tableau, Microstrategy.
* Developed and evangelized best practices for statistical analysis of Big Data.
* Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client. 
* Developed deep learning algorithm that generated hedging strategies providing 15% ROI per month with a standard deviation of 2.7%(results based on testing strategies on real data for 3 months)
* Designed the Enterprise Conceptual, Logical, and Physical Data Model for Bulk Data StorageSystem using Embarcadero ER Studio, the data models were designed in 3NF.
* ed on machine learning on large size data using Spark and MapReduce. 
* Collaborated with data engineers and operation team to implement ETL process, wrote and optimized SQL queries to perform data extraction to fit the analytical requirements.
* Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from RedShift.
* Explored and analyzed the customer specific features by using SparkSQL.
* Performed data imputation using Scikit-learn package in Python.
* Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches. 
* Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning.
* Developed Spark/Scala,SAS and R programs for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources.
* Conducted analysis on assessing customer consuming behaviours and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-MeansClustering and Hierarchical Clustering.
* Implement deep learning algorithms to identify fraudulent transactions
* Built regression models include: Lasso, Ridge, SVRand XGboost to predict Customer Life Time Value.
* Built classification models include: Logistic Regression, SVM, Decision Tree, RandomForest to predict Customer Churn Rate.
* Used F-Score, AUC/ROC, Confusion Matrix, MAE, RMSE to evaluate different Model performance.

Environment: AWS RedShift, EC2, EMR, Hadoop Frame, S3,HDFS, Spark(Pyspark, MLlib, Spark SQL), Python 3.x (Scikit-Learn/Scipy/Numpy/Pandas/Matplotlib/Seaborn),Tableau Desktop (9.x/10.x), Tableau Server (9.x/10.x), Machine Learning (Regressions, KNN, SVM, Decision Tree, Random Forest, XGboost,LightGBM, Collaborative filtering, Ensemble),Deep Learning, Teradata, Git 2.x, Agile/SCRUM

Role: Data scientist/Machine learning Engineer
Client: Johnson and Johnson, Raritan, NJ				Jan 2017  Apr 2018

Description:
Johnson & Johnson is an investment holding company with interests in health care products. It engages in research and development, manufacture and sale of personal care hygienic products, pharmaceuticals and surgical equipment. The company operates through the following business segments.
Responsibilities:
* Tackled highly imbalanced Fraud dataset using undersampling, oversampling with SMOTE and cost sensitive algorithms with Python Scikit-learn.
* Wrote complex Spark SQL queries for data analysis to meet business requirement.
* Developed MapReduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS.
* Building Optimization models using Machine Learning, Deep Learning algorithms.
* ed on data cleaning and ensured Data Quality, consistency, integrity using Pandas, Numpy.
* Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing.
* Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn.
* Performed feature engineering, performed NLP by using some techniques like Word2Vec, BOW (Bag of Words), Tf-Idf, Word2Vec, Doc2Vec.  
* Performed Naïve Bayes, KNN, Logistic Regression, RandomForest, SVMandXGboost to identify whether a loan will default or not.
* Implemented Ensemble of Ridge, Lasso Regression and XGboost to predict the potential loan default loss.
* Used various Metrics (RMSE, MAE, F-Score, ROC and AUC) to evaluate the performance of each model. 
* Performed data cleaning and feature selection using MLlib package in PySpark and ing with deep learning frames.
* Actively involved in all phases of data science project life cycle including Data Extraction, Data Cleaning, Data Visualization and building Models.  
* Experience in ing with languages Python and R.
* Developed text mining models using Tensor Flow&NLP (NLTK, SpaCy and CoreNLP) on call transactions & social media interaction data for existing customer management.  
* Experienced in Agile methodology and SCRUM process. 
* Experience in Extract, Transfer and Load process using ETL  like Data Stage, Data Integrator and SSIS for Data migration and Data Warehousing projects.  
* Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSAS, SSIS and SSRS.  
* Used big data  Spark(Pyspark, SparkSQL and MLlib) to conduct Realtime analysis of loan default based onAWS.
Environment:MS SQL Server 2014, Teradata, ETL, SSIS, Alteryx, Tableau (Desktop 9.x/Server 9.x), Python3.x(Scikit-Learn/Scipy/Numpy/Pandas), Machine Learning (Naïve Bayes, KNN, Regressions, Random Forest, SVM, XGboost, Ensemble), AWS Redshift, Deep Learning, Spark(PySpark, MLlib, Spark SQL), Hadoop 2.x, Map Reduce, HDFS, SharePoint.

Role: Data Scientist
Client: RetailMeNot INC, Austin, TX                                         Nov 2015  Dec2016

Description:RetailMeNot, Inc. is a leading digital savings destination connecting consumers with retailers, restaurants and brands, both online and in-store. The company enables consumers across the globe to find hundreds of thousands of digital offers and discounted gift cards to save money while they shop or dine out.
Responsibilities:
* Gathered, analyzed, documented and translated application requirements into data models and Supports standardization of documentation and the adoption of standards and practices related to data and applications.
* Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Sqoop, Pig, Flume, Hive, MapReduce and HDFS.
* Automated csv to chatbot friendly Json transformation by writing NLP scripts to minimize development time by 20%.
* Wrote user defined functions (UDFs) in Hive to manipulate strings, dates and other data.
* Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python.
* Applied clustering algorithms i.e. Hierarchical, K-means usingScikit and Scipy. 
* Created logical data model from the conceptual model and it's conversion into the physical database design using ERWIN. 
* Mapped business needs/requirements to subject area model and to logical enterprise model. 
* ed with DBA's to create a best fit physical data model from the logical data model
* Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities. 
* Enforced referential integrity in the OLTPData Model for consistent relationship between tables and efficient database design. 
* Developed the data warehouse model (star schema) for the proposed central model for the project. 
* Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using ERWIN tool.
* ed on the Snow-flaking the Dimensions to remove redundancy. 
* ed in using Teradata14  like Fast Load, Multi Load, T Pump, Fast Export, TeradataParallel Transporter (TPT) and BTEQ. 
* Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required. 
* Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems.

Environment: Machine learning(KNN, Clustering, Regressions, Random Forest, SVM,Ensemble), Linux, Python 2.x (Scikit-Learn/Scipy/Numpy/Pandas), R, Tableau (Desktop 8.x/Server 8.x), Hadoop, Map Reduce,HDFS, Hive, Pig, HBase,Sqoop, Flume,Oracle 11g, SQL Server 2012.


Role: BI Developer/Data Analyst
Client: Deutsche Bank, New York City, NYMay 2014  Oct 2015

Description: Deutsche Bank is a leading global investment bank with a strong and profitable private clients franchise. The Project was to implement machine learning techniques and develop statistical models to identify loan default pattern and predict potential default loss for the company.

 Responsibilities:

* Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.	
* Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2. 
* Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions. 
* Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index. 
* Created SSIS packages using Pivot Transformation, Fuzzy Lookup, Derived Columns, ConditionSplit, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task.
* Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services (SSIS).
* Developed and implemented several types of Financial Reports (Income Statement, Profit& Loss Statement, EBIT, ROIC Reports) by using SSRS. 
* Collaborated with database engineers to implement ETL process, wrote and optimized SQL queries to perform data extraction and merging from SQL server database.
* Created Complex ETL Packages using SSIS to extract data from staging tables to partitioned tables with incremental load.  
* Gathered, analyzed, and translated business requirements, communicated with other departments to collected client business requirements and access available data.  
* Migrating data from Legacy system to SQL Server using SQL Server Integration Services 2012.
* Used C# scripts to map records.  
*  Involved in writing complex SQL Queries, Stored Procedures, Triggers, Views, Cursors, Joins, Constraints, DDL, DML and User Defined Functions to implement the business logic and created clustered and non-clustered indexes.  
* Created and modified Stored Procedures, Functions, and Indexes.  
* Developed SQL Scripts to Insert/Update and Delete data in MS SQL database tables.  
* Created various ad-hoc SQL queries for customer reports, executive management reports and types of report types like tables, matrix, sub reports etc.  
* Designed and developed new reports and maintained existing reports using Microsoft SQLReporting Services (SSRS) and Microsoft Excel to support the firm's strategy and management. 
Created sub-reports, drill down reports,  reports, parameterized reports, and ad-hoc reports using SSRS.
* Used SAS/SQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements. 
* Used SAS for pre-processing data, SQL queries, Data Analysis, generating reports, Graphics, and Statistical analyses.
* Provided statistical research analyses and Data Modeling support for mortgage product.
* Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysisusing SAS programming.

Environment: SQL Server 2008 R2, DB2,Oracle,SQL Server Management Studio, SAS/ BASE, SAS/SQL, SAS/Enterprise Guide, MS BI Suite(SSIS/SSRS), T-SQL, SharePoint 2010, Visual Studio 2010, Agile/SCRUM

Role: Data Analyst
Client: Exceloid Soft Systems, India  Jan 2013  Apr 2014

Description: By implementing Exceloid's made-for-future technological strategies excellent ing with Exceloid Soft Systems in the initial days of our retail journey in India. I think they are among the best Openbravo specialist we ed with in India.
 Responsibilities:
* Wrote SQL queries for data validation on the backend systems and used various  like TOAD&DBVisualizer for DBMS(Oracle).
* Perform Data analysis, Backend Database testing, Data Modeling and Developing SQL Queries to solve problems and meet user's need for Database management in Data Warehouse.
* Utilize object-oriented languages, concepts, database design, star schemas and databases.
* Create algorithms as needed to manage and implement proposed solutions. 
* Participate in test planning and test execution for functional, system, integration, regression, UAT (User Acceptance Testing), load and performance testing.
*  with test automation  for recording/coding in Database, and execute in regression testing cycles. 
* Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server. 
* ing with Databases DB2, Oracle DM, SQL Server for Database testing and maintenance. 
* Involved in writing and executing User Acceptance Testing (UAT) with end users.
* Involved in Post- Implementation validations after the changes have been to the Data Marts. 
* Chart out Graphs, and Reports alike in QC to point out the percentage of Test Cases passed, and thereby to point out the percentage of Quality achieved and uploading the status daily to ART reports an in-house tool. 
* Performed extensive Data Validation, Data Verification against Data Warehouse. 
* Used UNIX to check the Data marts, Tables and Updates made to the tables. 
* Writing advanced SQL Queries to query the data from Data marts and Landings to verify the changes has been made. 
* Involved in Client requirement gathering, participated in discussion & brain storming sessions and documented requirements. 
* Validating and profilingFlat File Data into Teradata tables using UNIX Shell scripts. 
* Actively participated Functional, System and User Acceptance testing on all builds and supervised releases to ensure system / functionality integrity. 
* Closely interacted with designers and software developers to understand application functionality and navigational flow and keep them updated about Business user sentiments. 
* Interacted with developers to resolve different Quality Related Issues.
* Wrote and executed manual test cases for functional, GUI, and regression testing of the application to make sure that new enhancements do not break ing features 
* Writing and executing Manual test cases in HP Quality Center.
* Wrote test plans for positive and negative scenarios for GUI and functional testing
* Involved in writing SQL queries and stored procedures using Query Analyzer and matched the results retrieved from the batch log files 
* Created Project Charter documents & Detailed Requirement document and reviewed with Development & other stake holders. 

Environment: Subversion, TortoiseSVN, Jira, Agile-Scrum, Web Services, Mainframe, Oracle, Perl, UNIX, LINUX, Shell Scripts, UML, Quality Center, RequisitePro, SQL, MS Visio, MS Project, Excel, Power Point, Word, SharePoint, Win XP/7 Enterprise.

Role: Data Analyst/Data Modeler
Client: ZEN3 Info Solutions, India                             May 2011  Dec 2012


Description: Zen3 is a leading software solutions group developing innovative solutions for media, travel and  industries.
  Responsibilities:
* Data analysis and reporting using MY SQL, MS Power Point, MS Access and SQL assistant.
* Involved in MY SQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome.
* Used DB2 Adapters to integrate between Oracle database and Microsoft SQL database in order to transfer data.
* Designed the data marts using the Ralph Kimball's DimensionalData Mart modeling methodology using ER Studio. 
* Involved in writing T-SQL, ing on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration.
* Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems. 
* Initiated and conducted JAD sessions inviting various teams to finalize the required data fields and their formats. 
* Involved in designing and implementing the Data Extraction (XML DATA stream) procedures. 
* Created base tables, views, and index. Built a complex Oracle procedure in PL/SQL for extract, loading, transforming the data into the warehouse via DBMSScheduler from the internal data. 
* Involved in writing scripts for loading data to target data Warehouse using BTEQ, Fast Load, MultiLoad.
* Create ETL scripts using Regular Expressions and custom  (Informatica, Pentaho, and Sync Sort) to ETL data. 
* Developed SQLService Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).  
* Extensively involved in Recovery process for capturing the incremental changes in the source systems for updating in the staging area and data warehouse respectively 
* Strong knowledge of Entity-Relationship concept, Facts and dimensions tables, slowly changing dimensions and Dimensional Modeling (Star Schema and Snow Flake Schema).
* Involved in loading data between Netezza tables using NZSQL utility. 
* ed on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling. 
* Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle8.x wait events, time consuming SQL queries, table space growth, and database growth.

Environment: ER Studio, MY SQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, Informatica MDM, SSIS, SSRS, SSAS, ETL, MDM, 3NF and De-normalization, Teradata, Oracle8.x, (Star Schema and Snow Flake Schema) etc.








",Data Scientist,resume,"   * Around 8 years of hands on experience and comprehensive industry knowledge of Machine Learning, Statistical Modeling, Deep Learning, Data Analytics, Data Modeling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP), Artificial Intelligence algorithms, Business Intelligence. * Having good experience in Analytics Models like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and PostgreSQL,Erwin. * Strong knowledge in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation and maintenance.  * Experienced in Data Modeling techniques employing Data Warehousing concepts like star/snowflake schema and Extended Star. * Expertise in applying Data Mining techniques and optimization techniques in B2B and B2C industries. * Expertise in writing functional specifications, translating business requirements to  specifications, created/maintained/modified database design document with detailed description of logical entities and physical tables. * Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system.  * Expertise in Data Analysis, Data Migration, Data Profiling, Data Cleansing, Transformation, Integration, Data Import, and Data Export through the use of multiple ETL  such as Informatica Power Center. * Proficient in Machine Learning, Data/Text Mining, Statistical Analysis&Predictive Modeling.  * Good Knowledge and experience in deep learning algorithms such as Artificial Neural net (ANN), Convolutional Neural Net (CNN) and Recurrent Neural Net (RNN), LSTM and RNN based speech recognition using Tensor Flow.   * Excellent knowledge and experience in OLTP/OLAP System Study with focus on Oracle Hyperion Suite of , developing Database Schemas like Star schema and Snowflake schema (Fact Tables, Dimension Tables) used in relational, dimensional and multidimensional modeling, physical and logical Data Modeling using Erwin tool. * Used Cognitive Science in Machine Learning for Neurofeedback training which is essential for intentional control of brain rhythms. * Experienced in building data models using machine learning techniques for Classification, Regression, Clustering and Associative mining.  * Good Knowledge on Natural Language Processing (NLP) and Time Series Analysis and Forecasting using ARIMA model in Python and R. * Enabling rapid insights generation from adverse event Data via cognitive  to increase the translational research capabilities * ing experience in Hadoop ecosystem and Apache Spark frame such as HDFS, MapReduce,HiveQL, SparkSQL, PySpark. * Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includes services like EC2, S3, and EMR. * Proficient in data visualization  such as Tableau, Python Matplotlib, R Shiny to create visually powerful and actionable interactive reports and dashboards. * Expertise in building, publishing customized interactive reports and dashboards with customized parameters and user-filters using Tableau(9.x/10.x). * Experienced in Agile methodology and SCRUM process. * Strong business sense and abilities to communicate data insights to both  and non clients. * Proficient in Python, experience building, and product ionizing end-to-end systems. * Strong programming expertise (preferably in Python) and strong in Database SQL. * Solid coding and engineering  preferably in Machine Learning. * Exposure to python and python packages. * Be a valued contributor in shaping the future of our products and services.     Bachelor of Computer Science      Databases MySQL, PostgreSQL, Oracle, HBase, Amazon Redshift, MS SQL Server 2016/2014/2012/2008 R2/2008, Teradata Statistical Methods Hypothetical Testing, ANOVA, Time Series,Confidence Intervals, Bayes Law, PrincipalComponent Analysis (PCA), Dimensionality Reduction, Cross-Validation, Auto-correlation Machine Learning Regression analysis, Bayesian Method, Decision Tree, Random Forests, SupportVector Machine, Neural Net, SentimentAnalysis, K-Means Clustering, KNN andEnsemble Method Hadoop Ecosystem	 Hadoop 2.x, Spark 2.x, MapReduce, Hive, HDFS, Sqoop, Flume Reporting  Tableau Suite of  10.x, 9.x, 8.x which includes Desktop, Server and Online, Server Reporting Services(SSRS) Languages Python (2.x/3.x), R, SAS, SQL, T-SQL Operating Systems PowerShell, UNIX/UNIX Shell Scripting , Linux and Windows Data Analytics  Python (numpy, scipy, pandas, Gensim, Keras), R (Caret, Weka, ggplot). Data Visualization Tableau, Visualization packages, Matplotlib, Seaborn, ggplot2, Microsoft Office. R Package dplyr, sqldf, data table, Random Forest, gbm, caret, elastic net and all sort of Machine Learning Packages. :   Role: Sr.Data scientist/Machine learning Engineer Client: Best Buy, Minneapolis, MN  May 2018-Till Date  Description:Best Buy Co., Inc. is an American multinational consumer electronics retailer headquartered in Richfield, Minnesota. It was originally founded by Richard M. Schulze and James Wheeler in 1966 as an audio specialty store called Sound of Music. In 1983, it was re-branded under its current name with an emphasis placed on consumer electronics.Best Buy is the largest specialty retailer in the United States consumer electronics retail industry.  Responsibilities: * Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLlib, R a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. and Utilized the engine to increase user lifetime by 45% and triple user conversations for target categories.  * Participated in features engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learnpre-processing. * Performing statistical analysis on textual data. Building Machine learning/ Deep Learning models in the domain of Natural Language * Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes. * Application of various Machine Learning algorithms and statistical modeling like decision trees, regression models, neural nets, SVM, clustering to identify Volume using the scikit-learn package in python, Matlab. * Create and build Dockers images for prototype deep learning models running on local GPU. * Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data. Created various types of data visualizations using Python and Tableau. * Developed and implemented predictive models using machine learning algorithms such as linear regression, classification, multivariate regression, Naive Bayes, RandomForest, K-meansclustering, KNN, PCA and regularization for Data Analysis. * Performed Data Collection, Data Cleaning, Data Visualization and developing Machine Learning   Algorithms by using several packages: Numpy, Pandas, Scikit-learn and Matplotlib. * Implemented various data pre-processing techniques to manipulate the unstructured, structured data   and imbalanced data like SMOTE.   * Clustered customers' actions data by using K-means clustering and Hierarchical clustering, then   segmented them into different groups for further analyses. * Built Support Vector Machine algorithms for detecting the fraud and dishonest behaviors of customers   by using several packages: Scikit-learn, Numpy, Pandas in Python. * Designed and developed NLP models for sentiment analysis.  * Led discussions with users to gather business processes requirements and data requirements to develop a variety of Conceptual, Logical and Physical Data Models. Expert in BusinessIntelligence and Data Visualization : Tableau, Microstrategy. * Developed and evangelized best practices for statistical analysis of Big Data. * Designed and implemented system architecture for Amazon EC2 based cloud-hosted solution for client.  * Developed deep learning algorithm that generated hedging strategies providing 15% ROI per month with a standard deviation of 2.7%(results based on testing strategies on real data for 3 months) * Designed the Enterprise Conceptual, Logical, and Physical Data Model for Bulk Data StorageSystem using Embarcadero ER Studio, the data models were designed in 3NF. * ed on machine learning on large size data using Spark and MapReduce.  * Collaborated with data engineers and operation team to implement ETL process, wrote and optimized SQL queries to perform data extraction to fit the analytical requirements. * Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from RedShift. * Explored and analyzed the customer specific features by using SparkSQL. * Performed data imputation using Scikit-learn package in Python. * Let the implementation of new statistical algorithms and operators on Hadoop and SQL platforms and utilized optimizations techniques, linear regressions, K-means clustering, Native Bayes and other approaches.  * Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning. * Developed Spark/Scala,SAS and R programs for regular expression (regex) project in the Hadoop/Hive environment with Linux/Windows for big data resources. * Conducted analysis on assessing customer consuming behaviours and discover value of customers with RMF analysis; applied customer segmentation with clustering algorithms such as K-MeansClustering and Hierarchical Clustering. * Implement deep learning algorithms to identify fraudulent transactions * Built regression models include: Lasso, Ridge, SVRand XGboost to predict Customer Life Time Value. * Built classification models include: Logistic Regression, SVM, Decision Tree, RandomForest to predict Customer Churn Rate. * Used F-Score, AUC/ROC, Confusion Matrix, MAE, RMSE to evaluate different Model performance.  Environment: AWS RedShift, EC2, EMR, Hadoop Frame, S3,HDFS, Spark(Pyspark, MLlib, Spark SQL), Python 3.x (Scikit-Learn/Scipy/Numpy/Pandas/Matplotlib/Seaborn),Tableau Desktop (9.x/10.x), Tableau Server (9.x/10.x), Machine Learning (Regressions, KNN, SVM, Decision Tree, Random Forest, XGboost,LightGBM, Collaborative filtering, Ensemble),Deep Learning, Teradata, Git 2.x, Agile/SCRUM  Role: Data scientist/Machine learning Engineer Client: Johnson and Johnson, Raritan, NJ				Jan 2017  Apr 2018  Description: Johnson & Johnson is an investment holding company with interests in health care products. It engages in research and development, manufacture and sale of personal care hygienic products, pharmaceuticals and surgical equipment. The company operates through the following business segments. Responsibilities: * Tackled highly imbalanced Fraud dataset using undersampling, oversampling with SMOTE and cost sensitive algorithms with Python Scikit-learn. * Wrote complex Spark SQL queries for data analysis to meet business requirement. * Developed MapReduce/Spark Python modules for predictive analytics & machine learning in Hadoop on AWS. * Building Optimization models using Machine Learning, Deep Learning algorithms. * ed on data cleaning and ensured Data Quality, consistency, integrity using Pandas, Numpy. * Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with Scikit-learn preprocessing. * Improved fraud prediction performance by using random forest and gradient boosting for feature selection with Python Scikit-learn. * Performed feature engineering, performed NLP by using some techniques like Word2Vec, BOW (Bag of Words), Tf-Idf, Word2Vec, Doc2Vec.   * Performed Naïve Bayes, KNN, Logistic Regression, RandomForest, SVMandXGboost to identify whether a loan will default or not. * Implemented Ensemble of Ridge, Lasso Regression and XGboost to predict the potential loan default loss. * Used various Metrics (RMSE, MAE, F-Score, ROC and AUC) to evaluate the performance of each model.  * Performed data cleaning and feature selection using MLlib package in PySpark and ing with deep learning frames. * Actively involved in all phases of data science project life cycle including Data Extraction, Data Cleaning, Data Visualization and building Models.   * Experience in ing with languages Python and R. * Developed text mining models using Tensor Flow&NLP (NLTK, SpaCy and CoreNLP) on call transactions & social media interaction data for existing customer management.   * Experienced in Agile methodology and SCRUM process.  * Experience in Extract, Transfer and Load process using ETL  like Data Stage, Data Integrator and SSIS for Data migration and Data Warehousing projects.   * Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSAS, SSIS and SSRS.   * Used big data  Spark(Pyspark, SparkSQL and MLlib) to conduct Realtime analysis of loan default based onAWS. Environment:MS SQL Server 2014, Teradata, ETL, SSIS, Alteryx, Tableau (Desktop 9.x/Server 9.x), Python3.x(Scikit-Learn/Scipy/Numpy/Pandas), Machine Learning (Naïve Bayes, KNN, Regressions, Random Forest, SVM, XGboost, Ensemble), AWS Redshift, Deep Learning, Spark(PySpark, MLlib, Spark SQL), Hadoop 2.x, Map Reduce, HDFS, SharePoint.  Role: Data Scientist Client: RetailMeNot INC, Austin, TX                                         Nov 2015  Dec2016  Description:RetailMeNot, Inc. is a leading digital savings destination connecting consumers with retailers, restaurants and brands, both online and in-store. The company enables consumers across the globe to find hundreds of thousands of digital offers and discounted gift cards to save money while they shop or dine out. Responsibilities: * Gathered, analyzed, documented and translated application requirements into data models and Supports standardization of documentation and the adoption of standards and practices related to data and applications. * Participated in Data Acquisition with Data Engineer team to extract historical and real-time data by using Sqoop, Pig, Flume, Hive, MapReduce and HDFS. * Automated csv to chatbot friendly Json transformation by writing NLP scripts to minimize development time by 20%. * Wrote user defined functions (UDFs) in Hive to manipulate strings, dates and other data. * Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python. * Applied clustering algorithms i.e. Hierarchical, K-means usingScikit and Scipy.  * Created logical data model from the conceptual model and it's conversion into the physical database design using ERWIN.  * Mapped business needs/requirements to subject area model and to logical enterprise model.  * ed with DBA's to create a best fit physical data model from the logical data model * Redefined many attributes and relationships in the reverse engineered model and cleansed unwanted tables/ columns as part of data analysis responsibilities.  * Enforced referential integrity in the OLTPData Model for consistent relationship between tables and efficient database design.  * Developed the data warehouse model (star schema) for the proposed central model for the project.  * Created 3NF business area data modeling with de-normalized physical implementation data and information requirements analysis using ERWIN tool. * ed on the Snow-flaking the Dimensions to remove redundancy.  * ed in using Teradata14  like Fast Load, Multi Load, T Pump, Fast Export, TeradataParallel Transporter (TPT) and BTEQ.  * Helped in migration and conversion of data from the Sybase database into Oracle database, preparing mapping documents and developing partial SQL scripts as required.  * Generated ad-hoc SQL queries using joins, database connections and transformation rules to fetch data from legacy Oracle and SQL Server database systems.  Environment: Machine learning(KNN, Clustering, Regressions, Random Forest, SVM,Ensemble), Linux, Python 2.x (Scikit-Learn/Scipy/Numpy/Pandas), R, Tableau (Desktop 8.x/Server 8.x), Hadoop, Map Reduce,HDFS, Hive, Pig, HBase,Sqoop, Flume,Oracle 11g, SQL Server 2012.   Role: BI Developer/Data Analyst Client: Deutsche Bank, New York City, NYMay 2014  Oct 2015  Description: Deutsche Bank is a leading global investment bank with a strong and profitable private clients franchise. The Project was to implement machine learning techniques and develop statistical models to identify loan default pattern and predict potential default loss for the company.   Responsibilities:  * Used SSIS to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse and Data Mart.	 * Maintained and developed complex SQL queries, stored procedures, views, functions and reports that meet customer requirements using Microsoft SQL Server 2008 R2.  * Created Views and Table-valued Functions, Common Table Expression (CTE), joins, complex subqueries to provide the reporting solutions.  * Optimized the performance of queries with modification in T-SQL queries, removed the unnecessary columns and redundant data, normalized tables, established joins and created index.  * Created SSIS packages using Pivot Transformation, Fuzzy Lookup, Derived Columns, ConditionSplit, Aggregate, Execute SQL Task, Data Flow Task and Execute Package Task. * Migrated data from SAS environment to SQL Server 2008 via SQL Integration Services (SSIS). * Developed and implemented several types of Financial Reports (Income Statement, Profit& Loss Statement, EBIT, ROIC Reports) by using SSRS.  * Collaborated with database engineers to implement ETL process, wrote and optimized SQL queries to perform data extraction and merging from SQL server database. * Created Complex ETL Packages using SSIS to extract data from staging tables to partitioned tables with incremental load.   * Gathered, analyzed, and translated business requirements, communicated with other departments to collected client business requirements and access available data.   * Migrating data from Legacy system to SQL Server using SQL Server Integration Services 2012. * Used C# scripts to map records.   *  Involved in writing complex SQL Queries, Stored Procedures, Triggers, Views, Cursors, Joins, Constraints, DDL, DML and User Defined Functions to implement the business logic and created clustered and non-clustered indexes.   * Created and modified Stored Procedures, Functions, and Indexes.   * Developed SQL Scripts to Insert/Update and Delete data in MS SQL database tables.   * Created various ad-hoc SQL queries for customer reports, executive management reports and types of report types like tables, matrix, sub reports etc.   * Designed and developed new reports and maintained existing reports using Microsoft SQLReporting Services (SSRS) and Microsoft Excel to support the firm's strategy and management.  Created sub-reports, drill down reports,  reports, parameterized reports, and ad-hoc reports using SSRS. * Used SAS/SQL to pull data out from databases and aggregate to provide detailed reporting based on the user requirements.  * Used SAS for pre-processing data, SQL queries, Data Analysis, generating reports, Graphics, and Statistical analyses. * Provided statistical research analyses and Data Modeling support for mortgage product. * Perform analyses such as regression analysis, logistic regression, discriminant analysis, cluster analysisusing SAS programming.  Environment: SQL Server 2008 R2, DB2,Oracle,SQL Server Management Studio, SAS/ BASE, SAS/SQL, SAS/Enterprise Guide, MS BI Suite(SSIS/SSRS), T-SQL, SharePoint 2010, Visual Studio 2010, Agile/SCRUM  Role: Data Analyst Client: Exceloid Soft Systems, India  Jan 2013  Apr 2014  Description: By implementing Exceloid's made-for-future technological strategies excellent ing with Exceloid Soft Systems in the initial days of our retail journey in India. I think they are among the best Openbravo specialist we ed with in India.  Responsibilities: * Wrote SQL queries for data validation on the backend systems and used various  like TOAD&DBVisualizer for DBMS(Oracle). * Perform Data analysis, Backend Database testing, Data Modeling and Developing SQL Queries to solve problems and meet user's need for Database management in Data Warehouse. * Utilize object-oriented languages, concepts, database design, star schemas and databases. * Create algorithms as needed to manage and implement proposed solutions.  * Participate in test planning and test execution for functional, system, integration, regression, UAT (User Acceptance Testing), load and performance testing. *  with test automation  for recording/coding in Database, and execute in regression testing cycles.  * Transferred data from various OLTP data sources, such as Oracle, MS Access, MS Excel, Flat files, CSV files into SQL Server.  * ing with Databases DB2, Oracle DM, SQL Server for Database testing and maintenance.  * Involved in writing and executing User Acceptance Testing (UAT) with end users. * Involved in Post- Implementation validations after the changes have been to the Data Marts.  * Chart out Graphs, and Reports alike in QC to point out the percentage of Test Cases passed, and thereby to point out the percentage of Quality achieved and uploading the status daily to ART reports an in-house tool.  * Performed extensive Data Validation, Data Verification against Data Warehouse.  * Used UNIX to check the Data marts, Tables and Updates made to the tables.  * Writing advanced SQL Queries to query the data from Data marts and Landings to verify the changes has been made.  * Involved in Client requirement gathering, participated in discussion & brain storming sessions and documented requirements.  * Validating and profilingFlat File Data into Teradata tables using UNIX Shell scripts.  * Actively participated Functional, System and User Acceptance testing on all builds and supervised releases to ensure system / functionality integrity.  * Closely interacted with designers and software developers to understand application functionality and navigational flow and keep them updated about Business user sentiments.  * Interacted with developers to resolve different Quality Related Issues. * Wrote and executed manual test cases for functional, GUI, and regression testing of the application to make sure that new enhancements do not break ing features  * Writing and executing Manual test cases in HP Quality Center. * Wrote test plans for positive and negative scenarios for GUI and functional testing * Involved in writing SQL queries and stored procedures using Query Analyzer and matched the results retrieved from the batch log files  * Created Project Charter documents & Detailed Requirement document and reviewed with Development & other stake holders.   Environment: Subversion, TortoiseSVN, Jira, Agile-Scrum, Web Services, Mainframe, Oracle, Perl, UNIX, LINUX, Shell Scripts, UML, Quality Center, RequisitePro, SQL, MS Visio, MS Project, Excel, Power Point, Word, SharePoint, Win XP/7 Enterprise.  Role: Data Analyst/Data Modeler Client: ZEN3 Info Solutions, India                             May 2011  Dec 2012   Description: Zen3 is a leading software solutions group developing innovative solutions for media, travel and  industries.   Responsibilities: * Data analysis and reporting using MY SQL, MS Power Point, MS Access and SQL assistant. * Involved in MY SQL, MS Power Point, MS Access Database design and design new database on Netezza which will have optimized outcome. * Used DB2 Adapters to integrate between Oracle database and Microsoft SQL database in order to transfer data. * Designed the data marts using the Ralph Kimball's DimensionalData Mart modeling methodology using ER Studio.  * Involved in writing T-SQL, ing on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing and Data Migration. * Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP systems.  * Initiated and conducted JAD sessions inviting various teams to finalize the required data fields and their formats.  * Involved in designing and implementing the Data Extraction (XML DATA stream) procedures.  * Created base tables, views, and index. Built a complex Oracle procedure in PL/SQL for extract, loading, transforming the data into the warehouse via DBMSScheduler from the internal data.  * Involved in writing scripts for loading data to target data Warehouse using BTEQ, Fast Load, MultiLoad. * Create ETL scripts using Regular Expressions and custom  (Informatica, Pentaho, and Sync Sort) to ETL data.  * Developed SQLService Broker to flow and sync of data from MS-I to Microsoft's master database management (MDM).   * Extensively involved in Recovery process for capturing the incremental changes in the source systems for updating in the staging area and data warehouse respectively  * Strong knowledge of Entity-Relationship concept, Facts and dimensions tables, slowly changing dimensions and Dimensional Modeling (Star Schema and Snow Flake Schema). * Involved in loading data between Netezza tables using NZSQL utility.  * ed on Data modeling using Dimensional Data Modeling, Star Schema/Snow Flake schema, and Fact & Dimensional, Physical & Logical data modeling.  * Generated Stats pack/AWR reports from Oracle database and analyzed the reports for Oracle8.x wait events, time consuming SQL queries, table space growth, and database growth.  Environment: ER Studio, MY SQL, MS Power Point, MS Access, MY SQL, MS Power Point, MS Access, Netezza, DB2, T-SQL, DTS, Informatica MDM, SSIS, SSRS, SSAS, ETL, MDM, 3NF and De-normalization, Teradata, Oracle8.x, (Star Schema and Snow Flake Schema) etc.         "
"Willing to relocate to: Los Angeles, CA - Laughlin, NV - Fort Mohave, AZ
 Experience

Consultant Business Intelligence and Pricing Analyst
Panasonic - Lake Forest, CA
September 2018 to July 2019
 Developed and led the initiative for the monthly executive report, which provided data-driveninsights to support decision-makers by utilizing Python libraries and MS Power BI for data visualization.
 
 Collaborated with cross-functional teams to understand the data challenges of the organization anddevelop solutions, which increased ancillary revenue by approximately 20%.
Consultant Data and Post Market Analyst
Johnson & Johnson - Orange County, California, US
July 2017 to September 2018
 Analyzed large dataset related to post-market complaints and implementing statistical models todiscover potential anomalies to ensure compliance with all applicable corporate and federal agencies using MS SQL, Excel, and Python. 
 Improved operational efficiency, which reduced costs by automating 40% of ad-hoc activities,reporting, and data visualization by utilizing Tableau and SSRS.
Business Insight Analyst
ATV Inc., Los Angeles County - Los Angeles, CA December 2016 to June 2017
 Performed thorough research and analysis on market pricing to determine the best target price forproducts and services using Excel, Python, SQL, and other proprietary . 
 Automated ETL process by 80% using Python scripts and SQL Server stored procedures.
Customer Project / Program Manager III
Hewlett Packard Enterprise - Hong Kong, HK
July 2012 to June 2015
 ed with one of the largest conglomerates in Australia and New Zealand to ensure thatorganizational s are achieved. 
 Managed project scope, schedule, resources, and costs using the most appropriate verificationtechniques. 
Supv, ITO Svc Delivery Consultant II 
 Managed a team of ETL analyst and database administrators using appropriate  and techniqueswhich resulted to consistently exceeding operational performance goals, and quality expectations. 
 Delivered SIP (Service Improvement Plans) to advance the quality of service and service deliveryeffectiveness by identifying opportunities from a large dataset and develop data-driven strategies.
Programmer Analyst III
Amkor  Inc - Manila
December 2006 to June 2012
 Used Oracle PL/SQL, VB.NET, and Webmethods to design and develop ETL solutions. 
 Pioneered in Customer Focus -SME Team, who performed data analysis, disaster recovery,and problem resolution for high priority incident. Reduced ticket resolution time to 15%. 
 Conducted and facilitated  training about the implementation and maintenance of newlydeveloped ETL application across the Global team.


Master of Business Administration in Business Statistics
Westcliff University - Irvine, CA January 2018 to Present
Graduate Certificate in Data Science and Predictive Analytics
University of California Irvine - Irvine, CA September 2016
Bachelor of Science in Computer Application
De La Salle University March 2006


Database, Db2, Mysql, Oracle, Sql, Elasticsearch, Informatica, Power bi, Teradata, .net, C/c++, C+
+, Html, Python, Visual basic, Visual basic 6, Xml, Xslt, Cognos, Ssrs, Business Intelligence, Excel, Microsoft Office, Powerpoint, access, testing, Visio, MS Office, SAP
Links


Certifications/Licenses

ITIL v3",Data Scientist,resume,"Willing to relocate to: Los Angeles, CA - Laughlin, NV - Fort Mohave, AZ  Experience  Consultant Business Intelligence and Pricing Analyst Panasonic - Lake Forest, CA September 2018 to July 2019  Developed and led the initiative for the monthly executive report, which provided data-driveninsights to support decision-makers by utilizing Python libraries and MS Power BI for data visualization.    Collaborated with cross-functional teams to understand the data challenges of the organization anddevelop solutions, which increased ancillary revenue by approximately 20%. Consultant Data and Post Market Analyst Johnson & Johnson - Orange County, California, US July 2017 to September 2018  Analyzed large dataset related to post-market complaints and implementing statistical models todiscover potential anomalies to ensure compliance with all applicable corporate and federal agencies using MS SQL, Excel, and Python.   Improved operational efficiency, which reduced costs by automating 40% of ad-hoc activities,reporting, and data visualization by utilizing Tableau and SSRS. Business Insight Analyst ATV Inc., Los Angeles County - Los Angeles, CA December 2016 to June 2017  Performed thorough research and analysis on market pricing to determine the best target price forproducts and services using Excel, Python, SQL, and other proprietary .   Automated ETL process by 80% using Python scripts and SQL Server stored procedures. Customer Project / Program Manager III Hewlett Packard Enterprise - Hong Kong, HK July 2012 to June 2015  ed with one of the largest conglomerates in Australia and New Zealand to ensure thatorganizational s are achieved.   Managed project scope, schedule, resources, and costs using the most appropriate verificationtechniques.  Supv, ITO Svc Delivery Consultant II   Managed a team of ETL analyst and database administrators using appropriate  and techniqueswhich resulted to consistently exceeding operational performance goals, and quality expectations.   Delivered SIP (Service Improvement Plans) to advance the quality of service and service deliveryeffectiveness by identifying opportunities from a large dataset and develop data-driven strategies. Programmer Analyst III Amkor  Inc - Manila December 2006 to June 2012  Used Oracle PL/SQL, VB.NET, and Webmethods to design and develop ETL solutions.   Pioneered in Customer Focus -SME Team, who performed data analysis, disaster recovery,and problem resolution for high priority incident. Reduced ticket resolution time to 15%.   Conducted and facilitated  training about the implementation and maintenance of newlydeveloped ETL application across the Global team.   Master of Business Administration in Business Statistics Westcliff University - Irvine, CA January 2018 to Present Graduate Certificate in Data Science and Predictive Analytics University of California Irvine - Irvine, CA September 2016 Bachelor of Science in Computer Application De La Salle University March 2006   Database, Db2, Mysql, Oracle, Sql, Elasticsearch, Informatica, Power bi, Teradata, .net, C/c++, C+ +, Html, Python, Visual basic, Visual basic 6, Xml, Xslt, Cognos, Ssrs, Business Intelligence, Excel, Microsoft Office, Powerpoint, access, testing, Visio, MS Office, SAP Links   Certifications/Licenses  ITIL v3"
"Self-motivated, harding  with strong creative and strategic problem-solving capabilities, communication , analytical  and strong commitment to detail.  
Effective resource planning, project planning, decision making, results delivery, and team building .  
Effectively manage stress and communicate openly in high-pressure situations.  
Flexible, fast-learning and multitask capable with skillful attention to detail and accuracy.
Authorized to  in the US for any employer
 Experience

Data Analyst/Data Manager
CDG, A Boeing Company - Rancho Cucamonga, CA July 2018 to Present
 We provide support to Boeing engineers and suppliers in need of data governance assistancethrough CSDT, the web-based company-wide data warehouse. 
 Update CSDT, REDARS release and ENOVIA with supplier drawings and documents, verifyingmetadata in system matches real-time documentation through CSDT and PDM, as provided 
 Provide updates and verification on important dates and users to certify that all data is documentedaccurately and in a timely fashion, as needed.  
 Solely developed and implemented the Data Management SharePoint site for the EngineeringServices group in a matter of weeks.
 Specialist/Scientist 2
Southern California Edison - Pomona, CA October 2016 to November 2017
 Analysis, data corrections, retriggering assets to field , correcting maintenance plans/orders -mainly related to data discrepancies prohibiting inspectors from being able to complete orders  Create missing measurement points, create maintenance plans, generate E6 notifications, upload inspections w/o creating E1 notifications or upload against removed equipment, set DLFL NOCO on E6 notifications with removed equipment, location corrections, and research uploading errors for resolution 
 Create orders at floc level for assets that did not get inspected due to CMS issue. 
 Client support related to data associated to all inspection programs. Application support related todata discrepancies (CMS, EMobile, DM, AUD, GESW, FIM, SAP). 
 Query/correct flocs assigned to Superior Floc ""OH"" and ""UG"" 
 Object type changes, correct maintenance plans (ODI and Intrusive) and assist with creatingmaintenance plans. 
 Compare SAP location group table to FIM location group extract. Make determination for new Grids/FIMs and decommission removed Grids/FIMs no longer on FIM. May need to move assets from one group to another and create new ODI/AGP MPs. 
 Key element of the team that built model for equipment failure to increase reliability.  Utilized ArcGIS in project to predict risk of failure of equipment, as well as analyzing data discrepancies. 
 Experience using AutoCad in business environment.
Analyst-Business 2
Southern California Edison - Pomona, CA
October 2015 to October 2016
 Key team member in development, analysis, implementation, and reporting on reliability basedmaintenance and compliance pilot where I was tasked with providing analysis and strategic recommendations to peers and senior organizational leaders. 
 Key analyst in developing new and innovative statistical analytics and modeling for various high-  for implementation across T&D business lines. 
 Developed and implemented statistical and data analytical presentation for chief executive officers. 
 Responsible for analyzing data and metrics that were submitted to the CPUC. 
 Significant team member in root cause analytical strategy involving high-visibility reliability metricsin which I was responsible for presenting our findings to departmental executives in a comprehensive and coherent manner. 
 Responsible for creating and maintaining ODI Obstruction and ODI Access compliance-relateddatabases, ensuring that all Maintenance inspections are completed in a timely and efficient manner.  Identified opportunity for process improvement with Scheduled Outage report and used that as an occasion to develop fellow team member in understanding and implementation of the report.  Effectively integrate  across business lines to develop and execute business plans, manage information, and provide exceptional customer service to my clients. 
 Responsible for managing the  under the scope of the  Management team by creatingand maintaining multiple data sources that help manage the  of the District and Contractor related .
Analyst-Program/Project 2
Southern California Edison - Pomona, CA November 2014 to October 2015
 Developed a new monthly Utilization report using Telogis, SAP, Microsoft Access, and Microsoft Excel.
 
 Developed two new reports that help track the number of ODI Access and ODI Obstructionsinspections created since May 2014 using SAP, BEx Analyzer, Microsoft Access and Microsoft Excel. 
 Key associate in the development of the new 2015 Metrics book, including automation and design in
Microsoft Excel and Adobe PDF. 
 Designed and created the new graphical interface in the 2015 Metrics book. 
 Instrumental in the development of a high-level pilot program that will potentially help the companyreduce risk, including sifting through large data sets from SAP and BEx, running queries, formulas and automation in Microsoft Access and Microsoft Excel. 
 Continuously share data processes and report processing with peers by creating job aids andautomating the reports prior to handing them off. 
 Integral part of Data Validation team where I build queries in Microsoft Access to analyze large datasets pulled from SAP and BEx Analyzer, taken the lead on some milestones for the team, reviewed and updated peer's , and identified trends in data discrepancies. 
 Provided support to the newly developed Metrics database by trouble shooting issues, validatingcode and functionality, and identifying gaps in data flow to Metrics template.
Analyst-Program/Project 1
Southern California Edison - Pomona, CA
January 2014 to November 2014
- SW 
 Using BEx Analyzer, SAP & Excel, I create Ad-Hoc Reports out of confidential data that is thenprovided to the Law & Claims Department in a timely manner. 
 Generate and create reports in Excel that are then arranged in PowerPoint to provide information forthe Circuit Reliability Project, used in several areas of SCE management. 
 I routinely track and prioritize Record Corrections in SAP. I analyze the data, apply inspections,research the Maintenance Plan, research mapping in FIM and eWorld prior to making recommendations as to further course of action. 
 Research, develop and test job aids to assist co-ers in completing  in the most effectivemanner and making recommendations of ways to be more concise, effective and efficient. Utilize spreadsheets, databases, and material in Telogis, SAP, Excel, and Access. 
 Develop queries and maintain databases in Access for the E1P1 Report and Supervisor Field Report.  Provide effective support to co-ers and peers with little or no supervision, freeing PM&A management and staff to focus attention on other necessary  within the organization. 
 Experience reviewing, updating and preparing reports such as the E1P1 report, using Access, Excel,SAP and BEx Analyzer. 
 Analyze data in SAP in order to resolve problems and situations that arise in the field while ingon notifications for PM&A. 
 Review and assess  orders in SAP/BI Query to provide accurate data to ODI Supervisors for DailyPending Report. 
 Simultaneously  on notifications, claims, data requests, run reports and assist other analystswithin the PM&A  group. 
 Demonstrated the ability to effectively integrate  from the ODI Daily Pending report that I run toprocessing E2 notifications to gathering data for Claims. 
 Communicate clearly with different  groups to validate the data input into SAP and mapping.  Take initiative to effectively increase productivity by creating automations for the  that I  on using VBA. 
 Provide assistance without hesitation to other personnel when needed. 
 Complete ODI and UDI inspections, resulting in accurate data entered into database to aid PM&A intracking and managing process. 
 Analyze and track data for processing E2 notifications, Claims processing, ODI/AGP Daily Pending
Reports and E1P1 Report using SAP, Excel and Access. 
 Experience creating and maintaining documents and files for use in monitoring, tracking andtrending costs, budget variance and resources for T&D Dashboard for PM&A management using Excel formulas and functions. 
 Experience monitoring, tracking and trending resources, schedules and status for ODI Supervisorsusing Excel, SAP and BI Query. 
 Simultaneously create ODI Daily Pending Report, review T&D Dashboard, process E2 notifications,create Ad-Hoc reports for the Law & Claims Department, create PowerPoints for the System Reliability group and support various analysts within the PM&A group. 
 Create Ad-Hoc reports for the Law & Claims Department and provide daily compliance reports for theODI supervisors. 
 Support PM&A analysts by reviewing and updating job aids using Excel, VBA, Telogis, Access, Wordand other online resources. 
 Provide daily, bi-weekly and quarterly reports to assist T&D management in various groups. 
 Gather documents and research data using databases, files, spreadsheets and mapping for Claimsprocessing, notifications, E1P1 Report, field supervisor report, Fleet Frequency of Use report, ODI Daily
Pending Report and Circuit Reliability Report. 
 Perform analysis, data entry and record preparation for E2 notifications, claims and F.O.Ps. 
 Experience researching and reconciling online records using Telogis, eWorld and FIM for Fleet
Frequency of Use report, processing E2 notifications and processing Claims. 
 Experience using Word, Excel, and PowerPoint to create job aids and various reports, includingClaims, Circuit Reliability, etc. 
 Understanding of OMS readings that assist in the compilation of the data for the Circuit Reliabilityproject. 
 Experience ing with Transmission Resource Planning and Cost Efficiency database. 
 Ability to interface effectively and collaborate with peers while ing on multiple . 
 Routinely exceed deadlines and expectations set for  and reports. 
 Experience ing with T&D policies, procedures, and practices. 
 Experience following Edison safety protocols and safe  practices. 
 Demonstrated strong ethics, personal mastery and interpersonal .
Education

Master of Science degree in Statistics
Cal Poly Pomona 2018
Bachelor of Arts degree in Mathematics
CSU Fresno 2013
Sierra High School 2007


Analyzer (4 years), Excel. (4 years), Microsoft Access (4 years), SAP (4 years), Arcgis (1 year), Autocad (Less than 1 year), Apple, Esri Arcgis, Excel, SQL, Business Intelligence, access
Additional Information

Computer Application : 
 Microsoft Excel 
 Microsoft Access 
 Microsoft PowerPoint 
 Microsoft Word 
 Microsoft Outlook 
 Microsoft Visio 
 SnagIt 
 SAP 
 VBA 
 Lotus Notes 
 BEx Analyzer (Business Analyzer-BI) 
 Telogis 
 Facilities Inventory Mapping (FIM) 
 eWorld 
 SQL 
 OMS 
 CSS 
 SAS 
 R 
 Minitab 
 SPSS",Data Scientist,resume,"Self-motivated, harding  with strong creative and strategic problem-solving capabilities, communication , analytical  and strong commitment to detail.   Effective resource planning, project planning, decision making, results delivery, and team building .   Effectively manage stress and communicate openly in high-pressure situations.   Flexible, fast-learning and multitask capable with skillful attention to detail and accuracy. Authorized to  in the US for any employer  Experience  Data Analyst/Data Manager CDG, A Boeing Company - Rancho Cucamonga, CA July 2018 to Present  We provide support to Boeing engineers and suppliers in need of data governance assistancethrough CSDT, the web-based company-wide data warehouse.   Update CSDT, REDARS release and ENOVIA with supplier drawings and documents, verifyingmetadata in system matches real-time documentation through CSDT and PDM, as provided   Provide updates and verification on important dates and users to certify that all data is documentedaccurately and in a timely fashion, as needed.    Solely developed and implemented the Data Management SharePoint site for the EngineeringServices group in a matter of weeks.  Specialist/Scientist 2 Southern California Edison - Pomona, CA October 2016 to November 2017  Analysis, data corrections, retriggering assets to field , correcting maintenance plans/orders -mainly related to data discrepancies prohibiting inspectors from being able to complete orders  Create missing measurement points, create maintenance plans, generate E6 notifications, upload inspections w/o creating E1 notifications or upload against removed equipment, set DLFL NOCO on E6 notifications with removed equipment, location corrections, and research uploading errors for resolution   Create orders at floc level for assets that did not get inspected due to CMS issue.   Client support related to data associated to all inspection programs. Application support related todata discrepancies (CMS, EMobile, DM, AUD, GESW, FIM, SAP).   Query/correct flocs assigned to Superior Floc ""OH"" and ""UG""   Object type changes, correct maintenance plans (ODI and Intrusive) and assist with creatingmaintenance plans.   Compare SAP location group table to FIM location group extract. Make determination for new Grids/FIMs and decommission removed Grids/FIMs no longer on FIM. May need to move assets from one group to another and create new ODI/AGP MPs.   Key element of the team that built model for equipment failure to increase reliability.  Utilized ArcGIS in project to predict risk of failure of equipment, as well as analyzing data discrepancies.   Experience using AutoCad in business environment. Analyst-Business 2 Southern California Edison - Pomona, CA October 2015 to October 2016  Key team member in development, analysis, implementation, and reporting on reliability basedmaintenance and compliance pilot where I was tasked with providing analysis and strategic recommendations to peers and senior organizational leaders.   Key analyst in developing new and innovative statistical analytics and modeling for various high-  for implementation across T&D business lines.   Developed and implemented statistical and data analytical presentation for chief executive officers.   Responsible for analyzing data and metrics that were submitted to the CPUC.   Significant team member in root cause analytical strategy involving high-visibility reliability metricsin which I was responsible for presenting our findings to departmental executives in a comprehensive and coherent manner.   Responsible for creating and maintaining ODI Obstruction and ODI Access compliance-relateddatabases, ensuring that all Maintenance inspections are completed in a timely and efficient manner.  Identified opportunity for process improvement with Scheduled Outage report and used that as an occasion to develop fellow team member in understanding and implementation of the report.  Effectively integrate  across business lines to develop and execute business plans, manage information, and provide exceptional customer service to my clients.   Responsible for managing the  under the scope of the  Management team by creatingand maintaining multiple data sources that help manage the  of the District and Contractor related . Analyst-Program/Project 2 Southern California Edison - Pomona, CA November 2014 to October 2015  Developed a new monthly Utilization report using Telogis, SAP, Microsoft Access, and Microsoft Excel.    Developed two new reports that help track the number of ODI Access and ODI Obstructionsinspections created since May 2014 using SAP, BEx Analyzer, Microsoft Access and Microsoft Excel.   Key associate in the development of the new 2015 Metrics book, including automation and design in Microsoft Excel and Adobe PDF.   Designed and created the new graphical interface in the 2015 Metrics book.   Instrumental in the development of a high-level pilot program that will potentially help the companyreduce risk, including sifting through large data sets from SAP and BEx, running queries, formulas and automation in Microsoft Access and Microsoft Excel.   Continuously share data processes and report processing with peers by creating job aids andautomating the reports prior to handing them off.   Integral part of Data Validation team where I build queries in Microsoft Access to analyze large datasets pulled from SAP and BEx Analyzer, taken the lead on some milestones for the team, reviewed and updated peer's , and identified trends in data discrepancies.   Provided support to the newly developed Metrics database by trouble shooting issues, validatingcode and functionality, and identifying gaps in data flow to Metrics template. Analyst-Program/Project 1 Southern California Edison - Pomona, CA January 2014 to November 2014 - SW   Using BEx Analyzer, SAP & Excel, I create Ad-Hoc Reports out of confidential data that is thenprovided to the Law & Claims Department in a timely manner.   Generate and create reports in Excel that are then arranged in PowerPoint to provide information forthe Circuit Reliability Project, used in several areas of SCE management.   I routinely track and prioritize Record Corrections in SAP. I analyze the data, apply inspections,research the Maintenance Plan, research mapping in FIM and eWorld prior to making recommendations as to further course of action.   Research, develop and test job aids to assist co-ers in completing  in the most effectivemanner and making recommendations of ways to be more concise, effective and efficient. Utilize spreadsheets, databases, and material in Telogis, SAP, Excel, and Access.   Develop queries and maintain databases in Access for the E1P1 Report and Supervisor Field Report.  Provide effective support to co-ers and peers with little or no supervision, freeing PM&A management and staff to focus attention on other necessary  within the organization.   Experience reviewing, updating and preparing reports such as the E1P1 report, using Access, Excel,SAP and BEx Analyzer.   Analyze data in SAP in order to resolve problems and situations that arise in the field while ingon notifications for PM&A.   Review and assess  orders in SAP/BI Query to provide accurate data to ODI Supervisors for DailyPending Report.   Simultaneously  on notifications, claims, data requests, run reports and assist other analystswithin the PM&A  group.   Demonstrated the ability to effectively integrate  from the ODI Daily Pending report that I run toprocessing E2 notifications to gathering data for Claims.   Communicate clearly with different  groups to validate the data input into SAP and mapping.  Take initiative to effectively increase productivity by creating automations for the  that I  on using VBA.   Provide assistance without hesitation to other personnel when needed.   Complete ODI and UDI inspections, resulting in accurate data entered into database to aid PM&A intracking and managing process.   Analyze and track data for processing E2 notifications, Claims processing, ODI/AGP Daily Pending Reports and E1P1 Report using SAP, Excel and Access.   Experience creating and maintaining documents and files for use in monitoring, tracking andtrending costs, budget variance and resources for T&D Dashboard for PM&A management using Excel formulas and functions.   Experience monitoring, tracking and trending resources, schedules and status for ODI Supervisorsusing Excel, SAP and BI Query.   Simultaneously create ODI Daily Pending Report, review T&D Dashboard, process E2 notifications,create Ad-Hoc reports for the Law & Claims Department, create PowerPoints for the System Reliability group and support various analysts within the PM&A group.   Create Ad-Hoc reports for the Law & Claims Department and provide daily compliance reports for theODI supervisors.   Support PM&A analysts by reviewing and updating job aids using Excel, VBA, Telogis, Access, Wordand other online resources.   Provide daily, bi-weekly and quarterly reports to assist T&D management in various groups.   Gather documents and research data using databases, files, spreadsheets and mapping for Claimsprocessing, notifications, E1P1 Report, field supervisor report, Fleet Frequency of Use report, ODI Daily Pending Report and Circuit Reliability Report.   Perform analysis, data entry and record preparation for E2 notifications, claims and F.O.Ps.   Experience researching and reconciling online records using Telogis, eWorld and FIM for Fleet Frequency of Use report, processing E2 notifications and processing Claims.   Experience using Word, Excel, and PowerPoint to create job aids and various reports, includingClaims, Circuit Reliability, etc.   Understanding of OMS readings that assist in the compilation of the data for the Circuit Reliabilityproject.   Experience ing with Transmission Resource Planning and Cost Efficiency database.   Ability to interface effectively and collaborate with peers while ing on multiple .   Routinely exceed deadlines and expectations set for  and reports.   Experience ing with T&D policies, procedures, and practices.   Experience following Edison safety protocols and safe  practices.   Demonstrated strong ethics, personal mastery and interpersonal . Education  Master of Science degree in Statistics Cal Poly Pomona 2018 Bachelor of Arts degree in Mathematics CSU Fresno 2013 Sierra High School 2007   Analyzer (4 years), Excel. (4 years), Microsoft Access (4 years), SAP (4 years), Arcgis (1 year), Autocad (Less than 1 year), Apple, Esri Arcgis, Excel, SQL, Business Intelligence, access Additional Information  Computer Application :   Microsoft Excel   Microsoft Access   Microsoft PowerPoint   Microsoft Word   Microsoft Outlook   Microsoft Visio   SnagIt   SAP   VBA   Lotus Notes   BEx Analyzer (Business Analyzer-BI)   Telogis   Facilities Inventory Mapping (FIM)   eWorld   SQL   OMS   CSS   SAS   R   Minitab   SPSS"
"
Remote Statistician and Data Scientist Bremerton, WA

I have my Master of Applied Statistics and am seeking remote employment as a statistician or data scientist. Through my schooling I have experience in multiple statistical methods including Linear Regression, Multiple Linear Regression, Analysis of Variance, Multiple Analysis of Variance, Experimental Design. I am trained and experienced in Minitab, SAS, and R programming, and I have the ability to learn any statistical analysis software required for .  
I am a former math educator at the public secondary level and collegiate level. My experience in  helps me communicate statistical results in terms that are understandable to people from all fields of expertise. I also have extensive experience with creating and giving presentations and communicating with my management and customers. I am confident that I am a valuable asset to any data analysis team.
Authorized to  in the US for any employer
 Experience

Math Tutor
Leslie's Tutoring
October 2016 to Present
 Support students in secondary level schools in academic and organizational areas 
 Help students increase self-confidence regarding their ability to succeed 
 Designed, Administered, and Interpreted a statistical analysis comparing perceptions of the successof tutoring at the center
Adjunct Faculty
NORTHWEST COLLEGE OF ART & DESIGN - Tacoma, WA
May 2017 to August 2018
Instructed QP 351, Quantitative Reasoning, a core math requirement course for students pursuing a Bachelor of Fine Arts. 
 Designed a comprehensive curriculum that focused on students' using collegiate level mathematics to improve decision making and knowledge in daily life.
Administrative Assistant
Peninsula Electric Corp. - Poulsbo, WA September 2016 to August 2018
 Completed weekly payroll for all employees 
 Invoiced customers, received payments, entered inventory using Quickbooks software 
 Designed and administered monthly safety training program
Math Teacher
Sangaree Middle School - Ladson, SC
July 2015 to January 2016
 Taught 7th Grade and 7th Grade Accelerated Math 
 Created weekly teaching plans within peer teacher groups 
 Analyzed student data to adjust instruction
Math Teacher
Emmett School District - Emmett, ID
January 2012 to June 2015
 Taught 7th Grade, 7th Grade Accelerated, 8th Grade, 8th Grade Geometry at the Middle School 
 Taught Algebra 1 and Geometry at the High School 
 Actively taught using AVID techniques in conjunction with Common Core math curriculum 
 Member of the Emmett Middle School AVID CommitteeCore Subject Teacher 
 Aligned Emmett Math curriculum with Common Core standards in a district curriculum initiative 
 Created vertical alignment with curricular expectations for all middle grades through Algebra 1  ed on creating unified final exams and unit tests to create consistency in al expectations  
 Organized the end of year Awards Assembly for Emmett Middle School 2012-2015 
 Founded and Coached a Middle School Academic Team


Master's in Applied Statistics
Pennsylvania State University-Main Campus - State College, PA
January 2017 to May 2019
Bachelor's in Mathematics Secondary 
Boise State University - Boise, ID August 2008 to December 2011


SAS (2 years), R (1 year), Microsoft Office, Organizational , Communications, Powerpoint,
Teaching, Filing, problem solving, Typing, Data Presentation, Excel, MS Office
Certifications/Licenses

Driver's License
Assessments

High School Classroom Management  Highly Proficient
August 2019
Minimizing classroom disruption and engaging students.
Full results: https://share.indeedassessments.com/share_assignment/drsvobncaf5wgvlx
Logic & Verbal Reasoning  Expert
August 2019
Understanding the meaning of text, and identifying the relationships among words or concepts.
Full results: https://share.indeedassessments.com/share_assignment/55w3hznjs8o4bkuz
Verbal Communication  Expert
August 2019
Speaking clearly, correctly, and concisely.
Full results: https://share.indeedassessments.com/share_assignment/enouluzkbhlhrkim
Problem Solving  Expert
August 2019
Analyzing relevant information when solving problems.
Full results: https://share.indeedassessments.com/share_assignment/l9qhy5pgvj74e5rp
Critical Thinking  Expert
August 2019
Using logic to solve problems.
Full results: https://share.indeedassessments.com/share_assignment/vsei0-w54o3-av0f
Data Analysis  Expert
August 2019
Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/vfnyqiubwki52n80
 Support  Highly Proficient
August 2019
Applying protocols to identify errors and solutions in order to maintain system function.
Full results: https://share.indeedassessments.com/share_assignment/tegnzuaok0tc5hug
Research  Expert
August 2019
Following protocols, interpreting statistics and graphs, identifying errors, and choosing research methodology.
Full results: https://share.indeedassessments.com/share_assignment/d3iqhuhhdd-dwzyp
Management & Leadership : Planning & Execution  Expert
August 2019
Planning and managing resources to accomplish organizational goals.
Full results: https://share.indeedassessments.com/share_assignment/n-7zuzc-tuibibet
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.",Data Scientist,resume," Remote Statistician and Data Scientist Bremerton, WA  I have my Master of Applied Statistics and am seeking remote employment as a statistician or data scientist. Through my schooling I have experience in multiple statistical methods including Linear Regression, Multiple Linear Regression, Analysis of Variance, Multiple Analysis of Variance, Experimental Design. I am trained and experienced in Minitab, SAS, and R programming, and I have the ability to learn any statistical analysis software required for .   I am a former math educator at the public secondary level and collegiate level. My experience in  helps me communicate statistical results in terms that are understandable to people from all fields of expertise. I also have extensive experience with creating and giving presentations and communicating with my management and customers. I am confident that I am a valuable asset to any data analysis team. Authorized to  in the US for any employer  Experience  Math Tutor Leslie's Tutoring October 2016 to Present  Support students in secondary level schools in academic and organizational areas   Help students increase self-confidence regarding their ability to succeed   Designed, Administered, and Interpreted a statistical analysis comparing perceptions of the successof tutoring at the center Adjunct Faculty NORTHWEST COLLEGE OF ART & DESIGN - Tacoma, WA May 2017 to August 2018 Instructed QP 351, Quantitative Reasoning, a core math requirement course for students pursuing a Bachelor of Fine Arts.   Designed a comprehensive curriculum that focused on students' using collegiate level mathematics to improve decision making and knowledge in daily life. Administrative Assistant Peninsula Electric Corp. - Poulsbo, WA September 2016 to August 2018  Completed weekly payroll for all employees   Invoiced customers, received payments, entered inventory using Quickbooks software   Designed and administered monthly safety training program Math Teacher Sangaree Middle School - Ladson, SC July 2015 to January 2016  Taught 7th Grade and 7th Grade Accelerated Math   Created weekly teaching plans within peer teacher groups   Analyzed student data to adjust instruction Math Teacher Emmett School District - Emmett, ID January 2012 to June 2015  Taught 7th Grade, 7th Grade Accelerated, 8th Grade, 8th Grade Geometry at the Middle School   Taught Algebra 1 and Geometry at the High School   Actively taught using AVID techniques in conjunction with Common Core math curriculum   Member of the Emmett Middle School AVID CommitteeCore Subject Teacher   Aligned Emmett Math curriculum with Common Core standards in a district curriculum initiative   Created vertical alignment with curricular expectations for all middle grades through Algebra 1  ed on creating unified final exams and unit tests to create consistency in al expectations    Organized the end of year Awards Assembly for Emmett Middle School 2012-2015   Founded and Coached a Middle School Academic Team   Master's in Applied Statistics Pennsylvania State University-Main Campus - State College, PA January 2017 to May 2019 Bachelor's in Mathematics Secondary  Boise State University - Boise, ID August 2008 to December 2011   SAS (2 years), R (1 year), Microsoft Office, Organizational , Communications, Powerpoint, Teaching, Filing, problem solving, Typing, Data Presentation, Excel, MS Office Certifications/Licenses  Driver's License Assessments  High School Classroom Management  Highly Proficient August 2019 Minimizing classroom disruption and engaging students. Full results: https://share.indeedassessments.com/share_assignment/drsvobncaf5wgvlx Logic & Verbal Reasoning  Expert August 2019 Understanding the meaning of text, and identifying the relationships among words or concepts. Full results: https://share.indeedassessments.com/share_assignment/55w3hznjs8o4bkuz Verbal Communication  Expert August 2019 Speaking clearly, correctly, and concisely. Full results: https://share.indeedassessments.com/share_assignment/enouluzkbhlhrkim Problem Solving  Expert August 2019 Analyzing relevant information when solving problems. Full results: https://share.indeedassessments.com/share_assignment/l9qhy5pgvj74e5rp Critical Thinking  Expert August 2019 Using logic to solve problems. Full results: https://share.indeedassessments.com/share_assignment/vsei0-w54o3-av0f Data Analysis  Expert August 2019 Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. Full results: https://share.indeedassessments.com/share_assignment/vfnyqiubwki52n80  Support  Highly Proficient August 2019 Applying protocols to identify errors and solutions in order to maintain system function. Full results: https://share.indeedassessments.com/share_assignment/tegnzuaok0tc5hug Research  Expert August 2019 Following protocols, interpreting statistics and graphs, identifying errors, and choosing research methodology. Full results: https://share.indeedassessments.com/share_assignment/d3iqhuhhdd-dwzyp Management & Leadership : Planning & Execution  Expert August 2019 Planning and managing resources to accomplish organizational goals. Full results: https://share.indeedassessments.com/share_assignment/n-7zuzc-tuibibet Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field."
"To continue learning and innovating the world in science, medicine, and humanities as a developing human for the benefit of the world. 
The opportunity to gain meaningful experience is always possible with the right environment. As I explore career paths and creative ways of learning, I find meaningful  in seeing mission statements fulfilled and visions being executed.
 Experience

Activities Director
HUMANITARIAN EFFORTS - San Diego, CA
August 2017 to Present
San Diego, CA, USA; Los Angeles, CA, USA: San Jose, Costa Rica; Azores, Portugal 
 
 Nazarene Youth International: Annual District Youth Camp - Activities Director 
 Loves Program:  and Community - Portugal Missions Trip 
Achievement 
 City Height's Health Disparities Clinic: Medical Volunteer
RESEARCH LABORATORY MANAGER
Abilities - Loma Linda, CA
June 2018 to June 2019
91232 
Laboratory: in Vivo surgical procedures, IHC, Cryosectioning, Advanced Microscopy Imaging 
 Project Manager for NIH R21 Grant, Two MD-PHD graduate  
 Neuroscience Research in Vagal nerve stimulation, Traumatic Brain Injury, Hypoxia 
Research : SPSS, Data Analysis, Prism, 
 Created research culture to market healthy place environment and productivity 
Python, PubMed, NCBI 
 Collaborated with Biochemistry, Pediatrics, Neurosurgery departments with other 
Health Equity Mentor Program
UCSD School of Medicine
May 2018 to May 2018
Science  Mentor
PLNU - San Diego, CA
May 2018 to May 2018
San Diego, USA 
 Ocean Discovery Institute: Discovery Fellowship Program - Science  Mentor 
The Best Oral Presentation - May 2018 
WCBSUR Conference, California, USA
STUDENT MANAGER
place Culture - San Diego, CA
August 2014 to May 2018
71622 Social Media incorporation, Emotional Intelligence, 
Leadership 
 Oversaw GDP, purchasing logistics, organization and marketing of Dining Program 
 Marketed and Developed new dining programs for student well-being 
Office Application: Microsoft Office, Grant 
 Collaborative integration of campus coffee shop, café, and dining hall Budgets, Inventory, and Project Management


CERTIFICATE
Chaffey College - San Diego, CA August 2019
B.S Honors in Scholar
Point Loma Nazarene University
August 2014 to May 2018
Links",Data Scientist,resume,"To continue learning and innovating the world in science, medicine, and humanities as a developing human for the benefit of the world.  The opportunity to gain meaningful experience is always possible with the right environment. As I explore career paths and creative ways of learning, I find meaningful  in seeing mission statements fulfilled and visions being executed.  Experience  Activities Director HUMANITARIAN EFFORTS - San Diego, CA August 2017 to Present San Diego, CA, USA; Los Angeles, CA, USA: San Jose, Costa Rica; Azores, Portugal     Nazarene Youth International: Annual District Youth Camp - Activities Director   Loves Program:  and Community - Portugal Missions Trip  Achievement   City Height's Health Disparities Clinic: Medical Volunteer RESEARCH LABORATORY MANAGER Abilities - Loma Linda, CA June 2018 to June 2019 91232  Laboratory: in Vivo surgical procedures, IHC, Cryosectioning, Advanced Microscopy Imaging   Project Manager for NIH R21 Grant, Two MD-PHD graduate    Neuroscience Research in Vagal nerve stimulation, Traumatic Brain Injury, Hypoxia  Research : SPSS, Data Analysis, Prism,   Created research culture to market healthy place environment and productivity  Python, PubMed, NCBI   Collaborated with Biochemistry, Pediatrics, Neurosurgery departments with other  Health Equity Mentor Program UCSD School of Medicine May 2018 to May 2018 Science  Mentor PLNU - San Diego, CA May 2018 to May 2018 San Diego, USA   Ocean Discovery Institute: Discovery Fellowship Program - Science  Mentor  The Best Oral Presentation - May 2018  WCBSUR Conference, California, USA STUDENT MANAGER place Culture - San Diego, CA August 2014 to May 2018 71622 Social Media incorporation, Emotional Intelligence,  Leadership   Oversaw GDP, purchasing logistics, organization and marketing of Dining Program   Marketed and Developed new dining programs for student well-being  Office Application: Microsoft Office, Grant   Collaborative integration of campus coffee shop, café, and dining hall Budgets, Inventory, and Project Management   CERTIFICATE Chaffey College - San Diego, CA August 2019 B.S Honors in Scholar Point Loma Nazarene University August 2014 to May 2018 Links"
"? Highly productive and effective Financial Analyst with over four years of experience ing in thefinancial field. 
? Exceptional command of language and communication , both written and oral. 
? Unique ability to give full attention to people, utilize effective listening  and ask appropriatequestions. 
? Strong analytical and reasoning  and ability to identify the strengths and weaknesses ofalternative solutions. 
? Extensive experience in developing constructive, cooperative,  ing relationshipswith others and maintaining them over time.
Authorized to  in the US for any employer
 Experience

QUANTITATIVE ANALYST
Quantopian - Boston, MA
July 2016 to Present
Building low latency liquidity taking or market making strategies from end-to-end. 
? Developing mathematical models to solve difficult stochastic problems. 
? Analyzing convergence and boundedness properties of algorithms and estimates. 
? Translating your models to fast computational methods. 
? Collaborating with researchers and developers to implement all of the above.
SR. FINANCIAL ANALYST
Houghton Mifflin - Boston, MA December 2014 to June 2016
Performed complex accounting functions for business units 
? Coordinated, prepare and review month-end closing process, accounting entries and documents ? Developed annual budgets - coordinated the development, implementation and control of budgets by preparing, balancing, compiling, and entering budget data 
? Created quarterly forecasts 
? Performed statistical analysis of revenue and expense versus budgets 
? Monitored and evaluated budgets and forecast 
? Analyzed financial results to reconcile variances and develop qualitative explanations, and identifypotential risks to IT plans. 
? Interacted with vendors and support deal negotiations using statistical regression analysis forpredictability 
? Liaised with Legal and HMH executives in efforts to help achieve goals and mission of firm.
FINANCE ASSOCIATE
Bain Capital - Boston, MA
November 2014 to December 2014
Two month contracted position. 
? ed with the Financial Analyst team to transition smoothly from Great Plains to day, andwrote VBA scripts and macros to further aid the team in minimizing the time of completion. 
? Performed Balance Sheet and Income statement validation and variance analysis and provided auditsupport.
FX & SECURITIES TRADER
FINRA SERIES - Boston, MA
July 2014 to November 2014
Executed purchases and sales of investment products in a timely and efficient manner. 
? Explained trading procedures to reps and other customers, answers questions and provides quotes. 
? Researched using Statistical software equity price movements for future price action. 
? Ensured that all trades meet internal and external compliance standards. Checks to verify properlicensing of outside reps. 
? Developed proprietary application MSITEQ to analyze market volatility.
FINANCIAL ACCOUNTING REPORTING Specialist
BNY Mellon - Everett, MA
December 2012 to July 2014
Supported complete, accurate, and timely valuation of financial statements for daily and monthly values. Utilized VBA to produce accurate and timely Net Asset Values and month-end close consolidation. 
? Supported a mix of large complex institutional client relationships requiring the understanding of alldaily, weekly and monthly standard and customized client accounting and reporting requirements. ? Frequent/daily contact with entry/mid-level client staff with respect to cash, accounting, and reporting of daily and monthly client activities and initiatives. 
? Answered all client processing, accounting and reporting questions, either directly or throughresearch with other functional areas, which required a solid understand of all functional groups that support CARS. ? Provided  support to clients and interpreted business performance drivers and opportunities for enhancements.


Bachelor of Science in Economics
Salem State University 2012


ACCESS (Less than 1 year), EXCEL (Less than 1 year), FINANCIAL MODELING (Less than 1 year),
HYPERION (Less than 1 year), MS EXCEL (Less than 1 year), Python, C, R
Additional Information

?  : MS Excel (Pivot Tables, Macros, VBA script, If and Analysis, (V, H)LOOKUP), Access and PowerPoint, 
Financial Modeling SAP R3, SAP BW, BPC, Hyperion, SAP,",Data Scientist,resume,"? Highly productive and effective Financial Analyst with over four years of experience ing in thefinancial field.  ? Exceptional command of language and communication , both written and oral.  ? Unique ability to give full attention to people, utilize effective listening  and ask appropriatequestions.  ? Strong analytical and reasoning  and ability to identify the strengths and weaknesses ofalternative solutions.  ? Extensive experience in developing constructive, cooperative,  ing relationshipswith others and maintaining them over time. Authorized to  in the US for any employer  Experience  QUANTITATIVE ANALYST Quantopian - Boston, MA July 2016 to Present Building low latency liquidity taking or market making strategies from end-to-end.  ? Developing mathematical models to solve difficult stochastic problems.  ? Analyzing convergence and boundedness properties of algorithms and estimates.  ? Translating your models to fast computational methods.  ? Collaborating with researchers and developers to implement all of the above. SR. FINANCIAL ANALYST Houghton Mifflin - Boston, MA December 2014 to June 2016 Performed complex accounting functions for business units  ? Coordinated, prepare and review month-end closing process, accounting entries and documents ? Developed annual budgets - coordinated the development, implementation and control of budgets by preparing, balancing, compiling, and entering budget data  ? Created quarterly forecasts  ? Performed statistical analysis of revenue and expense versus budgets  ? Monitored and evaluated budgets and forecast  ? Analyzed financial results to reconcile variances and develop qualitative explanations, and identifypotential risks to IT plans.  ? Interacted with vendors and support deal negotiations using statistical regression analysis forpredictability  ? Liaised with Legal and HMH executives in efforts to help achieve goals and mission of firm. FINANCE ASSOCIATE Bain Capital - Boston, MA November 2014 to December 2014 Two month contracted position.  ? ed with the Financial Analyst team to transition smoothly from Great Plains to day, andwrote VBA scripts and macros to further aid the team in minimizing the time of completion.  ? Performed Balance Sheet and Income statement validation and variance analysis and provided auditsupport. FX & SECURITIES TRADER FINRA SERIES - Boston, MA July 2014 to November 2014 Executed purchases and sales of investment products in a timely and efficient manner.  ? Explained trading procedures to reps and other customers, answers questions and provides quotes.  ? Researched using Statistical software equity price movements for future price action.  ? Ensured that all trades meet internal and external compliance standards. Checks to verify properlicensing of outside reps.  ? Developed proprietary application MSITEQ to analyze market volatility. FINANCIAL ACCOUNTING REPORTING Specialist BNY Mellon - Everett, MA December 2012 to July 2014 Supported complete, accurate, and timely valuation of financial statements for daily and monthly values. Utilized VBA to produce accurate and timely Net Asset Values and month-end close consolidation.  ? Supported a mix of large complex institutional client relationships requiring the understanding of alldaily, weekly and monthly standard and customized client accounting and reporting requirements. ? Frequent/daily contact with entry/mid-level client staff with respect to cash, accounting, and reporting of daily and monthly client activities and initiatives.  ? Answered all client processing, accounting and reporting questions, either directly or throughresearch with other functional areas, which required a solid understand of all functional groups that support CARS. ? Provided  support to clients and interpreted business performance drivers and opportunities for enhancements.   Bachelor of Science in Economics Salem State University 2012   ACCESS (Less than 1 year), EXCEL (Less than 1 year), FINANCIAL MODELING (Less than 1 year), HYPERION (Less than 1 year), MS EXCEL (Less than 1 year), Python, C, R Additional Information  ?  : MS Excel (Pivot Tables, Macros, VBA script, If and Analysis, (V, H)LOOKUP), Access and PowerPoint,  Financial Modeling SAP R3, SAP BW, BPC, Hyperion, SAP,"
"Business Data Analyst who maximizes productivity, profitability, and competitiveness by transforming  plans into powerful business solutions. Distinguished for both  and business acumen, and for an ability to bridge communication gaps between , business, and executive audiences. Earned undergraduate degree in petroleum engineering and master degree in business data analytics. Expertise in Data Modeling, Data Mining/manipulation, SQL, python, R language, Data Visualization, and Data Warehousing with experience in handling multiple  and meeting tight deadlines.
 Experience

Data Analyst
Dude Solutions Inc - Grand Junction, CO
September 2018 to Present
 Streamline the reporting process through tuning SQL and SAS scripts and automate the dailyreporting process 
 Handle data manipulation and data cleaning on all utility data for successful database population.  Provide assistance to implementation teams with initial account configuration by utilizing, creating, and maintaining import templates, utility data migration tool sets, and data loads. Actively participate, when needed, in client facing calls to set proper expectations and establish needs regarding client data. 
 Optimize production databases by ing with implementation teams to address data problems.  Perform pre-and-post job inspections, consistently complete preventative maintenance procedures, and maintain support equipment as well as rig up and down service line equipment at the well site.  Utilize expertise in transforming all 3rd party utility data received from client in a consumable format for the purposes of importing into Dude Solutions Software (Energy Manager) from existing SQL databases. 
 Demonstrate strong problem-solving, critical thinking, and organizational . 
 Efficiently communicate the entire process successes and failures to internal and externalstakeholders to identify potential areas of business improvement, which require cross-departmental collaboration. 
 Document and monitor large utility data sets for producing business intelligence (BI) solutions thatidentify relationships and trends in utility data. Help team to prioritize  according to business value. 
 Perform database administration duties: backups, restoration, performance monitoring, and tuning. 
 Provide data solutions such as business partner data pulls, report design, and data visualizations.  Leverage advanced statistical and data analytic techniques to support/implement new data efforts by analyzing and producing comprehensive reports that provide competitive and actionable information.  Use optimization  at a quickening pace for preventative maintenance forecasting, integrated business planning, business partner collaboration, and risk analytics. Ensure the merging of large utility data sets. 
 Manage project through status meetings, weekly reports, identifying risks, and tracking issues 
 Served customers and businesses by running and viewing reports 
 Designed analytical solutions 
 Execute and validate test cases 
 Responsible for specifications, implementations, and analytics 
 Prepared business models, flowcharts, and diagrams 
 Efficiently collaborate with team members to develop and recommend new business intelligenceservices.
Maintenance Assistant
Halliburton - Fort Lupton, CO
July 2017 to October 2017
 Performed basic safety/repair procedures using processes defined for assigned jobs. Strictlymonitored preventive/predictive maintenance procedures on the Halliburton and other pumping equipment. 
 Completed preventative maintenance procedures, performed pre-and-post job inspections, andmaintained support equipment as well as rig up and down service line equipment at the well site.  Under general supervision, inspected, maintained, diagnosed, troubleshot, and repaired a wide variety of high-pressure pumping equipment, which included: the Halliburton HT-150, HT-400, HT-2000, Grizzly (HQ-2000), and Bearcat as well as a variety of other manufacturers pumping equipment. 
 Managed the process for maintenance inspections, repairs, and tests required by the company andcustomers. Performed basic troubleshooting on some mechanical, hydraulic, and pneumatic systems.
Information  Tech
University of Wyoming - Laramie, WY
July 2014 to May 2017
 Provided  assistance and support for incoming queries and issues related to computersystems, software, and hardware. Deeply experienced in responding to queries either in person or over the phone. 
 Diagnosed computer errors and provided  support. Trained end-users on how to setup anduse new . Performed basic administration of Windows OS and iOS - fixing any related issues. 
 Troubleshot software, net issues, and hardware i.e. desktops, laptops, printers, and otherperipherals. Continuously backed-up and restored the organization's data files and electronic systems.
 
 Installed, configured, and upgraded PC software and operating systems. Performed setup ofenterprise LAN and WAN. Deeply experienced in installing and using anti-spyware and anti-virus software.


Master's in Science in Business Data Analytics
Maryville University - Saint Louis, MI December 2018
Bachelor of Science in Petroleum Engineering
University of Wyoming - Laramie, WY May 2017


Database, Relational database, Sql, Power bi, Saas, Tableau, Ios, Python, Vba, Data warehouse, Matlab, Linux, Sap, Power BI, Excel, Microsoft office, Powerpoint, Word, MS Office
Additional Information

Business Data Analyst who maximizes productivity, profitability, and competitiveness by transforming  plans into powerful business solutions. Distinguished for both  and business acumen, and for an ability to bridge communication gaps between , business, and executive audiences. Earned undergraduate degree in petroleum engineering and master degree in business data analytics. Expertise in Data Modeling, Data Mining/manipulation, SQL, python, R language, Data Visualization, and Data Warehousing with experience in handling multiple  and meeting tight deadlines. 
 
 Predictive/Exploratory Model Design 
 Advanced Graph Database and SQL  
 Excellent Communication  
 Expert in Data Mining, Query Optimization, Scripting, and ing with Large Datasets 
 Data Analytical Mindset 
 Problem-Solving  
 Advanced Data Warehousing  
 Advanced Visualization  (Tableau and Microsoft Power BI) 
 
 Exceptional Interpersonal and Communication  - Proficient in maintaining long-term businessrelationships while successfully interfacing with a wide range of people from diverse backgrounds.  Problem Solving - Proven ability to troubleshoot and develop both creative and innovative solutions to project challenges. Successfully manages change for improved performance and greater team efficiency. 
  Ethic and ism - Solid  standards and excellent record of dependability.
Maintains a clear focus on achieving bottom-line results and team integrity while ensuring progress. 
 
  
 
Advanced Microsoft Power BI Skill Advanced R language Skill 
Advanced Python Language Skill Advanced SQL Database Skill 
Advanced Graph Database Skill (Neo4j Software) Advanced Azure Server Skill 
Advanced Experience in SQL Query and Data Warehouse Advanced Tableau skill 
Advanced SAP IS - Oil and Gas (Downstream) Skill Expert VBA Excel Skill 
Proficiency in relational database  and  
Advanced  in Petrel E&P Software and MATLAB 
Experienced ing with several operating systems (iOS, Windows, and Linux) 
Experience with SaaS or analytics software 
Advanced  in Microsoft Office applications (Word, Excel, PowerPoint, Project, etc.)",Data Scientist,resume,"Business Data Analyst who maximizes productivity, profitability, and competitiveness by transforming  plans into powerful business solutions. Distinguished for both  and business acumen, and for an ability to bridge communication gaps between , business, and executive audiences. Earned undergraduate degree in petroleum engineering and master degree in business data analytics. Expertise in Data Modeling, Data Mining/manipulation, SQL, python, R language, Data Visualization, and Data Warehousing with experience in handling multiple  and meeting tight deadlines.  Experience  Data Analyst Dude Solutions Inc - Grand Junction, CO September 2018 to Present  Streamline the reporting process through tuning SQL and SAS scripts and automate the dailyreporting process   Handle data manipulation and data cleaning on all utility data for successful database population.  Provide assistance to implementation teams with initial account configuration by utilizing, creating, and maintaining import templates, utility data migration tool sets, and data loads. Actively participate, when needed, in client facing calls to set proper expectations and establish needs regarding client data.   Optimize production databases by ing with implementation teams to address data problems.  Perform pre-and-post job inspections, consistently complete preventative maintenance procedures, and maintain support equipment as well as rig up and down service line equipment at the well site.  Utilize expertise in transforming all 3rd party utility data received from client in a consumable format for the purposes of importing into Dude Solutions Software (Energy Manager) from existing SQL databases.   Demonstrate strong problem-solving, critical thinking, and organizational .   Efficiently communicate the entire process successes and failures to internal and externalstakeholders to identify potential areas of business improvement, which require cross-departmental collaboration.   Document and monitor large utility data sets for producing business intelligence (BI) solutions thatidentify relationships and trends in utility data. Help team to prioritize  according to business value.   Perform database administration duties: backups, restoration, performance monitoring, and tuning.   Provide data solutions such as business partner data pulls, report design, and data visualizations.  Leverage advanced statistical and data analytic techniques to support/implement new data efforts by analyzing and producing comprehensive reports that provide competitive and actionable information.  Use optimization  at a quickening pace for preventative maintenance forecasting, integrated business planning, business partner collaboration, and risk analytics. Ensure the merging of large utility data sets.   Manage project through status meetings, weekly reports, identifying risks, and tracking issues   Served customers and businesses by running and viewing reports   Designed analytical solutions   Execute and validate test cases   Responsible for specifications, implementations, and analytics   Prepared business models, flowcharts, and diagrams   Efficiently collaborate with team members to develop and recommend new business intelligenceservices. Maintenance Assistant Halliburton - Fort Lupton, CO July 2017 to October 2017  Performed basic safety/repair procedures using processes defined for assigned jobs. Strictlymonitored preventive/predictive maintenance procedures on the Halliburton and other pumping equipment.   Completed preventative maintenance procedures, performed pre-and-post job inspections, andmaintained support equipment as well as rig up and down service line equipment at the well site.  Under general supervision, inspected, maintained, diagnosed, troubleshot, and repaired a wide variety of high-pressure pumping equipment, which included: the Halliburton HT-150, HT-400, HT-2000, Grizzly (HQ-2000), and Bearcat as well as a variety of other manufacturers pumping equipment.   Managed the process for maintenance inspections, repairs, and tests required by the company andcustomers. Performed basic troubleshooting on some mechanical, hydraulic, and pneumatic systems. Information  Tech University of Wyoming - Laramie, WY July 2014 to May 2017  Provided  assistance and support for incoming queries and issues related to computersystems, software, and hardware. Deeply experienced in responding to queries either in person or over the phone.   Diagnosed computer errors and provided  support. Trained end-users on how to setup anduse new . Performed basic administration of Windows OS and iOS - fixing any related issues.   Troubleshot software, net issues, and hardware i.e. desktops, laptops, printers, and otherperipherals. Continuously backed-up and restored the organization's data files and electronic systems.    Installed, configured, and upgraded PC software and operating systems. Performed setup ofenterprise LAN and WAN. Deeply experienced in installing and using anti-spyware and anti-virus software.   Master's in Science in Business Data Analytics Maryville University - Saint Louis, MI December 2018 Bachelor of Science in Petroleum Engineering University of Wyoming - Laramie, WY May 2017   Database, Relational database, Sql, Power bi, Saas, Tableau, Ios, Python, Vba, Data warehouse, Matlab, Linux, Sap, Power BI, Excel, Microsoft office, Powerpoint, Word, MS Office Additional Information  Business Data Analyst who maximizes productivity, profitability, and competitiveness by transforming  plans into powerful business solutions. Distinguished for both  and business acumen, and for an ability to bridge communication gaps between , business, and executive audiences. Earned undergraduate degree in petroleum engineering and master degree in business data analytics. Expertise in Data Modeling, Data Mining/manipulation, SQL, python, R language, Data Visualization, and Data Warehousing with experience in handling multiple  and meeting tight deadlines.     Predictive/Exploratory Model Design   Advanced Graph Database and SQL    Excellent Communication    Expert in Data Mining, Query Optimization, Scripting, and ing with Large Datasets   Data Analytical Mindset   Problem-Solving    Advanced Data Warehousing    Advanced Visualization  (Tableau and Microsoft Power BI)     Exceptional Interpersonal and Communication  - Proficient in maintaining long-term businessrelationships while successfully interfacing with a wide range of people from diverse backgrounds.  Problem Solving - Proven ability to troubleshoot and develop both creative and innovative solutions to project challenges. Successfully manages change for improved performance and greater team efficiency.    Ethic and ism - Solid  standards and excellent record of dependability. Maintains a clear focus on achieving bottom-line results and team integrity while ensuring progress.         Advanced Microsoft Power BI Skill Advanced R language Skill  Advanced Python Language Skill Advanced SQL Database Skill  Advanced Graph Database Skill (Neo4j Software) Advanced Azure Server Skill  Advanced Experience in SQL Query and Data Warehouse Advanced Tableau skill  Advanced SAP IS - Oil and Gas (Downstream) Skill Expert VBA Excel Skill  Proficiency in relational database  and   Advanced  in Petrel E&P Software and MATLAB  Experienced ing with several operating systems (iOS, Windows, and Linux)  Experience with SaaS or analytics software  Advanced  in Microsoft Office applications (Word, Excel, PowerPoint, Project, etc.)"
"  with around 7 years of experience in all phases of diverse  specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure. 
 ed on analyzing large datasets on distributed databases and developing Machine Learningalgorithms to gain operational insights and present them to the leadership. 
 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervisedand unsupervised modeling. 
 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies. 
 Expert in using of statistical  and programming languages (R, Python, Java, SQL, UNIX) 
 Adapted statistical programming languages like R and Python. 
 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms. 
 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool. 
 Created dashboards as part of Data Visualization using Tableau. 
 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool. 
 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.  Performed multiple Data Mining techniques and derive new insights from the data. 
 Creative problem-solver with strong analytical, leadership, and communication . 
 Proficient in Python, R, Scala, Java, SQL, and C. 
 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured
Data, Data Acquisition, Data Validation, and Predictive Modeling. 
 Experience in building Machine Learning, Sequential Modeling, Natural Language Processing (NLP). 
 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis,
Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering,
Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics. 
 Experienced in formulating and solving discrete and continuous optimization problems. 
 Able to research statistical machine learning, supervised learning, and classification methods.  Strong mathematical and statistical modeling and computer programming  in an innovative manner. 
 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine
(SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA),
Regression, Naïve Bayes, Support Vector Machines. 
 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets. 
 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights 
 Development of clear analytical reports which directly address strategic goals. 
 Identified and learn applicable new techniques independently as needed. 
 Able to  comfortably and effectively within an interdisciplinary research environment. 
 Experienced with validation of machine learning ensemble classifiers. 
 Utilized the online datasets to implement machine learning models using Spark ML for buildingprototypes.
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data scientist
Walmart - Jersey City, NJ
September 2017 to September 2017
Responsibilities: 
* Identified relevant data sources and sets to mine for client's business needs from large structuredand unstructured datasets and variables. 
* Identify and integrate new datasets from various data sources including Oracle, DB2, SQLServer,AWS, Azure by employing languages such as SPARK, HDFS, Hive and PIG Latin by ing closely with the data engineering team to strategize and execute the development of models 
* Performed data cleaning including transforming variables and dealing with missing value andensured data quality, consistency, integrity using Pandas, NumPy. 
* ed on Multiple datasets containing two billion values which are structured and unstructured dataabout web applications usage and online customer surveys. 
* Performed Data cleaning process applied Backward - Forward filling methods on dataset for handlingmissing values. 
* ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
* Design built and deployed a set of python modelling APIs for customer analytics, which integratemultiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs. 
* Supported client by developing Machine Learning Algorithms on Big Data using PySpark to analyzetransaction fraud, Cluster Analysis etc. 
* Used classification techniques including Random Forest and Logistic Regression to quantify thelikelihood of each user referring. 
* Designed and implemented end-to-end systems for Data Analytics and Automation, integratingcustom, visualization  using R, Tableau, and Power BI. 
* Implemented a Python-based distributed random forest via PySpark and MLlib. 
* Performed data wrangling methods using R programming and data visualizations using Tableau. 
* ed on Amazon Web Services (AWS) cloud services to do machine learning on big data. 
* Experience ing with Big Data  such as Hadoop - HDFS and MapReduce, Hive QL, Sqoop, Pig
Latin and Apache Spark (PySpark) 
* Collaborating with the project managers and business owners to understand their organizationalprocesses and help design the necessary reports. 
 
Environment: Python, R, SQL, HADOOP (HDFS) Horton s, AWS, SPARK, Hive, Tableau, MySQL
Data Scientist
Anthem or United Health Groups - Atlanta, GA
June 2015 to November 2016
Responsibilities: 
 Translated business questions into research s, design and conduct analyses, developfindings and synthesize recommendations to deliver valuable, relevant, and actionable insights.  Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings) 
 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developingvarious machine learning models such Random forest. 
 The missing data in the dataset is handled using Imputer method in SkLearn library. 
 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encodermethods in sklearn library. 
 Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for modeling. 
 Defined a generic classification function, which takes a model as input and determines the Accuracyand Cross-Validation scores. 
 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standardmachine learning datasets. 
 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenuefor future 
 ed with applied statistics and applied mathematics  for performance optimization. 
 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores.  Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms. 
 Used cross-validation to test the models with different batches of data to optimize the models andprevent over fitting. 
 Analyzed the SQL scripts and designed the solution to implement using PySpark and developedscripts as per the requirement. 
 ed with Tableau in order to represent the data in visual format and better describe the problemwith solutions. 
 
Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL
Data Analyst
Discover - Brookfield, WI
October 2013 to June 2015
Responsibilities: 
 Designed, implemented and automated modeling and analysis procedures on existing andexperimentally created data. 
 Increased pace & confidence of learning algorithm by combining state of the art statistical methods. 
 provided expertise and assistance in integrating advanced analytics into ongoing business processes
 
 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easilymaintainable format. 
 Collaborated with data engineers and operation team to implement the ETL process, wrote andoptimized SQL queries to perform data extraction to fit the analytical requirements. 
 Performed univariate and multivariate analysis on the data to identify any underlying pattern in thedata and associations between the variables. 
 ed on feature generating, PCA, feature normalization and label encoding with Scikit-learnpreprocessing. 
 Used Python (NumPy, SciPy, pandas, Scikit-learn, seaborn) and R to develop a variety of models andalgorithms for analytic purposes. 
 Analyzed the partitioned and bucketed data and compute various metrics for reporting 
 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to developoperational and visual reports for KPI monitoring. 
 Data analysis and visualization using (Python, R) 
 Collaborate with UI engineers, project managers, and designers to develop web portal thataggregates reports from various sources and . 
 Focused on front end features, browser manipulation, and cross-browser compatibility. 
 Used Agile Scrum for BI  across different clients, which allowed for production prototyping,rapid deployment and transparency. 
 
Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.
Data Analyst
VEDA Solutions PVT LTD - Hyderabad, Telangana
September 2012 to October 2013
Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%. 
 
Responsibilities: 
 ed on various phases of data mining, data collection, data cleaning, developing models,validation, and visualization. 
 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python. 
 Performed Statistical Analysis and Hypothesis Testing in Excel by using Data Analysis Tool. 
 Integrated data from disparate sources, mined large data set to identify patterns using predictiveanalysis 
 Conducted intermediate and advanced statistical analysis, such as linear regression, ANOVA, time-series analysis, classification models, and forecasting future sales. 
 Applied concepts of probability distribution and statistical inference on the given dataset to unearthinteresting findings using comparison, T-test, F-test, R-squared, P-value etc. 
 Used various libraries and developed various matching learning algorithms using Pandas, NumPy,
Seaborn, Scipy, matplotlib, Scikit-learn in python. 
 Identified patterns, data quality issues, and opportunities and leveraged insights by communicatingopportunities with business partners. 
 Designed Predictive analysis algorithms using Historical Data. 
 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linearregression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN. 
 ed on Python modules for machine learning and predictive analytics. 
 ed on Reporting tool to Test, Validate Data Integrity of Reports. 
 
Environment: Python, IPython, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.
Education

Bachelor's


PYTHON (4 years), SQL (4 years), Hadoop (2 years), HADOOP (2 years), MACHINE LEARNING (2 years)
Additional Information

 : 
Programming languages Python, Java, R. C 
Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB,
R-Studio 
Data Science 
Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception, Keras, TensFlow, PyTorch 
 
Operating systems Windows, Linux, Mac 
Databases MySQL, MS SQL Server, NoSQL 
Cloud Technologies AWS, Azure. 
Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL",Data Scientist,resume,"  with around 7 years of experience in all phases of diverse  specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.   ed on analyzing large datasets on distributed databases and developing Machine Learningalgorithms to gain operational insights and present them to the leadership.   Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervisedand unsupervised modeling.   Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.   Expert in using of statistical  and programming languages (R, Python, Java, SQL, UNIX)   Adapted statistical programming languages like R and Python.   Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.   Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.   Created dashboards as part of Data Visualization using Tableau.   Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.   Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.  Performed multiple Data Mining techniques and derive new insights from the data.   Creative problem-solver with strong analytical, leadership, and communication .   Proficient in Python, R, Scala, Java, SQL, and C.   Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling.   Experience in building Machine Learning, Sequential Modeling, Natural Language Processing (NLP).   Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics.   Experienced in formulating and solving discrete and continuous optimization problems.   Able to research statistical machine learning, supervised learning, and classification methods.  Strong mathematical and statistical modeling and computer programming  in an innovative manner.   Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, Naïve Bayes, Support Vector Machines.   Experienced in AWS cloud computing, Spark, and capable of ing with large datasets.   Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights   Development of clear analytical reports which directly address strategic goals.   Identified and learn applicable new techniques independently as needed.   Able to  comfortably and effectively within an interdisciplinary research environment.   Experienced with validation of machine learning ensemble classifiers.   Utilized the online datasets to implement machine learning models using Spark ML for buildingprototypes. Willing to relocate: Anywhere Sponsorship required to  in the US  Experience  Data scientist Walmart - Jersey City, NJ September 2017 to September 2017 Responsibilities:  * Identified relevant data sources and sets to mine for client's business needs from large structuredand unstructured datasets and variables.  * Identify and integrate new datasets from various data sources including Oracle, DB2, SQLServer,AWS, Azure by employing languages such as SPARK, HDFS, Hive and PIG Latin by ing closely with the data engineering team to strategize and execute the development of models  * Performed data cleaning including transforming variables and dealing with missing value andensured data quality, consistency, integrity using Pandas, NumPy.  * ed on Multiple datasets containing two billion values which are structured and unstructured dataabout web applications usage and online customer surveys.  * Performed Data cleaning process applied Backward - Forward filling methods on dataset for handlingmissing values.  * ed on different data formats such as JSON, XML and performed machine learning algorithms inPython.  * Design built and deployed a set of python modelling APIs for customer analytics, which integratemultiple machine learning techniques for various user behavior prediction and support multiple marketing segmentation programs.  * Supported client by developing Machine Learning Algorithms on Big Data using PySpark to analyzetransaction fraud, Cluster Analysis etc.  * Used classification techniques including Random Forest and Logistic Regression to quantify thelikelihood of each user referring.  * Designed and implemented end-to-end systems for Data Analytics and Automation, integratingcustom, visualization  using R, Tableau, and Power BI.  * Implemented a Python-based distributed random forest via PySpark and MLlib.  * Performed data wrangling methods using R programming and data visualizations using Tableau.  * ed on Amazon Web Services (AWS) cloud services to do machine learning on big data.  * Experience ing with Big Data  such as Hadoop - HDFS and MapReduce, Hive QL, Sqoop, Pig Latin and Apache Spark (PySpark)  * Collaborating with the project managers and business owners to understand their organizationalprocesses and help design the necessary reports.    Environment: Python, R, SQL, HADOOP (HDFS) Horton s, AWS, SPARK, Hive, Tableau, MySQL Data Scientist Anthem or United Health Groups - Atlanta, GA June 2015 to November 2016 Responsibilities:   Translated business questions into research s, design and conduct analyses, developfindings and synthesize recommendations to deliver valuable, relevant, and actionable insights.  Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)   Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developingvarious machine learning models such Random forest.   The missing data in the dataset is handled using Imputer method in SkLearn library.   Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encodermethods in sklearn library.   Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for modeling.   Defined a generic classification function, which takes a model as input and determines the Accuracyand Cross-Validation scores.   Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standardmachine learning datasets.   Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenuefor future   ed with applied statistics and applied mathematics  for performance optimization.   ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores.  Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms.   Used cross-validation to test the models with different batches of data to optimize the models andprevent over fitting.   Analyzed the SQL scripts and designed the solution to implement using PySpark and developedscripts as per the requirement.   ed with Tableau in order to represent the data in visual format and better describe the problemwith solutions.    Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL Data Analyst Discover - Brookfield, WI October 2013 to June 2015 Responsibilities:   Designed, implemented and automated modeling and analysis procedures on existing andexperimentally created data.   Increased pace & confidence of learning algorithm by combining state of the art statistical methods.   provided expertise and assistance in integrating advanced analytics into ongoing business processes    Parsed data, producing concise conclusions from raw data in a clean, well-structured and easilymaintainable format.   Collaborated with data engineers and operation team to implement the ETL process, wrote andoptimized SQL queries to perform data extraction to fit the analytical requirements.   Performed univariate and multivariate analysis on the data to identify any underlying pattern in thedata and associations between the variables.   ed on feature generating, PCA, feature normalization and label encoding with Scikit-learnpreprocessing.   Used Python (NumPy, SciPy, pandas, Scikit-learn, seaborn) and R to develop a variety of models andalgorithms for analytic purposes.   Analyzed the partitioned and bucketed data and compute various metrics for reporting   Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to developoperational and visual reports for KPI monitoring.   Data analysis and visualization using (Python, R)   Collaborate with UI engineers, project managers, and designers to develop web portal thataggregates reports from various sources and .   Focused on front end features, browser manipulation, and cross-browser compatibility.   Used Agile Scrum for BI  across different clients, which allowed for production prototyping,rapid deployment and transparency.    Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop. Data Analyst VEDA Solutions PVT LTD - Hyderabad, Telangana September 2012 to October 2013 Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.    Responsibilities:   ed on various phases of data mining, data collection, data cleaning, developing models,validation, and visualization.   Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.   Performed Statistical Analysis and Hypothesis Testing in Excel by using Data Analysis Tool.   Integrated data from disparate sources, mined large data set to identify patterns using predictiveanalysis   Conducted intermediate and advanced statistical analysis, such as linear regression, ANOVA, time-series analysis, classification models, and forecasting future sales.   Applied concepts of probability distribution and statistical inference on the given dataset to unearthinteresting findings using comparison, T-test, F-test, R-squared, P-value etc.   Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.   Identified patterns, data quality issues, and opportunities and leveraged insights by communicatingopportunities with business partners.   Designed Predictive analysis algorithms using Historical Data.   Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linearregression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.   ed on Python modules for machine learning and predictive analytics.   ed on Reporting tool to Test, Validate Data Integrity of Reports.    Environment: Python, IPython, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc. Education  Bachelor's   PYTHON (4 years), SQL (4 years), Hadoop (2 years), HADOOP (2 years), MACHINE LEARNING (2 years) Additional Information   :  Programming languages Python, Java, R. C  Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio  Data Science  Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception, Keras, TensFlow, PyTorch    Operating systems Windows, Linux, Mac  Databases MySQL, MS SQL Server, NoSQL  Cloud Technologies AWS, Azure.  Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL"
"Motivated, team-oriented and responsible Data Scientist with 2 years of experience in analyzing market data by using machine learning techniques. Strong knowledge and experience in Predictive analytics, Machine Learning, Deep Learning, Data Visualization and Statistical Modeling.
 Experience

Data Scientist
Akuna Capital - Chicago, IL
January 2018 to Present
Responsibilities: 
 Optimized the existing trading strategies and participated in developing market prediction andtrading strategies using machine learning algorithms such as Linear regression, K-Nearest Neighbors
(KNN), Decision trees, Boosting and Recurrent Neural Nets (RNN). 
 Optimized the existing portfolio optimizer with historical data to improve the performance ofportfolios and asset selection. Designed the new portfolio optimizer with optimization machine learning algorithms for the improvement of portfolio construction. 
 Ranked and selected candidate features by using Principal Component Analysis (PCA). 
 Participated in developing quantitative models using statistical and machine learning algorithms todescribe market behavior. 
 Developed trade signal based on prediction model to guide the trade direction. 
 Evaluated the executed trading strategies based on the trade results for the future optimization.  ed with analytics and trader team to further improve and refine quantitative . 
 Acquisition of new data sets, wrangling and maintenance of new and existing data sets.


Master of Statistics in Statistics
North Carolina State University - Raleigh, NC May 2016 to June 2018
Bachelor of Science in Mathematics in Mathematics
Zhejiang University - Hangzhou, CN
September 2011 to August 2014


Decision trees, Linear regression, Logistic regression, Machine learning, Neural nets, Random forests, Svm, Support vector machines, Natural language processing, Nlp, Natural, Python, Keras, Matplotlib, Numpy, Pandas, Tensorflow, Ms access, Ms sql server, Sql server, Excel, SQL, Business Intelligence, testing, access
Additional Information

  
Programming Languages: Python, R, SAS, SQL 
Packages: Pandas, NumPy, Scikit-learn, SciPy, Seaborn, Matplotlib, NLTK, TensorFlow, Keras 
Databases: MS SQL Server, MySQL, MS Access 
Machine Learning : Linear Regression, Logistic Regression, Decision Trees, Naive Bayes, KNearest Neighbors, Random Forests, Support Vector Machines (SVM), Convolutional Neural Nets
(CNN), Recurrent Neural Nets (RNN), Generative Adversarial Net (GAN), 
XGBoost, Natural Language Processing (NLP)",Data Scientist,resume,"Motivated, team-oriented and responsible Data Scientist with 2 years of experience in analyzing market data by using machine learning techniques. Strong knowledge and experience in Predictive analytics, Machine Learning, Deep Learning, Data Visualization and Statistical Modeling.  Experience  Data Scientist Akuna Capital - Chicago, IL January 2018 to Present Responsibilities:   Optimized the existing trading strategies and participated in developing market prediction andtrading strategies using machine learning algorithms such as Linear regression, K-Nearest Neighbors (KNN), Decision trees, Boosting and Recurrent Neural Nets (RNN).   Optimized the existing portfolio optimizer with historical data to improve the performance ofportfolios and asset selection. Designed the new portfolio optimizer with optimization machine learning algorithms for the improvement of portfolio construction.   Ranked and selected candidate features by using Principal Component Analysis (PCA).   Participated in developing quantitative models using statistical and machine learning algorithms todescribe market behavior.   Developed trade signal based on prediction model to guide the trade direction.   Evaluated the executed trading strategies based on the trade results for the future optimization.  ed with analytics and trader team to further improve and refine quantitative .   Acquisition of new data sets, wrangling and maintenance of new and existing data sets.   Master of Statistics in Statistics North Carolina State University - Raleigh, NC May 2016 to June 2018 Bachelor of Science in Mathematics in Mathematics Zhejiang University - Hangzhou, CN September 2011 to August 2014   Decision trees, Linear regression, Logistic regression, Machine learning, Neural nets, Random forests, Svm, Support vector machines, Natural language processing, Nlp, Natural, Python, Keras, Matplotlib, Numpy, Pandas, Tensorflow, Ms access, Ms sql server, Sql server, Excel, SQL, Business Intelligence, testing, access Additional Information     Programming Languages: Python, R, SAS, SQL  Packages: Pandas, NumPy, Scikit-learn, SciPy, Seaborn, Matplotlib, NLTK, TensorFlow, Keras  Databases: MS SQL Server, MySQL, MS Access  Machine Learning : Linear Regression, Logistic Regression, Decision Trees, Naive Bayes, KNearest Neighbors, Random Forests, Support Vector Machines (SVM), Convolutional Neural Nets (CNN), Recurrent Neural Nets (RNN), Generative Adversarial Net (GAN),  XGBoost, Natural Language Processing (NLP)"
"   
: 	 	 	 	 	 	 	 	 	 	 	 
 A highly motivated, detail-oriented and performance-driven  with relevant years of industrial experience in Python development. Seeking full time opportunity in an organization where I can effectively utilize my Analytical, Machine Learning, Software Development, Problem solving and NLP  relevant in moving the organization forward. 
:   
      University of Illinois at Chicago, Chicago, IL  	 	 	 	 	            	                                   May 2019 Master of Science (MS)  Computer Science (Teaching Assistant) 	 	 	 	 	              GPA - 3.44/4 
Computer Algorithms, Machine Learning, Data Mining, Neural Nets, NLP, Computer Vision, Recommendation Systems 
      SRM University, India   	 	 	 	 	 	 	 	  	                   June 2013 
	Bachelors  Computer Science   	              	 	 	 	 	                             GPA: 8.42/10 
Data Structures, Analysis of Algorithms, Database, Software Engineering, Web Programming, Java 
 :                                                                                                  
Programming Languages: Python (Numpy, Pandas, NLTK, OpenCV, Matplotlib, Scikit-learn, TensorFlow), Matlab, C++, R 
Database : SQL, PostgreSQL 
Other : Django, Flask, REST, Git, Hadoop, Tableau, AWS, ETL, Snowflake 
Data Science: Predictive Modeling, Linear/Logistics regression, Bagging, NLP  and corpora (NLTK, Stanford CoreNLP), Random forest, Bayesian Learning, SVM, PCA, Clustering, Neural Nets (CNN, RNN, LSTM) Operating Systems: Linux, Unix, Mac OS X, Windows 
CORE :          ? Machine Learning ? ETL and Snowflake Datawarehouse ? Neural Nets/Deep Learning ? Statistical Analysis ? Data Structures & Algorithms ? Data Mining & Text Mining 
* Natural Language Processing (NLP) 	? Database 
 DATA SCIENCE ACADEMIC RESEARCH EXPERIENCE:   	 	 	 	 	 	 AUG 2017-May 2019 ? Sentiment Analysis of Twitter Corpus - Python, Scikit-learn, NLTK, NLP, Pandas, Numpy, TensorFlow  	  o Cleaned, structured, analysed the data and generated features followed by application of ML and DL models such as Naïve Bayes, SVM, K-nearest neighbours, Decision trees, Random Forest, Logistic Regression, PCA, CNN & LSTM 
o Build a classifier to determine the sentiment of the corpus  means positivity and negativity with 84% accuracy 
* Housing Sale Price prediction based on 79 initial features Python, NLTK, Scikit-learn, pandas, Numpy, TensorFlow o Cleaned and pre-processed the raw data, performed statistical analysis and applied feature engineering o Predicted Sale Prices to minimize the Cost function using Machine learning models & Neural Nets o Explored Linear regression with regularization methods, Random Forest and Deep Learning models 
* Designed Automatic License Plate Recognition Using Computer Vision and Neural Net o It takes the image of a vehicle as the input and outputs the characters written on its license plate 
o Utilised various Image processing algorithms like Connected Components, Contour detection, thresholding, 
       Filtering techniques, Edge detection and detection of boundary box ? Question/answering system in NLP, employing only text using category database o Found the category of each question using Named Entity Recognition and other rules o Created Parse tree and dependency tree of questions to understand and generalise the pattern o Develop a SQL Query to query relevant answer from the corresponding category database 
* Designed a neural net in Python without using pre-defined libraries with 400 hidden neurons for digit classification using the backpropagation algorithm on MNIST data set and achieved accuracy of more than 95% INDUSTRIAL EXPERIENCE:  
CCC Information Services, Chicago, IL 	 	 	              	 	                                   Jun 2018-Aug 2018 Data Science Intern - Python (OpenCV, Pandas, Numpy, scikit-learn), Linux, Machine Learning, Neural Nets (CNN) ? Automated data collection, pre-processing and cleaning for Neural net application 
* Used Image processing to automate the panel segmentation process for any car image by outlining the panels 
	Quadeye Securities  Financial Firm, India  	 	 	 	 	 	 	 Aug 2016Aug 2017 
Software Developer - Python, JSON, REST, Linux, UNIX, SQL, GIT, Django, Pandas, Numpy, AWS 
* Automated the monitoring process and error tracking process across 45 servers for a high frequency trading company 
	Maplelabs by Xoriant Solutions, India   	 	 	 	 	                                    Jul 2015Jun 2016 
Software Engineer - Python, Django, JSON, REST, SQL, Linux, Unix, GIT, ETL 
* Proficiently developed neting application to automate the installation and upgradation of software images and configuration files on switches, connected in different topologies 
	Wipro , India  	 	 	 	 	 	 	 	 	    Dec 2013Jul 2015 
Project Engineer  International Clients  Python, Greenplum, SQL, C, Linux ? Provided support and handled issues using logs of Greenplum DB by Pivotal ",Data Scientist,resume,"    : 	 	 	 	 	 	 	 	 	 	 	   A highly motivated, detail-oriented and performance-driven  with relevant years of industrial experience in Python development. Seeking full time opportunity in an organization where I can effectively utilize my Analytical, Machine Learning, Software Development, Problem solving and NLP  relevant in moving the organization forward.  :          University of Illinois at Chicago, Chicago, IL  	 	 	 	 	            	                                   May 2019 Master of Science (MS)  Computer Science (Teaching Assistant) 	 	 	 	 	              GPA - 3.44/4  Computer Algorithms, Machine Learning, Data Mining, Neural Nets, NLP, Computer Vision, Recommendation Systems        SRM University, India   	 	 	 	 	 	 	 	  	                   June 2013  	Bachelors  Computer Science   	              	 	 	 	 	                             GPA: 8.42/10  Data Structures, Analysis of Algorithms, Database, Software Engineering, Web Programming, Java   :                                                                                                   Programming Languages: Python (Numpy, Pandas, NLTK, OpenCV, Matplotlib, Scikit-learn, TensorFlow), Matlab, C++, R  Database : SQL, PostgreSQL  Other : Django, Flask, REST, Git, Hadoop, Tableau, AWS, ETL, Snowflake  Data Science: Predictive Modeling, Linear/Logistics regression, Bagging, NLP  and corpora (NLTK, Stanford CoreNLP), Random forest, Bayesian Learning, SVM, PCA, Clustering, Neural Nets (CNN, RNN, LSTM) Operating Systems: Linux, Unix, Mac OS X, Windows  CORE :          ? Machine Learning ? ETL and Snowflake Datawarehouse ? Neural Nets/Deep Learning ? Statistical Analysis ? Data Structures & Algorithms ? Data Mining & Text Mining  * Natural Language Processing (NLP) 	? Database   DATA SCIENCE ACADEMIC RESEARCH EXPERIENCE:   	 	 	 	 	 	 AUG 2017-May 2019 ? Sentiment Analysis of Twitter Corpus - Python, Scikit-learn, NLTK, NLP, Pandas, Numpy, TensorFlow  	  o Cleaned, structured, analysed the data and generated features followed by application of ML and DL models such as Naïve Bayes, SVM, K-nearest neighbours, Decision trees, Random Forest, Logistic Regression, PCA, CNN & LSTM  o Build a classifier to determine the sentiment of the corpus  means positivity and negativity with 84% accuracy  * Housing Sale Price prediction based on 79 initial features Python, NLTK, Scikit-learn, pandas, Numpy, TensorFlow o Cleaned and pre-processed the raw data, performed statistical analysis and applied feature engineering o Predicted Sale Prices to minimize the Cost function using Machine learning models & Neural Nets o Explored Linear regression with regularization methods, Random Forest and Deep Learning models  * Designed Automatic License Plate Recognition Using Computer Vision and Neural Net o It takes the image of a vehicle as the input and outputs the characters written on its license plate  o Utilised various Image processing algorithms like Connected Components, Contour detection, thresholding,         Filtering techniques, Edge detection and detection of boundary box ? Question/answering system in NLP, employing only text using category database o Found the category of each question using Named Entity Recognition and other rules o Created Parse tree and dependency tree of questions to understand and generalise the pattern o Develop a SQL Query to query relevant answer from the corresponding category database  * Designed a neural net in Python without using pre-defined libraries with 400 hidden neurons for digit classification using the backpropagation algorithm on MNIST data set and achieved accuracy of more than 95% INDUSTRIAL EXPERIENCE:   CCC Information Services, Chicago, IL 	 	 	              	 	                                   Jun 2018-Aug 2018 Data Science Intern - Python (OpenCV, Pandas, Numpy, scikit-learn), Linux, Machine Learning, Neural Nets (CNN) ? Automated data collection, pre-processing and cleaning for Neural net application  * Used Image processing to automate the panel segmentation process for any car image by outlining the panels  	Quadeye Securities  Financial Firm, India  	 	 	 	 	 	 	 Aug 2016Aug 2017  Software Developer - Python, JSON, REST, Linux, UNIX, SQL, GIT, Django, Pandas, Numpy, AWS  * Automated the monitoring process and error tracking process across 45 servers for a high frequency trading company  	Maplelabs by Xoriant Solutions, India   	 	 	 	 	                                    Jul 2015Jun 2016  Software Engineer - Python, Django, JSON, REST, SQL, Linux, Unix, GIT, ETL  * Proficiently developed neting application to automate the installation and upgradation of software images and configuration files on switches, connected in different topologies  	Wipro , India  	 	 	 	 	 	 	 	 	    Dec 2013Jul 2015  Project Engineer  International Clients  Python, Greenplum, SQL, C, Linux ? Provided support and handled issues using logs of Greenplum DB by Pivotal "
"I am an ambitious individual who is excited to start a career in an analyst position. I excel in statistical programming, including data analysis, data visualization, and predictive modeling using R and RStudio. I am excited to put these  to use, help a company with their data needs, and to pursue a career in the statistics field.
Willing to relocate to: Ontario, CA - Lake Elsinore, CA - Irvine, CA
Authorized to  in the US for any employer
 Experience

Notetaker for Disability Services
Disability Services - Riverside, CA January 2017 to Present
 Acted as a liaison between the students and the professors ensuring the students had what theyneeded to be successful 
 Attended select classes for individuals with disabilities in order to take notes on lectures 
 Kept confidentiality between myself and the students to provide normalcy in their 
Teacher's Assistant/Grader
California Baptist University - Riverside, CA January 2018 to May 2018
 Graded home and quizzes and input the scores to the grading system so students could keeptrack of their grades 
 Frequently discussed with the professor about assignment deadlines and rubrics
Hardlines Team Member
Target - Corona, CA
July 2016 to November 2016
 Assisted customers with finding the correct merchandise 
 Collaborated with a team to effectively (re)stock and organize the products resulting in more sales 
 Accelerated the checkout process by helping at the registers and providing cashier services


Bachelor of Science in Applied Statistical Analysis in Actuarial Science
California Baptist University - Riverside, CA
September 2015 to May 2019


Microsoft Office (7 years), Python (Less than 1 year), R and R Studio (3 years), Latex (1 year), Excel, Powerpoint
Links


Additional Information

Some of my strengths are: 
Hard-ing  
Discipline  
Leader 
Team player  
Organization  
Attention to Detail 
Data Visualization 
Data Analytics  
Quick learner  
Predictive modeling 
Good Communicator 
Data Interpretation",Data Scientist,resume,"I am an ambitious individual who is excited to start a career in an analyst position. I excel in statistical programming, including data analysis, data visualization, and predictive modeling using R and RStudio. I am excited to put these  to use, help a company with their data needs, and to pursue a career in the statistics field. Willing to relocate to: Ontario, CA - Lake Elsinore, CA - Irvine, CA Authorized to  in the US for any employer  Experience  Notetaker for Disability Services Disability Services - Riverside, CA January 2017 to Present  Acted as a liaison between the students and the professors ensuring the students had what theyneeded to be successful   Attended select classes for individuals with disabilities in order to take notes on lectures   Kept confidentiality between myself and the students to provide normalcy in their  Teacher's Assistant/Grader California Baptist University - Riverside, CA January 2018 to May 2018  Graded home and quizzes and input the scores to the grading system so students could keeptrack of their grades   Frequently discussed with the professor about assignment deadlines and rubrics Hardlines Team Member Target - Corona, CA July 2016 to November 2016  Assisted customers with finding the correct merchandise   Collaborated with a team to effectively (re)stock and organize the products resulting in more sales   Accelerated the checkout process by helping at the registers and providing cashier services   Bachelor of Science in Applied Statistical Analysis in Actuarial Science California Baptist University - Riverside, CA September 2015 to May 2019   Microsoft Office (7 years), Python (Less than 1 year), R and R Studio (3 years), Latex (1 year), Excel, Powerpoint Links   Additional Information  Some of my strengths are:  Hard-ing   Discipline   Leader  Team player   Organization   Attention to Detail  Data Visualization  Data Analytics   Quick learner   Predictive modeling  Good Communicator  Data Interpretation"
" Experience

Research Analyst
North East Independent School District - San Antonio, TX
August 2019 to Present
 Review external research requests 
 Digitize data reports using Tableau-dashboards/stories 
 Create surveys using Qualtrics
Junior Data Analyst (Contract Position-Remote)
Pactera  International Limited
June 2019 to August 2019
 Utilized the tool Address Point Judge to analyze large quantities of address data. 
 Verified accuracy while demonstrating the ability to understand, follow, and apply software developerguidelines.
Statistical Consultant
Sportavida LLC - San Antonio, TX
February 2019 to March 2019
 Prepared descriptive statistics and graphics for a season of college football data using SASprogramming. 
 Prepared regression analyses showing relationships between measures such as a fatigue biomarkerand biomarkers indicative of general stress (cortisol levels) and specific tissue stress (muscle, softtissue, cardiac).
Graduate Teaching Assistant I
University of Texas at San Antonio-Department of Data Management and Statistics - San Antonio, TX January 2016 to December 2017
 Held office hours for undergraduate students each week. 
 Maintained email correspondence with professors and undergraduate students on a daily basis. 
 Graded home/ and exams and submitted grades in Blackboard. 
 Proctored exams and aided professors in the department with exams when needed. 
 Helped fellow teaching assistants with grading and inputting grades into Blackboard. 
 Assisted undergraduate students with questions regarding statistics home and .
Data Maintenance - San Antonio, TX
August 2014 to September 2015
Oblate Missions-Missionary Association of Mary Immaculate, San Antonio, TX 
? Retrieved donor account numbers for pledges by searching for and matching donor names andaddresses. 
? Maintained a database of donor accounts by inputting, removing, and revising requests fromreturned donation pledges. 
? Corrected mailing addresses and created mailing labels in Microsoft Word. 
? Balanced pledges using StudioEnterprise software and generated basic excel spreadsheets in orderto keep track of data entry errors.
Office Clerk
Denton, Navarro, Rocha, Bernal, Hyde & Zech, P.C - San Antonio, TX
April 2014 to July 2014
Recorded reimbursable office expenses using QuickBooks. 
? Resolved error issues with Sage Timeslips and QuickBooks by troubleshooting with support. 
? Scanned, copied, and emailed legal documents to secretaries and the office administrator. ? Determined postage for outgoing legal mail and delivered it to the post office at the end of each day.


Master of Science in Applied Statistics in Applied Statistics
University of Texas at San Antonio - San Antonio, TX
August 2018
Bachelor of Arts in Mathematics in Mathematics
St. Mary's University - San Antonio, TX
August 2014",Data Scientist,resume," Experience  Research Analyst North East Independent School District - San Antonio, TX August 2019 to Present  Review external research requests   Digitize data reports using Tableau-dashboards/stories   Create surveys using Qualtrics Junior Data Analyst (Contract Position-Remote) Pactera  International Limited June 2019 to August 2019  Utilized the tool Address Point Judge to analyze large quantities of address data.   Verified accuracy while demonstrating the ability to understand, follow, and apply software developerguidelines. Statistical Consultant Sportavida LLC - San Antonio, TX February 2019 to March 2019  Prepared descriptive statistics and graphics for a season of college football data using SASprogramming.   Prepared regression analyses showing relationships between measures such as a fatigue biomarkerand biomarkers indicative of general stress (cortisol levels) and specific tissue stress (muscle, softtissue, cardiac). Graduate Teaching Assistant I University of Texas at San Antonio-Department of Data Management and Statistics - San Antonio, TX January 2016 to December 2017  Held office hours for undergraduate students each week.   Maintained email correspondence with professors and undergraduate students on a daily basis.   Graded home/ and exams and submitted grades in Blackboard.   Proctored exams and aided professors in the department with exams when needed.   Helped fellow teaching assistants with grading and inputting grades into Blackboard.   Assisted undergraduate students with questions regarding statistics home and . Data Maintenance - San Antonio, TX August 2014 to September 2015 Oblate Missions-Missionary Association of Mary Immaculate, San Antonio, TX  ? Retrieved donor account numbers for pledges by searching for and matching donor names andaddresses.  ? Maintained a database of donor accounts by inputting, removing, and revising requests fromreturned donation pledges.  ? Corrected mailing addresses and created mailing labels in Microsoft Word.  ? Balanced pledges using StudioEnterprise software and generated basic excel spreadsheets in orderto keep track of data entry errors. Office Clerk Denton, Navarro, Rocha, Bernal, Hyde & Zech, P.C - San Antonio, TX April 2014 to July 2014 Recorded reimbursable office expenses using QuickBooks.  ? Resolved error issues with Sage Timeslips and QuickBooks by troubleshooting with support.  ? Scanned, copied, and emailed legal documents to secretaries and the office administrator. ? Determined postage for outgoing legal mail and delivered it to the post office at the end of each day.   Master of Science in Applied Statistics in Applied Statistics University of Texas at San Antonio - San Antonio, TX August 2018 Bachelor of Arts in Mathematics in Mathematics St. Mary's University - San Antonio, TX August 2014"
"Data Mining Analyst and SAP application engineer in a mid-size e-Commerce company more than 11 years, one year analysis and prediction experience with Python, A resourceful engineer with strong understanding of machine learning approaches and big data ecosystems. Proactive in learning new concepts and apply in , highly self-motivated, able to  well with a team as well as independently.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Mining Analyst
Newegg Inc. - Industry, CA
September 2018 to Present
1) Monitor operating metrics 
2) Explore and analyze the business data sets 
3) Development and optimize predictive models, 
4) Build visualization to illustrate trends 
5) Customer Segmentation for market promotions and A/B test 
6) With SMOTE to process the imbalanced data for PO anti-Fraud
SAP Application Engineer
Newegg Inc. - Industry, CA
November 2007 to August 2018
1) Configure SAP ERP/CRM system, maintain SAP server, troubleshoot SAP issue 
2) Handle change request and user role in FI/CO,MM/SD and HR module, support for end-user 
3) Monitor and optimize SAP production system performance, enhance system security 
4) Research SAP HAHA and S/4 application, deploy testing environment 
5) Upgrade SAP hardware and software component 
6) Implement data interface between SAP and external system 
7) Data volume management, archive transaction data 
8) Support and compliant with SOX ITGC audit activities 
9) SAP R/3 system backup/restore, system copy and client copy 
10) SAP Concur integrate with ECC


Bachelor's in Information and Computing Science
Xiangtan University - Xiangtan, Hunan
September 1999 to June 2003
Bachelor's


SAP (10+ years), Data Mining, machine learning (1 year), Python, Java R, SQL, Tableau, Scikit-Learn, Hadoop, Spark (1 year)
Certifications/Licenses

SAP Certified  Associate - System Administration(Oracle DB) with SAP NetWeaver 7.0 EhP2
May 2014 to Present
LPIC-1
October 2015
Linux System Administration",Data Scientist,resume,"Data Mining Analyst and SAP application engineer in a mid-size e-Commerce company more than 11 years, one year analysis and prediction experience with Python, A resourceful engineer with strong understanding of machine learning approaches and big data ecosystems. Proactive in learning new concepts and apply in , highly self-motivated, able to  well with a team as well as independently. Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Data Mining Analyst Newegg Inc. - Industry, CA September 2018 to Present 1) Monitor operating metrics  2) Explore and analyze the business data sets  3) Development and optimize predictive models,  4) Build visualization to illustrate trends  5) Customer Segmentation for market promotions and A/B test  6) With SMOTE to process the imbalanced data for PO anti-Fraud SAP Application Engineer Newegg Inc. - Industry, CA November 2007 to August 2018 1) Configure SAP ERP/CRM system, maintain SAP server, troubleshoot SAP issue  2) Handle change request and user role in FI/CO,MM/SD and HR module, support for end-user  3) Monitor and optimize SAP production system performance, enhance system security  4) Research SAP HAHA and S/4 application, deploy testing environment  5) Upgrade SAP hardware and software component  6) Implement data interface between SAP and external system  7) Data volume management, archive transaction data  8) Support and compliant with SOX ITGC audit activities  9) SAP R/3 system backup/restore, system copy and client copy  10) SAP Concur integrate with ECC   Bachelor's in Information and Computing Science Xiangtan University - Xiangtan, Hunan September 1999 to June 2003 Bachelor's   SAP (10+ years), Data Mining, machine learning (1 year), Python, Java R, SQL, Tableau, Scikit-Learn, Hadoop, Spark (1 year) Certifications/Licenses  SAP Certified  Associate - System Administration(Oracle DB) with SAP NetWeaver 7.0 EhP2 May 2014 to Present LPIC-1 October 2015 Linux System Administration"
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Scientist
BESHTON SOFTWARE INC - Santa Clara, CA
May 2019 to Present
1. Cardiovascular Disease(CVD) Prediction 
Implemented data cleaning and standardization for datasets consisting of 70,000 records of patients data in 12 features, including medical history and patients behavior. 
Performed exploratory data analysis(EDA) to detect outliers, and presented to non-experts 
Build binary classifier with Python to predict presence or absence of cardiovascular disease, achieving 73% accuracy 
  
2. Document Classification 
Build multinomial classifier with Python to classify legal documents (with 41 classes). 
Extracted features from documents with cutting edge method such as LDA, TF-IDF and Word2vec. Trained the extracted data with various models including Random Forest and Softmax, achieving 65% accuracy. 
         
3. Rating Behavior Prediction 
Implemented data extraction and cleaning from Guokr using Python- Over 100,000 questions, answers and rating results. 
Extracted features for text mining and performed different models with TensorFlow for users rating prediction. 
Built algorithm for multilayer random-grouping and decision-making strategy using Python
Kaggle Competition: House Prices Prediction
Kaggle
January 2019 to February 2019
Used Python to standardize and clean datasets with 79 explanatory variables describing every aspect of residential homes. Used MXNET to build a multilayer perceptron model to predict house prices, achieving 12% RMSE 
Kaggle Competition: Leaf Classification with Neural Net
Kaggle
October 2017 to December 2017
Used TensorFlow to build a Multinomial Logistic Regression with a single layer for leaf classification and extend the case to a multilayer Convolutional Neural Net.  
Used Python to prepare and clean data for machine learning, implement Random Forest and K-Nearest Neighbors algorithm on the datasets and compare these models. 


Master of Science in Information Systems
Claremont Graduate University - Claremont, CA
June 2018
Master of Science in Mathematics
Claremont Graduate University - Claremont, CA December 2015
Bachelor of Science in Financial Management
Southwestern University of Finance and Economics
June 2013


Matlab (2 years), Python (2 years), Sql (2 years), access, Excel, Business Intelligence, MS Office, testing
Additional Information

Machine learning: TensorFlow, MXNET, NLP, AWS, Google Cloud,  
Programming: Python, R, MySQL, Tableau 
Mathematics: Statistics, Time Series, AB Testing, ",Data Scientist,resume,"Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Data Scientist BESHTON SOFTWARE INC - Santa Clara, CA May 2019 to Present 1. Cardiovascular Disease(CVD) Prediction  Implemented data cleaning and standardization for datasets consisting of 70,000 records of patients data in 12 features, including medical history and patients behavior.  Performed exploratory data analysis(EDA) to detect outliers, and presented to non-experts  Build binary classifier with Python to predict presence or absence of cardiovascular disease, achieving 73% accuracy     2. Document Classification  Build multinomial classifier with Python to classify legal documents (with 41 classes).  Extracted features from documents with cutting edge method such as LDA, TF-IDF and Word2vec. Trained the extracted data with various models including Random Forest and Softmax, achieving 65% accuracy.            3. Rating Behavior Prediction  Implemented data extraction and cleaning from Guokr using Python- Over 100,000 questions, answers and rating results.  Extracted features for text mining and performed different models with TensorFlow for users rating prediction.  Built algorithm for multilayer random-grouping and decision-making strategy using Python Kaggle Competition: House Prices Prediction Kaggle January 2019 to February 2019 Used Python to standardize and clean datasets with 79 explanatory variables describing every aspect of residential homes. Used MXNET to build a multilayer perceptron model to predict house prices, achieving 12% RMSE  Kaggle Competition: Leaf Classification with Neural Net Kaggle October 2017 to December 2017 Used TensorFlow to build a Multinomial Logistic Regression with a single layer for leaf classification and extend the case to a multilayer Convolutional Neural Net.   Used Python to prepare and clean data for machine learning, implement Random Forest and K-Nearest Neighbors algorithm on the datasets and compare these models.    Master of Science in Information Systems Claremont Graduate University - Claremont, CA June 2018 Master of Science in Mathematics Claremont Graduate University - Claremont, CA December 2015 Bachelor of Science in Financial Management Southwestern University of Finance and Economics June 2013   Matlab (2 years), Python (2 years), Sql (2 years), access, Excel, Business Intelligence, MS Office, testing Additional Information  Machine learning: TensorFlow, MXNET, NLP, AWS, Google Cloud,   Programming: Python, R, MySQL, Tableau  Mathematics: Statistics, Time Series, AB Testing, "
" 
Experienced data scientist currently co-founding a startup building a platform, including full lifecycle of data science: data management, data mining, model training, predicting, implementing and reusing. Skilled in predictive analytics and prescriptive analytics using Python, SQL, and Spark Pipeline.  
 
Languages:   	 
Python, R, SQL 
Data/Modeling : 
Pandas, Numpy, Scipy, NLTK, SparkML, Scikit-learn, Keras, Regular Expression (RE)  
Algorithms:   	 
Classification, Regression, Clustering, Neural Nets, Time series 
Visualization:  	 
Tableau, Gephi, Matplotlib, Seaborn, Plotly 
Other:  	 	 
AWS (EC2, RDS, Beanstalk, Pipeline), Git, T-test, Experimental Design 
   
 
EXPERIENCE 

Data Scientist & Co-Founder| Eiffo Analytics LLC | Jersey City, NJ 	        Dec 2018  Present 
Providing data science as a service, empowering all level  with automatic data solution, helping data scientist implement and manage predictive models. Won the third place in Viva  2019. https://www.eiffo-analytics.com/ 
 Designing and building automated end-to-end data pipelines including processing, modeling and deploying, etc. 
 Improving data reliability, efficiency and quality by data type recognition and data enrichment. 
 Implementing data science code on Django frame. Constructing automatic deploying pipeline on AWS. 
 Reducing model training time from 136s to 10s using parallelized computing and Spark.   
 Building a reinforcement learning mechanism to continuously improving performance and optimizing machine learning. 
 Leading 3 data scientists ing on pipelines for classification, regression, fraud detection and time series.  
 
Data Scientist Intern | Linchpin Inc. | New York, NY 	    Sep 2018 - Dec 2018 
 Contributed to the risk management system for insurance by brainstorming potential risk factors for restaurants.  
 Identified 10 new data points; collected data and enriched the databases by 200% using APIs with python. 
 Improved user experience by a fuzzy search tool; generated applications by querying data using SQL with Python.  
 
Research Assistant | Stevens Institute of Tech | Hoboken, NJ 	             May 2018 - Dec 2018 
 Participated in ongoing research to predict frauds based on 10-k financial reports using NLP and neural nets.  
 Parsed and analyzed over 100,000 text files using natural language processing (NLP) and RE.  
 Trained bassline classification models (Naïve Bayes, SVM, CNN) using python with Scikit-learn and Keras.  
 Constructed a nested word vector, after data cleaning, for the usage of RNN and attention model.   
Graduate Assistant | Stevens Institute of Tech | Hoboken, NJ 	Jan 2018  May 2019 
 ed as a graduate teaching assistant for 2 professors in 3 semesters; instructed and helped over 200 students.  
 Developed a case study <Advertising Analytics - Click Prediction> with Spark pipelines using SparkML and SparkSql.  
 Coached on Statistics course teaching experimental design, A/B test, and hypothetical test.    
 
 

 
KKBox's Churn Prediction 	 	 	 	 	 	 	 	 	 	 	  Fall 2018  
 
 Analyzed customer behavior and predicted subscription churn, which will reduce 90% risk of losing existing customers.  
 Built a ML Pipeline for Xgboost classifier and increased performance by 8% by hyperparameters tuning. 
 
Fraud Detection on IEEE Membership  	 	 	 	 	 	 	               Spring 2018 
 
 Selected by IEEE in partnership with Stevens to build a fraud detection system alerting them on membership abuse. 
 Parsed 1T log files (Unstructured data) in combined with metadata (Json) to determine patterns and abnormal behaviors.  
 Created a dashboard using Tableau to visualize and analyze the abnormal pattern; presented to help decision making.  
 
Site Selection of Chase Bank Branch 	 	 	 	 	 	 	 	 	 Fall 2017 
 Suggested on site selection strategy based on 12 key metrics using machine learning models with Python. 
 Collected and constructed the dataset from scratch; built 7 classification models with highest accuracy of 90%.  
 
 

Stevens Institute of  |Hoboken, USA | GPA:4.0/4.0                                            	  May 2019 
Master of Science in Business Intelligence & Analytics (Data Science) | Outstanding Academic Achievement Award  Relevant Course: Machine Learning, Data Mining, Big Data, Cognitive Computing, Deep Learning, Databases  
 
Hubei University of Economics | Wuhan, China | GPA:3.69/4.0  	 	 	 	 	          June 2016 
Bachelors in Financial Management | Award: First Prize Scholarship (top 1%) ",Data Scientist,resume,"  Experienced data scientist currently co-founding a startup building a platform, including full lifecycle of data science: data management, data mining, model training, predicting, implementing and reusing. Skilled in predictive analytics and prescriptive analytics using Python, SQL, and Spark Pipeline.     Languages:   	  Python, R, SQL  Data/Modeling :  Pandas, Numpy, Scipy, NLTK, SparkML, Scikit-learn, Keras, Regular Expression (RE)   Algorithms:   	  Classification, Regression, Clustering, Neural Nets, Time series  Visualization:  	  Tableau, Gephi, Matplotlib, Seaborn, Plotly  Other:  	 	  AWS (EC2, RDS, Beanstalk, Pipeline), Git, T-test, Experimental Design        EXPERIENCE   Data Scientist & Co-Founder| Eiffo Analytics LLC | Jersey City, NJ 	        Dec 2018  Present  Providing data science as a service, empowering all level  with automatic data solution, helping data scientist implement and manage predictive models. Won the third place in Viva  2019. https://www.eiffo-analytics.com/   Designing and building automated end-to-end data pipelines including processing, modeling and deploying, etc.   Improving data reliability, efficiency and quality by data type recognition and data enrichment.   Implementing data science code on Django frame. Constructing automatic deploying pipeline on AWS.   Reducing model training time from 136s to 10s using parallelized computing and Spark.     Building a reinforcement learning mechanism to continuously improving performance and optimizing machine learning.   Leading 3 data scientists ing on pipelines for classification, regression, fraud detection and time series.     Data Scientist Intern | Linchpin Inc. | New York, NY 	    Sep 2018 - Dec 2018   Contributed to the risk management system for insurance by brainstorming potential risk factors for restaurants.    Identified 10 new data points; collected data and enriched the databases by 200% using APIs with python.   Improved user experience by a fuzzy search tool; generated applications by querying data using SQL with Python.     Research Assistant | Stevens Institute of Tech | Hoboken, NJ 	             May 2018 - Dec 2018   Participated in ongoing research to predict frauds based on 10-k financial reports using NLP and neural nets.    Parsed and analyzed over 100,000 text files using natural language processing (NLP) and RE.    Trained bassline classification models (Naïve Bayes, SVM, CNN) using python with Scikit-learn and Keras.    Constructed a nested word vector, after data cleaning, for the usage of RNN and attention model.    Graduate Assistant | Stevens Institute of Tech | Hoboken, NJ 	Jan 2018  May 2019   ed as a graduate teaching assistant for 2 professors in 3 semesters; instructed and helped over 200 students.    Developed a case study <Advertising Analytics - Click Prediction> with Spark pipelines using SparkML and SparkSql.    Coached on Statistics course teaching experimental design, A/B test, and hypothetical test.            KKBox's Churn Prediction 	 	 	 	 	 	 	 	 	 	 	  Fall 2018      Analyzed customer behavior and predicted subscription churn, which will reduce 90% risk of losing existing customers.    Built a ML Pipeline for Xgboost classifier and increased performance by 8% by hyperparameters tuning.    Fraud Detection on IEEE Membership  	 	 	 	 	 	 	               Spring 2018     Selected by IEEE in partnership with Stevens to build a fraud detection system alerting them on membership abuse.   Parsed 1T log files (Unstructured data) in combined with metadata (Json) to determine patterns and abnormal behaviors.    Created a dashboard using Tableau to visualize and analyze the abnormal pattern; presented to help decision making.     Site Selection of Chase Bank Branch 	 	 	 	 	 	 	 	 	 Fall 2017   Suggested on site selection strategy based on 12 key metrics using machine learning models with Python.   Collected and constructed the dataset from scratch; built 7 classification models with highest accuracy of 90%.        Stevens Institute of  |Hoboken, USA | GPA:4.0/4.0                                            	  May 2019  Master of Science in Business Intelligence & Analytics (Data Science) | Outstanding Academic Achievement Award  Relevant Course: Machine Learning, Data Mining, Big Data, Cognitive Computing, Deep Learning, Databases     Hubei University of Economics | Wuhan, China | GPA:3.69/4.0  	 	 	 	 	          June 2016  Bachelors in Financial Management | Award: First Prize Scholarship (top 1%) "
"Around 8 Years of experience and comprehensive industry knowledge of Machine Learning, Statistical
Modeling, Data Analytics, DataModeling, Data Architecture, Data Analysis, Data Mining, Text Mining &
Natural Language Processing (NLP), Artificial Intelligence algorithms, Business Intelligence, Analytics
Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Java, Spark,
Scala, MS Excel, SQL and Postgre SQL, Erwin. 
Having a good experience in Big Data, Hadoop, No SQL database (MongoDb, HBase), Data
Warehousing, Business Intelligence, Data Analytics & ETL concepts. 
Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research.
Comfortable with R, Python, Java, SAS and Weka, MATLAB, Relational databases. Deep understanding
& exposure of Big Data Eco-system. 
Expert in creating PL/SQL Schema objects like Packages, Procedures, Functions, Subprograms,
Triggers, Views, Materialized Views, Indexes, Constraints, Sequences, Exception Handling, Dynamic
SQL/Cursors, Native Compilation, Collection Types, Record Type, Object Type using SQL Developer. 
Have hands on experience in Hadoop, Hive, Hbase, Map Reduce, Pig, Oozie, R, Sqoop, Flume,
Zookeeper, Ambari, YARN, Tezand SAP Hana. 
Strong experience and knowledge in Data Visualization with Tableau creating: Line and scatter plots,
Bar Charts, Histograms, Pie chart, Dot charts, Box plots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc. 
Strong experience in Big Data  like Spark, SparkSQL, PySpark, Hadoop, HDFS, Hive. 
Proficient at building robust Machine Learning, Deep Learning models, Convolution Neural Nets
(CNN), Recurrent Neural Nets (RNN), LSTM using Tensor Flow and Keras. 
Adept in analyzing large datasets using Apache Spark, PySpark, Spark ML and Amazon Web Services (AWS). 
Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in the infrastructure to provide data summarization and Proficient in HiveQL, SparkSQL, PySpark. 
In depth knowledge in using of spark machine learning library MLlib. 
Hands on experience integrating R with Hadoop ecosystem using rhdfs, hiver, rhbase packages. 
Hands on experience in SAP Hana, HDFS and R integration. 
Hands on experience in making Web application in R using shiny package. 
Hands on experience in PHP, Python, MySql, PostgreSQL, and MongoDB. 
Knowledge on Scala, Spark andJaql. 
Having experience in Data Quality Management to get, clean, process, and cross-verify the data in multiple sources. 
Domain Knowledge on E-Commerce, E-Learning, Travel, Health Care and Gaming. 
I am an Active Team Player, Quick Learner, Planned and Committed Personality. 
Experience in Amazon - EMR, EC2 and S3 cloud services. 
Involved in installing/configuring Hadoop 1.0 and its Eco system  in CentOS6.x. ed with the admin team in upgrading Hadoop 1.0 to 2.0 using Apache Ambari 2.0.1 and configured with HUE. 
ed up to 20 nodes, with dedicated nodes for namenode, Jobtracker, Secondary node. 
Handled a data load up to 20 TB. 
Extracted data from log files into HDFS using Flume. 
Developed Oozie flow for scheduling and orchestrating the ETL process. 
Extracted data from SAP Hana, MsSql and MySqlintoHDFS using Sqoop. 
Experienced in Teradata RDBMS using Fast load, Fast Export, Multi load, T pump and Teradata SQL
Assistance and BTEQ Teradata utilities. 
Expertise R user with knowledge of statistical programming languagesSAS. 
Created and ed on Sqoop (version 1.4.3) jobs with incremental load to populate Hive External tables. 
Developed Hive (version 0.10) scripts for end user / analyst requirements to perform ad hoc analysis. 
Very good understanding of Partitions, Bucketing concepts in Hive and designed both Managed and
External tables in Hive to optimize performance. 
Solved performance issues in Hive and Pig scripts with understanding of Joins, Group and aggregation and how does it translate to MapReduce jobs. 
Used Tez execution to speed up the query execution time in Hive. 
Good experience with both MapReduce 1 (Job Tracker) and MapReduce 2 (YARN) setups. 
Good experience in monitoring and managing cluster using Ambari through Nagios and Ganglia. 
Experience with ing in Agile/SCRUM software environments. 
Highly motivated team player with excellent interpersonal , effective communication, analytical and presentation .
 Experience

Sr. Data Scientist/Machine Learning Engineer
Allianz life - Minneapolis, MN August 2018 to Present
Description:Allianz Life Insurance Company of North America (Allianz) is a leading provider of retirement solutions, including fixed and variable annuities and life insurance for individuals. Guarantees are backed by the financial strength and claims-paying ability of the issuing company. Variable annuity guarantees do not apply to the performance of the variable subaccounts, which will fluctuate with market conditions. 
Responsibilities: 
? Massively involved in Data Architect role to review business requirement and compose source totarget data mapping documents. 
? Expertise and experience in domains like Retail Solutions, Finance, Healthcare, Banking, Digitaladvertisement and e-commerce. 
? Responsible for the dataarchitecture design delivery, data model development, review, approval and
Data warehouse implementation. 
? Set strategy and oversee design for significant data modelling , such as Enterprise Logical
Models, Conformed Dimensions, and Enterprise Hierarchy. 
? Analyzed existing Conceptual and Physicaldatamodels and altered them using Erwin to supportenhancements. 
? Applied feature engineering according to feature importance by Random Forest and correlationamong features by L1 and L2 algorithms. 
? Designed the LogicalDataModel using Erwin with the entities and attributes for each subject areas. 
? Architectural Design in BigData, Hadoop  and provide for a designer that is an idea-driven. 
? Skilled in Data chunking, Data profiling, Data Cleansing, Data mapping, creating flows and Data
Validation using data integration  like Informatica during the ETL and ELT processes. 
? Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark 2.0 (Scala, PySpark, MLlib)to develop variety of models and algorithms for analytic purposes. 
? Developed Map Reduce jobs written in java, using Hive for data cleaning and pre-processing. ? Used big data  Spark (Pyspark, SparkSQL, Mllib) to conduct real time analysis of loan default based on AWS. 
? Developed and configured on InformaticaMDM hub supports the MasterDataManagement (MDM),
BusinessIntelligence (BI) and Datawarehousing platforms to meet business needs. 
? Loaded data into Hive Tables from HadoopDistributed File System (HDFS) to provide SQL access onHadoop data 
? Used AgileMethodology of Data Warehouse development. 
? Design and implement data ingestion techniques for real time and batch processes for structuredand unstructured data sources into Hadoopecosystems and HDFSclusters. 
? Designed and developedarchitecture for data servicesecosystem spanning Relational, NoSQL, andBigData . 
? Responsible for data identification, collection, exploration, and cleaning for modeling, participate inmodel development 
? Implemented multi-datacenter and multi-rack Cassandra cluster. 
? Created HBase tables to load large sets of structured, semi-structured and unstructured data comingfrom NoSQL and a variety of portfolios. 
? Involved in data model reviews as dataarchitect with business analysts and business users withexplanation of the data model to make sure it is in-line with business requirements. 
? Created Entity relationships diagrams, data flow diagrams and enforced all referential integrityconstraints using Rational Rose 
? ed with the ETL team to document the SSIS packages for data extraction to Warehouseenvironment for reporting purposes. 
? Developed data Mart for the base data in Star Schema, Snow-FlakeSchema involved in developingthe data warehouse for the database. 
? Involved in Dataloading using PL\SQLScripts and SQLServer Integration Services packages ? Established data governance, monitoring of DataQuality and clear documentation for facile implementation. 
? Involved in the validation of the OLAP, Unittesting and System Testing of the OLAP Report
Functionality and data displayed in the reports. 
? Generated ad-hocSQLqueries using joins, database connections and transformation rules to fetchdata from Teradata database. 
? Created HBase tables to load large sets of structured, semi-structured and unstructureddata comingfrom UNIX, NoSQL and a variety of portfolios. 
? ed on AmazonRedshift and AWS and architecting a solution to load data creates data modelsand run BI on it. 
? ed on AWS and architecting a solution to load data creates data models and run BI on it. 
? Created UNIXscripts for file transfer and file manipulation 
? Directed to Create Dashboards based on the business requirement using SSRS/Cognos and helpeddevelopment team in knowledge about the requirement. 
? Handled importing of data from various data sources, performed transformations using Hive,
MapReduce, loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop 
? ed with various Teradata15  and utilities like Teradata Viewpoint, MultiLoad, ARC,
TeradataAdministrator, BTEQ and other Teradata Utilities. 
? Involved in several facets of MDM implementations including DataProfiling, Metadataacquisition anddata migration. 
? Developed predictive models using Decision Tree, Random Forest, Naïve Bayes, Logistic Regression,Cluster Analysis, and Neural Nets to predict analytical Online Advertising Pricing Model to maximize client's net revenues, predict accurate Revenue per Click estimates and build a fraud traffic detection system to flag potential bot sessions that cause inflated billings to the client's customers. ? Extensively used AginityNetezza  bench to perform various DML,DDL operations on Netezza database. 
? Created DDLscripts using Erwin and source to target mappings to bring the data from source to thewarehouse. 
? Lead database level tuning and optimization in support of application development teams on an ad-hoc basis. 
 
Environment: Erwin r9.6, Python, SQL, Oracle 12c,Netezza, SQL Server, Informatica, Java, SSRS, PL/ SQL, T-SQL, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, logistic regression, Hadoop, Hive, Teradata, random forest, OLAP, Azure, MariaDB, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, Cassandra, MapReduce, AWS, Assistant 15.0, Flat Files.
Sr. Data Scientist/Data Analyst
State of MA Boston - Boston, MA
May 2017 to July 2018
Description:Mass.gov is committed to achieving meaningful accessibility to this online environment for all users, including users with disabilities. We follow specific Commonwealth enterprise standards designed to meet the needs of our citizens with disabilities. The Commonwealth enterprise standards are generally based on standards used by the federal government for technology accessibility for people with disabilities, as well as web content accessibility guidelines developed by the World Wide Web Consortium (W3C). 
Responsibilities: 
? Evaluating the data analytics opportunities to improve the efficiency of claims handling process likeFraud Detection 
? Utilized various data analysis and data visualization  to accomplish data analysis, report designand report delivery. 
? Trained different ML algorithms models, including Logistic, Tree-Based, SVM, Knn and GBM, multipletimes after repeat model evaluations by confusion matrix and cross-validation aimed at finding out optimal parameter and hyper parameter to ensure prediction accuracy. 
? Create statistical models based on researched information to provide conclusions that will guide thecompany and the industry into the future. 
? Implemented PySpark jobs for batch processing to handle massive volume of data from various datasources - Bloomberg, Government publications, unstructured news articles, etc. and data persisted in
HDFS. Configured a CI/CD pipeline in Kubernetes and DockerSwarm. 
? Performed data cleaning and feature selection using MLLib package in PySpark and ing withdeep learning frames such as Caffe, Neon etc. 
? Taking care of missing data after import and encoding the categorical data, when needed. ? Splitting the data into training set, test set and scaling the data in training set and test set, if necessary. 
? Creatively communicated and presented models to business customers and executives, utilizing avariety of formats and visualization methodologies. 
? Impact of marketing tactics on sales and then forecast the impact of future sets of tactics. 
? Developed Scala and SQL code to extract data from various databases 
? Used R and python for Exploratory Data Analysis and Hypothesis test to compare and identify theeffectiveness of Creative Campaigns. 
? Used Scala, Python, R and SQL to create Statistical algorithms involving Linear Regression, Logistic
Regression, Random forest, Decision trees, Support Vector Machine for estimating the risks. 
? Developed statistical models to forecast inventory and procurement cycles. 
? Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behaviour. 
? Created pipelines for data ingestion and from various channels, through the scripts written inHive&Java. 
?  with a range of proprietary, industry standard, and open source data stores to assemble andorganize and analyze data. 
? Mapped customers to revenue to predict the revenue (if any) from a new prospective customer. 
? Visualizations, Summary Reports and Presentations using R and Tableau. 
? Uploaded data to HadoopHive and combined new tables with existing databases. 
? Involved in converting Hive/SQL queries into Spark transformations using SparkRDDs, and Scala. 
? Developed pyspark code and Spark-SQL/Streaming for faster testing and processing of data. 
? Supported Map Reduce Programs those are running on the cluster. 
? Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. 
? Scheduled jobs and flow scheduler to manage Hadoop jobs. 
? Loaded the aggregated data into Data Mart for reporting, dash boarding and ad-hoc analysis using
Tableau and developed a self-service BI solution for quicker turnaround of insights. 
? Maintained SQL scripts to create and populate tables in data warehouse for daily reporting acrossdepartments. 
 
Environment: R 3.x, Python 2.x, Tableau 9, SQL Server 2012, Spark/Scala, SBT, Hive, Sqoop, Spark ML.
Sr. Data Scientist
Coventry Health Care - Downers Grove, IL
January 2016 to April 2017
Description:Coventry Health Care, Inc. is a diversified national insurer in the United States. Based in North Bethesda, Maryland, Coventry operates health plans, insurance companies, net rental and ers' compensation services companies. 
Responsibilities: 
? Participated in all phases of project life cycle including data collection, data mining, data cleaning,model building and validation, as well as report creating. 
? Utilized MapReduce and PySpark programs to process data for analysis reports. 
? ed on data cleaning to ensure data quality, consistency, and integrity using Pandas/Numpy. ? Performed data pre-processing on messy data including imputation, normalization, scaling, feature engineering etc. using Scikit-Learn. 
? ed on different data formats such as JSON, XML and performed ML algorithms in Python. ? Conducted exploratory data analysis using PythonMatplotlib and Seaborn to identify underlying patterns and correlations between features. 
? Built classification models based on Logistic Regression, Decision Trees, Random Forest Support
Vector Machine, and Ensemble algorithms to predict the probability of absence of patients. 
? Applied various metrics like recall, precision, F-Score, ROC, and AUC to evaluate the performance ofeach model and k-fold cross-validation to test the models with different batches of data to optimize the models. 
? Involving in creating data frames in Hadoop system, Spark using PySpark and then applying Hive/
SQL queries into Spark transformations using Spark RDDs, Python libraries. 
? Utilized PySpark, Spark Streaming, MLlib, in Spark ecosystem with a broad variety of machinelearning methods including classifications, regressions, dimensionally reduction etc. 
? Implemented and tested the model on AWSEC2, collaborated with development team to get the bestalgorithm and parameters. 
? ed on Naïve Bayes algorithms for Agent Fraud Detection using R. 
? Performed data visualization and design dashboards with Tableau and generated complex reportsincluding chars, summaries, and graphs to interpret the findings to the team and stakeholders. 
 
Environment: Python (Scikit-Learn/Keras/Scipy/Numpy/Pandas/ Matplotlib/Seaborn), Machine Learning
(Linear and Non-linear Regressions, Deep Learning, SVM, Decision Tree, Random Forest, XGboost, Ensemble and KNN), MS SQL Server 2017, AWS Redshift, S3, Hadoop Frame, HDFS, Spark (Pyspark, MLlib, Spark SQL), Tableau Desktop and Tableau Server.
Data Analyst/Data Modeler
Aetna - San Diego, CA
March 2014 to December 2015
Description:Aetna is committed to providing individuals, employers, health care professionals, producers and others with innovative benefits, products and services. Aetna is now a subsidiary company of CVS Health Corporation. 
Responsibilities: 
? Analyzed and reviewed functional specifications and requirements to determine best data designapproach and translate business requirements into data models. 
? Created models for various schemas and created the metadata in order to deploy the models intomicro strategy to be able to reuse the definitions enterprise wide. 
? Performed data Ingestion for the incoming web feeds into the Data lake store which includes bothstructured and unstructured data. 
? Implemented Predictive analytics and machine learning algorithms to forecast key metrics inthe form of designed dashboards on to AWS (S3/EC2) and Django platform for the company's core business. 
? Create the architectural artifacts for the Enterprise Data Warehouse and the Operational Dashboard,such as Entity Relationship Diagrams (ERD), the DDL scripts, the Conceptual Data Model, and technical as well as business documents. 
? Conducted data profiling to insure that the available data could support business needs. edwith the developers on resolving the reported bugs and various technical issues. 
? Involved in requirements gathering activities analyze, and document business processes andfundamentals, and strategic data needs. 
? Created data source views from MYSQL and HADOOP data sources. 
? Migrated our retired systems to leverage new systems and customized according to the businessrequirement. 
? Enforced database naming standards and maintained user domains. 
? Supported data conversion activities and coordinated the resolution of conversion and datamigration issues. 
? Created and maintained Data Dictionary and pursued to reach consensus. 
? Created data lineages and mappings for Data Lake schemas. 
? Ensured Error logs and audit tables are generated and populated properly. 
? Involved in troubleshooting, resolving and escalating data related issues and validating data toimprove data quality. 
? Tracking and reporting the issues to project team and management. 
? Created mapping for horizontal data lineages for various systems. 
? Contribute in the development of knowledge transfer documentation. 
? Used Python, R and SQL to create Statistical algorithms involving Linear Regression, Logistic
Regression, Random forest, Decision trees for estimating the risks. 
? Managed change requests by following change request management process for the project. ? Involved in preparing a simple and detailed user guide and training manual for the application and for an intended novice user. 
Environment:Erwin 9.64, MS Access, Micro strategy, MySql, Erwin, Oracle10g, HeidiSql, Hadoop, Toad 12.5,MS Visio, SVN
Data Analyst
Symbiosys  - Visakhapatnam
December 2012 to February 2014
Description: Symbiosys  was founded in 2001 and is a 100% export oriented unit, registered in the Visakhapatnam Special Economic Zone (VSEZ). We provide high-quality services and solution to our clients worldwide. 
 
Responsibilities: 
? Extensively ed on Informatica PowerCenter Transformations such as Source Qualifier, Lookup,
Filter, Expression, Router, Joiner, Update Strategy, Rank, Aggregator, Sequence Generator etc. ? Proficiency in using Informatica PowerCenter tool to design data conversions from wide variety of sources. 
? Expertise in transforming business requirements into analytical models, designing algorithms,building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data. 
? Proficient in using Informatica flow manager, flow monitor to create, schedule and controlflows, tasks, and sessions. 
? Created pivot tables and ran VLOOKUP's in Excel as a part of data validation. 
? Used Informatica PowerCenter for extraction, loading and transformation (ETL) of data in the datawarehouse. 
? ed on data analysis, data discrepancy reduction in the source and target schemas. 
? Designed and developed complex mappings, from varied transformation logic like Unconnected and
Connected lookups, Router, Filter, Expression, Aggregator, Joiner, Update Strategy and more. ? Preparation of System requirements (SRS), Database specifications (DBS), Software design document (SDD). 
? Responsible for the maintenance of few applications in PowerBuilder 10.2 
? Involved in using SQLServer 2005 for fixed the production issues in the background. 
? Coordination and Quality activities on delivery 
? Involved in testing with validation of all fields, functions, programs, agents from front end and backend code reviews across the application. 
? Involved in preparation program specifications, unit tests, test cases and user manual documents. 
 
Environment: Informatica 8.x, PowerBuilder 10.2, SQL Server 2005.
Java Developer
Clover InfoTech - Mumbai, Maharashtra
October 2011 to November 2012
Description: BOLSTER Solutions was established in 2011 and is based in Hyderabad. Our project clients vary in size from start-ups and SMEs to larger international companies. Our main area of expertise is building complex, secure and database-driven web-based applications. Last year we ed for 26 clients, completing over 50 , 65% of our business came from repeat orders from existing clients. 
Responsibilities: 
? Involved in each phase of Software Development Life Cycle(SDLC) models like Requirementgathering and analysis, Design, Implementation, Testing, Deployment and Maintenance. 
? Developed Login, Policy and Claims Screens for customers using HTML 5, CSS3, JavaScript, AJAX, JSP,and jQuery. 
? Used Core Java to develop Business Logic. 
? ML models developed: Customer Survival Analysis for better targeting, Member Engagement callcenter optimization, Financial Forecasting for product realization. 
? Involved in the development of business module applications using J2EE  like Servlets,JSP. 
? Designed and developed the web-tier using JSP's, Servlets frame. 
? Used various Core Java concepts such as Multi-Threading, Exception Handling, Collection APIs toimplement various features and enhancements. 
? Strong experience in design & development of applications using Java/J2EE components such as JavaServer Pages (JSP). 
? Developed EJB MDB's and message Queue's using JMS technology. 
? EJB Session Beans were used to process requests from the user interface and CMP entity beans wereused to interact with the persistence layer. 
? Developed stored procedures, triggers, and queries using PLSQL in SQL Server. 
? Use Spring MVC as frame and JavaScript for client-side view, used frames for client-sidedata validation, creating dynamic web pages-Ajax, jQuery. Developed model classes based on the forms to be displayed on the UI. 
? ML Algorithms used: Logistic/Linear Regression, Random Forest, XG-Boost, K-Means Clustering e.t.c.
 
? Implemented various design patterns in the project such as Business Delegate, Data Transfer
Object, Data Access Object, Service Locator and Singleton. 
? Used SQL statements and procedures to fetch the data from the database. 
? Developed test cases and performed unit test using JUnit Frame. 
? Used CVS as version control and ANT scripts to fetch, build, and deploy application to developmentenvironment. 
 
Environment:Java, HTML, CSS, JavaScript, MySQL, Struts, EJB, Spring MVC.
Education

Bachelor's

Data modeling, Db2, Ms access, Sql server, Sql server 2012, Oracle, Pl/sql, Sql, Sybase, Sqoop, Hbase,
Flume, Hadoop, Tableau server, Teradata, C++, Hadoop, Hbase, Hive, Html
Additional Information

Technical : 
 
Data Modeling  Erwin r 9.6/9.5, ER/Studio 9.7, Star-Schema Modeling, Snowflake-Schema
Modeling, FACT and dimension tables, Pivot Tables. 
Databases Oracle 11g/12c, MS Access, SQL Server 2012/2014, Sybase and DB2, Teradata, Hive. 
Big Data  Hadoop, Hive, Spark, Pig, HBase, Sqoop, Flume. 
BI  Tableau 7.0/8.2, Tableau server 8.2, Tableau Reader 8.1,SAP Business Objects, Crystal Reports
 
Packages Microsoft Office 2010, Microsoft Project 2010, SAP and Microsoft Visio, Share point Portal Server 
Operating Systems Microsoft Windows 8/7/XP, Linux and UNIX. 
Languages SQL, PL/SQL, ASP, Visual Basic, XML, Python, SQL, T-SQL, SQL Server, C, C++, JAVA, HTML , UNIX shell scripting, PERL, R. 
Applications Toad for Oracle, Oracle SQL Developer, MS Word, MS Excel, MS Power Point, Teradata, Designer 6i. 
Methodologies RAD, JAD, RUP, UML, System Development Life Cycle (SDLC), Waterfall Model. 
Data Modeling  Erwin r 9.6/9.5, ER/Studio 9.7, Star-Schema Modeling, Snowflake-Schema
Modeling, FACT and dimension tables, Pivot Tables. 
Databases Oracle 11g/12c, MS Access, SQL Server 2012/2014, Sybase and DB2, Teradata14/15, Hive. 
Big Data  Hadoop, Hive, Spark, Pig, HBase, Sqoop, Flume. 
BI  Tableau 7.0/8.2, Tableau server 8.2, Tableau Reader 8.1,SAP Business Objects, Crystal Reports
 
Packages Microsoft Office 2010, Microsoft Project 2010, SAP and Microsoft Visio, Share point Portal
Server",Data Scientist,resume,"Around 8 Years of experience and comprehensive industry knowledge of Machine Learning, Statistical Modeling, Data Analytics, DataModeling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP), Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Java, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin.  Having a good experience in Big Data, Hadoop, No SQL database (MongoDb, HBase), Data Warehousing, Business Intelligence, Data Analytics & ETL concepts.  Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, Java, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system.  Expert in creating PL/SQL Schema objects like Packages, Procedures, Functions, Subprograms, Triggers, Views, Materialized Views, Indexes, Constraints, Sequences, Exception Handling, Dynamic SQL/Cursors, Native Compilation, Collection Types, Record Type, Object Type using SQL Developer.  Have hands on experience in Hadoop, Hive, Hbase, Map Reduce, Pig, Oozie, R, Sqoop, Flume, Zookeeper, Ambari, YARN, Tezand SAP Hana.  Strong experience and knowledge in Data Visualization with Tableau creating: Line and scatter plots, Bar Charts, Histograms, Pie chart, Dot charts, Box plots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc.  Strong experience in Big Data  like Spark, SparkSQL, PySpark, Hadoop, HDFS, Hive.  Proficient at building robust Machine Learning, Deep Learning models, Convolution Neural Nets (CNN), Recurrent Neural Nets (RNN), LSTM using Tensor Flow and Keras.  Adept in analyzing large datasets using Apache Spark, PySpark, Spark ML and Amazon Web Services (AWS).  Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in the infrastructure to provide data summarization and Proficient in HiveQL, SparkSQL, PySpark.  In depth knowledge in using of spark machine learning library MLlib.  Hands on experience integrating R with Hadoop ecosystem using rhdfs, hiver, rhbase packages.  Hands on experience in SAP Hana, HDFS and R integration.  Hands on experience in making Web application in R using shiny package.  Hands on experience in PHP, Python, MySql, PostgreSQL, and MongoDB.  Knowledge on Scala, Spark andJaql.  Having experience in Data Quality Management to get, clean, process, and cross-verify the data in multiple sources.  Domain Knowledge on E-Commerce, E-Learning, Travel, Health Care and Gaming.  I am an Active Team Player, Quick Learner, Planned and Committed Personality.  Experience in Amazon - EMR, EC2 and S3 cloud services.  Involved in installing/configuring Hadoop 1.0 and its Eco system  in CentOS6.x. ed with the admin team in upgrading Hadoop 1.0 to 2.0 using Apache Ambari 2.0.1 and configured with HUE.  ed up to 20 nodes, with dedicated nodes for namenode, Jobtracker, Secondary node.  Handled a data load up to 20 TB.  Extracted data from log files into HDFS using Flume.  Developed Oozie flow for scheduling and orchestrating the ETL process.  Extracted data from SAP Hana, MsSql and MySqlintoHDFS using Sqoop.  Experienced in Teradata RDBMS using Fast load, Fast Export, Multi load, T pump and Teradata SQL Assistance and BTEQ Teradata utilities.  Expertise R user with knowledge of statistical programming languagesSAS.  Created and ed on Sqoop (version 1.4.3) jobs with incremental load to populate Hive External tables.  Developed Hive (version 0.10) scripts for end user / analyst requirements to perform ad hoc analysis.  Very good understanding of Partitions, Bucketing concepts in Hive and designed both Managed and External tables in Hive to optimize performance.  Solved performance issues in Hive and Pig scripts with understanding of Joins, Group and aggregation and how does it translate to MapReduce jobs.  Used Tez execution to speed up the query execution time in Hive.  Good experience with both MapReduce 1 (Job Tracker) and MapReduce 2 (YARN) setups.  Good experience in monitoring and managing cluster using Ambari through Nagios and Ganglia.  Experience with ing in Agile/SCRUM software environments.  Highly motivated team player with excellent interpersonal , effective communication, analytical and presentation .  Experience  Sr. Data Scientist/Machine Learning Engineer Allianz life - Minneapolis, MN August 2018 to Present Description:Allianz Life Insurance Company of North America (Allianz) is a leading provider of retirement solutions, including fixed and variable annuities and life insurance for individuals. Guarantees are backed by the financial strength and claims-paying ability of the issuing company. Variable annuity guarantees do not apply to the performance of the variable subaccounts, which will fluctuate with market conditions.  Responsibilities:  ? Massively involved in Data Architect role to review business requirement and compose source totarget data mapping documents.  ? Expertise and experience in domains like Retail Solutions, Finance, Healthcare, Banking, Digitaladvertisement and e-commerce.  ? Responsible for the dataarchitecture design delivery, data model development, review, approval and Data warehouse implementation.  ? Set strategy and oversee design for significant data modelling , such as Enterprise Logical Models, Conformed Dimensions, and Enterprise Hierarchy.  ? Analyzed existing Conceptual and Physicaldatamodels and altered them using Erwin to supportenhancements.  ? Applied feature engineering according to feature importance by Random Forest and correlationamong features by L1 and L2 algorithms.  ? Designed the LogicalDataModel using Erwin with the entities and attributes for each subject areas.  ? Architectural Design in BigData, Hadoop  and provide for a designer that is an idea-driven.  ? Skilled in Data chunking, Data profiling, Data Cleansing, Data mapping, creating flows and Data Validation using data integration  like Informatica during the ETL and ELT processes.  ? Used Python 3.X (numpy, scipy, pandas, scikit-learn, seaborn) and Spark 2.0 (Scala, PySpark, MLlib)to develop variety of models and algorithms for analytic purposes.  ? Developed Map Reduce jobs written in java, using Hive for data cleaning and pre-processing. ? Used big data  Spark (Pyspark, SparkSQL, Mllib) to conduct real time analysis of loan default based on AWS.  ? Developed and configured on InformaticaMDM hub supports the MasterDataManagement (MDM), BusinessIntelligence (BI) and Datawarehousing platforms to meet business needs.  ? Loaded data into Hive Tables from HadoopDistributed File System (HDFS) to provide SQL access onHadoop data  ? Used AgileMethodology of Data Warehouse development.  ? Design and implement data ingestion techniques for real time and batch processes for structuredand unstructured data sources into Hadoopecosystems and HDFSclusters.  ? Designed and developedarchitecture for data servicesecosystem spanning Relational, NoSQL, andBigData .  ? Responsible for data identification, collection, exploration, and cleaning for modeling, participate inmodel development  ? Implemented multi-datacenter and multi-rack Cassandra cluster.  ? Created HBase tables to load large sets of structured, semi-structured and unstructured data comingfrom NoSQL and a variety of portfolios.  ? Involved in data model reviews as dataarchitect with business analysts and business users withexplanation of the data model to make sure it is in-line with business requirements.  ? Created Entity relationships diagrams, data flow diagrams and enforced all referential integrityconstraints using Rational Rose  ? ed with the ETL team to document the SSIS packages for data extraction to Warehouseenvironment for reporting purposes.  ? Developed data Mart for the base data in Star Schema, Snow-FlakeSchema involved in developingthe data warehouse for the database.  ? Involved in Dataloading using PL\SQLScripts and SQLServer Integration Services packages ? Established data governance, monitoring of DataQuality and clear documentation for facile implementation.  ? Involved in the validation of the OLAP, Unittesting and System Testing of the OLAP Report Functionality and data displayed in the reports.  ? Generated ad-hocSQLqueries using joins, database connections and transformation rules to fetchdata from Teradata database.  ? Created HBase tables to load large sets of structured, semi-structured and unstructureddata comingfrom UNIX, NoSQL and a variety of portfolios.  ? ed on AmazonRedshift and AWS and architecting a solution to load data creates data modelsand run BI on it.  ? ed on AWS and architecting a solution to load data creates data models and run BI on it.  ? Created UNIXscripts for file transfer and file manipulation  ? Directed to Create Dashboards based on the business requirement using SSRS/Cognos and helpeddevelopment team in knowledge about the requirement.  ? Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and Extracted the data from Oracle into HDFS using Sqoop  ? ed with various Teradata15  and utilities like Teradata Viewpoint, MultiLoad, ARC, TeradataAdministrator, BTEQ and other Teradata Utilities.  ? Involved in several facets of MDM implementations including DataProfiling, Metadataacquisition anddata migration.  ? Developed predictive models using Decision Tree, Random Forest, Naïve Bayes, Logistic Regression,Cluster Analysis, and Neural Nets to predict analytical Online Advertising Pricing Model to maximize client's net revenues, predict accurate Revenue per Click estimates and build a fraud traffic detection system to flag potential bot sessions that cause inflated billings to the client's customers. ? Extensively used AginityNetezza  bench to perform various DML,DDL operations on Netezza database.  ? Created DDLscripts using Erwin and source to target mappings to bring the data from source to thewarehouse.  ? Lead database level tuning and optimization in support of application development teams on an ad-hoc basis.    Environment: Erwin r9.6, Python, SQL, Oracle 12c,Netezza, SQL Server, Informatica, Java, SSRS, PL/ SQL, T-SQL, Tableau, MLlib, regression, Cluster analysis, Scala NLP, Spark, Kafka, MongoDB, logistic regression, Hadoop, Hive, Teradata, random forest, OLAP, Azure, MariaDB, SAP CRM, HDFS, ODS, NLTK, SVM, JSON, Tableau, XML, Cassandra, MapReduce, AWS, Assistant 15.0, Flat Files. Sr. Data Scientist/Data Analyst State of MA Boston - Boston, MA May 2017 to July 2018 Description:Mass.gov is committed to achieving meaningful accessibility to this online environment for all users, including users with disabilities. We follow specific Commonwealth enterprise standards designed to meet the needs of our citizens with disabilities. The Commonwealth enterprise standards are generally based on standards used by the federal government for technology accessibility for people with disabilities, as well as web content accessibility guidelines developed by the World Wide Web Consortium (W3C).  Responsibilities:  ? Evaluating the data analytics opportunities to improve the efficiency of claims handling process likeFraud Detection  ? Utilized various data analysis and data visualization  to accomplish data analysis, report designand report delivery.  ? Trained different ML algorithms models, including Logistic, Tree-Based, SVM, Knn and GBM, multipletimes after repeat model evaluations by confusion matrix and cross-validation aimed at finding out optimal parameter and hyper parameter to ensure prediction accuracy.  ? Create statistical models based on researched information to provide conclusions that will guide thecompany and the industry into the future.  ? Implemented PySpark jobs for batch processing to handle massive volume of data from various datasources - Bloomberg, Government publications, unstructured news articles, etc. and data persisted in HDFS. Configured a CI/CD pipeline in Kubernetes and DockerSwarm.  ? Performed data cleaning and feature selection using MLLib package in PySpark and ing withdeep learning frames such as Caffe, Neon etc.  ? Taking care of missing data after import and encoding the categorical data, when needed. ? Splitting the data into training set, test set and scaling the data in training set and test set, if necessary.  ? Creatively communicated and presented models to business customers and executives, utilizing avariety of formats and visualization methodologies.  ? Impact of marketing tactics on sales and then forecast the impact of future sets of tactics.  ? Developed Scala and SQL code to extract data from various databases  ? Used R and python for Exploratory Data Analysis and Hypothesis test to compare and identify theeffectiveness of Creative Campaigns.  ? Used Scala, Python, R and SQL to create Statistical algorithms involving Linear Regression, Logistic Regression, Random forest, Decision trees, Support Vector Machine for estimating the risks.  ? Developed statistical models to forecast inventory and procurement cycles.  ? Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behaviour.  ? Created pipelines for data ingestion and from various channels, through the scripts written inHive&Java.  ?  with a range of proprietary, industry standard, and open source data stores to assemble andorganize and analyze data.  ? Mapped customers to revenue to predict the revenue (if any) from a new prospective customer.  ? Visualizations, Summary Reports and Presentations using R and Tableau.  ? Uploaded data to HadoopHive and combined new tables with existing databases.  ? Involved in converting Hive/SQL queries into Spark transformations using SparkRDDs, and Scala.  ? Developed pyspark code and Spark-SQL/Streaming for faster testing and processing of data.  ? Supported Map Reduce Programs those are running on the cluster.  ? Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata.  ? Scheduled jobs and flow scheduler to manage Hadoop jobs.  ? Loaded the aggregated data into Data Mart for reporting, dash boarding and ad-hoc analysis using Tableau and developed a self-service BI solution for quicker turnaround of insights.  ? Maintained SQL scripts to create and populate tables in data warehouse for daily reporting acrossdepartments.    Environment: R 3.x, Python 2.x, Tableau 9, SQL Server 2012, Spark/Scala, SBT, Hive, Sqoop, Spark ML. Sr. Data Scientist Coventry Health Care - Downers Grove, IL January 2016 to April 2017 Description:Coventry Health Care, Inc. is a diversified national insurer in the United States. Based in North Bethesda, Maryland, Coventry operates health plans, insurance companies, net rental and ers' compensation services companies.  Responsibilities:  ? Participated in all phases of project life cycle including data collection, data mining, data cleaning,model building and validation, as well as report creating.  ? Utilized MapReduce and PySpark programs to process data for analysis reports.  ? ed on data cleaning to ensure data quality, consistency, and integrity using Pandas/Numpy. ? Performed data pre-processing on messy data including imputation, normalization, scaling, feature engineering etc. using Scikit-Learn.  ? ed on different data formats such as JSON, XML and performed ML algorithms in Python. ? Conducted exploratory data analysis using PythonMatplotlib and Seaborn to identify underlying patterns and correlations between features.  ? Built classification models based on Logistic Regression, Decision Trees, Random Forest Support Vector Machine, and Ensemble algorithms to predict the probability of absence of patients.  ? Applied various metrics like recall, precision, F-Score, ROC, and AUC to evaluate the performance ofeach model and k-fold cross-validation to test the models with different batches of data to optimize the models.  ? Involving in creating data frames in Hadoop system, Spark using PySpark and then applying Hive/ SQL queries into Spark transformations using Spark RDDs, Python libraries.  ? Utilized PySpark, Spark Streaming, MLlib, in Spark ecosystem with a broad variety of machinelearning methods including classifications, regressions, dimensionally reduction etc.  ? Implemented and tested the model on AWSEC2, collaborated with development team to get the bestalgorithm and parameters.  ? ed on Naïve Bayes algorithms for Agent Fraud Detection using R.  ? Performed data visualization and design dashboards with Tableau and generated complex reportsincluding chars, summaries, and graphs to interpret the findings to the team and stakeholders.    Environment: Python (Scikit-Learn/Keras/Scipy/Numpy/Pandas/ Matplotlib/Seaborn), Machine Learning (Linear and Non-linear Regressions, Deep Learning, SVM, Decision Tree, Random Forest, XGboost, Ensemble and KNN), MS SQL Server 2017, AWS Redshift, S3, Hadoop Frame, HDFS, Spark (Pyspark, MLlib, Spark SQL), Tableau Desktop and Tableau Server. Data Analyst/Data Modeler Aetna - San Diego, CA March 2014 to December 2015 Description:Aetna is committed to providing individuals, employers, health care professionals, producers and others with innovative benefits, products and services. Aetna is now a subsidiary company of CVS Health Corporation.  Responsibilities:  ? Analyzed and reviewed functional specifications and requirements to determine best data designapproach and translate business requirements into data models.  ? Created models for various schemas and created the metadata in order to deploy the models intomicro strategy to be able to reuse the definitions enterprise wide.  ? Performed data Ingestion for the incoming web feeds into the Data lake store which includes bothstructured and unstructured data.  ? Implemented Predictive analytics and machine learning algorithms to forecast key metrics inthe form of designed dashboards on to AWS (S3/EC2) and Django platform for the company's core business.  ? Create the architectural artifacts for the Enterprise Data Warehouse and the Operational Dashboard,such as Entity Relationship Diagrams (ERD), the DDL scripts, the Conceptual Data Model, and technical as well as business documents.  ? Conducted data profiling to insure that the available data could support business needs. edwith the developers on resolving the reported bugs and various technical issues.  ? Involved in requirements gathering activities analyze, and document business processes andfundamentals, and strategic data needs.  ? Created data source views from MYSQL and HADOOP data sources.  ? Migrated our retired systems to leverage new systems and customized according to the businessrequirement.  ? Enforced database naming standards and maintained user domains.  ? Supported data conversion activities and coordinated the resolution of conversion and datamigration issues.  ? Created and maintained Data Dictionary and pursued to reach consensus.  ? Created data lineages and mappings for Data Lake schemas.  ? Ensured Error logs and audit tables are generated and populated properly.  ? Involved in troubleshooting, resolving and escalating data related issues and validating data toimprove data quality.  ? Tracking and reporting the issues to project team and management.  ? Created mapping for horizontal data lineages for various systems.  ? Contribute in the development of knowledge transfer documentation.  ? Used Python, R and SQL to create Statistical algorithms involving Linear Regression, Logistic Regression, Random forest, Decision trees for estimating the risks.  ? Managed change requests by following change request management process for the project. ? Involved in preparing a simple and detailed user guide and training manual for the application and for an intended novice user.  Environment:Erwin 9.64, MS Access, Micro strategy, MySql, Erwin, Oracle10g, HeidiSql, Hadoop, Toad 12.5,MS Visio, SVN Data Analyst Symbiosys  - Visakhapatnam December 2012 to February 2014 Description: Symbiosys  was founded in 2001 and is a 100% export oriented unit, registered in the Visakhapatnam Special Economic Zone (VSEZ). We provide high-quality services and solution to our clients worldwide.    Responsibilities:  ? Extensively ed on Informatica PowerCenter Transformations such as Source Qualifier, Lookup, Filter, Expression, Router, Joiner, Update Strategy, Rank, Aggregator, Sequence Generator etc. ? Proficiency in using Informatica PowerCenter tool to design data conversions from wide variety of sources.  ? Expertise in transforming business requirements into analytical models, designing algorithms,building models, developing data mining and reporting solutions that scale across a massive volume of structured and unstructured data.  ? Proficient in using Informatica flow manager, flow monitor to create, schedule and controlflows, tasks, and sessions.  ? Created pivot tables and ran VLOOKUP's in Excel as a part of data validation.  ? Used Informatica PowerCenter for extraction, loading and transformation (ETL) of data in the datawarehouse.  ? ed on data analysis, data discrepancy reduction in the source and target schemas.  ? Designed and developed complex mappings, from varied transformation logic like Unconnected and Connected lookups, Router, Filter, Expression, Aggregator, Joiner, Update Strategy and more. ? Preparation of System requirements (SRS), Database specifications (DBS), Software design document (SDD).  ? Responsible for the maintenance of few applications in PowerBuilder 10.2  ? Involved in using SQLServer 2005 for fixed the production issues in the background.  ? Coordination and Quality activities on delivery  ? Involved in testing with validation of all fields, functions, programs, agents from front end and backend code reviews across the application.  ? Involved in preparation program specifications, unit tests, test cases and user manual documents.    Environment: Informatica 8.x, PowerBuilder 10.2, SQL Server 2005. Java Developer Clover InfoTech - Mumbai, Maharashtra October 2011 to November 2012 Description: BOLSTER Solutions was established in 2011 and is based in Hyderabad. Our project clients vary in size from start-ups and SMEs to larger international companies. Our main area of expertise is building complex, secure and database-driven web-based applications. Last year we ed for 26 clients, completing over 50 , 65% of our business came from repeat orders from existing clients.  Responsibilities:  ? Involved in each phase of Software Development Life Cycle(SDLC) models like Requirementgathering and analysis, Design, Implementation, Testing, Deployment and Maintenance.  ? Developed Login, Policy and Claims Screens for customers using HTML 5, CSS3, JavaScript, AJAX, JSP,and jQuery.  ? Used Core Java to develop Business Logic.  ? ML models developed: Customer Survival Analysis for better targeting, Member Engagement callcenter optimization, Financial Forecasting for product realization.  ? Involved in the development of business module applications using J2EE  like Servlets,JSP.  ? Designed and developed the web-tier using JSP's, Servlets frame.  ? Used various Core Java concepts such as Multi-Threading, Exception Handling, Collection APIs toimplement various features and enhancements.  ? Strong experience in design & development of applications using Java/J2EE components such as JavaServer Pages (JSP).  ? Developed EJB MDB's and message Queue's using JMS technology.  ? EJB Session Beans were used to process requests from the user interface and CMP entity beans wereused to interact with the persistence layer.  ? Developed stored procedures, triggers, and queries using PLSQL in SQL Server.  ? Use Spring MVC as frame and JavaScript for client-side view, used frames for client-sidedata validation, creating dynamic web pages-Ajax, jQuery. Developed model classes based on the forms to be displayed on the UI.  ? ML Algorithms used: Logistic/Linear Regression, Random Forest, XG-Boost, K-Means Clustering e.t.c.   ? Implemented various design patterns in the project such as Business Delegate, Data Transfer Object, Data Access Object, Service Locator and Singleton.  ? Used SQL statements and procedures to fetch the data from the database.  ? Developed test cases and performed unit test using JUnit Frame.  ? Used CVS as version control and ANT scripts to fetch, build, and deploy application to developmentenvironment.    Environment:Java, HTML, CSS, JavaScript, MySQL, Struts, EJB, Spring MVC. Education  Bachelor's  Data modeling, Db2, Ms access, Sql server, Sql server 2012, Oracle, Pl/sql, Sql, Sybase, Sqoop, Hbase, Flume, Hadoop, Tableau server, Teradata, C++, Hadoop, Hbase, Hive, Html Additional Information  Technical :    Data Modeling  Erwin r 9.6/9.5, ER/Studio 9.7, Star-Schema Modeling, Snowflake-Schema Modeling, FACT and dimension tables, Pivot Tables.  Databases Oracle 11g/12c, MS Access, SQL Server 2012/2014, Sybase and DB2, Teradata, Hive.  Big Data  Hadoop, Hive, Spark, Pig, HBase, Sqoop, Flume.  BI  Tableau 7.0/8.2, Tableau server 8.2, Tableau Reader 8.1,SAP Business Objects, Crystal Reports   Packages Microsoft Office 2010, Microsoft Project 2010, SAP and Microsoft Visio, Share point Portal Server  Operating Systems Microsoft Windows 8/7/XP, Linux and UNIX.  Languages SQL, PL/SQL, ASP, Visual Basic, XML, Python, SQL, T-SQL, SQL Server, C, C++, JAVA, HTML , UNIX shell scripting, PERL, R.  Applications Toad for Oracle, Oracle SQL Developer, MS Word, MS Excel, MS Power Point, Teradata, Designer 6i.  Methodologies RAD, JAD, RUP, UML, System Development Life Cycle (SDLC), Waterfall Model.  Data Modeling  Erwin r 9.6/9.5, ER/Studio 9.7, Star-Schema Modeling, Snowflake-Schema Modeling, FACT and dimension tables, Pivot Tables.  Databases Oracle 11g/12c, MS Access, SQL Server 2012/2014, Sybase and DB2, Teradata14/15, Hive.  Big Data  Hadoop, Hive, Spark, Pig, HBase, Sqoop, Flume.  BI  Tableau 7.0/8.2, Tableau server 8.2, Tableau Reader 8.1,SAP Business Objects, Crystal Reports   Packages Microsoft Office 2010, Microsoft Project 2010, SAP and Microsoft Visio, Share point Portal Server"
"Applied mathematics graduate student currently pursuing a Master of Arts degree in an ABETaccredited program. Obtained a Bachelor of Science degree in Aerospace Engineering in 2015. Possesses , and analytical and  . Loyal and focused, with problem solving and critical thinking . Ready for a career with opportunities to contribute to company goals and s.
 Experience

Applications Engineer
Spectra Quest, Inc - Richmond, VA
July 2015 to November 2017
* Conducted experiments, analyzed machine vibration data, and wrote  reports. 
* Tested prototype software and electrical components and systems. 
* Developed test procedures for new and existing products. 
* Recommended design modifications that enhance performance and eliminate malfunctions. 
* Processed digital signals and sensor data. 
* Proofread and edited product manuals. 
* Calibrated proximity probes, force transducer signal conditioner, and torque sensors. 
* Balanced high speed motors and provided the balance reports. 
* Performed electrical fabrication  including soldering, wiring, and BNC cable installation. 
* Provided  support to worldwide customers and resolved hardware and software issues. * Provided on-site training. 
* Assisted hands-on machine vibration analysis course. 
* Created and provided sales quotations. 
* Created  orders and shipping documents. 
* Assembled simulators, data acquisition, sensors, and embedded computer. 
* Fabricated faulted components such as bearings for experimental fault analysis. 
* Attended conferences and symposiums. 
 Turbomachinery & Pump Symposia (Houston, TX - Sep 2015 & 2016) 
 IMAC - Society for Experimental Mechanics (Orlando, FL - Jan 2016) 
 ASEE (New Orleans, LA - June 2016, Columbus, OH - June 2017) 
 MFP (Virginia Beach, VA - May 2017) 
 IMVAC (Orlando, FL - Nov 2017)
Engineering Intern
Diablo Canyon Power Plant - Avila Beach, CA
June 2009 to November 2009
* Monitored equipment vibration using SKF Microlog Analyzer AX Series and GX Series on a daily basis.* Reported results from vibration monitoring to the Predictive Maintenance Engineering division for analysis. 
* Developed a spreadsheet for each piece of equipment to identify routes, motor data, pump data, andtypes of bearings.
Outage Electrical Utility er
Diablo Canyon Power Plant - Avila Beach, CA
January 2009 to March 2009
* Gained knowledge of power plant operations and protocols. 
* Assisted electricians modify and repair electrical breakers. 
* Delivered supplies to on-site facilities from the warehouse. 
* Operated forklift. 
 
Intern, Research Experiences for Undergraduates 
Program in Robotics and Autonomous System
Intern, Research Experiences for Undergraduates
California Polytechnic State University - San Luis Obispo, CA June 2008 to August 2008
* Collaborated within a group to modify software in a robot to remotely walk towards a target. * Implemented the Sick LIDAR module to communicate with software in a robot via Bluetooth. * Researched Denavit-Hartenberg convention to assign coordinate frames to the robot.


Bachelor of Science degree in Aerospace Engineering
California Polytechnic State University - San Luis Obispo, CA
May 2020
Associate of Science degree in Mathematics
Cuesta College - San Luis Obispo, CA
June 2015
Master of Arts in Applied Mathematics
California State University - Fullerton, CA


Matlab (7 years), Python (1 year), Excel (10+ years), Outlook (3 years), Powerpoint (5 years), Word
(10+ years), Publisher (Less than 1 year), R (Less than 1 year), Latex (Less than 1 year), testing, Microsoft Office
Certifications/Licenses

ISO Vibration Analyst Certification Level II
July 2017 to Present
Vibration Institute
Commercial Pilot Helicopter License
March 2004 to Present
Federal Aviation Administration
Private Pilot Helicopter License
October 2000 to March 2004
Federal Aviation Administration
Assessments

Data Analysis  Expert
August 2019
Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/la7pv5a-aez0jftt
Problem Solving  Highly Proficient
August 2019
Analyzing relevant information when solving problems.
Full results: https://share.indeedassessments.com/share_assignment/4p0o8kttrd14qbvp
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Additional Information

Written and verbal bilingual Japanese communication , and experience translating  documents and digital presentations.",Data Scientist,resume,"Applied mathematics graduate student currently pursuing a Master of Arts degree in an ABETaccredited program. Obtained a Bachelor of Science degree in Aerospace Engineering in 2015. Possesses , and analytical and  . Loyal and focused, with problem solving and critical thinking . Ready for a career with opportunities to contribute to company goals and s.  Experience  Applications Engineer Spectra Quest, Inc - Richmond, VA July 2015 to November 2017 * Conducted experiments, analyzed machine vibration data, and wrote  reports.  * Tested prototype software and electrical components and systems.  * Developed test procedures for new and existing products.  * Recommended design modifications that enhance performance and eliminate malfunctions.  * Processed digital signals and sensor data.  * Proofread and edited product manuals.  * Calibrated proximity probes, force transducer signal conditioner, and torque sensors.  * Balanced high speed motors and provided the balance reports.  * Performed electrical fabrication  including soldering, wiring, and BNC cable installation.  * Provided  support to worldwide customers and resolved hardware and software issues. * Provided on-site training.  * Assisted hands-on machine vibration analysis course.  * Created and provided sales quotations.  * Created  orders and shipping documents.  * Assembled simulators, data acquisition, sensors, and embedded computer.  * Fabricated faulted components such as bearings for experimental fault analysis.  * Attended conferences and symposiums.   Turbomachinery & Pump Symposia (Houston, TX - Sep 2015 & 2016)   IMAC - Society for Experimental Mechanics (Orlando, FL - Jan 2016)   ASEE (New Orleans, LA - June 2016, Columbus, OH - June 2017)   MFP (Virginia Beach, VA - May 2017)   IMVAC (Orlando, FL - Nov 2017) Engineering Intern Diablo Canyon Power Plant - Avila Beach, CA June 2009 to November 2009 * Monitored equipment vibration using SKF Microlog Analyzer AX Series and GX Series on a daily basis.* Reported results from vibration monitoring to the Predictive Maintenance Engineering division for analysis.  * Developed a spreadsheet for each piece of equipment to identify routes, motor data, pump data, andtypes of bearings. Outage Electrical Utility er Diablo Canyon Power Plant - Avila Beach, CA January 2009 to March 2009 * Gained knowledge of power plant operations and protocols.  * Assisted electricians modify and repair electrical breakers.  * Delivered supplies to on-site facilities from the warehouse.  * Operated forklift.    Intern, Research Experiences for Undergraduates  Program in Robotics and Autonomous System Intern, Research Experiences for Undergraduates California Polytechnic State University - San Luis Obispo, CA June 2008 to August 2008 * Collaborated within a group to modify software in a robot to remotely walk towards a target. * Implemented the Sick LIDAR module to communicate with software in a robot via Bluetooth. * Researched Denavit-Hartenberg convention to assign coordinate frames to the robot.   Bachelor of Science degree in Aerospace Engineering California Polytechnic State University - San Luis Obispo, CA May 2020 Associate of Science degree in Mathematics Cuesta College - San Luis Obispo, CA June 2015 Master of Arts in Applied Mathematics California State University - Fullerton, CA   Matlab (7 years), Python (1 year), Excel (10+ years), Outlook (3 years), Powerpoint (5 years), Word (10+ years), Publisher (Less than 1 year), R (Less than 1 year), Latex (Less than 1 year), testing, Microsoft Office Certifications/Licenses  ISO Vibration Analyst Certification Level II July 2017 to Present Vibration Institute Commercial Pilot Helicopter License March 2004 to Present Federal Aviation Administration Private Pilot Helicopter License October 2000 to March 2004 Federal Aviation Administration Assessments  Data Analysis  Expert August 2019 Interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. Full results: https://share.indeedassessments.com/share_assignment/la7pv5a-aez0jftt Problem Solving  Highly Proficient August 2019 Analyzing relevant information when solving problems. Full results: https://share.indeedassessments.com/share_assignment/4p0o8kttrd14qbvp Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field. Additional Information  Written and verbal bilingual Japanese communication , and experience translating  documents and digital presentations."
"  with around 5 years of experience in all phases of diverse  specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure. 
 ed on analyzing large datasets on distributed databases and developing Machine Learningalgorithms to gain operational insights and present them to the leadership. 
 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervisedand unsupervised modeling. 
 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies. 
 Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX) 
 Adapted statistical programming languages like R and Python 
 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms. 
 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool. 
 Created dashboards as part of Data Visualization using Tableau. 
 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool. 
 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark. 
 Performed multiple Data Mining techniques and derive new insights from the data. 
 Skilled in Machine Learning, Statistical Modeling, and Big Data. 
 Creative problem-solver with strong analytical, leadership, and communication  
 Proficient in Python, R, Scala, Java, SQL, and C 
 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured
Data, Data Acquisition, Data Validation, and Predictive Modeling 
 Data Science Specialties include: Machine Learning, Sequential Modeling, Natural LanguageProcessing (NLP) 
 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis,
Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering,
Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics 
 Experienced in stochastic optimization and regression with machine learning algorithms 
 Experienced in formulating and solving discrete and continuous optimization problems 
 Able to research statistical machine learning, supervised learning, and classification methods  Strong mathematical and statistical modeling and computer programming  in an innovative manner 
 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine
(SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA),
Regression, Naïve Bayes, Support Vector Machines 
 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets 
 Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets,TensorFlow, Keras 
 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights 
 Development of clear analytical reports which directly address strategic goals 
 Identified and learn applicable new techniques independently as needed 
 Able to  comfortably and effectively within an interdisciplinary research environment 
 Experienced with validation of machine learning ensemble classifiers 
 Utilized the online datasets to implement machine learning models using Spark ML for buildingprototypes
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Science Engineer
Jefferies LLC - Jersey City, NJ February 2019 to Present
Responsibilities: 
 Coded in Python with selenium and automated website data scraping 
 Scripted using R for cleaning, merging and extraction of relevant data 
 Created interactive visualization using Tableau and performed data analysis to report findings andtrends 
 Analyzed massive data models with tables having over 100s of millions of records to draw insightsand useful information. 
 Architect complex database systems on Hadoop to scale out data development processes. 
 Explore  and gather insights from Amfam's operational data stores and data warehouses
(stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau) 
 Designed and developed scripts to test data and find data defects. 
 Determined the quality of data, verify accuracy of information and ensure that the data is fit formodeling purposes. 
 Transformed data elements and attributes into usable form based on business requirements.  Blend data sets at different granularity levels using analytical queries, window functions and SQL joins. 
 Identified data duplicates and develop means to remove them. 
 Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries,MapReduce or python. 
 Designed and develop data pipelines to preprocess modeling data such as handle null values andclean up defective data attributes. 
 Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and
CSVs into hive tables or data frames. 
 Explored and determined ways to organize data in Hive tables for fast read and writes through hivetable partitions and buckets for optimized performance. 
 Developed programs to store data in appropriate file formats and logical grouping in tables. 
 Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed datamarts on Hadoop 
 Maintained development activities in version control and create updated documentation. 
 
Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL
Data Scientist
Anthem - Atlanta, GA
June 2018 to February 2019
Responsibilities: 
 Translated business questions into research s, design and conduct analyses, developfindings and synthesize recommendations to deliver valuable, relevant, and actionable insights  Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings) 
 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developingvarious machine learning models such Random forest 
 The missing data in the dataset is handled using Imputer method in SkLearn library 
 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encodermethods in sklearn library 
 Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for modeling 
 Defined a generic classification function, which takes a model as input and determines the Accuracyand Cross-Validation scores 
 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standardmachine learning datasets 
 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenuefor future 
 ed with applied statistics and applied mathematics  for performance optimization 
 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores  Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms 
 Used cross-validation to test the models with different batches of data to optimize the models andprevent over fitting 
 Analyzed the SQL scripts and designed the solution to implement using PySpark and developedscripts as per the requirement 
 ed with Tableau in order to represent the data in visual format and better describe the problemwith solutions. 
 
Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL
Data Scientist
US Bank - Brookfield, WI
September 2017 to June 2018
Responsibilities: 
 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to developoperational and visual reports for KPI monitoring 
 Data analysis and visualization (Python, R) 
 Designed, implemented and automated modeling and analysis procedures on existing andexperimentally created data 
 Increased pace & confidence of learning algorithm by combining state of the art  andstatistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes 
 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easilymaintainable format 
 Implemented Topic Modelling, linear classifier models 
 Collaborate with UI engineers, project managers, and designers to develop web portal thataggregates reports from various sources and  
 Member of Data Science team tasked with helping clients turn data into a strategic asset 
 Focused on front end features, browser manipulation, and cross-browser compatibility. 
 Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and
DOM to develop reporting portal. 
 Used Agile Scrum for BI  across different clients, which allowed for production prototyping,rapid deployment and transparency. 
 
Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.
Junior Data Scientist
Ordnance Factory Board
January 2016 to July 2017
Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%. 
 
Responsibilities: 
 ed on various phases of data mining- data collection, data cleaning, developing models,validation, visualization. 
 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python. 
 Performed Data Manipulation and Aggregation from various sources including HDFS and createdvarious Predictive and Descriptive analytics using R and Tableau. 
 Used various libraries and developed various matching learning algorithms using Pandas, NumPy,
Seaborn, Scipy, matplotlib, Scikit-learn in python. 
 Designed Predictive analysis algorithms using Historical Data. 
 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linearregression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN. 
 ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in
Hadoop on AWS. 
 ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports. 
 
Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.
Data Science Intern
iPrism Technologies - Hyderabad, Telangana
April 2015 to December 2015
Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate. 
Responsibilities: 
 Collected data from end client, performed ETL and defined the uniform standard format 
 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basicfields 
 Performed string formatting on the dataset converting hours from date format to a numerical integer
 
 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the datasetsuch as day of week, age, hour and number of screens. 
 Developed and implemented predictive models like Logistic Regression, Decision Tree, Support
Vector Machine (SVM) to predict the probability of enrollment 
 Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the finalmodel based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment  Tuned the hyper parameters of the above models using Grid Search to find the optimum models 
 Designed and implemented K-Fold Cross-validation to test and verify the model's significance  Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure. 
 
Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop
Education

Master's in Computer Engineering
Arizona State University - Tempe, AZ


PYTHON (3 years), SQL (3 years), Hadoop (3 years), HADOOP (3 years), MYSQL (2 years)
Additional Information

 : 
Programming languages Python, Java, R. C 
Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio 
Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural
Nets, Machine Perception 
Operating systems Windows, Linux. 
Databases MySQL, MS SQL Server, NoSQL 
Web and Cloud Technologies AWS, HTML5 
Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL",Data Scientist,resume,"  with around 5 years of experience in all phases of diverse  specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.   ed on analyzing large datasets on distributed databases and developing Machine Learningalgorithms to gain operational insights and present them to the leadership.   Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervisedand unsupervised modeling.   Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.   Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX)   Adapted statistical programming languages like R and Python   Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.   Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.   Created dashboards as part of Data Visualization using Tableau.   Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.  Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.   Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.  Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.   Performed multiple Data Mining techniques and derive new insights from the data.   Skilled in Machine Learning, Statistical Modeling, and Big Data.   Creative problem-solver with strong analytical, leadership, and communication    Proficient in Python, R, Scala, Java, SQL, and C   Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling   Data Science Specialties include: Machine Learning, Sequential Modeling, Natural LanguageProcessing (NLP)   Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics   Experienced in stochastic optimization and regression with machine learning algorithms   Experienced in formulating and solving discrete and continuous optimization problems   Able to research statistical machine learning, supervised learning, and classification methods  Strong mathematical and statistical modeling and computer programming  in an innovative manner   Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, Naïve Bayes, Support Vector Machines   Experienced in AWS cloud computing, Spark, and capable of ing with large datasets   Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets,TensorFlow, Keras   Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights   Development of clear analytical reports which directly address strategic goals   Identified and learn applicable new techniques independently as needed   Able to  comfortably and effectively within an interdisciplinary research environment   Experienced with validation of machine learning ensemble classifiers   Utilized the online datasets to implement machine learning models using Spark ML for buildingprototypes Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Data Science Engineer Jefferies LLC - Jersey City, NJ February 2019 to Present Responsibilities:   Coded in Python with selenium and automated website data scraping   Scripted using R for cleaning, merging and extraction of relevant data   Created interactive visualization using Tableau and performed data analysis to report findings andtrends   Analyzed massive data models with tables having over 100s of millions of records to draw insightsand useful information.   Architect complex database systems on Hadoop to scale out data development processes.   Explore  and gather insights from Amfam's operational data stores and data warehouses (stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau)   Designed and developed scripts to test data and find data defects.   Determined the quality of data, verify accuracy of information and ensure that the data is fit formodeling purposes.   Transformed data elements and attributes into usable form based on business requirements.  Blend data sets at different granularity levels using analytical queries, window functions and SQL joins.   Identified data duplicates and develop means to remove them.   Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries,MapReduce or python.   Designed and develop data pipelines to preprocess modeling data such as handle null values andclean up defective data attributes.   Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and CSVs into hive tables or data frames.   Explored and determined ways to organize data in Hive tables for fast read and writes through hivetable partitions and buckets for optimized performance.   Developed programs to store data in appropriate file formats and logical grouping in tables.   Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed datamarts on Hadoop   Maintained development activities in version control and create updated documentation.    Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL Data Scientist Anthem - Atlanta, GA June 2018 to February 2019 Responsibilities:   Translated business questions into research s, design and conduct analyses, developfindings and synthesize recommendations to deliver valuable, relevant, and actionable insights  Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)   Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developingvarious machine learning models such Random forest   The missing data in the dataset is handled using Imputer method in SkLearn library   Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encodermethods in sklearn library   Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for modeling   Defined a generic classification function, which takes a model as input and determines the Accuracyand Cross-Validation scores   Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standardmachine learning datasets   Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenuefor future   ed with applied statistics and applied mathematics  for performance optimization   ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores  Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms   Used cross-validation to test the models with different batches of data to optimize the models andprevent over fitting   Analyzed the SQL scripts and designed the solution to implement using PySpark and developedscripts as per the requirement   ed with Tableau in order to represent the data in visual format and better describe the problemwith solutions.    Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL Data Scientist US Bank - Brookfield, WI September 2017 to June 2018 Responsibilities:   Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to developoperational and visual reports for KPI monitoring   Data analysis and visualization (Python, R)   Designed, implemented and automated modeling and analysis procedures on existing andexperimentally created data   Increased pace & confidence of learning algorithm by combining state of the art  andstatistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes   Parsed data, producing concise conclusions from raw data in a clean, well-structured and easilymaintainable format   Implemented Topic Modelling, linear classifier models   Collaborate with UI engineers, project managers, and designers to develop web portal thataggregates reports from various sources and    Member of Data Science team tasked with helping clients turn data into a strategic asset   Focused on front end features, browser manipulation, and cross-browser compatibility.   Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and DOM to develop reporting portal.   Used Agile Scrum for BI  across different clients, which allowed for production prototyping,rapid deployment and transparency.    Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop. Junior Data Scientist Ordnance Factory Board January 2016 to July 2017 Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.    Responsibilities:   ed on various phases of data mining- data collection, data cleaning, developing models,validation, visualization.   Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.   Performed Data Manipulation and Aggregation from various sources including HDFS and createdvarious Predictive and Descriptive analytics using R and Tableau.   Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.   Designed Predictive analysis algorithms using Historical Data.   Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linearregression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.   ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in Hadoop on AWS.   ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports.    Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc. Data Science Intern iPrism Technologies - Hyderabad, Telangana April 2015 to December 2015 Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate.  Responsibilities:   Collected data from end client, performed ETL and defined the uniform standard format   Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basicfields   Performed string formatting on the dataset converting hours from date format to a numerical integer    Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the datasetsuch as day of week, age, hour and number of screens.   Developed and implemented predictive models like Logistic Regression, Decision Tree, Support Vector Machine (SVM) to predict the probability of enrollment   Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the finalmodel based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment  Tuned the hyper parameters of the above models using Grid Search to find the optimum models   Designed and implemented K-Fold Cross-validation to test and verify the model's significance  Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.    Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop Education  Master's in Computer Engineering Arizona State University - Tempe, AZ   PYTHON (3 years), SQL (3 years), Hadoop (3 years), HADOOP (3 years), MYSQL (2 years) Additional Information   :  Programming languages Python, Java, R. C  Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio  Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception  Operating systems Windows, Linux.  Databases MySQL, MS SQL Server, NoSQL  Web and Cloud Technologies AWS, HTML5  Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL"
"Doctor of Philosophy. Research area Plant Ecology & Evolution. Chinese Academy of Sciences/World Agroforestry Centre (ICRAF). 
2011  2015  	Master of Science in Ecology. Research area Plant Ecology & Evolution 
Xishuangbanna Tropical Botanical Garden, Chinese Academy of Sciences. 
2006  2008  	Bachelor of Science in Forestry. Faculty of Forestry, National University of Laos. 
2002  2005 	Associate Degree in Forestry. Faculty of Forestry, National University of Laos. 
 
PROFRESSIONAL EXPERIENCE 

2016  2017 	Associate Statistic Instructor for World banks sub-Project in Laos. 
2014  2016  	Team leader and researcher. The Land Degradation Surveillance Frame 
(LDSF), Mekong Sentinel Landscape Project, ICRAF and Consultative Group for International Agricultural Research (CGIAR) collaborative project. Led the fields (vegetation surveys, collecting soil samples and etc), plant identification, data management, data analysis, writing publications. 
2012  2013 	Non-timber Forest Product Advisor and Assistant Project Manager to Triangle Generation Humanitaire. 
2009  2011  	Lao plant database developer, Curriculum developer, and Lecturer assistant. 
2005  2011  Researcher and Biodiversity Consultant to International non-Government Organizations and Government of Lao PDR. 
 
 AND STATISTICAL SOFTWARE 
 
 
 
 
 
Advance statistical analysis and machine learning in R (8 years). 
Linear modelling, Structural Equation Modelling, Spatial analysis in R. 
Net Analysis, Path analysis, Factor analysis, cluster analysis in R. 
 
 
  
 
Regressions, ANOVA and etc in R. Data management, manipulation, visualization in R. 
Phylogenetic analysis in R, MEGA5.  Experimental design, Land navigation Forest inventory, Plant identification. 
GIS (QGIS) and spatial analysis in R. 
 
LANGUAGES 

Lao (native), English (fluent), Thai (fluent), Chinese mandarin (basic). 
 
RECENT SCIENTIFIC PAPERS 

1. Satdichanh, M., Ma, H., Yan, K., Dossa, G. G. O., Winowiecki, L., Vågen, T.-G.,  Harrison, R. D. (2019). Phylogenetic diversity correlated with above?ground biomass production during forest succession: Evidence from tropical forests in Southeast Asia. Journal of Ecology, 107(3), 14191432. doi:10.1111/1365-2745.13112. 
2. Satdichanh, M., Millet, J., Heinimann, A., Nanthavong, K., & Harrison, R. D. (2015). Using Plant Functional Traits and Phylogenies to Understand Patterns of Plant Community Assembly in a Seasonal Tropical Forest in Lao PDR. PLOS ONE, 10(6), e0130151. doi:10.1371/journal.pone.0130151. ",Data Scientist,resume,"Doctor of Philosophy. Research area Plant Ecology & Evolution. Chinese Academy of Sciences/World Agroforestry Centre (ICRAF).  2011  2015  	Master of Science in Ecology. Research area Plant Ecology & Evolution  Xishuangbanna Tropical Botanical Garden, Chinese Academy of Sciences.  2006  2008  	Bachelor of Science in Forestry. Faculty of Forestry, National University of Laos.  2002  2005 	Associate Degree in Forestry. Faculty of Forestry, National University of Laos.    PROFRESSIONAL EXPERIENCE   2016  2017 	Associate Statistic Instructor for World banks sub-Project in Laos.  2014  2016  	Team leader and researcher. The Land Degradation Surveillance Frame  (LDSF), Mekong Sentinel Landscape Project, ICRAF and Consultative Group for International Agricultural Research (CGIAR) collaborative project. Led the fields (vegetation surveys, collecting soil samples and etc), plant identification, data management, data analysis, writing publications.  2012  2013 	Non-timber Forest Product Advisor and Assistant Project Manager to Triangle Generation Humanitaire.  2009  2011  	Lao plant database developer, Curriculum developer, and Lecturer assistant.  2005  2011  Researcher and Biodiversity Consultant to International non-Government Organizations and Government of Lao PDR.     AND STATISTICAL SOFTWARE            Advance statistical analysis and machine learning in R (8 years).  Linear modelling, Structural Equation Modelling, Spatial analysis in R.  Net Analysis, Path analysis, Factor analysis, cluster analysis in R.           Regressions, ANOVA and etc in R. Data management, manipulation, visualization in R.  Phylogenetic analysis in R, MEGA5.  Experimental design, Land navigation Forest inventory, Plant identification.  GIS (QGIS) and spatial analysis in R.    LANGUAGES   Lao (native), English (fluent), Thai (fluent), Chinese mandarin (basic).    RECENT SCIENTIFIC PAPERS   1. Satdichanh, M., Ma, H., Yan, K., Dossa, G. G. O., Winowiecki, L., Vågen, T.-G.,  Harrison, R. D. (2019). Phylogenetic diversity correlated with above?ground biomass production during forest succession: Evidence from tropical forests in Southeast Asia. Journal of Ecology, 107(3), 14191432. doi:10.1111/1365-2745.13112.  2. Satdichanh, M., Millet, J., Heinimann, A., Nanthavong, K., & Harrison, R. D. (2015). Using Plant Functional Traits and Phylogenies to Understand Patterns of Plant Community Assembly in a Seasonal Tropical Forest in Lao PDR. PLOS ONE, 10(6), e0130151. doi:10.1371/journal.pone.0130151. "
" Data Scientist with around 5 years of experience in areas including Data Analysis, Statistical Analysis,Machine Learning, Deep Learning, Data mining with large data sets of structured and unstructured data 
 Experienced in using various Python libraries (Beautiful Soup, NumPy, Scipy, matplotlib, python-twistter, Pandas, MySQL DB for database connectivity). 
 Experience in building end to end data science solutions using R, Python, SQL and Tableau byleveraging machine learning based algorithms, Statistical Modeling, Data Mining, Natural Language
Processing (NLP) and Data Visualization. 
 Adept and deep understanding of Statistical Modeling, Multivariate Analysis, model testing, problemanalysis, model comparison and validation. 
 Adapted with Python and OOP concepts such as Inheritance, Polymorphism, Abstraction, Association,etc. 
 Sound understanding of Deep learning using CNN, RNN, ANN, reinforcement learning, transferlearning. 
 Experienced in developing machine learning models for real-world problems using R and python  Experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics, data wrangling. 
 ed on theoretical foundations and practical hands-on  related to supervised learning(linear and logistic regression, boosted decision trees, Support Vector Machines (SVM), neural nets(NN), NLP), unsupervised learning (clustering, dimensionality reduction, recommender systems), probability & statistics, experiment analysis, confidence intervals, A/B testing, algorithms and data structures. 
 Excellent understanding of machine learning techniques and algorithms, such as K-NN, Naive Bayes,
SVM, Decision Forests, Random forest etc. 
 Experience with command-line scripting, data structures and algorithms. 
 Experienced in processing large datasets with Spark using Python. 
 Solid understanding of big data  like Hadoop, Spark, HDFS, MapReduce, Pig and Hive. 
 Experience with machine learning  and libraries such as Scikit-learn, R, Spark and Weka 
 Experience ing with large, real world data (Unsupervised Data) - big, messy, incomplete, full oferrors 
 Hands-on experience with NLP, mining of structured, semi-structured, and unstructured data 
 Expertise in database Performance Tuning using Oracle Hints, explain plan, TKPROF, Partitioning andIndexes 
 Experience in manipulating the large data sets with R packages like tidyr, tidyverse, dplyr reshape,lubridate, Caret and visualizing the data using lattice and ggplot2 packages. 
 Intensive hands-on Boot camp on Data Analytics course spanning from Statistics to Programmingincluding data engineering, data visualization, machine learning and programming in R, SQL.  Strong background in Machine Learning, Predictive Analysis and Data Mining with a broad understanding of Supervised and Unsupervised learning techniques and algorithms (eg: Regression, K-
NN, SVM, Naïve Bayes, Decision trees, Clustering, etc.) 
 Proficient in data visualization  such as Tableau 10.5, Power BI 2.30, Python Matplotlib/Seaborn,R ggplot2/Shiny to generate charts like Box Plot, Scatter Chart, Pie Chart and Histogram etc., and to create visually impactful and actionable interactive reports and dashboards. 
 Experience in using Teradata ETL  and utilizes such as BTEQ, MLOAD, FASTLOAD, TPT, FastExport. 
 Experience with  such as R Programming, visualizations, SAS, Open Source etc. 
 Strong experience writing stored procedures, functions, triggers and adhoc queries using PL/SQL 
 Experienced in integration of various relational and non-relational sources such as DB2, Oracle,
Netezza, SQL Server, NoSQL, COBOL, XML and Flat Files, to Netezza database. 
 Extensive experience in Normalization (1NF, 2NF, 3NF and BCNF) and De-normalization techniquesfor improved database performance Data Warehouse/Data Mart environments.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Sr. Data Scientist/Machine Learning Engineer
Nike - Hillsboro, OR
April 2018 to Present
Responsibilities: 
 ings on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
 Setup storage and data analysis  in Amazon Web Services (AWS) cloud computing infrastructure.
 
 Implemented end-to-end systems for Data Analytics, Data Automation and integrated with customvisualization  using R, Mahout, Hadoop and MongoDB. 
 ed as Data Architects and IT Architects to understand the movement of data and its storage andER Studio 9.7. 
 ed with several R packages including knitr, dplyr, SparkR, Causal Infer, Space-Time. 
 Coded R functions to interface with Caffe Deep Learning Frame. 
 Used Pandas, Numpy, Seaborn, Scipy, Matplotlib, Sci-kit-learn, and NLTK in Python for developingvarious machine learning algorithms. 
 Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments. 
 Utilized domain knowledge and application portfolio knowledge to play a key role in defining thefuture state of large, business  programs. 
 Machine Learning algorithms such as decision trees and random forest were used in this process topredict the urgency of the problem statement received by the company, this was done by calculating the weighted totals of the polarity and subjectivity of the problem statements and classifying each statement accordingly. 
 Text data received in the problem statements was converted into numerical/ordinal data usingparameters like polarity and subjectivity by developing a mathematical model to integrate the two statistics. 
 Installed and used Caffe Deep Learning Frame 
 Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python,a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc. 
 Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC'susing Scala, Spark SQL and MLlib libraries. 
 Used Data Quality Validation techniques to validate Critical Data Elements (CDE) and identifiedvarious anomalies. 
 Developed various Qlik-View Data Models by extracting and using the data from various sources files,DB2, Excel, Flat Files and Big data. 
 Participated in all phases of Datamining, Data-collection, Data-Cleaning, Developing-Models,
Validation, Visualization and Performed Gap Analysis. 
 Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task
Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts. 
 As Architect delivered various complex OLAP Databases/Cubes, Scorecards, Dashboards and Reports.
 
 Programmed a utility in Python that used multiple packages (Scipy, Numpy, Pandas) 
 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN,Naive Bayes. 
 Designed both 3NF data models for ODS, OLTP systems and Dimensional Data Models using Star andSnowflake Schemas. 
 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so thatwe would be able to assign each document a response label for further classification. 
 Created SQL tables with referential integrity and developed queries using SQL, SQL PLUS and PL/SQL.
 
 Designed and developed Use Case, Activity Diagrams, Sequence Diagrams, OOD (Object orientedDesign) using UML and Visio. 
 
Environment: AWS, R, Informatica, Machine learning-Algorithms, Anaconda, Market Basket Analysis,
Sentiment Analysis, Polarity, Predictive Analytics, Deep Learning- Algorithms, CNN, HCNN, Python,
Data Mining, Data Collection, Data Cleaning, Validation, HDFS, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Map-Reduce, Rational Rose, SQL, and MongoDB.
Data Scientist
Vanguard - Malvern, PA
August 2017 to March 2018
Responsibilities: 
 Implementation of machine learning methods, optimization, and visualization. Mathematical methodsof statistics such as Regression Models, Decision Tree, Naïve Bayes, Ensemble Classifier, Hierarchical
Clustering and Semi-Supervised Learning on different datasets using Python. 
 Researched and implemented various Machine Learning Algorithms using the R language. 
 Devised a machine learning algorithm using Python for facial recognition. 
 Used R for a prototype on a sample data exploration to identify the best algorithmic approach andthen wrote Scala scripts using spark machine learning module. 
 Used Scala scripts for spark machine learning libraries API execution for decision trees, ALS, logisticand linear regressions algorithms. 
 ed on Migrating an On-premises virtual machine to Azure Resource Manager Subscription with
Azure Site Recovery. 
 Provide consulting and cloud architecture for premier customers and internal  running on MS
Azure platform for high availability of services, low operational costs. 
 Develop structured, efficient and error-free codes for Big Data requirements using my knowledge in
Hadoop and its Eco-system. 
 Development of web service using Windows Communication Foundation and.Net to receive andprocess XML files and deploy on Cloud Service on Microsoft Azure. 
 ed on various methods including data fusion and machine learning and improved the accuracyof distinguished right rules from potential rules. 
 Developed Merge jobs in Python to extract and load data into a MySQL database. 
 Used Test driven approach for developing the application and Implemented the unit tests using
Python Unit test frame. 
 Wrote unit test cases in Python and Objective-C for other API calls in the customer frames. 
 Tested with various Machine Learning algorithms like Support Vector Machine (SVM), Random Forest,
Trees with XGBoost concluded Decision Trees as a champion model. 
 Built models using Statistical techniques like Bayesian HMM and Machine Learning classificationmodels like XGBoost, SVM, and Random Forest. 
 ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
 
Environment: Machine Learning, R Language, Hadoop, Big Data, Azure, Python, Java, J2EE, Spring,
Struts, JSF, Dojo, JavaScript, DB2, CRUD, PL/ SQL, JDBC, coherence, MongoDB, Apache CXF, soap, Web Services, Eclipse
Data Scientist
Line -Vision - Boston, MA
January 2017 to July 2017
Responsibilities: 
 Analyzed large data sets apply machine learning techniques and develop predictive models,statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques. 
 Analyzed pre-existing predictive model developed by advanced analytics team and factorsconsidered during model development. 
 Experienced in all phases of data mining; data collection, data cleaning, developing models,validation and visualization. 
 Analyzed metadata and processed data to get better insights of the data. 
 Created initial data visualizations in tableau to provide basic insights of data to the takeholders. 
 Application of various machine learning algorithms and statistical modeling like decision trees,regression models, clustering, SVM to identify Volume using scikit-learn package in Python. 
 Conducted regular communications with leaders of other teams to get better understanding of thedata at a deeper level. 
 Analyzed dataset of 14M record count and reduced it to 1.3M by filtering out rows with duplicatecustomer IDs and removed outliers using boxplots and univariate algorithms. 
 Performed extensive exploratory data analysis using Teradata to improve the quality of the datasetand developed Machine Learning algorithms using Python for predicting the model quality and created
Data Visualizations using Tableau. 
 Developed visualizations using R packages like ggplot2, choroplethr to identify patterns and trends inthe preprocessed data. 
 Experienced in RStudio packages and Python libraries like SciKit-Learn to improve the modelaccuracy from 65% to 86%. 
 Experienced in various Python libraries like Pandas, One dimensional NumPy and Two dimensionalNumPy. 
 Experienced in using PyTorch library and implementing natural language processing. 
 Developed data visualizations in Tableau to display day to day accuracy of the model with newlyincoming 
 data. 
 Hold a point-of-view on the strengths and limitations of statistical models and analyses in variousbusiness contexts and can evaluate and effectively communicate the uncertainty in the results.  Used Keras library to build and train deep learning models and fetched good results. 
 Propensity model developed that was beneficial with a greater ROI compared to other models.
Achieved 0.95 million dollars ROI per cycle with cycle duration of one quarter year. 
 Implemented complete data science project involving data acquisition, data wrangling, exploratorydata analysis (EDA), model development and model evaluation. 
 
Environment: MS Access, SQL Server, Teradata, Advanced SQL, RStudio (ggplot2, caret), Python (Pandas, NumPy, Sci-kit learn), Machine Learning (Logistic Regression, Decision trees, SVM, Random forest), PyTorch, Keras, Tableau, Excel
Data Analyst
Qualex - IN
April 2014 to November 2016
Responsibilities: 
 Collected data from end client, performed ETL and defined the uniform standard format 
 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basicfields 
 Performed string formatting on the dataset converting hours from date format to a numerical integer
 
 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the datasetsuch as day of week, age, hour and number of screens. 
 Developed and implemented predictive models like Logistic Regression, Decision Tree, SupportVector 
 Machine (SVM) to predict the probability of enrollment 
 Used Ensemble learning methods like Random Forest, Bagging, Gradient Boosting, picked the finalmodel based on confusion matrix, ROC, AUC predicted the probability of customer enrollment  ed on missing value imputation, outlier identification with statistical methodologies using
Pandas 
 NumPy 
 Tuned the hyper parameters of the above models using Grid Search to find the optimum models 
 Designed and implemented K-Fold Cross-validation to test and verify the model's significance  Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure. 
 Use  extensively like R, Python, ODS, DB2, Metadata, MS Excel etc. to analyze data frommultiple perspectives and was able to provide a robust Machine Learning algorithm. 
 Created new  and business processes that simplify, standardize and enables operationalexcellence. 
 Used  like Tableau for drilling-down data, creating insightful reports and garnering actionablebusiness insights. 
 
Environment: Tableau report builder, MS Outlook, SQL Server 2012/2014, Python (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop.
Education

Master's in information systems in information systems
University of Texas at Arlington - Arlington, TX
Skills

Cassandra, Hdfs, Mapreduce, Kafka, Db2, Hadoop, Machine learning, Nlp, Deep learning, Logistic regression, Neural nets, Random forest, Api, Git, Hadoop, Hive, Mapreduce, Pig, Python, Flask
Additional Information

TECHNICAL SKILLS: 
Machine Learning Neural Nets, Deep Learning, NLP, Recommendation Systems, IoT 
Software Development Agile, Scrum, Jira, Wiki, Git, SVN, AWS, Predix, Microsoft Azure, Third Party API integration, Unit Testing, Code coverage 
Database MySQL, MSSQL, DB2, PostgreSQL, Cassandra, HDFS 
Python Scipy, NumPy, IPython, Scikit-learn, Pyspark, Pandas, Flask, Tensor flow, keras 
R Recommender lab, Random forest, glm, rpart, xgboost 
SAS Logistic Regression, Decision Tree, Proc 
Visualization Tableau, PowerBI, ggplot, matplotlib 
Operating System Windows, Linux, Unix, Macintosh HD, Red Hat 
Data Modeling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer 
Hadoop Ecosystem Hadoop, Hive, HDFS, MapReduce, Pig, Kafka",Data Scientist,resume," Data Scientist with around 5 years of experience in areas including Data Analysis, Statistical Analysis,Machine Learning, Deep Learning, Data mining with large data sets of structured and unstructured data   Experienced in using various Python libraries (Beautiful Soup, NumPy, Scipy, matplotlib, python-twistter, Pandas, MySQL DB for database connectivity).   Experience in building end to end data science solutions using R, Python, SQL and Tableau byleveraging machine learning based algorithms, Statistical Modeling, Data Mining, Natural Language Processing (NLP) and Data Visualization.   Adept and deep understanding of Statistical Modeling, Multivariate Analysis, model testing, problemanalysis, model comparison and validation.   Adapted with Python and OOP concepts such as Inheritance, Polymorphism, Abstraction, Association,etc.   Sound understanding of Deep learning using CNN, RNN, ANN, reinforcement learning, transferlearning.   Experienced in developing machine learning models for real-world problems using R and python  Experienced in Agile Methodologies, Scrum stories and sprints experience in a Python based environment, along with data analytics, data wrangling.   ed on theoretical foundations and practical hands-on  related to supervised learning(linear and logistic regression, boosted decision trees, Support Vector Machines (SVM), neural nets(NN), NLP), unsupervised learning (clustering, dimensionality reduction, recommender systems), probability & statistics, experiment analysis, confidence intervals, A/B testing, algorithms and data structures.   Excellent understanding of machine learning techniques and algorithms, such as K-NN, Naive Bayes, SVM, Decision Forests, Random forest etc.   Experience with command-line scripting, data structures and algorithms.   Experienced in processing large datasets with Spark using Python.   Solid understanding of big data  like Hadoop, Spark, HDFS, MapReduce, Pig and Hive.   Experience with machine learning  and libraries such as Scikit-learn, R, Spark and Weka   Experience ing with large, real world data (Unsupervised Data) - big, messy, incomplete, full oferrors   Hands-on experience with NLP, mining of structured, semi-structured, and unstructured data   Expertise in database Performance Tuning using Oracle Hints, explain plan, TKPROF, Partitioning andIndexes   Experience in manipulating the large data sets with R packages like tidyr, tidyverse, dplyr reshape,lubridate, Caret and visualizing the data using lattice and ggplot2 packages.   Intensive hands-on Boot camp on Data Analytics course spanning from Statistics to Programmingincluding data engineering, data visualization, machine learning and programming in R, SQL.  Strong background in Machine Learning, Predictive Analysis and Data Mining with a broad understanding of Supervised and Unsupervised learning techniques and algorithms (eg: Regression, K- NN, SVM, Naïve Bayes, Decision trees, Clustering, etc.)   Proficient in data visualization  such as Tableau 10.5, Power BI 2.30, Python Matplotlib/Seaborn,R ggplot2/Shiny to generate charts like Box Plot, Scatter Chart, Pie Chart and Histogram etc., and to create visually impactful and actionable interactive reports and dashboards.   Experience in using Teradata ETL  and utilizes such as BTEQ, MLOAD, FASTLOAD, TPT, FastExport.   Experience with  such as R Programming, visualizations, SAS, Open Source etc.   Strong experience writing stored procedures, functions, triggers and adhoc queries using PL/SQL   Experienced in integration of various relational and non-relational sources such as DB2, Oracle, Netezza, SQL Server, NoSQL, COBOL, XML and Flat Files, to Netezza database.   Extensive experience in Normalization (1NF, 2NF, 3NF and BCNF) and De-normalization techniquesfor improved database performance Data Warehouse/Data Mart environments. Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Sr. Data Scientist/Machine Learning Engineer Nike - Hillsboro, OR April 2018 to Present Responsibilities:   ings on different data formats such as JSON, XML and performed machine learning algorithms inPython.   Setup storage and data analysis  in Amazon Web Services (AWS) cloud computing infrastructure.    Implemented end-to-end systems for Data Analytics, Data Automation and integrated with customvisualization  using R, Mahout, Hadoop and MongoDB.   ed as Data Architects and IT Architects to understand the movement of data and its storage andER Studio 9.7.   ed with several R packages including knitr, dplyr, SparkR, Causal Infer, Space-Time.   Coded R functions to interface with Caffe Deep Learning Frame.   Used Pandas, Numpy, Seaborn, Scipy, Matplotlib, Sci-kit-learn, and NLTK in Python for developingvarious machine learning algorithms.   Demonstrated experience in design and implementation of Statistical models, Predictive models,enterprise data model, metadata solution and data life cycle management in both RDBMS, Big Data environments.   Utilized domain knowledge and application portfolio knowledge to play a key role in defining thefuture state of large, business  programs.   Machine Learning algorithms such as decision trees and random forest were used in this process topredict the urgency of the problem statement received by the company, this was done by calculating the weighted totals of the polarity and subjectivity of the problem statements and classifying each statement accordingly.   Text data received in the problem statements was converted into numerical/ordinal data usingparameters like polarity and subjectivity by developing a mathematical model to integrate the two statistics.   Installed and used Caffe Deep Learning Frame   Utilized Spark, Scala, Hadoop, HBase, Cassandra, MongoDB, Kafka, Spark Streaming, MLLib, Python,a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.   Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC'susing Scala, Spark SQL and MLlib libraries.   Used Data Quality Validation techniques to validate Critical Data Elements (CDE) and identifiedvarious anomalies.   Developed various Qlik-View Data Models by extracting and using the data from various sources files,DB2, Excel, Flat Files and Big data.   Participated in all phases of Datamining, Data-collection, Data-Cleaning, Developing-Models, Validation, Visualization and Performed Gap Analysis.   Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, and MapReduce concepts.   As Architect delivered various complex OLAP Databases/Cubes, Scorecards, Dashboards and Reports.    Programmed a utility in Python that used multiple packages (Scipy, Numpy, Pandas)   Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN,Naive Bayes.   Designed both 3NF data models for ODS, OLTP systems and Dimensional Data Models using Star andSnowflake Schemas.   Updated Python scripts to match training data with our database stored in AWS Cloud Search, so thatwe would be able to assign each document a response label for further classification.   Created SQL tables with referential integrity and developed queries using SQL, SQL PLUS and PL/SQL.    Designed and developed Use Case, Activity Diagrams, Sequence Diagrams, OOD (Object orientedDesign) using UML and Visio.    Environment: AWS, R, Informatica, Machine learning-Algorithms, Anaconda, Market Basket Analysis, Sentiment Analysis, Polarity, Predictive Analytics, Deep Learning- Algorithms, CNN, HCNN, Python, Data Mining, Data Collection, Data Cleaning, Validation, HDFS, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Vision, Map-Reduce, Rational Rose, SQL, and MongoDB. Data Scientist Vanguard - Malvern, PA August 2017 to March 2018 Responsibilities:   Implementation of machine learning methods, optimization, and visualization. Mathematical methodsof statistics such as Regression Models, Decision Tree, Naïve Bayes, Ensemble Classifier, Hierarchical Clustering and Semi-Supervised Learning on different datasets using Python.   Researched and implemented various Machine Learning Algorithms using the R language.   Devised a machine learning algorithm using Python for facial recognition.   Used R for a prototype on a sample data exploration to identify the best algorithmic approach andthen wrote Scala scripts using spark machine learning module.   Used Scala scripts for spark machine learning libraries API execution for decision trees, ALS, logisticand linear regressions algorithms.   ed on Migrating an On-premises virtual machine to Azure Resource Manager Subscription with Azure Site Recovery.   Provide consulting and cloud architecture for premier customers and internal  running on MS Azure platform for high availability of services, low operational costs.   Develop structured, efficient and error-free codes for Big Data requirements using my knowledge in Hadoop and its Eco-system.   Development of web service using Windows Communication Foundation and.Net to receive andprocess XML files and deploy on Cloud Service on Microsoft Azure.   ed on various methods including data fusion and machine learning and improved the accuracyof distinguished right rules from potential rules.   Developed Merge jobs in Python to extract and load data into a MySQL database.   Used Test driven approach for developing the application and Implemented the unit tests using Python Unit test frame.   Wrote unit test cases in Python and Objective-C for other API calls in the customer frames.   Tested with various Machine Learning algorithms like Support Vector Machine (SVM), Random Forest, Trees with XGBoost concluded Decision Trees as a champion model.   Built models using Statistical techniques like Bayesian HMM and Machine Learning classificationmodels like XGBoost, SVM, and Random Forest.   ed on different data formats such as JSON, XML and performed machine learning algorithms inPython.    Environment: Machine Learning, R Language, Hadoop, Big Data, Azure, Python, Java, J2EE, Spring, Struts, JSF, Dojo, JavaScript, DB2, CRUD, PL/ SQL, JDBC, coherence, MongoDB, Apache CXF, soap, Web Services, Eclipse Data Scientist Line -Vision - Boston, MA January 2017 to July 2017 Responsibilities:   Analyzed large data sets apply machine learning techniques and develop predictive models,statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques.   Analyzed pre-existing predictive model developed by advanced analytics team and factorsconsidered during model development.   Experienced in all phases of data mining; data collection, data cleaning, developing models,validation and visualization.   Analyzed metadata and processed data to get better insights of the data.   Created initial data visualizations in tableau to provide basic insights of data to the takeholders.   Application of various machine learning algorithms and statistical modeling like decision trees,regression models, clustering, SVM to identify Volume using scikit-learn package in Python.   Conducted regular communications with leaders of other teams to get better understanding of thedata at a deeper level.   Analyzed dataset of 14M record count and reduced it to 1.3M by filtering out rows with duplicatecustomer IDs and removed outliers using boxplots and univariate algorithms.   Performed extensive exploratory data analysis using Teradata to improve the quality of the datasetand developed Machine Learning algorithms using Python for predicting the model quality and created Data Visualizations using Tableau.   Developed visualizations using R packages like ggplot2, choroplethr to identify patterns and trends inthe preprocessed data.   Experienced in RStudio packages and Python libraries like SciKit-Learn to improve the modelaccuracy from 65% to 86%.   Experienced in various Python libraries like Pandas, One dimensional NumPy and Two dimensionalNumPy.   Experienced in using PyTorch library and implementing natural language processing.   Developed data visualizations in Tableau to display day to day accuracy of the model with newlyincoming   data.   Hold a point-of-view on the strengths and limitations of statistical models and analyses in variousbusiness contexts and can evaluate and effectively communicate the uncertainty in the results.  Used Keras library to build and train deep learning models and fetched good results.   Propensity model developed that was beneficial with a greater ROI compared to other models. Achieved 0.95 million dollars ROI per cycle with cycle duration of one quarter year.   Implemented complete data science project involving data acquisition, data wrangling, exploratorydata analysis (EDA), model development and model evaluation.    Environment: MS Access, SQL Server, Teradata, Advanced SQL, RStudio (ggplot2, caret), Python (Pandas, NumPy, Sci-kit learn), Machine Learning (Logistic Regression, Decision trees, SVM, Random forest), PyTorch, Keras, Tableau, Excel Data Analyst Qualex - IN April 2014 to November 2016 Responsibilities:   Collected data from end client, performed ETL and defined the uniform standard format   Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basicfields   Performed string formatting on the dataset converting hours from date format to a numerical integer    Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the datasetsuch as day of week, age, hour and number of screens.   Developed and implemented predictive models like Logistic Regression, Decision Tree, SupportVector   Machine (SVM) to predict the probability of enrollment   Used Ensemble learning methods like Random Forest, Bagging, Gradient Boosting, picked the finalmodel based on confusion matrix, ROC, AUC predicted the probability of customer enrollment  ed on missing value imputation, outlier identification with statistical methodologies using Pandas   NumPy   Tuned the hyper parameters of the above models using Grid Search to find the optimum models   Designed and implemented K-Fold Cross-validation to test and verify the model's significance  Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.   Use  extensively like R, Python, ODS, DB2, Metadata, MS Excel etc. to analyze data frommultiple perspectives and was able to provide a robust Machine Learning algorithm.   Created new  and business processes that simplify, standardize and enables operationalexcellence.   Used  like Tableau for drilling-down data, creating insightful reports and garnering actionablebusiness insights.    Environment: Tableau report builder, MS Outlook, SQL Server 2012/2014, Python (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop. Education  Master's in information systems in information systems University of Texas at Arlington - Arlington, TX Skills  Cassandra, Hdfs, Mapreduce, Kafka, Db2, Hadoop, Machine learning, Nlp, Deep learning, Logistic regression, Neural nets, Random forest, Api, Git, Hadoop, Hive, Mapreduce, Pig, Python, Flask Additional Information  TECHNICAL SKILLS:  Machine Learning Neural Nets, Deep Learning, NLP, Recommendation Systems, IoT  Software Development Agile, Scrum, Jira, Wiki, Git, SVN, AWS, Predix, Microsoft Azure, Third Party API integration, Unit Testing, Code coverage  Database MySQL, MSSQL, DB2, PostgreSQL, Cassandra, HDFS  Python Scipy, NumPy, IPython, Scikit-learn, Pyspark, Pandas, Flask, Tensor flow, keras  R Recommender lab, Random forest, glm, rpart, xgboost  SAS Logistic Regression, Decision Tree, Proc  Visualization Tableau, PowerBI, ggplot, matplotlib  Operating System Windows, Linux, Unix, Macintosh HD, Red Hat  Data Modeling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer  Hadoop Ecosystem Hadoop, Hive, HDFS, MapReduce, Pig, Kafka"
" 
Over 6+ years of experience in areas including Data Analyst, Statistical Analysis, Machine Learning, Deep Learning with large data sets of structured and unstructured data in travel services, strong functional knowledge, business processes, and latest market trends and manufactory industries. 

* Developed predictive models using Decision Tree, Random Forest, Naïve Bayes, Logistic Regression, Cluster Analysis, and Neural Nets. 
* Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling, and data visualization with large data sets of structured and unstructured data, created ER diagrams and schema.
* Expert in the entire Data Science process life cycle including Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Machine Learning Algorithms, Validation, and Visualization.
* Strong knowledge in Statistical methodologies such as Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions and Time Series Analysis.
* Proficient in Python and its libraries such as NumPy, Pandas, Scikit-Learn, Matplotlib and Seaborn.
* Expert in prepossessing data in Pandas using visualization, data cleaning and engineering methods such as looking for Correlations, Imputations, Scaling and Handling Categories
* Experience in building various machine learning models using algorithms such as Linear Regression, Gradient Descent, Support Vector Machines (SVM), Logistic Regression, KNN, Decision Tree, Ensembles such as Random Forrest, AdaBoost, Gradient Boosting Trees.
* Experienced the full software lifecycle in SDLC, Agile, and Scrum methodologies. 
* Strong SQL programming , with experience in ing with functions, packages, and triggers. 
* Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, natural language processing (NLP), etc. 
* Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS, SSAS, SSRS. 
* Expert in developing Data Conversions/Migration from Legacy System of various sources (flat files, Oracle, Non-Oracle Database) to Oracle system Using SQL LOADER, External table and Calling Appropriate Interface tables and API's Informatica. 
* Strong ing experience on Teradata query performance tuning by analyzingCPU, AMP Distribution, Table Skewness, and IO metrics.
* Excellent Data Mining  who can sift the grain from large datasets of Structured and Unstructured data, identify the patterns within data, analyze data and interpret results into actionable insights and business values.
* Good understanding of ing on Artificial Neural Nets and Deep Learning models using Theano and Tensor Flow packages using in Python.
* Extensive ing experience with Python including Scikit-learn, SciPy, Pandas, and NumPy developing machine learning models, manipulating and handling data. 
* Transformed traditional environment to virtualized environments with AWS-EC2, S3, EBS, ELB, and EBS. 
*  to build a fully automated, highly elastic cloud orchestration frame on AWS.
* Extensively ed on Teradata Utility  like BTEQ, Fast Load, Fast Export, Multi-Load, TPUMP, and TPT.
* Experience in developing and analyzing data models, involved in writing simple and complex SQL queries to extract data from the database for data analysis and testing 
* Strong knowledge in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation, and maintenance with timely delivery against deadlines 
* Proficient knowledge of statistics, mathematics, machine learning, recommendation algorithms and analytics with an excellent understanding of business operations and analytics  for effective analysis of data.
* Excellent communication . Successfully ing in fast-paced multitasking environment both independently and in a collaborative team, a self-motivated enthusiastic learner. 



Bachelor of Engineering.

 


Programming & Scripting Languages 
Python, C++, C, Java.
Databases
MS-Access, Oracle 12c/11g/10g/9i,Mysql,DB2
BI 
Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1.
Machine Learning Libraries
TensorFlow, Keras, PyTorch, NumPy, OpenCV, Scikit-Learn, SciPy, Pandas
Data Modelling 
Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer. 
Teradata Utilities
BTEQ, Fast Load, Fast Export, Multi-load, TPUMP, and TPT 
Database 
Toad, SQL Developer, PL/SQL Developer, SQL Developer, Informatica Power Center 9.5.1.
Operating Systems 
Windows (10, 7, Vista), XP, UNIX, Linux. 
Project 1                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                       
Automotive Lighting,Auburn hills, Michigan.    			Apr 2018 - Till Date

Description:Automotive Lighting (AL) is a German-based company founded in 1999 as a 50-50 joint venture between the Italian Magneti-Marelli and the German Robert Bosch GmbH. In 2001 MagnetiMarelli raised its share to 75% after the acquisition of Seima Group. 
Responsibilities:
* Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos.
* Designed an object detection program by utilizing Python, YOLOmodel, and TensorFlow to identify objects by drawing the boundaries of each identified object in an image.  
* Filtered the discovered boundaries by implementing a non-max suppression algorithm to achieve an optimal bounding box per identified object.
* Evaluated models using Cross-Validation, Log loss function, ROC curves and used AUC for feature selection and elastic  like Elasticsearch, Kibana,etc
* Addressed overfitting by implementing the algorithm regularization methods like L2 and L1.
* Implemented statistical modeling with XGBoostmachinelearning software package using Python to determine the predicted probabilities of each model.
* Created master data for modeling by combining various tables and derived fields from client data and students LORs, essays and various performance metrics.
* Formulated a basis for variable selection and GridSearch, KFold for optimal hyperparameters
* Used NumPy, SciPy, pandas, NTLK(Natural Language Processing Toolkit),matplotlib to build the model.
* Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams.
* Application of various Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing(NLP), supervised and unsupervised, regression models.
* Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in python and build models using deep learning frames.
* Created deep learning models using TensorFlow and Keras by combining all tests as a single normalized score and predict residency attainment of students.
* Used XGB classifier if the feature is a categorical variable and XGBregressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing.
* Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning frame.
* ed with the Data Governance group in creating a custom data dictionary template to be used across the various business lines.
* Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution.
* Interface with other  teams to load (ETL), extract and transform data from a wide variety of data sources
* Provides input and recommendations on  issues to Business & Data Analysts, BI Engineers, and Data Scientists.
Environment: Python 2.x,3.x, Hive, AWS, Linux, Tableau Desktop, Microsoft Excel, NLP, Deep learning frames such as TensorFlow, Keras, Boosting algorithms, DB2, R, Python, Visio, HP ALM, Agile.

Project 2                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                       
Theron  Solutions, Chicago.ILJun 2017-March 2018

Description:Theron  Solutions. LLC provides customers with one strategic team that partners to audit, design, build and maintain purpose-built front and back-end solutions that meet high-growth companies' distinct needs.
Responsibilities:
* Analyzed and solved business problems and found patterns and insights within structured and unstructured data.
* Implemented advanced computer vision techniques like distortion correction, thresholding techniques, and the sliding window method to identify the lane markings to highlight the entire lane.
* Tested the algorithm in a video to ensure that the lane boundaries are accurately identified.
* Utilized a diverse array of  and  as needed, to deliver insights such as R, SAS, MATLAB, Tableau and more.
* Detected near-duplicated news by applying NLP methods and developing machine learning models like label spreading and clustering.
* Employed the output of the semantic segmentation to perform drivable space estimation in 3D, lane estimation and to filter errors in the output of the 2D object detectors.
* prototyping and experimenting with Algorithms and integrating into a production system for different business needs.
* Implemented Porter Stemmer (Natural Language Tool Kit) and NLP bag of words model (Count Vectorizer) to prepare the data.
* Implemented number of customer clustering models and these clusters are plotted visually using Tableau legends for the higher management.
* Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments.
* ed closely with the Data Governance Office team in assessing the source systems for project deliverables.
* Developed SQL procedures to synchronize the dynamic data generated from GTID systems with the Azure SQL Server.
* Process automation using Python/R scripts with Oracle database to generate and write the results in the production environment on a weekly basis.
* Used Data Quality validation techniques to validate Critical Data elements (CDE) and identified various anomalies.
* Performing Data Validation / Data Reconciliation between the disparate source and target systems for various projects.
* Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos.
* Provides input and recommendations on  issues to Business & Data Analysts, BI Engineers, and Data Scientists.

Environment: SAS, R, MLIB, Python, Data Governance, MDM, MATLAB, Tableau,Azure SQL Server.

Project 3                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                       
Global Atlantic Financial Group, Brighton, MA.                                                                                Oct 2016-May 2017

Description:Global Atlantic Financial Group, through its subsidiaries, offers a broad range of retirement, life and reinsurance products designed to help our customers address financial challenges with confidence. A variety of options help Americans customize a strategy to fulfill their protection, accumulation, income, wealth transfer and end-of-life needs.
Responsibilities:
* Used various approaches to collect the business requirements and ed with the business users for ETL application enhancements by conducting various JRD sessions to meet the job requirements
* Designed data profiles for processing, including running PL/SQL queries and using R for Data Acquisition and Data Integrity which consists of Datasets Comparing and Dataset schema checks
* Performed exploratory data analysis like calculation of descriptive statistics, detection of outliers, assumptions testing, factor analysis, etc., in R
* Applied Clustering Algorithms such as K-Means to categorize customers into certain groups
* Implemented Key Performance Indicator (KPI) Objects, Actions, Hierarchies and Attribute Relationships for added functionality and better performance of SSAS Warehouse
* Used Tableau and designed various charts and tables for data analysis and creating various analytical Dashboards to showcase the data to managers
* Performed data management, including creating SQL Server Report Services to develop reusable code and an automatic reporting system and designed user acceptance test to provide end with an opportunity to give constructive feedback.

Environment:R/R Studio, SAS, Oracle Database 11g, Oracle BI , Tableau, MS-Excel.


Project 4                                                                                                                                                                        Data Analyst                                                                                                                                                                                                                                       
Sterling Insurance, Brentwood, TN.						Oct 15 - Sep 2016

Description: Since 1895, the mission of Sterling Insurance Company has been to provide unexcelled service and sound insurance protection to its policyholders. And for over a century, weve delivered on our promise. In our first one hundred years, weve succeeded by doing two things exceedingly well. First, weve built a solid financial base on which our policyholders can rely.

Responsibilities:
* Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions.
* Retrieving data from SQLServer database by writing SQL queries like stored procedure, temp table, view.
* ed with the DBA group to create a Best-Fit Physical Data Model from the Logical Data Model using Forward engineering using Erwin.
* Connected Database with Jupyter notebook for Modeling and Tableau for visualization and reporting.
* ed on fraud detection analysis on loan applications using the history of loan taking with supervised learning methods.
* Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Logistic Regression, Random Forest, Gradient BoostDecisionTree, and NeuralNet.
* Experienced in performing feature engineering such as PCA for high dimensional datasets, important feature selection by Tree-based models.
* Perform model tuning and selection by using cross-validation, parameters tuning to prevent overfitting.
* Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.

Environment: SQL Server 2008, Python 2.x (NumPy/Pandas/Scikit-Learn), GitHub.

Project 5                                                                                                                                                                      Data Analyst
Amity IT Solutions, Hyderabad, India.                                                                                               May 2013 - Sep 2015

Description: A Gwalior based IT Solutions and Software Services Development Company. Providing cost-effective, innovative and robust business solutions. We focus on combining web-based application development, market-savvy design, and reliable business strategy to advance a clients business goals. 
Responsibilities:
* Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity.
* ed closely with stakeholders to understand, define, document business questions needed.
* Review system/application requirements (functional specifications), test results and metrics for quality and completeness.
* Designed and Developed Oracle PL/SQL Procedures and UNIXShellScripts for Data Import/Export and Data Conversions.
* Have Used InformaticaData Quality as an ETL tool to transform the data from various sources and bring them into one common format and load them into the target database for the analysis purpose from Data Warehouse.

Environment: Oracle PL/SQL, UNIX Shell Scripts, Data Import/Export, Informatica Data Quality.


",Data Scientist,resume,"  Over 6+ years of experience in areas including Data Analyst, Statistical Analysis, Machine Learning, Deep Learning with large data sets of structured and unstructured data in travel services, strong functional knowledge, business processes, and latest market trends and manufactory industries.   * Developed predictive models using Decision Tree, Random Forest, Naïve Bayes, Logistic Regression, Cluster Analysis, and Neural Nets.  * Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling, and data visualization with large data sets of structured and unstructured data, created ER diagrams and schema. * Expert in the entire Data Science process life cycle including Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Machine Learning Algorithms, Validation, and Visualization. * Strong knowledge in Statistical methodologies such as Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions and Time Series Analysis. * Proficient in Python and its libraries such as NumPy, Pandas, Scikit-Learn, Matplotlib and Seaborn. * Expert in prepossessing data in Pandas using visualization, data cleaning and engineering methods such as looking for Correlations, Imputations, Scaling and Handling Categories * Experience in building various machine learning models using algorithms such as Linear Regression, Gradient Descent, Support Vector Machines (SVM), Logistic Regression, KNN, Decision Tree, Ensembles such as Random Forrest, AdaBoost, Gradient Boosting Trees. * Experienced the full software lifecycle in SDLC, Agile, and Scrum methodologies.  * Strong SQL programming , with experience in ing with functions, packages, and triggers.  * Excellent understanding of machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, natural language processing (NLP), etc.  * Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio SSIS, SSAS, SSRS.  * Expert in developing Data Conversions/Migration from Legacy System of various sources (flat files, Oracle, Non-Oracle Database) to Oracle system Using SQL LOADER, External table and Calling Appropriate Interface tables and API's Informatica.  * Strong ing experience on Teradata query performance tuning by analyzingCPU, AMP Distribution, Table Skewness, and IO metrics. * Excellent Data Mining  who can sift the grain from large datasets of Structured and Unstructured data, identify the patterns within data, analyze data and interpret results into actionable insights and business values. * Good understanding of ing on Artificial Neural Nets and Deep Learning models using Theano and Tensor Flow packages using in Python. * Extensive ing experience with Python including Scikit-learn, SciPy, Pandas, and NumPy developing machine learning models, manipulating and handling data.  * Transformed traditional environment to virtualized environments with AWS-EC2, S3, EBS, ELB, and EBS.  *  to build a fully automated, highly elastic cloud orchestration frame on AWS. * Extensively ed on Teradata Utility  like BTEQ, Fast Load, Fast Export, Multi-Load, TPUMP, and TPT. * Experience in developing and analyzing data models, involved in writing simple and complex SQL queries to extract data from the database for data analysis and testing  * Strong knowledge in all phases of the SDLC (Software Development Life Cycle) from analysis, design, development, testing, implementation, and maintenance with timely delivery against deadlines  * Proficient knowledge of statistics, mathematics, machine learning, recommendation algorithms and analytics with an excellent understanding of business operations and analytics  for effective analysis of data. * Excellent communication . Successfully ing in fast-paced multitasking environment both independently and in a collaborative team, a self-motivated enthusiastic learner.     Bachelor of Engineering.      Programming & Scripting Languages  Python, C++, C, Java. Databases MS-Access, Oracle 12c/11g/10g/9i,Mysql,DB2 BI  Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1. Machine Learning Libraries TensorFlow, Keras, PyTorch, NumPy, OpenCV, Scikit-Learn, SciPy, Pandas Data Modelling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer.  Teradata Utilities BTEQ, Fast Load, Fast Export, Multi-load, TPUMP, and TPT  Database  Toad, SQL Developer, PL/SQL Developer, SQL Developer, Informatica Power Center 9.5.1. Operating Systems  Windows (10, 7, Vista), XP, UNIX, Linux.  Project 1                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                        Automotive Lighting,Auburn hills, Michigan.    			Apr 2018 - Till Date  Description:Automotive Lighting (AL) is a German-based company founded in 1999 as a 50-50 joint venture between the Italian Magneti-Marelli and the German Robert Bosch GmbH. In 2001 MagnetiMarelli raised its share to 75% after the acquisition of Seima Group.  Responsibilities: * Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos. * Designed an object detection program by utilizing Python, YOLOmodel, and TensorFlow to identify objects by drawing the boundaries of each identified object in an image.   * Filtered the discovered boundaries by implementing a non-max suppression algorithm to achieve an optimal bounding box per identified object. * Evaluated models using Cross-Validation, Log loss function, ROC curves and used AUC for feature selection and elastic  like Elasticsearch, Kibana,etc * Addressed overfitting by implementing the algorithm regularization methods like L2 and L1. * Implemented statistical modeling with XGBoostmachinelearning software package using Python to determine the predicted probabilities of each model. * Created master data for modeling by combining various tables and derived fields from client data and students LORs, essays and various performance metrics. * Formulated a basis for variable selection and GridSearch, KFold for optimal hyperparameters * Used NumPy, SciPy, pandas, NTLK(Natural Language Processing Toolkit),matplotlib to build the model. * Formulated several graphs to show the performance of the students by demographics and their mean score in different USMLE exams. * Application of various Artificial Intelligence(AI)/machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing(NLP), supervised and unsupervised, regression models. * Performed Data Cleaning, features scaling, features engineering using pandas and NumPy packages in python and build models using deep learning frames. * Created deep learning models using TensorFlow and Keras by combining all tests as a single normalized score and predict residency attainment of students. * Used XGB classifier if the feature is a categorical variable and XGBregressor for continuous variables and combined it using FeatureUnion and FunctionTransfomer methods of Natural Language Processing. * Created data layers as signals to Signal Hub to predict new unseen data with performance not less than the static model build using deep learning frame. * ed with the Data Governance group in creating a custom data dictionary template to be used across the various business lines. * Create statistical models using distributed and standalone models to build various diagnostics, predictive and prescriptive solution. * Interface with other  teams to load (ETL), extract and transform data from a wide variety of data sources * Provides input and recommendations on  issues to Business & Data Analysts, BI Engineers, and Data Scientists. Environment: Python 2.x,3.x, Hive, AWS, Linux, Tableau Desktop, Microsoft Excel, NLP, Deep learning frames such as TensorFlow, Keras, Boosting algorithms, DB2, R, Python, Visio, HP ALM, Agile.  Project 2                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                        Theron  Solutions, Chicago.ILJun 2017-March 2018  Description:Theron  Solutions. LLC provides customers with one strategic team that partners to audit, design, build and maintain purpose-built front and back-end solutions that meet high-growth companies' distinct needs. Responsibilities: * Analyzed and solved business problems and found patterns and insights within structured and unstructured data. * Implemented advanced computer vision techniques like distortion correction, thresholding techniques, and the sliding window method to identify the lane markings to highlight the entire lane. * Tested the algorithm in a video to ensure that the lane boundaries are accurately identified. * Utilized a diverse array of  and  as needed, to deliver insights such as R, SAS, MATLAB, Tableau and more. * Detected near-duplicated news by applying NLP methods and developing machine learning models like label spreading and clustering. * Employed the output of the semantic segmentation to perform drivable space estimation in 3D, lane estimation and to filter errors in the output of the 2D object detectors. * prototyping and experimenting with Algorithms and integrating into a production system for different business needs. * Implemented Porter Stemmer (Natural Language Tool Kit) and NLP bag of words model (Count Vectorizer) to prepare the data. * Implemented number of customer clustering models and these clusters are plotted visually using Tableau legends for the higher management. * Used T-SQL queries to pull the data from disparate systems and Data warehouse in different environments. * ed closely with the Data Governance Office team in assessing the source systems for project deliverables. * Developed SQL procedures to synchronize the dynamic data generated from GTID systems with the Azure SQL Server. * Process automation using Python/R scripts with Oracle database to generate and write the results in the production environment on a weekly basis. * Used Data Quality validation techniques to validate Critical Data elements (CDE) and identified various anomalies. * Performing Data Validation / Data Reconciliation between the disparate source and target systems for various projects. * Writing complex SQL queries for validating the data against different kinds of reports generated by Cognos. * Provides input and recommendations on  issues to Business & Data Analysts, BI Engineers, and Data Scientists.  Environment: SAS, R, MLIB, Python, Data Governance, MDM, MATLAB, Tableau,Azure SQL Server.  Project 3                                                                                                                                                                      Data Scientist                                                                                                                                                                                                                                        Global Atlantic Financial Group, Brighton, MA.                                                                                Oct 2016-May 2017  Description:Global Atlantic Financial Group, through its subsidiaries, offers a broad range of retirement, life and reinsurance products designed to help our customers address financial challenges with confidence. A variety of options help Americans customize a strategy to fulfill their protection, accumulation, income, wealth transfer and end-of-life needs. Responsibilities: * Used various approaches to collect the business requirements and ed with the business users for ETL application enhancements by conducting various JRD sessions to meet the job requirements * Designed data profiles for processing, including running PL/SQL queries and using R for Data Acquisition and Data Integrity which consists of Datasets Comparing and Dataset schema checks * Performed exploratory data analysis like calculation of descriptive statistics, detection of outliers, assumptions testing, factor analysis, etc., in R * Applied Clustering Algorithms such as K-Means to categorize customers into certain groups * Implemented Key Performance Indicator (KPI) Objects, Actions, Hierarchies and Attribute Relationships for added functionality and better performance of SSAS Warehouse * Used Tableau and designed various charts and tables for data analysis and creating various analytical Dashboards to showcase the data to managers * Performed data management, including creating SQL Server Report Services to develop reusable code and an automatic reporting system and designed user acceptance test to provide end with an opportunity to give constructive feedback.  Environment:R/R Studio, SAS, Oracle Database 11g, Oracle BI , Tableau, MS-Excel.   Project 4                                                                                                                                                                        Data Analyst                                                                                                                                                                                                                                        Sterling Insurance, Brentwood, TN.						Oct 15 - Sep 2016  Description: Since 1895, the mission of Sterling Insurance Company has been to provide unexcelled service and sound insurance protection to its policyholders. And for over a century, weve delivered on our promise. In our first one hundred years, weve succeeded by doing two things exceedingly well. First, weve built a solid financial base on which our policyholders can rely.  Responsibilities: * Participated in all phases of data acquisition, data cleaning, developing models, validation, and visualization to deliver data science solutions. * Retrieving data from SQLServer database by writing SQL queries like stored procedure, temp table, view. * ed with the DBA group to create a Best-Fit Physical Data Model from the Logical Data Model using Forward engineering using Erwin. * Connected Database with Jupyter notebook for Modeling and Tableau for visualization and reporting. * ed on fraud detection analysis on loan applications using the history of loan taking with supervised learning methods. * Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models and utilized algorithms such as Logistic Regression, Random Forest, Gradient BoostDecisionTree, and NeuralNet. * Experienced in performing feature engineering such as PCA for high dimensional datasets, important feature selection by Tree-based models. * Perform model tuning and selection by using cross-validation, parameters tuning to prevent overfitting. * Ensemble methods were used to increase the accuracy of the training model with different Bagging and Boosting methods.  Environment: SQL Server 2008, Python 2.x (NumPy/Pandas/Scikit-Learn), GitHub.  Project 5                                                                                                                                                                      Data Analyst Amity IT Solutions, Hyderabad, India.                                                                                               May 2013 - Sep 2015  Description: A Gwalior based IT Solutions and Software Services Development Company. Providing cost-effective, innovative and robust business solutions. We focus on combining web-based application development, market-savvy design, and reliable business strategy to advance a clients business goals.  Responsibilities: * Involved in Data mapping specifications to create and execute detailed system test plans. The data mapping specifies what data will be extracted from an internal data warehouse, transformed and sent to an external entity. * ed closely with stakeholders to understand, define, document business questions needed. * Review system/application requirements (functional specifications), test results and metrics for quality and completeness. * Designed and Developed Oracle PL/SQL Procedures and UNIXShellScripts for Data Import/Export and Data Conversions. * Have Used InformaticaData Quality as an ETL tool to transform the data from various sources and bring them into one common format and load them into the target database for the analysis purpose from Data Warehouse.  Environment: Oracle PL/SQL, UNIX Shell Scripts, Data Import/Export, Informatica Data Quality.   "
" 

University of California at Berkeley  	 	 	 	                Graduated: May 2017 
Master of Arts in Statistics 
 GPA: 3.72/4.00 
 Course: Advanced Probability, Advanced Statistics, Statistical Computing with R, Linear Models, Time Series 
 
University of California at Santa Barbara                                                     Graduated: June 2013 Bachelor of Science in Financial Mathematics and Statistics 
 Cumulative GPA: 3.96/4.00 
 Relevant Course: Stochastic Processes, Numerical Analysis, Microeconomic Theory, Partial 
Differential Equations and Fourier Series, SAS Base Programming 
 Experience 

Applied Underwriters, Foster City, CA  	 	 	 	    June 2018  September 2018 
Financial Analyst Intern 
 Maintained and Updated data files relating to rate filings. 
 Helped in evaluating terrorism risk for client locations. 
 
Project Experience 

Portfolio Optimization Project                                                                        Feb 2017  May 2017 
 Studied the effectiveness of Portfolio Optimization Techniques, starting with those created by Harry Markowitz. 
 ed with a small team to try and make a portfolio that would outdo a naive portfolio. 
 Used convex optimization with 5 years worth of monthly stock data to create an optimal portfolio, then measured its future performance. 
 Performed convex optimization using cvxpy package in Python. 
Honors 

Rama Thogarati Memorial Award                                                                                             2013 
 	Awarded each year to a Senior Undergraduate at UCSB for academic excellence in the Department of Statistics and Applied Probability. 
  

Programming :  R, Python, SAS, C, MATLAB, Latex, Excel, SQL 
 ",Data Scientist,resume,"   University of California at Berkeley  	 	 	 	                Graduated: May 2017  Master of Arts in Statistics   GPA: 3.72/4.00   Course: Advanced Probability, Advanced Statistics, Statistical Computing with R, Linear Models, Time Series    University of California at Santa Barbara                                                     Graduated: June 2013 Bachelor of Science in Financial Mathematics and Statistics   Cumulative GPA: 3.96/4.00   Relevant Course: Stochastic Processes, Numerical Analysis, Microeconomic Theory, Partial  Differential Equations and Fourier Series, SAS Base Programming   Experience   Applied Underwriters, Foster City, CA  	 	 	 	    June 2018  September 2018  Financial Analyst Intern   Maintained and Updated data files relating to rate filings.   Helped in evaluating terrorism risk for client locations.    Project Experience   Portfolio Optimization Project                                                                        Feb 2017  May 2017   Studied the effectiveness of Portfolio Optimization Techniques, starting with those created by Harry Markowitz.   ed with a small team to try and make a portfolio that would outdo a naive portfolio.   Used convex optimization with 5 years worth of monthly stock data to create an optimal portfolio, then measured its future performance.   Performed convex optimization using cvxpy package in Python.  Honors   Rama Thogarati Memorial Award                                                                                             2013   	Awarded each year to a Senior Undergraduate at UCSB for academic excellence in the Department of Statistics and Applied Probability.      Programming :  R, Python, SAS, C, MATLAB, Latex, Excel, SQL   "
"Willing to relocate: Anywhere
 Experience

Student Specialist III
University of Alabama in Huntsville - Huntsville, AL May 2017 to December 2018
Office of Information : Re-imaging of instructor machines and installing a new Operating System on the PC. Triaged and resolved numerous tickets related to Software Issues on the Instructor machines. 
Online Learning: Edited tests and quizzes for online courses on Sociology, Mythology, History and Management. Captioned lecture videos for improved accessibility. Designed posters for events and created online forms for surveys.
Event Manager
May 2015 to May 2015
Managed multiple events like singing, cooking and electrical engineering competitions while coordinating with volunteers and competitors. All events were successful without delays.
Student Volunteer
HELPING HANDS OF HUNTSVILLE - Huntsville, AL
Cleaned up lake in Huntsville and university duck pond .Helping Hands of Huntsville is a fall service tradition. It allows new and current students to engage in direct service  with a variety of focuses to help students understand the importance of giving back.


M.S. in Electrical Engineering
University of Alabama in Huntsville - Huntsville, AL December 2018
Visvesvaraya Technological University May 2016


Dimensionality reduction, Machine learning, Pca, Supervised learning, Big data analytics, Clustering,
Data analytics, Mapreduce, Python, Matplotlib, Numpy, Pandas, Ec2, Sql
Links

Additional Information

. 
Machine Learning: Regression, Classification and Clustering. 
Big Data Analytics: Dimensionality reduction like PCA, LDA. Supervised and un-Supervised Learning for feature engineering. 
 Programming Language: Python (NumPy, Pandas, Matplotlib, SciKit-Learn). 
 SQL querying. 
 Linux 
 Tableau, AWS's S3 and EC2. 
 Hadoop HDFS and MapReduce.",Data Scientist,resume,"Willing to relocate: Anywhere  Experience  Student Specialist III University of Alabama in Huntsville - Huntsville, AL May 2017 to December 2018 Office of Information : Re-imaging of instructor machines and installing a new Operating System on the PC. Triaged and resolved numerous tickets related to Software Issues on the Instructor machines.  Online Learning: Edited tests and quizzes for online courses on Sociology, Mythology, History and Management. Captioned lecture videos for improved accessibility. Designed posters for events and created online forms for surveys. Event Manager May 2015 to May 2015 Managed multiple events like singing, cooking and electrical engineering competitions while coordinating with volunteers and competitors. All events were successful without delays. Student Volunteer HELPING HANDS OF HUNTSVILLE - Huntsville, AL Cleaned up lake in Huntsville and university duck pond .Helping Hands of Huntsville is a fall service tradition. It allows new and current students to engage in direct service  with a variety of focuses to help students understand the importance of giving back.   M.S. in Electrical Engineering University of Alabama in Huntsville - Huntsville, AL December 2018 Visvesvaraya Technological University May 2016   Dimensionality reduction, Machine learning, Pca, Supervised learning, Big data analytics, Clustering, Data analytics, Mapreduce, Python, Matplotlib, Numpy, Pandas, Ec2, Sql Links  Additional Information  .  Machine Learning: Regression, Classification and Clustering.  Big Data Analytics: Dimensionality reduction like PCA, LDA. Supervised and un-Supervised Learning for feature engineering.   Programming Language: Python (NumPy, Pandas, Matplotlib, SciKit-Learn).   SQL querying.   Linux   Tableau, AWS's S3 and EC2.   Hadoop HDFS and MapReduce."
"Im searching for a savvy company to leverage my proven data science and public relations  in multiple flows including Finance, Chain Supply, Sales, Charity, Social Media Marketing and many other departments.  
 
Todays business-to-business and business-to-consumer cultures survive and thrive on valuable insights drawn from data analytics and innovative thinking. Id like to be part of your team. 
 
I am confident that my talents will be a positive addition to your organization. I appreciate your time and attention for this new opportunity.
Authorized to  in the US for any employer
 Experience

Senior Data Scientist
Western States Pharmacy Coalition - Lake Forest, CA
February 2018 to May 2019
Highly experienced business insights and analytics  with expert presentations  who delivers clear and concise operational, tactical, and strategic initiatives to increase profits.  
 
Innovative data scientist (mining, tracking and visualization) with growth minded problem solving  identifies simple solutions to leverage remarkable results. 
 
 Achieved $14 million in savings via customized NDC comparison database. 
 Identified more than 80K prospects; filtered to 7K high value targets.       Increased revenue $3 million with $24.8 million on deck. 
 
Created and streamlined 71 reports for Sales, Marketing, and Finance to help independent pharmacies maintain and increase profits.
Senior Analyst, Business Insights & Analytics
AmerisourceBergen - Orange, CA
February 2001 to November 2017
 Generated more than $2.1 Billion in revenue. 
 Saved more than $420 Million in operating expenses. 
 : Automation 45; Procedures 51; Templates 59  all still used today 
 
Promoted multiple times and remained successful through mergers, acquisitions, multiple platform and software conversions. 
 
Provided senior leadership and key decision makers with information to support numerous strategic initiatives and strategies. 
 
Interpreted business reports, trend analyses and projections to create action plans for segmentation sales teams to offer mutually profitable solutions. 
 
Streamlined, designed, developed and implemented best practices to improve business operations efficiencies and profitability for Supply Chain and Finance. 
 
Oversaw Reporting, Data Governance and Analytics teams to ensure day-to-day activities and valuable insights were delivered. 
 
Conducted hundreds of competitive pricing comparisons and dispensing data analyses for new business (request for proposals) to strategic accounts. 
 
Provided formal and informal trainings and shops for internal and external customers.


High school or equivalent


Tableau Desktop (2 years), Business Objects Advanced Reporting (10+ years), Salesforce (6 years),
Sigma Six (3 years), Microsoft Office Certifications (Excel, Access, Word, Outlook, and PowerPoint) (10+ years)
Links

Certifications/Licenses

Tableau Desktop Training I
June 2018 to Present
Training from Tableau
Tableau Prep Training I
December 2018 to Present
Training from Tableau
Tableau Calculations, Maps, and Visualizations
February 2018 to Present
Training via oft
Business Objects Advanced Reporting
August 2016 to Present
Training from SAP
SalesForce.com Wave Analytics
August 2016 to Present
Training from SalesForce.com
Business Objects Reporting
April 2016 to Present
Training from SAP
Six Sigma (WB)
April 2016 to Present
Training from Aveta Business Institute
SalesForce.com Reporting
April 2013 to Present
Training from SalesForce.com
Distinguished Toastmaster
December 2010 to Present
Training from Toastmasters International
Microsoft 2007 Expert (Excel, Access, Word, Outlook and PowerPoint)
September 2009 to Present
Certification from Microsoft
MicroStrategy Narrowcast Developer and Web Interface Designer
August 2005 to Present
Training from MicroStrategy
MicroStrategy SQL Essentials and Engine Specialist Track
June 2005 to Present
Training from MicroStrategy
Master Microsoft Office User Specialist (MOUS) 2000
February 2002 to Present
Certification from Microsoft
Microsoft 2000 Expert (Excel, Access, Word, Outlook and PowerPoint)
February 2002 to Present
Certification from Microsoft
Master Microsoft Office User Specialist 1997
April 2001 to Present
Certification from Microsoft
Microsoft 1997 Expert (Excel, Access, Word, Outlook and PowerPoint)
April 2001 to Present
Certification from Microsoft
Assessments

Spreadsheets with Microsoft Excel  Highly Proficient
June 2019
Excel knowledge including common , PivotTables, conditional & nested formulas, and custom visuals.
Full results: https://share.indeedassessments.com/share_assignment/wmq6e7p3iosi8c-u
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Additional Information

Additional Career, Contract and Project Experience  
 
Childrens Hospital of Orange County CHOC, U.S. Healths, Kaiser Permanente, Proctor & Gamble,
Book Publicist of Southern California, Toastmasters International, American Red Cross and Mensa. 
 
Core Competencies 
 Excellent communication, leadership, time management and organizational . 
 Flexible to  effectively solo, as part of a team, or manage large or small groups. 
 Committed to quality process improvement methods, such as SMART, Six Sigma, PDCA,
Demming, KPI Metrics, POC, modeling and others. 
 Certified in Microsoft Office Suite  Excel, Access, Outlook, Word and PowerPoint. 
 Savvy Social Media Campaigns, MailChimp, Google Business Analytics, Facebook Insights, GoTo
Meeting, Zoom Administrator, and Webinars. 
 Knowledgeable about predictive models, statistical analysis, data warehouse and visualization: Business Objects, PowerBI, Qlik, Tableau, SalesForce, MicroStrategy and more 
 
Instructor 
Chapman University 
University of Phoenix 
Whittier College 
 
Talents 
Magic Castle Magician Member 
Distinguished Toastmaster 
Successful Inventor 
Published Author 
Host of FDTV 
Webmaster 
 
MAJOR ACCOMPLISHMENTS LAST 10 YEARS 
 
One Fiscal Year  
 Generated more than $2.1M in revenue opportunity. 
 Saved more than $500,000 in operating expenses. 
 Saved more than 21,000 man-hours (137 associates in 18 departments). 
 
High Level  
 Created 3 revenue generating reports totaling $2M for strategic accounts. 
 $930,000 annual opportunity  Health Systems B2G ACAP pilot. 
 $864,000 annual opportunity  IPBG Blue Box Opportunity Report. 
 $288,000 annual opportunity  Ingles Alt Size Sub Option B. 
 Saved more than 900 hours and more than $20,000 in operating expenses. 
 Streamlined 27 internal processes to be more efficient. 
 Automated 64 reports in Business Objects Reporting Tool. 
 Trained 14 associates across 6 departments. 
 
Local and Enterprise-wide Awards and Achievements, 20072017 
Individual 206 + Team 129 = Total 335 
 
KEY  (Alpha Order) 
 
? AHP Track n Trend 
? AHP Unit Dose Initiative 
? Automate NTI-GPN Opt Outs 
? Automated Weekly Updates 
? B2G Safe ACAP 
? BOBJ and DAX Formulas 
? BOBJ Scheduler 
? Brand to Generic Projections 
? CCE Reports and Templates 
? Cost of Goods Savings 
? Cost of Shopping 
? Costco Inventory Management 
? CTCA Pricing Analysis 
? Customer Compliance Tool 
? Email Etiquette 
? Estimator Calculator 
? Excel Learning Lab 
? FDB-ABC-PRxO Equivalents 
? FTS and ACAP Statistics 
? GPIs for RFPs 
? Kaboom! Playground 
? Key Shortage Report 
? Large Bottle Savings 
? Margin Analysis Template 
? Profit Per Script 
? QRC and Training 
? Rebate Comparison Model 
? Rebate Value Proposition 
? Save Big Buy Big 
? Substitution Resolution 
? Total Value Scorecard 
 ",Data Scientist,resume,"Im searching for a savvy company to leverage my proven data science and public relations  in multiple flows including Finance, Chain Supply, Sales, Charity, Social Media Marketing and many other departments.     Todays business-to-business and business-to-consumer cultures survive and thrive on valuable insights drawn from data analytics and innovative thinking. Id like to be part of your team.    I am confident that my talents will be a positive addition to your organization. I appreciate your time and attention for this new opportunity. Authorized to  in the US for any employer  Experience  Senior Data Scientist Western States Pharmacy Coalition - Lake Forest, CA February 2018 to May 2019 Highly experienced business insights and analytics  with expert presentations  who delivers clear and concise operational, tactical, and strategic initiatives to increase profits.     Innovative data scientist (mining, tracking and visualization) with growth minded problem solving  identifies simple solutions to leverage remarkable results.     Achieved $14 million in savings via customized NDC comparison database.   Identified more than 80K prospects; filtered to 7K high value targets.       Increased revenue $3 million with $24.8 million on deck.    Created and streamlined 71 reports for Sales, Marketing, and Finance to help independent pharmacies maintain and increase profits. Senior Analyst, Business Insights & Analytics AmerisourceBergen - Orange, CA February 2001 to November 2017  Generated more than $2.1 Billion in revenue.   Saved more than $420 Million in operating expenses.   : Automation 45; Procedures 51; Templates 59  all still used today    Promoted multiple times and remained successful through mergers, acquisitions, multiple platform and software conversions.    Provided senior leadership and key decision makers with information to support numerous strategic initiatives and strategies.    Interpreted business reports, trend analyses and projections to create action plans for segmentation sales teams to offer mutually profitable solutions.    Streamlined, designed, developed and implemented best practices to improve business operations efficiencies and profitability for Supply Chain and Finance.    Oversaw Reporting, Data Governance and Analytics teams to ensure day-to-day activities and valuable insights were delivered.    Conducted hundreds of competitive pricing comparisons and dispensing data analyses for new business (request for proposals) to strategic accounts.    Provided formal and informal trainings and shops for internal and external customers.   High school or equivalent   Tableau Desktop (2 years), Business Objects Advanced Reporting (10+ years), Salesforce (6 years), Sigma Six (3 years), Microsoft Office Certifications (Excel, Access, Word, Outlook, and PowerPoint) (10+ years) Links  Certifications/Licenses  Tableau Desktop Training I June 2018 to Present Training from Tableau Tableau Prep Training I December 2018 to Present Training from Tableau Tableau Calculations, Maps, and Visualizations February 2018 to Present Training via oft Business Objects Advanced Reporting August 2016 to Present Training from SAP SalesForce.com Wave Analytics August 2016 to Present Training from SalesForce.com Business Objects Reporting April 2016 to Present Training from SAP Six Sigma (WB) April 2016 to Present Training from Aveta Business Institute SalesForce.com Reporting April 2013 to Present Training from SalesForce.com Distinguished Toastmaster December 2010 to Present Training from Toastmasters International Microsoft 2007 Expert (Excel, Access, Word, Outlook and PowerPoint) September 2009 to Present Certification from Microsoft MicroStrategy Narrowcast Developer and Web Interface Designer August 2005 to Present Training from MicroStrategy MicroStrategy SQL Essentials and Engine Specialist Track June 2005 to Present Training from MicroStrategy Master Microsoft Office User Specialist (MOUS) 2000 February 2002 to Present Certification from Microsoft Microsoft 2000 Expert (Excel, Access, Word, Outlook and PowerPoint) February 2002 to Present Certification from Microsoft Master Microsoft Office User Specialist 1997 April 2001 to Present Certification from Microsoft Microsoft 1997 Expert (Excel, Access, Word, Outlook and PowerPoint) April 2001 to Present Certification from Microsoft Assessments  Spreadsheets with Microsoft Excel  Highly Proficient June 2019 Excel knowledge including common , PivotTables, conditional & nested formulas, and custom visuals. Full results: https://share.indeedassessments.com/share_assignment/wmq6e7p3iosi8c-u Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field. Additional Information  Additional Career, Contract and Project Experience     Childrens Hospital of Orange County CHOC, U.S. Healths, Kaiser Permanente, Proctor & Gamble, Book Publicist of Southern California, Toastmasters International, American Red Cross and Mensa.    Core Competencies   Excellent communication, leadership, time management and organizational .   Flexible to  effectively solo, as part of a team, or manage large or small groups.   Committed to quality process improvement methods, such as SMART, Six Sigma, PDCA, Demming, KPI Metrics, POC, modeling and others.   Certified in Microsoft Office Suite  Excel, Access, Outlook, Word and PowerPoint.   Savvy Social Media Campaigns, MailChimp, Google Business Analytics, Facebook Insights, GoTo Meeting, Zoom Administrator, and Webinars.   Knowledgeable about predictive models, statistical analysis, data warehouse and visualization: Business Objects, PowerBI, Qlik, Tableau, SalesForce, MicroStrategy and more    Instructor  Chapman University  University of Phoenix  Whittier College    Talents  Magic Castle Magician Member  Distinguished Toastmaster  Successful Inventor  Published Author  Host of FDTV  Webmaster    MAJOR ACCOMPLISHMENTS LAST 10 YEARS    One Fiscal Year    Generated more than $2.1M in revenue opportunity.   Saved more than $500,000 in operating expenses.   Saved more than 21,000 man-hours (137 associates in 18 departments).    High Level    Created 3 revenue generating reports totaling $2M for strategic accounts.   $930,000 annual opportunity  Health Systems B2G ACAP pilot.   $864,000 annual opportunity  IPBG Blue Box Opportunity Report.   $288,000 annual opportunity  Ingles Alt Size Sub Option B.   Saved more than 900 hours and more than $20,000 in operating expenses.   Streamlined 27 internal processes to be more efficient.   Automated 64 reports in Business Objects Reporting Tool.   Trained 14 associates across 6 departments.    Local and Enterprise-wide Awards and Achievements, 20072017  Individual 206 + Team 129 = Total 335    KEY  (Alpha Order)    ? AHP Track n Trend  ? AHP Unit Dose Initiative  ? Automate NTI-GPN Opt Outs  ? Automated Weekly Updates  ? B2G Safe ACAP  ? BOBJ and DAX Formulas  ? BOBJ Scheduler  ? Brand to Generic Projections  ? CCE Reports and Templates  ? Cost of Goods Savings  ? Cost of Shopping  ? Costco Inventory Management  ? CTCA Pricing Analysis  ? Customer Compliance Tool  ? Email Etiquette  ? Estimator Calculator  ? Excel Learning Lab  ? FDB-ABC-PRxO Equivalents  ? FTS and ACAP Statistics  ? GPIs for RFPs  ? Kaboom! Playground  ? Key Shortage Report  ? Large Bottle Savings  ? Margin Analysis Template  ? Profit Per Script  ? QRC and Training  ? Rebate Comparison Model  ? Rebate Value Proposition  ? Save Big Buy Big  ? Substitution Resolution  ? Total Value Scorecard   "
"Data and research scientist with Physics PhD and machine learning certificate. Self-motivated, detail oriented, quick learner, and a team player. Able to think out of the box when tackling new challenges.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Postdoctoral scholar
University of California Irvine - Irvine, CA September 2016 to July 2018
Repair/rebuilt instruments, designed experiments, data acquisition, signal processing, and image analysis.
Lecturer
University of California Irvine - Irvine, CA 2016 to 2018
Postdoctoral researcher
Physics Department, Harvard
September 2014 to September 2015
Designed and implemented experiments, and data analysis using Igor in a fast-paced environment.
Research Assistant
Physics Department, UMASS Amherst
September 2009 to May 2014
Developed an automated experimental setup, built and tested models, data mining using Matlab. 
20+ peer-reviewed publications, presentations, and conference papers(google scholar). 
Successful collaborations with 30+ scientists with chemistry and material science backgrounds.


Doctor of Philosophy in Physics
University of Massachusetts Amherst - Amherst, MA
May 2014
M. Sc. in Physics
University of Tehran - Tehran, IR May 2009
B. Sc. in Physics
Amirkabir University - Tehran, IR May 2006


Deep learning, Linear regression, Logistic regression, Machine learning, Random forest, Svm, Hadoop,
Html, Javascript, Python, Keras, Matplotlib, Numpy, Vba, Data manipulation, Mysql, Clustering, Hadoop,
Mongodb, Matlab
Links


Certifications/Licenses

Data Bootcamp, UCI
April 2019 to Present
Machine Learning, Coursera
July 2019 to Present
Additional Information

  
- Programing and Data Manipulation: Python(pandas, numpy, matplotlib, SQLAlchemy, seaborn,sklearn, keras), VBA, MATLAB, IGOR, familiar with R, Hadoop, Spark 
- Machine Learning Techniques: Linear Regression, Classification (Logistic Regression, Decision Tree, 
KNN, SVM), Random Forest, Clustering, and Deep learning. - Visualization: Tableau, HTML, Javascript, Leaflet, Geomap. - Databases: MySQL, mongoDB.",Data Scientist,resume,"Data and research scientist with Physics PhD and machine learning certificate. Self-motivated, detail oriented, quick learner, and a team player. Able to think out of the box when tackling new challenges. Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Postdoctoral scholar University of California Irvine - Irvine, CA September 2016 to July 2018 Repair/rebuilt instruments, designed experiments, data acquisition, signal processing, and image analysis. Lecturer University of California Irvine - Irvine, CA 2016 to 2018 Postdoctoral researcher Physics Department, Harvard September 2014 to September 2015 Designed and implemented experiments, and data analysis using Igor in a fast-paced environment. Research Assistant Physics Department, UMASS Amherst September 2009 to May 2014 Developed an automated experimental setup, built and tested models, data mining using Matlab.  20+ peer-reviewed publications, presentations, and conference papers(google scholar).  Successful collaborations with 30+ scientists with chemistry and material science backgrounds.   Doctor of Philosophy in Physics University of Massachusetts Amherst - Amherst, MA May 2014 M. Sc. in Physics University of Tehran - Tehran, IR May 2009 B. Sc. in Physics Amirkabir University - Tehran, IR May 2006   Deep learning, Linear regression, Logistic regression, Machine learning, Random forest, Svm, Hadoop, Html, Javascript, Python, Keras, Matplotlib, Numpy, Vba, Data manipulation, Mysql, Clustering, Hadoop, Mongodb, Matlab Links   Certifications/Licenses  Data Bootcamp, UCI April 2019 to Present Machine Learning, Coursera July 2019 to Present Additional Information     - Programing and Data Manipulation: Python(pandas, numpy, matplotlib, SQLAlchemy, seaborn,sklearn, keras), VBA, MATLAB, IGOR, familiar with R, Hadoop, Spark  - Machine Learning Techniques: Linear Regression, Classification (Logistic Regression, Decision Tree,  KNN, SVM), Random Forest, Clustering, and Deep learning. - Visualization: Tableau, HTML, Javascript, Leaflet, Geomap. - Databases: MySQL, mongoDB."
"Data Analyst Intern at V-ETS with 4 years of experience in IT. Skilled in Machine learning, python, data
 
science and database functioning. Committed to helping organizations advance by helping them to develop strategic plans based on predictive modeling and findings.
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Analyst Intern
V-ets - Washington, DC
June 2019 to Present
Project:  
 Building an application which converts audio conversations between medical practitioner andpatients into text.  
 Improve the accuracy of the conversion using machine learning techniques. 
 
: AWS Transcribe, AWS S3, EC2, AWS RDS, Speech to Text, Python, Project Management, NLP 
Senior System Engineer(Machine Learning domain)
Infosys Limited - Mysore, Karnataka
January 2015 to May 2018
Job Function Machine Learning, Data Analytics, Python, PLM domain 
 
Project Fraud risk prediction of online money transfer. 
 
Impact Model with 94% accuracy to save $ 1 million per annum. 
 Led a team of 4 and executed a project to predict money transfer frauds for a South Asian bankingfirm with a customer base over 10 million customers. 
 Aggregated data from Hadoop cluster and dumped into MySql database. Interfaced dB with Jupyternotebook, performed data transformation using complex SQL queries. 
 Visualizations using MATPLOTLIB and SEABORN libraries for effective client communication. 
 Used Pivot tables and lookup functions in MS Excel for weekly report preparation. 
 Applied Logistic regression, Random forest and Neural nets to predict fraud. 
 LASSO variable selection, exploratory data analysis to shortlist from 370 variables to a smallerintuitive set of 39 features. 
 Implemented Feature Selection (Chi -Square, p-Value for finding association among dependent,independent variables) 
 Data Transformation (Data Preprocessing- Handling missing values), Interactive Decision Tree.  Assisted product team in hosting a web based app. which was interfaced with the Statistical model built by our team. 
 
Project Duplicate Ads detection for one of India's largest online B2B marketplace 
 
Impact Reduced data storage cost, duplication rate abridged to 2.4% from 11%. 
 Data cleaning, stemming using NLTK Snowball to remove stop words/punctuations. 
 Computed TF-IDF similarities for text, hash similarities for images, nGrams Features for title anddescription, Fuzzywuzzy distances 
 Applied XGBoost (94% accuracy) on 587 final features with 1000 estimators, replaced NAN. 
 Applied Nnet (91% accuracy) with 3 hidden layers, 800 hidden units with 60% drop out. 
 Used ensemble of models to take a weighted average for final predictions of duplicate Ad's 
 
Project Enovia(Matrixone) upgrade to 3DEX 2017 
 
 End to End implementation: Analysis, Design, Development and Maintenance.  Performed unit testing, code review, QA, UAT and production support. 
 
Project Application Rehosting (Advitium) 
 Installation of application from scratch in HP data center (Windows 2008 R2) 
 Implemented knowledge of SMTP, LDAP, Active Directory, Ports, Load Balancer during datamigration.
System Engineer Trainee
Infosys - Mysore
July 2014 to December 2014
Learning: Java, Python, Data Mining, Data base(SQL), HTML & CSS as a part of generic training 


Master's in Data Science
University at Buffalo - Buffalo, NY August 2018 to Present
Bachelor's in Mechanical Engineering
College of Engineering Roorkee - Roorkee, Uttarakhand
August 2010 to July 2014


Data analysis, Sql server, Sql, Decision trees, Lda, Logistic regression, Pca, Svm, Bayesian, Hadoop,
Predictive analytics, C++, Hadoop, Python, Ggplot2, Keras, Matplotlib, Numpy, Pandas, Tensorflow,
Excel, Python (3 years), R (2 years), AWS (1 year), Tableau (1 year)
Links

Certifications/Licenses

Neural Nets and Deep Learning by deeplearning.ai on Coursera
June 2019 to Present
- Understand the major  trends driving Deep Learning 
- Be able to build, train and apply fully connected deep neural nets  
- Know how to implement efficient (vectorized) neural nets  
- Understand the key parameters in a neural net's architecture
Marketing Analytics in r statistical modeling
March 2019 to Present
Supervised Learning with scikit-learn
January 2019 to Present
Unsupervised Learning in Python
January 2019 to Present
Intermediate Python for Data Science
June 2018 to Present
Intro to Python for Data Science
June 2018 to Present
Google Analytics for Beginners March 2019 to March 2022
Improving Deep Neural Nets Hyperparameter tuning, Regularization and Optimization
July 2019 to Present
This course will teach you the ""magic"" of getting deep learning to  well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow.  
 
After 3 weeks, you will:  
- Understand industry best-practices for building deep learning applications.  
- Be able to effectively use the common neural net ""tricks"", including initialization, L2 and dropoutregularization, Batch normalization, gradient checking,  
- Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradientdescent, Momentum, RMSprop and Adam, and check for their convergence.  
- Understand new best-practices for the deep learning era of how to set up train/dev/test sets andanalyze bias/variance 
- Be able to implement a neural net in TensorFlow.  
 
This is the second course of the Deep Learning Specialization.
Structuring Machine Learning 
July 2019 to Present
You will learn how to build a successful machine learning project. If you aspire to be a  leader in AI, and know how to set direction for your team's , this course will show you how. 
 
Much of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two ""flight simulators"" that let you practice decision-making as a machine learning project leader. This provides ""industry experience"" that you might otherwise get only after years of ML  experience. 
 
After 2 weeks, you will:  
- Understand how to diagnose errors in a machine learning system, and  
- Be able to prioritize the most promising directions for reducing error 
- Understand complex ML settings, such as mismatched training/test sets, and comparing to and/orsurpassing human-level performance 
- Know how to apply end-to-end learning, transfer learning, and multi-task learning 
 
I've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time. 
 
This is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization.
Assessments

Data Analysis  Proficient
April 2019
Measures a candidate's skill in interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data.
Full results: https://share.indeedassessments.com/share_assignment/4n1d8najsxiyrffw
Problem Solving  Expert
April 2019
Analyzing relevant information when solving problems.
Full results: https://share.indeedassessments.com/share_assignment/aaf2omtxquk4l9jp
Critical Thinking  Expert
June 2019
Using logic to solve problems.
Full results: https://share.indeedassessments.com/share_assignment/y7nb5ttzrdb9nkxn
Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field.
Additional Information

ACADEMIC  : 
 
Project: Cervical cancer risk factors for classification (Kaggle). 
 
Description: Predict cancer risk based on features related to medical domain. 
  
Stages Data: Pre-processing (replacing NA/? values, normalizing, oversampling data), Data
Visualization using R, Classification Models (XGBoost (cross validation), Random forest, Confusion Matrix, Type II errors  
 
Results: Random forest (92.4% accuracy after oversampling) XGBoost (94.33% accuracy after oversampling, 100% specificity). 
 
Project: Kick-Starters (crowdfunding)  analysis (Kaggle). 
 
Description: Determine the most supported  and predict the success/ failure of future . 
 
Visualization Heat Map, World map, Word cloud, Box plots 
  
Models Logistic Regression, Gradient boosting, Random forest. 
 
  
Project: Petfinder.my Adoption Prediction(Kaggle). 
 
Description: Developing algorithms to predict adoptability of pets in Malaysia.  
 
Visualization: Bar plots, Distribution charts, Pie charts, Histogram, Word Cloud  
 
Models: CatBoost Classifier (70% accuracy without parameters tuning). ",Data Scientist,resume,"Data Analyst Intern at V-ETS with 4 years of experience in IT. Skilled in Machine learning, python, data   science and database functioning. Committed to helping organizations advance by helping them to develop strategic plans based on predictive modeling and findings. Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Data Analyst Intern V-ets - Washington, DC June 2019 to Present Project:    Building an application which converts audio conversations between medical practitioner andpatients into text.    Improve the accuracy of the conversion using machine learning techniques.    : AWS Transcribe, AWS S3, EC2, AWS RDS, Speech to Text, Python, Project Management, NLP  Senior System Engineer(Machine Learning domain) Infosys Limited - Mysore, Karnataka January 2015 to May 2018 Job Function Machine Learning, Data Analytics, Python, PLM domain    Project Fraud risk prediction of online money transfer.    Impact Model with 94% accuracy to save $ 1 million per annum.   Led a team of 4 and executed a project to predict money transfer frauds for a South Asian bankingfirm with a customer base over 10 million customers.   Aggregated data from Hadoop cluster and dumped into MySql database. Interfaced dB with Jupyternotebook, performed data transformation using complex SQL queries.   Visualizations using MATPLOTLIB and SEABORN libraries for effective client communication.   Used Pivot tables and lookup functions in MS Excel for weekly report preparation.   Applied Logistic regression, Random forest and Neural nets to predict fraud.   LASSO variable selection, exploratory data analysis to shortlist from 370 variables to a smallerintuitive set of 39 features.   Implemented Feature Selection (Chi -Square, p-Value for finding association among dependent,independent variables)   Data Transformation (Data Preprocessing- Handling missing values), Interactive Decision Tree.  Assisted product team in hosting a web based app. which was interfaced with the Statistical model built by our team.    Project Duplicate Ads detection for one of India's largest online B2B marketplace    Impact Reduced data storage cost, duplication rate abridged to 2.4% from 11%.   Data cleaning, stemming using NLTK Snowball to remove stop words/punctuations.   Computed TF-IDF similarities for text, hash similarities for images, nGrams Features for title anddescription, Fuzzywuzzy distances   Applied XGBoost (94% accuracy) on 587 final features with 1000 estimators, replaced NAN.   Applied Nnet (91% accuracy) with 3 hidden layers, 800 hidden units with 60% drop out.   Used ensemble of models to take a weighted average for final predictions of duplicate Ad's    Project Enovia(Matrixone) upgrade to 3DEX 2017     End to End implementation: Analysis, Design, Development and Maintenance.  Performed unit testing, code review, QA, UAT and production support.    Project Application Rehosting (Advitium)   Installation of application from scratch in HP data center (Windows 2008 R2)   Implemented knowledge of SMTP, LDAP, Active Directory, Ports, Load Balancer during datamigration. System Engineer Trainee Infosys - Mysore July 2014 to December 2014 Learning: Java, Python, Data Mining, Data base(SQL), HTML & CSS as a part of generic training    Master's in Data Science University at Buffalo - Buffalo, NY August 2018 to Present Bachelor's in Mechanical Engineering College of Engineering Roorkee - Roorkee, Uttarakhand August 2010 to July 2014   Data analysis, Sql server, Sql, Decision trees, Lda, Logistic regression, Pca, Svm, Bayesian, Hadoop, Predictive analytics, C++, Hadoop, Python, Ggplot2, Keras, Matplotlib, Numpy, Pandas, Tensorflow, Excel, Python (3 years), R (2 years), AWS (1 year), Tableau (1 year) Links  Certifications/Licenses  Neural Nets and Deep Learning by deeplearning.ai on Coursera June 2019 to Present - Understand the major  trends driving Deep Learning  - Be able to build, train and apply fully connected deep neural nets   - Know how to implement efficient (vectorized) neural nets   - Understand the key parameters in a neural net's architecture Marketing Analytics in r statistical modeling March 2019 to Present Supervised Learning with scikit-learn January 2019 to Present Unsupervised Learning in Python January 2019 to Present Intermediate Python for Data Science June 2018 to Present Intro to Python for Data Science June 2018 to Present Google Analytics for Beginners March 2019 to March 2022 Improving Deep Neural Nets Hyperparameter tuning, Regularization and Optimization July 2019 to Present This course will teach you the ""magic"" of getting deep learning to  well. Rather than the deep learning process being a black box, you will understand what drives performance, and be able to more systematically get good results. You will also learn TensorFlow.     After 3 weeks, you will:   - Understand industry best-practices for building deep learning applications.   - Be able to effectively use the common neural net ""tricks"", including initialization, L2 and dropoutregularization, Batch normalization, gradient checking,   - Be able to implement and apply a variety of optimization algorithms, such as mini-batch gradientdescent, Momentum, RMSprop and Adam, and check for their convergence.   - Understand new best-practices for the deep learning era of how to set up train/dev/test sets andanalyze bias/variance  - Be able to implement a neural net in TensorFlow.     This is the second course of the Deep Learning Specialization. Structuring Machine Learning  July 2019 to Present You will learn how to build a successful machine learning project. If you aspire to be a  leader in AI, and know how to set direction for your team's , this course will show you how.    Much of this content has never been taught elsewhere, and is drawn from my experience building and shipping many deep learning products. This course also has two ""flight simulators"" that let you practice decision-making as a machine learning project leader. This provides ""industry experience"" that you might otherwise get only after years of ML  experience.    After 2 weeks, you will:   - Understand how to diagnose errors in a machine learning system, and   - Be able to prioritize the most promising directions for reducing error  - Understand complex ML settings, such as mismatched training/test sets, and comparing to and/orsurpassing human-level performance  - Know how to apply end-to-end learning, transfer learning, and multi-task learning    I've seen teams waste months or years through not understanding the principles taught in this course. I hope this two week course will save you months of time.    This is a standalone course, and you can take this so long as you have basic machine learning knowledge. This is the third course in the Deep Learning Specialization. Assessments  Data Analysis  Proficient April 2019 Measures a candidate's skill in interpreting and producing graphs, identifying trends, and drawing justifiable conclusions from data. Full results: https://share.indeedassessments.com/share_assignment/4n1d8najsxiyrffw Problem Solving  Expert April 2019 Analyzing relevant information when solving problems. Full results: https://share.indeedassessments.com/share_assignment/aaf2omtxquk4l9jp Critical Thinking  Expert June 2019 Using logic to solve problems. Full results: https://share.indeedassessments.com/share_assignment/y7nb5ttzrdb9nkxn Indeed Assessments provides  tests that are not indicative of a license or certification, or continued development in any  field. Additional Information  ACADEMIC  :    Project: Cervical cancer risk factors for classification (Kaggle).    Description: Predict cancer risk based on features related to medical domain.     Stages Data: Pre-processing (replacing NA/? values, normalizing, oversampling data), Data Visualization using R, Classification Models (XGBoost (cross validation), Random forest, Confusion Matrix, Type II errors     Results: Random forest (92.4% accuracy after oversampling) XGBoost (94.33% accuracy after oversampling, 100% specificity).    Project: Kick-Starters (crowdfunding)  analysis (Kaggle).    Description: Determine the most supported  and predict the success/ failure of future .    Visualization Heat Map, World map, Word cloud, Box plots     Models Logistic Regression, Gradient boosting, Random forest.       Project: Petfinder.my Adoption Prediction(Kaggle).    Description: Developing algorithms to predict adoptability of pets in Malaysia.     Visualization: Bar plots, Distribution charts, Pie charts, Histogram, Word Cloud     Models: CatBoost Classifier (70% accuracy without parameters tuning). "
" of : 
Positive and self-motivated Trilingual Russian, Japanese and English Experienced Licensed Massage Therapist, specializing in Chines Medical massage Tui Na, Swedish, Deep tissue, Hot Stones, Sport massage, Face lifting/acupressure, Chair, Prenatal, Cupping, Aromatherapy, Chakra Balance, Reiki looking to  at medical office 
as a massage therapist/chiropractor or acupuncture assistant or at spa. I have an experience at Coconuts, Sugar, Chocolate scrubs. I have management , teaching and translating/interpretations experience as well.
Authorized to  in the US for any employer
 Experience

Massage Therapist
Ichiban Massage Therapy - San Jacinto, CA
January 2018 to Present
Massages, scrubs.
Manager
Gprservice incorporated - Yokohama, Japan
October 2000 to Present
Developed export automotive products strategy and plan. Developed import products such Chaga
(health row material), strategy and plan. Developed import export and import wine strategy and plan. Interacted with Japanese, Russian and American customers and dealers through fax, skype,verbal and written communications to provide the information about the export and import products. Developed system implementations of planning, location sensitivity and vendor managed inventory, transportation planning and warehousing.Finding clients in Russia, Japan, USA, New Zealand and other countries. Goods descriptions, preparing contracts. Price negotiations. Select and interview job applicants. Maintaining good ing relationships with clients, vendors and government agency representatives. Attending trade shows.
Massage Therapist
Dr Wu Chiropractor - Chino, CA
January 2019 to September 2019
High quality deep tissue massages, pregnancy massages.
Massage Therapist
Dr Bark Acupuncture office - Fontana, CA
June 2019 to August 2019
30min,1hr massages. Deep tissue, Chinese medical massage.
Massage Therapist
Green spa - Chino Hills, CA
January 2019 to July 2019
Massage Therapy, pregnancy, scrubs, couple massages.
Massage Therapist
Southern California health clinic - Tarzana, CA June 2017 to December 2018
Massage Therapy.
Massage Therapist
Amore Day Spa - Temecula, CA
January 2018 to March 2018
Massages, scrubs.
Nursing Assistant
Sakura intermediated care - Los Angeles, CA
January 2017 to March 2017
Provides personal care, rehabilitative and supportive services. 
Assists in activities of daily living (personal hygiene, eating, dressing, ambulation). Supports the patient and provides care that encourages independence. Assists with medications. Vital signs.


Bachelor's in Doctor of acupuncture and oriental medicine
Pacific College of Oriental Medicine-San Diego - San Diego, CA
January 2018 to Present
CNA/EKG monitor technician in Nurse assistant/EKG monitor technician
St Jude Nursing school - Van Nuys, CA May 2015 to September 2015
Bachelor's in Management
Khabarovsk Economy university - Khabarovsk
September 1988 to May 1992


Deep Tissue, Marketing, Receptionist, Training, Swedish Massage, Microsoft (10+ years), Russian language (10+ years), Japanese language (10+ years), Management (10+ years), Inventory, Time Management, Fast learner, CPR, Outlook, Microsoft Word, Microsoft Excel, TuiNa Therapeutic massage
(5 years), Microsoft Office, Chinese medical massage Tui Na, Swedish massage, Pregnancy massages
Certifications/Licenses

Licensed Massage Therapist
CPR
Certified Nursing Assistant (CNA)
Massage Therapist",Data Scientist,resume," of :  Positive and self-motivated Trilingual Russian, Japanese and English Experienced Licensed Massage Therapist, specializing in Chines Medical massage Tui Na, Swedish, Deep tissue, Hot Stones, Sport massage, Face lifting/acupressure, Chair, Prenatal, Cupping, Aromatherapy, Chakra Balance, Reiki looking to  at medical office  as a massage therapist/chiropractor or acupuncture assistant or at spa. I have an experience at Coconuts, Sugar, Chocolate scrubs. I have management , teaching and translating/interpretations experience as well. Authorized to  in the US for any employer  Experience  Massage Therapist Ichiban Massage Therapy - San Jacinto, CA January 2018 to Present Massages, scrubs. Manager Gprservice incorporated - Yokohama, Japan October 2000 to Present Developed export automotive products strategy and plan. Developed import products such Chaga (health row material), strategy and plan. Developed import export and import wine strategy and plan. Interacted with Japanese, Russian and American customers and dealers through fax, skype,verbal and written communications to provide the information about the export and import products. Developed system implementations of planning, location sensitivity and vendor managed inventory, transportation planning and warehousing.Finding clients in Russia, Japan, USA, New Zealand and other countries. Goods descriptions, preparing contracts. Price negotiations. Select and interview job applicants. Maintaining good ing relationships with clients, vendors and government agency representatives. Attending trade shows. Massage Therapist Dr Wu Chiropractor - Chino, CA January 2019 to September 2019 High quality deep tissue massages, pregnancy massages. Massage Therapist Dr Bark Acupuncture office - Fontana, CA June 2019 to August 2019 30min,1hr massages. Deep tissue, Chinese medical massage. Massage Therapist Green spa - Chino Hills, CA January 2019 to July 2019 Massage Therapy, pregnancy, scrubs, couple massages. Massage Therapist Southern California health clinic - Tarzana, CA June 2017 to December 2018 Massage Therapy. Massage Therapist Amore Day Spa - Temecula, CA January 2018 to March 2018 Massages, scrubs. Nursing Assistant Sakura intermediated care - Los Angeles, CA January 2017 to March 2017 Provides personal care, rehabilitative and supportive services.  Assists in activities of daily living (personal hygiene, eating, dressing, ambulation). Supports the patient and provides care that encourages independence. Assists with medications. Vital signs.   Bachelor's in Doctor of acupuncture and oriental medicine Pacific College of Oriental Medicine-San Diego - San Diego, CA January 2018 to Present CNA/EKG monitor technician in Nurse assistant/EKG monitor technician St Jude Nursing school - Van Nuys, CA May 2015 to September 2015 Bachelor's in Management Khabarovsk Economy university - Khabarovsk September 1988 to May 1992   Deep Tissue, Marketing, Receptionist, Training, Swedish Massage, Microsoft (10+ years), Russian language (10+ years), Japanese language (10+ years), Management (10+ years), Inventory, Time Management, Fast learner, CPR, Outlook, Microsoft Word, Microsoft Excel, TuiNa Therapeutic massage (5 years), Microsoft Office, Chinese medical massage Tui Na, Swedish massage, Pregnancy massages Certifications/Licenses  Licensed Massage Therapist CPR Certified Nursing Assistant (CNA) Massage Therapist"
"* A data scientist  with 7 years of progressive experience in Data Analytics, StatisticalModeling, Visualization and Machine Learning. Excellent capability in collaboration, quick learning and adaptation. 
* Experience in Data mining with large datasets of Structured and Unstructured data, Data Acquisition,
Data Validation, Predictive modeling, Data Visualization. 
* Experience in integrating data, profiling, validating and data cleansing transformation and datavisualization using R and Python. 
* Theoretical foundations and practical hands-on  related to (i) supervised learning (linearand logistic regression, boosted decision trees, Support Vector Machines, neural nets, NLP), (ii) unsupervised learning (clustering, dimensionality reduction, recommender systems), (iii) probability & statistics, experiment analysis, confidence intervals, A/B testing, (iv) algorithms and data structures. 
* Extensive knowledge on Azure Data Lake and Azure Storage. 
* Experience in migration from heterogeneous sources including Oracle to MS SQL Server. 
* Hands on experience in design, management and visualization of databases using Oracle, MySQL andSQL Server. 
* In depth knowledge and hands on experience of Big Data / Hadoop ecosystem (MapReduce, HDFS,Hive, Pig and Sqoop). 
* Experience in Apache Spark, Kafka for Big Data Processing & Scala Functional programming. * Experience in manipulating the large data sets with R packages like tidyr, tidyverse, dplyr reshape, lubridate, Caret and visualizing the data using lattice and ggplot2 packages. * Experience in dimensionality reduction using techniques like PCA and LDA. 
* Intensive hands-on Boot camp on Data Analytics course spanning from Statistics to Programmingincluding data engineering, data visualization, machine learning and programming in R, SQL. 
* Experience in data analytics, predictive analysis like Classification, Regression, RecommenderSystems. 
* Good Exposure with Factor Analysis, Bagging and Boosting algorithms. 
* Experience in Descriptive Analysis Problems like Frequent Pattern Mining, Clustering, OutlierDetection. 
* ed on Machine Learning algorithms like Classification and Regression with KNN Model, Decision
Tree Model, Naïve Bayes Model, Logistic Regression, SVM Model and Latent Factor Model. 
* Hands-on experience on Python and libraries like Numpy, Pandas, Matplotlib, Seaborn, NLTK, Sci-Kitlearn, SciPy. 
* Expertise and knowledge in TensorFlow to do machine learning/deep learning package in python. 
* Good knowledge on Microsoft Azure SQL, Machine Learning and HDInsight. 
* Good Exposure on SAS analytics. 
* Good Exposure in deep learning with Tensor flow in python. 
* Good Knowledge on Natural Language Processing (NLP) and Time Series Analysis and Forecastingusing ARIMA model in Python and R. 
* Good knowledge in Tableau, Power BI for interactive data visualizations. 
* In-depth Understanding in NoSQL databases like MongoDB, HBase. 
* Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includesservices like EC2, S3, and EMR. 
* Experience and Knowledge in developing software using Java, C++ (Data Structures and Algorithms). 
* Good exposure in creating pivot tables and charts in Excel. 
* Experience in developing Custom Report and different types of Tabular Reports, Matrix Reports, Adhoc reports and distributed reports in multiple formats using SQL Server Reporting Services (SSRS). 
* Excellent Database administration (DBA)  including user authorizations, Database creation,Tables, indexes and backup creation. 
 
 SECTION 
 
Languages Java 8, Python, R Packages 
ggplot2, caret, dplyr, Rweka, gmodels, RCurl, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numPy, Seaborn, sciPy, matplot lib, sci-kit-learn, Beautiful Soup, Rpy2. 
 
Web  JDBC, HTML5, DHTML and XML, CSS3, Web Services, WSDL 
Data Modelling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer 
Big Data  Hadoop, Hive, HDFS, MapReduce, Pig, Kafka 
Databases SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS,
HBase, Teradata, Netezza, MongoDB, Cassandra. 
Reporting  
MS Office (Word/Excel/Power Point/ Visio), Tableau, Crystal reports XI, Business Intelligence, SSRS, Business Objects 5.x/ 6.x, Cognos7.0/6.0. 
 
ETL  Informatica Power Centre, SSIS. 
Version Control  SVM, GitHub 
Project Execution Methodologies 
Ralph Kimball and Bill Inmon data warehousing methodology, Rational Unified Process (RUP), Rapid Application Development (RAD), Joint Application Development (JAD). 
 
BI  
Tableau, Tableau Server, Tableau Reader, SAP Business Objects, OBIEE, QlikView, SAP Business Intelligence, Amazon Redshift, or Azure Data Warehouse 
 
Operating System Windows, Linux, Unix, Macintosh HD, Red Hat
 Experience

Data Scientist
AT & T INC - Plano, TX
January 2019 to Present
This Project is developed for Roaming Command Center to minimize cost what we pay to different service providers when AT & T customers roaming to their Carriers by using Optimization techniques and Machine Learning models. 
Responsibilities: - 
  on all aspects of creating predictive models including collecting requirements, establishanalytics  plan, data exploration, cleansing, and preparation, identifying features, selecting machine learning algorithms, building and testing models, iteratively improving solutions. 
 Build multiple time series forecasting models using tsfresh, Prophet, ARIMA time series models topredict future usage of voice call, mobile data and SMS for different countries using forecasting and regression methods in machine learning. 
 Use Feature engineering to and create new time series features like rolling means, Date Timefeatures, Lag features to improve forecasting Model. 
 Evaluate Forecasting Model accuracy by Using RMSE and MAPE to check error rate between actualand predicted values. 
 Build Forecasting model to predict future usage of units by daily, weekly and monthly and used
Jenkins jobs for periodically run python script and store output in database. 
 Integrated Forecasting model with MongoDB using PyMongo to get data for model and storepredictions result from model. 
 Build Forecasting model pipeline to automate process. 
 Used Matplotlib library to plot graph for all countries and understand trend of usage of data andenhance model accuracy. 
 Build big event feature and holidays feature for model to improve model accuracy. 
 Used Rolling Window method to build forecasting model. 
 Perform Linear programing methods and models, Genetic Algorithm for optimization models to findoptimal solution by creating different constraints and  functions for cost minimization.  Extracted the data from hive tables by writing efficient Hive queries using PySpark. 
 Developed Production Document which includes data pipeline, database schema, Jenkins Jobspipeline and API application properties. 
 Follow Agile Craft for project development and to track project performance 
Environment: HDFS, Hive, Pyspark, Python, tsfresh, Prophet, ARIMA, Gradient Boosting Model, Random forest regressor, Linear programing, Optimization, Genetic algorithm, Agile.
Data Scientist
Capital Group - Los Angeles, CA March 2018 to December 2018
This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims. 
 
Claims severity prediction in real-time 
Built classification models to predict the fraudulent claims by severity in real-time reducing the time for execution from 6 hours to 4 seconds. Implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products. Forwarded the high-risk claims for further investigation. 
 
Text analytics for fraud prediction 
Executed topic modeling for finding different topics based on the notes made corresponding to the claimant's claim. Attributed the resulting topics to classify into fraud and not fraud categories. 
 
Risk assessment prediction 
Incorporated models built in Python and R into the business processes using clustering techniques to assess the risk involved with a customer. The models built are used in assessing the premiums amount required to be paid by the customer. 
 
Customer churn/attrition prediction 
Developed models that predict whether a customer's propensity to churn leveraging the information related to insurance policies, demographics, claims, related to the customer, payment frequency, home ownership status, household tenure etc. 
 
Responsibilities: 
* Implemented Data Exploration to analyze patterns and to select features using Python SciPy. * Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups. 
* Built predictive models including Support Vector Machine, Random Forests and Naïve Bayes Classifierusing Python Scikit-Learn to predict the personalized product choice for each client. 
* Using R's dplyr and ggplot2 packages performed an extensive graphical visualization of overall data,including customized graphical representation of revenue reports, specific item sales statistics and visualization. 
* Designed and implemented cross-validation and statistical tests including Hypothetical Testing,
ANOVA, Auto-correlation to verify the models' significance. 
* Designed an A/B experiment for testing the business performance of the new recommendationsystem. 
* Supported MapReduce Programs running on the cluster. 
* Evaluated business requirements and prepared detailed specifications that follow project guidelinesrequired to develop written programs. 
* Configured Hadoop cluster with Namenode and slaves and formatted HDFS. 
* Used Oozie flow engine to run multiple Hive and Pig jobs. 
* Participated in Data Acquisition with Data Engineer team to extract historical and real-time data byusing Hadoop MapReduce and HDFS. 
* Performed Data Enrichment jobs to deal missing value, to normalize data, and to select features byusing HiveQL. 
* Developed multiple MapReduce jobs in java for data cleaning and pre-processing. 
* Analyzed the partitioned and bucketed data and compute various metrics for reporting. 
* Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume. 
* ed on loading the data from MySQL to HBase where necessary using Sqoop. 
* Developed Hive queries for Analysis across different banners. 
* Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data anduploaded to database. 
* Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuringlaunched instances with respect to specific applications. 
* Developed Hive queries for analysis, and exported the result set from Hive to MySQL using Sqoopafter processing the data. 
* Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior. 
* Created HBase tables to store various data formats of data coming from different portfolios. 
* ed on improving performance of existing Pig and Hive Queries. 
* Created reports and dashboards, by using D3.js and Tableau 9.x, to explain and communicate datainsights, significant features, models scores and performance of new recommendation system to both technical and business teams. 
* Utilize SQL, Excel and several Marketing/Web Analytics  (Google Analytics, AdWords) in order tocomplete business & marketing analysis and assessment. 
* Used Git 2.x for version control with Data Engineer team and Data Scientists colleagues. * Used Agile methodology and SCRUM process for project developing. 
 
Environment: HDFS, Hive, Scoop, Pig, Oozie, Amazon Web Services (AWS), Python 3.x (SciPy, ScikitLearn), Tableau 9.x, D3.js, SVM, Random Forests, Naïve Bayes Classifier, A/B experiment, Git 2.x, Agile/ SCRUM.
Sr. Data Scientist
PCCI Group - Dallas, TX
November 2016 to February 2018
PCCI provides services to healthcare industry which is characterized by its dynamism and the diversity of players by providing customized BPO solutions. Our services are designed to help healthcare insurers; medical device manufacturers, pharmaceutical companies and healthcare providers acquire new market shares and effectively support their current members. 
With a comprehensive understanding of your business, PCCI acts as a key point between your products/services and your members, delivering them a superior customer experience at every opportunity. 
We provide sales and customer service solutions. More specifically, our solutions can be customized to your organization's specific requirements, including: 
* Sales through programs enrollments 
* Customer service (Claim inquiries about billing & account) 
* Helpline services 
* Awareness campaigns 
* Appointment scheduling for labs, doctor offices, clinics visits 
* Data management 
* Technical support 
* Market research 
 
Responsibilities 
* Perform Data Profiling to learn about behavior with various features such as traffic pattern, location,Date and Time etc. 
* Extracted the data from hive tables by writing efficient Hive queries. 
* Performed preliminary data analysis using descriptive statistics and handled anomalies such asremoving duplicates and imputing missing values. 
* Analyze Data and Performed Data Preparation by applying historical model on the data set in
AZUREML. 
* Application of various machine learning algorithms and statistical modeling like decision trees, textanalytics, natural language processing (NLP), supervised and unsupervised, regression models, social net analysis, neural nets, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab. 
* Exploring DAG's, their dependencies and logs using Airflow pipelines for automation. 
* Performed data cleaning and feature selection using MLlib package in PySpark and ing with deeplearning frames such as TensorFlow, Keras etc. 
* Conducted a hybrid of Hierarchical and K-means Cluster Analysis using IBM SPSS and identifiedmeaningful segments of customers through a discovery approach. 
* Develop Spark/Scala, Python, R for regular expression (regex) project in the Hadoop/Hiveenvironment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data. 
* Evaluate models using Cross Validation, Log loss function, ROC curves and used AUC for featureselection and elastic  like Elastic Search, Kibana etc. 
*  with NLTK library to NLP data processing and finding the patterns. 
* Categorize comments into positive and negative clusters from different social neting sites using
Sentiment Analysis and Text Analytics. 
* Analyze traffic patterns by calculating autocorrelation with different time lags. 
* Ensure that the model has low False Positive Rate and Text classification and sentiment analysis forunstructured and semi-structured data. 
* Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1. 
* Use Principal Component Analysis in feature engineering to analyze high dimensional data. * Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior. 
* Perform Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route. 
* Implemented different models like Logistic Regression, Random Forest and Gradient-Boost Trees topredict whether a given die will pass or fail the test. 
* Perform data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation. 
* Use MLlib, Spark's Machine learning library to build and evaluate different models. 
* Perform Data Cleaning, features scaling, features engineering using pandas and numpy packages inpython. 
* Develop MapReduce pipeline for feature extraction using Hive and Pig. 
* Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Create various types of data visualizations using Python and Tableau. 
* Communicate the results with operations team for taking best decisions. 
* Collect data needs and requirements by Interacting with the other departments. 
 
Environment: Python 2.x, R, HDFS, Hadoop 2.3, Hive, Linux, Spark, IBM SPSS, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark.
Data Scientist
Cummins - Denver, CO
April 2015 to August 2016
SLM Corporation is a publicly traded U.S. corporation that provides consumer banking & is a student loan company with over 40 years of providing student loans for college, supporting graduate and undergraduate study, and more. The primary goal of the Customer Administration effort is to provide a single point of management for all customer data. 
 
Responsibilities: 
* Developing Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data
Management (MDM) Architecture involving OLTP, ODS and OLAP. 
* Providing source to target mappings to the ETL team to perform initial, full, and incremental loadsinto the target data mart. 
* Conducting JAD sessions, writing meeting minutes, collecting requirements from business users andanalyze based on the requirements. 
* Involved in defining the source to target data mappings, business rules, and data definitions. 
* Transformation on the files received from clients and consumed by Sql Server. 
* ing closely with the ETL, SSIS, SSRS Developers to explain the complex Data Transformationusing Logic. 
* ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005.
 
* Performing Data Profiling, Cleansing, Integration and extraction  
* Defining the list codes and code conversions between the source systems and the data mart using
Reference Data Management (RDM). 
* Applying data cleansing/data scrubbing techniques to ensure consistency amongst data sets. * Extensively using ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW. 
 
Environment: MS Excel, Agile, Oracle 11g, Sql Server, SOA, SSIS, SSRS, ETL, UNIX, T-SQL, HP Quality Center 11, RDM (Reference Data Management).
Data Scientist
John Wiley & Sons - Hoboken, NJ
January 2014 to March 2015
Wiley's Scientific, Technical, Medical, and Scholarly business, also known as Wiley-Blackwell, serves the world's research and scholarly communities, and is the largest publisher for  and scholarly societies. Wiley has been actively increasing its presence in the Higher Education Department by partnering with 3rd party Learning Management Systems (LMS). The project involved integration of Wiley PLUS E4 LMS with Blackboard LMS. 
Responsibilities: 
* Involved in complete Software Development Life Cycle (SDLC) process by analyzing businessrequirements and understanding the functional  flow of information from source systems to destination systems. 
* A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping,
Machine Learning, Python programming, SQL, Unix Commands, NoSQL, Hadoop. 
* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing variousmachine learning algorithms. 
* ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
* Analyzed sentimental data and detecting trend in customer usage and other services. 
* Analyzed and Prepared data, identify the patterns on dataset by applying historical models. 
* Collaborated with Senior Data Scientists for understanding of data. 
* Used Python and R scripting by implementing machine algorithms to predict the data and forecastthe data for better results. 
* Used Python and R scripting to visualize the data and implemented machine learning algorithms. 
* Experience in developing packages in R with a shiny interface. 
* Used predictive analysis to create models of customer behavior that are correlated positively withhistorical data and use these models to forecast future results. 
* Predicted user preference based on segmentation using General Additive Models, combined withfeature clustering, to understand non-linear patterns between user segmentation and related monthly platform usage features (time series data). 
* Perform data manipulation, data preparation, normalization, and predictive modeling. 
* Improve efficiency and accuracy by evaluating model in Python and R. 
* Used Python and R script for improvement of model. 
* Application of various machine learning algorithms and statistical modeling like Decision Trees,Random Forest, Regression Models, neural nets, SVM, clustering to identify Volume using scikitlearn package 
* Performed Data cleaning process applied Backward - Forward filling methods on dataset for handlingmissing values. 
* Developed a predictive model and validate Neural Net Classification model for predict thefeature label. 
* Performed Boosting method on predicted model for the improve efficiency of the model. 
* Presented Dashboards to Higher Management for more Insights using Power BI and Tableau. 
* Hands on experience in using HIVE, Hadoop, HDFS and Bigdata related topics. 
 
Environment: R/R studio, Python, Tableau, Hadoop, Hive, MS SQL Server, MS Access, MS Excel, Outlook, Power BI.
Data Reporting Analyst
C3-Solutions - Bengaluru, Karnataka
October 2012 to April 2013
* Designed and implemented an internal reporting tool named I-CUBE using Python to automate salesand financial operational data accessible through a built-in SharePoint for leaders globally. Used API for
I-Cube to extract sales data on an hourly-basis. 
* Built and customized interactive reports on forecasts, targets and actuals data using BI/ETL such as SAS, SSAS, SSIS in the CRM which slashed manual efforts by 8%. 
* Conducted operational analyses for business worth $3M ing through all phases such asrequirements gathering, developing use cases, data mapping and creating flow diagrams. * Accomplished data cleansing and analysis results using Excel pivot tables, VLOOKUPs, data validation, graphs and chart manipulation in Excel. 
* Designed complex SQL queries, Views, Stored Procedures, Functions and Triggers to handle databasemanipulation and performance. 
* Used SQL, PLSQL scripts for automating repeatable tasks of customer feedback survey datacollection and distribution which increased the departmental efficiency by 8%.
Education

M.S in Data Mining and Predictive Analytics. in Data Mining and Predictive Analytics
St. John's University - Queens, NY
B.E in Computer Science & Engineering. in Computer Science & Engineering
Gujarat Technological University - Gujarat, IN",Data Scientist,resume,"* A data scientist  with 7 years of progressive experience in Data Analytics, StatisticalModeling, Visualization and Machine Learning. Excellent capability in collaboration, quick learning and adaptation.  * Experience in Data mining with large datasets of Structured and Unstructured data, Data Acquisition, Data Validation, Predictive modeling, Data Visualization.  * Experience in integrating data, profiling, validating and data cleansing transformation and datavisualization using R and Python.  * Theoretical foundations and practical hands-on  related to (i) supervised learning (linearand logistic regression, boosted decision trees, Support Vector Machines, neural nets, NLP), (ii) unsupervised learning (clustering, dimensionality reduction, recommender systems), (iii) probability & statistics, experiment analysis, confidence intervals, A/B testing, (iv) algorithms and data structures.  * Extensive knowledge on Azure Data Lake and Azure Storage.  * Experience in migration from heterogeneous sources including Oracle to MS SQL Server.  * Hands on experience in design, management and visualization of databases using Oracle, MySQL andSQL Server.  * In depth knowledge and hands on experience of Big Data / Hadoop ecosystem (MapReduce, HDFS,Hive, Pig and Sqoop).  * Experience in Apache Spark, Kafka for Big Data Processing & Scala Functional programming. * Experience in manipulating the large data sets with R packages like tidyr, tidyverse, dplyr reshape, lubridate, Caret and visualizing the data using lattice and ggplot2 packages. * Experience in dimensionality reduction using techniques like PCA and LDA.  * Intensive hands-on Boot camp on Data Analytics course spanning from Statistics to Programmingincluding data engineering, data visualization, machine learning and programming in R, SQL.  * Experience in data analytics, predictive analysis like Classification, Regression, RecommenderSystems.  * Good Exposure with Factor Analysis, Bagging and Boosting algorithms.  * Experience in Descriptive Analysis Problems like Frequent Pattern Mining, Clustering, OutlierDetection.  * ed on Machine Learning algorithms like Classification and Regression with KNN Model, Decision Tree Model, Naïve Bayes Model, Logistic Regression, SVM Model and Latent Factor Model.  * Hands-on experience on Python and libraries like Numpy, Pandas, Matplotlib, Seaborn, NLTK, Sci-Kitlearn, SciPy.  * Expertise and knowledge in TensorFlow to do machine learning/deep learning package in python.  * Good knowledge on Microsoft Azure SQL, Machine Learning and HDInsight.  * Good Exposure on SAS analytics.  * Good Exposure in deep learning with Tensor flow in python.  * Good Knowledge on Natural Language Processing (NLP) and Time Series Analysis and Forecastingusing ARIMA model in Python and R.  * Good knowledge in Tableau, Power BI for interactive data visualizations.  * In-depth Understanding in NoSQL databases like MongoDB, HBase.  * Very good experience and knowledge in provisioning virtual clusters under AWS cloud which includesservices like EC2, S3, and EMR.  * Experience and Knowledge in developing software using Java, C++ (Data Structures and Algorithms).  * Good exposure in creating pivot tables and charts in Excel.  * Experience in developing Custom Report and different types of Tabular Reports, Matrix Reports, Adhoc reports and distributed reports in multiple formats using SQL Server Reporting Services (SSRS).  * Excellent Database administration (DBA)  including user authorizations, Database creation,Tables, indexes and backup creation.     SECTION    Languages Java 8, Python, R Packages  ggplot2, caret, dplyr, Rweka, gmodels, RCurl, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numPy, Seaborn, sciPy, matplot lib, sci-kit-learn, Beautiful Soup, Rpy2.    Web  JDBC, HTML5, DHTML and XML, CSS3, Web Services, WSDL  Data Modelling  Erwin r 9.6, 9.5, 9.1, 8.x, Rational Rose, ER/Studio, MS Visio, SAP Power designer  Big Data  Hadoop, Hive, HDFS, MapReduce, Pig, Kafka  Databases SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase, Teradata, Netezza, MongoDB, Cassandra.  Reporting   MS Office (Word/Excel/Power Point/ Visio), Tableau, Crystal reports XI, Business Intelligence, SSRS, Business Objects 5.x/ 6.x, Cognos7.0/6.0.    ETL  Informatica Power Centre, SSIS.  Version Control  SVM, GitHub  Project Execution Methodologies  Ralph Kimball and Bill Inmon data warehousing methodology, Rational Unified Process (RUP), Rapid Application Development (RAD), Joint Application Development (JAD).    BI   Tableau, Tableau Server, Tableau Reader, SAP Business Objects, OBIEE, QlikView, SAP Business Intelligence, Amazon Redshift, or Azure Data Warehouse    Operating System Windows, Linux, Unix, Macintosh HD, Red Hat  Experience  Data Scientist AT & T INC - Plano, TX January 2019 to Present This Project is developed for Roaming Command Center to minimize cost what we pay to different service providers when AT & T customers roaming to their Carriers by using Optimization techniques and Machine Learning models.  Responsibilities: -    on all aspects of creating predictive models including collecting requirements, establishanalytics  plan, data exploration, cleansing, and preparation, identifying features, selecting machine learning algorithms, building and testing models, iteratively improving solutions.   Build multiple time series forecasting models using tsfresh, Prophet, ARIMA time series models topredict future usage of voice call, mobile data and SMS for different countries using forecasting and regression methods in machine learning.   Use Feature engineering to and create new time series features like rolling means, Date Timefeatures, Lag features to improve forecasting Model.   Evaluate Forecasting Model accuracy by Using RMSE and MAPE to check error rate between actualand predicted values.   Build Forecasting model to predict future usage of units by daily, weekly and monthly and used Jenkins jobs for periodically run python script and store output in database.   Integrated Forecasting model with MongoDB using PyMongo to get data for model and storepredictions result from model.   Build Forecasting model pipeline to automate process.   Used Matplotlib library to plot graph for all countries and understand trend of usage of data andenhance model accuracy.   Build big event feature and holidays feature for model to improve model accuracy.   Used Rolling Window method to build forecasting model.   Perform Linear programing methods and models, Genetic Algorithm for optimization models to findoptimal solution by creating different constraints and  functions for cost minimization.  Extracted the data from hive tables by writing efficient Hive queries using PySpark.   Developed Production Document which includes data pipeline, database schema, Jenkins Jobspipeline and API application properties.   Follow Agile Craft for project development and to track project performance  Environment: HDFS, Hive, Pyspark, Python, tsfresh, Prophet, ARIMA, Gradient Boosting Model, Random forest regressor, Linear programing, Optimization, Genetic algorithm, Agile. Data Scientist Capital Group - Los Angeles, CA March 2018 to December 2018 This project was to support auditing team and claim department to improve accounting accuracy and reduce risk of fraudulent activities via providing machine learning and modeling solutions to identify suspicious insurance claims.    Claims severity prediction in real-time  Built classification models to predict the fraudulent claims by severity in real-time reducing the time for execution from 6 hours to 4 seconds. Implemented the models as a predictive solution for finding the fraudulent claims for the credit disability and debt protection products. Forwarded the high-risk claims for further investigation.    Text analytics for fraud prediction  Executed topic modeling for finding different topics based on the notes made corresponding to the claimant's claim. Attributed the resulting topics to classify into fraud and not fraud categories.    Risk assessment prediction  Incorporated models built in Python and R into the business processes using clustering techniques to assess the risk involved with a customer. The models built are used in assessing the premiums amount required to be paid by the customer.    Customer churn/attrition prediction  Developed models that predict whether a customer's propensity to churn leveraging the information related to insurance policies, demographics, claims, related to the customer, payment frequency, home ownership status, household tenure etc.    Responsibilities:  * Implemented Data Exploration to analyze patterns and to select features using Python SciPy. * Built Factor Analysis and Cluster Analysis models using Python SciPy to classify customers into different target groups.  * Built predictive models including Support Vector Machine, Random Forests and Naïve Bayes Classifierusing Python Scikit-Learn to predict the personalized product choice for each client.  * Using R's dplyr and ggplot2 packages performed an extensive graphical visualization of overall data,including customized graphical representation of revenue reports, specific item sales statistics and visualization.  * Designed and implemented cross-validation and statistical tests including Hypothetical Testing, ANOVA, Auto-correlation to verify the models' significance.  * Designed an A/B experiment for testing the business performance of the new recommendationsystem.  * Supported MapReduce Programs running on the cluster.  * Evaluated business requirements and prepared detailed specifications that follow project guidelinesrequired to develop written programs.  * Configured Hadoop cluster with Namenode and slaves and formatted HDFS.  * Used Oozie flow engine to run multiple Hive and Pig jobs.  * Participated in Data Acquisition with Data Engineer team to extract historical and real-time data byusing Hadoop MapReduce and HDFS.  * Performed Data Enrichment jobs to deal missing value, to normalize data, and to select features byusing HiveQL.  * Developed multiple MapReduce jobs in java for data cleaning and pre-processing.  * Analyzed the partitioned and bucketed data and compute various metrics for reporting.  * Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.  * ed on loading the data from MySQL to HBase where necessary using Sqoop.  * Developed Hive queries for Analysis across different banners.  * Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data anduploaded to database.  * Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuringlaunched instances with respect to specific applications.  * Developed Hive queries for analysis, and exported the result set from Hive to MySQL using Sqoopafter processing the data.  * Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.  * Created HBase tables to store various data formats of data coming from different portfolios.  * ed on improving performance of existing Pig and Hive Queries.  * Created reports and dashboards, by using D3.js and Tableau 9.x, to explain and communicate datainsights, significant features, models scores and performance of new recommendation system to both technical and business teams.  * Utilize SQL, Excel and several Marketing/Web Analytics  (Google Analytics, AdWords) in order tocomplete business & marketing analysis and assessment.  * Used Git 2.x for version control with Data Engineer team and Data Scientists colleagues. * Used Agile methodology and SCRUM process for project developing.    Environment: HDFS, Hive, Scoop, Pig, Oozie, Amazon Web Services (AWS), Python 3.x (SciPy, ScikitLearn), Tableau 9.x, D3.js, SVM, Random Forests, Naïve Bayes Classifier, A/B experiment, Git 2.x, Agile/ SCRUM. Sr. Data Scientist PCCI Group - Dallas, TX November 2016 to February 2018 PCCI provides services to healthcare industry which is characterized by its dynamism and the diversity of players by providing customized BPO solutions. Our services are designed to help healthcare insurers; medical device manufacturers, pharmaceutical companies and healthcare providers acquire new market shares and effectively support their current members.  With a comprehensive understanding of your business, PCCI acts as a key point between your products/services and your members, delivering them a superior customer experience at every opportunity.  We provide sales and customer service solutions. More specifically, our solutions can be customized to your organization's specific requirements, including:  * Sales through programs enrollments  * Customer service (Claim inquiries about billing & account)  * Helpline services  * Awareness campaigns  * Appointment scheduling for labs, doctor offices, clinics visits  * Data management  * Technical support  * Market research    Responsibilities  * Perform Data Profiling to learn about behavior with various features such as traffic pattern, location,Date and Time etc.  * Extracted the data from hive tables by writing efficient Hive queries.  * Performed preliminary data analysis using descriptive statistics and handled anomalies such asremoving duplicates and imputing missing values.  * Analyze Data and Performed Data Preparation by applying historical model on the data set in AZUREML.  * Application of various machine learning algorithms and statistical modeling like decision trees, textanalytics, natural language processing (NLP), supervised and unsupervised, regression models, social net analysis, neural nets, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.  * Exploring DAG's, their dependencies and logs using Airflow pipelines for automation.  * Performed data cleaning and feature selection using MLlib package in PySpark and ing with deeplearning frames such as TensorFlow, Keras etc.  * Conducted a hybrid of Hierarchical and K-means Cluster Analysis using IBM SPSS and identifiedmeaningful segments of customers through a discovery approach.  * Develop Spark/Scala, Python, R for regular expression (regex) project in the Hadoop/Hiveenvironment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.  * Evaluate models using Cross Validation, Log loss function, ROC curves and used AUC for featureselection and elastic  like Elastic Search, Kibana etc.  *  with NLTK library to NLP data processing and finding the patterns.  * Categorize comments into positive and negative clusters from different social neting sites using Sentiment Analysis and Text Analytics.  * Analyze traffic patterns by calculating autocorrelation with different time lags.  * Ensure that the model has low False Positive Rate and Text classification and sentiment analysis forunstructured and semi-structured data.  * Addressed over fitting by implementing of the algorithm regularization methods like L2 and L1.  * Use Principal Component Analysis in feature engineering to analyze high dimensional data. * Create and design reports that will use gathered metrics to infer and draw logical conclusions of past and future behavior.  * Perform Multinomial Logistic Regression, Random forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route.  * Implemented different models like Logistic Regression, Random Forest and Gradient-Boost Trees topredict whether a given die will pass or fail the test.  * Perform data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.  * Use MLlib, Spark's Machine learning library to build and evaluate different models.  * Perform Data Cleaning, features scaling, features engineering using pandas and numpy packages inpython.  * Develop MapReduce pipeline for feature extraction using Hive and Pig.  * Create Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Create various types of data visualizations using Python and Tableau.  * Communicate the results with operations team for taking best decisions.  * Collect data needs and requirements by Interacting with the other departments.    Environment: Python 2.x, R, HDFS, Hadoop 2.3, Hive, Linux, Spark, IBM SPSS, Tableau Desktop, SQL Server 2012, Microsoft Excel, Matlab, Spark SQL, Pyspark. Data Scientist Cummins - Denver, CO April 2015 to August 2016 SLM Corporation is a publicly traded U.S. corporation that provides consumer banking & is a student loan company with over 40 years of providing student loans for college, supporting graduate and undergraduate study, and more. The primary goal of the Customer Administration effort is to provide a single point of management for all customer data.    Responsibilities:  * Developing Data Mapping, Data Governance, Transformation and Cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS and OLAP.  * Providing source to target mappings to the ETL team to perform initial, full, and incremental loadsinto the target data mart.  * Conducting JAD sessions, writing meeting minutes, collecting requirements from business users andanalyze based on the requirements.  * Involved in defining the source to target data mappings, business rules, and data definitions.  * Transformation on the files received from clients and consumed by Sql Server.  * ing closely with the ETL, SSIS, SSRS Developers to explain the complex Data Transformationusing Logic.  * ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005.   * Performing Data Profiling, Cleansing, Integration and extraction   * Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).  * Applying data cleansing/data scrubbing techniques to ensure consistency amongst data sets. * Extensively using ETL methodology for supporting data extraction, transformations and loading processing, in a complex EDW.    Environment: MS Excel, Agile, Oracle 11g, Sql Server, SOA, SSIS, SSRS, ETL, UNIX, T-SQL, HP Quality Center 11, RDM (Reference Data Management). Data Scientist John Wiley & Sons - Hoboken, NJ January 2014 to March 2015 Wiley's Scientific, Technical, Medical, and Scholarly business, also known as Wiley-Blackwell, serves the world's research and scholarly communities, and is the largest publisher for  and scholarly societies. Wiley has been actively increasing its presence in the Higher Education Department by partnering with 3rd party Learning Management Systems (LMS). The project involved integration of Wiley PLUS E4 LMS with Blackboard LMS.  Responsibilities:  * Involved in complete Software Development Life Cycle (SDLC) process by analyzing businessrequirements and understanding the functional  flow of information from source systems to destination systems.  * A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, Unix Commands, NoSQL, Hadoop.  * Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing variousmachine learning algorithms.  * ed on different data formats such as JSON, XML and performed machine learning algorithms inPython.  * Analyzed sentimental data and detecting trend in customer usage and other services.  * Analyzed and Prepared data, identify the patterns on dataset by applying historical models.  * Collaborated with Senior Data Scientists for understanding of data.  * Used Python and R scripting by implementing machine algorithms to predict the data and forecastthe data for better results.  * Used Python and R scripting to visualize the data and implemented machine learning algorithms.  * Experience in developing packages in R with a shiny interface.  * Used predictive analysis to create models of customer behavior that are correlated positively withhistorical data and use these models to forecast future results.  * Predicted user preference based on segmentation using General Additive Models, combined withfeature clustering, to understand non-linear patterns between user segmentation and related monthly platform usage features (time series data).  * Perform data manipulation, data preparation, normalization, and predictive modeling.  * Improve efficiency and accuracy by evaluating model in Python and R.  * Used Python and R script for improvement of model.  * Application of various machine learning algorithms and statistical modeling like Decision Trees,Random Forest, Regression Models, neural nets, SVM, clustering to identify Volume using scikitlearn package  * Performed Data cleaning process applied Backward - Forward filling methods on dataset for handlingmissing values.  * Developed a predictive model and validate Neural Net Classification model for predict thefeature label.  * Performed Boosting method on predicted model for the improve efficiency of the model.  * Presented Dashboards to Higher Management for more Insights using Power BI and Tableau.  * Hands on experience in using HIVE, Hadoop, HDFS and Bigdata related topics.    Environment: R/R studio, Python, Tableau, Hadoop, Hive, MS SQL Server, MS Access, MS Excel, Outlook, Power BI. Data Reporting Analyst C3-Solutions - Bengaluru, Karnataka October 2012 to April 2013 * Designed and implemented an internal reporting tool named I-CUBE using Python to automate salesand financial operational data accessible through a built-in SharePoint for leaders globally. Used API for I-Cube to extract sales data on an hourly-basis.  * Built and customized interactive reports on forecasts, targets and actuals data using BI/ETL such as SAS, SSAS, SSIS in the CRM which slashed manual efforts by 8%.  * Conducted operational analyses for business worth $3M ing through all phases such asrequirements gathering, developing use cases, data mapping and creating flow diagrams. * Accomplished data cleansing and analysis results using Excel pivot tables, VLOOKUPs, data validation, graphs and chart manipulation in Excel.  * Designed complex SQL queries, Views, Stored Procedures, Functions and Triggers to handle databasemanipulation and performance.  * Used SQL, PLSQL scripts for automating repeatable tasks of customer feedback survey datacollection and distribution which increased the departmental efficiency by 8%. Education  M.S in Data Mining and Predictive Analytics. in Data Mining and Predictive Analytics St. John's University - Queens, NY B.E in Computer Science & Engineering. in Computer Science & Engineering Gujarat Technological University - Gujarat, IN"
"

:
* 10+ years of  IT experience and ed on various  implementations.
* Experience in various industries including Healthcare, Financial, Banking and Energy.
* experience in implementing data mining and statistical machine learning solutions to various business problems, performing Principal Component Analysis, SVM, implementing supervised and unsupervised machine learning models.
* of experience in implementing batch and real time descriptive analytical solutions using big data, visualizations and reporting .
* Built predictive analytics models to generate actionable insights.
* experience in Python, R and Tableau.
* Expertise with the  in Hadoop ecosystem including HDFS, Spark, Spark SQL, HBase, Hive, and Sqoop.
* Used Agile Methodology of Data Warehouse development using Kanbanize.
* Experience with leveraging Hadoop ecosystem components including Hive for Data Analysis, Sqoop for Data Migration between RDBMS systems and Hadoop.
* Design and implement Hadoop based data ingestion solutions and implement the capabilities and offerings of the enterprise Hadoop platform.
* Experience in Business Intelligence with SQL Server SSIS, SSAS, SSRS packages
* Experience in MySQL and MS SQL server administering and creating stored procedures.
* Proficient in analyzing and translating business requirements to  requirements and architecture.
* Developed and implemented data cleansing, data security, data profiling and datamonitoring processes.
* Developed Data Mapping, Data Governance, and Transformation and cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS.
* Extensive exposure to Functional as well as  aspects.

Authorized to  in United States for any employer



Doozer.com, GA (Remote)
Mar' 17 to Present
Data Scientist


Responsibilities:
* Reports creation and analysis at various levels for example Currency level report(CLR), Account Level Report(ALR) and other reports
* Developing fraud detection models using different machine learning techniques
* Defining the data streams and analyzing the data
* Developed the ETL process in Hadoop Platform
* Prepared High Level Logical Data Models using Erwin, and later translated the model into the physical model using the Forward Engineering technique.
* Loaded semi structured data into Hadoop File System (HDFS).
* Involved in writing T-SQL ing on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing, and Data Migration. involved in writing stored procedures and involved in writing ad-jacqueries for the datamining.
* Developed the Sqoop and Hive scripts for data transfer and data analysis.

Environment: Isolation Forest, Logistic Regression machine learning algorithms, Python, Cloudera, HDFS, Hive, Sqoop



VBGN  Inc., Seattle, WA
Oct' 15 to Feb' 17
Data Scientist

Responsibilities:
* Perform extensive data studies to understand the data behaviour
* Perform Principal Component Analysis using different PCA techniques
* Developing predictive models
* Applied machine learning algorithms like clustering and segmentation methods for product offerings
* Created Physical Data Analyst from the Logical Data Analyst using Compare and Merge Utility in ERStudio and ed with the naming standards utility.
* Developed classification models to predict the products and offers subscriptions
* Designed Python frames for machine learning models
* Involved from design to implementation of machine learning models
* Performed Import and export of data into HDFS and Hive using Sqoop and managed data coming from different sources
* Defining the requirements for data lakes/pipe lines
* ed on Data Analysis, Data profiling, and Data Modeling, data governance identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats.
* Analyzed and fixed the data related issues with help of Hive and Sqoop.
* Creating Hive tables, loading data and writing Hive queries.
* Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats
* Modeled various hive tables and optimized the access by designing partitions and bucketing.
* Involved in code review and bug fixing for improving the performance
* ed on the core and Spark SQL modules of Spark using programming languages like Python

Environment: K-means clustering, Hierarchical clustering, Logistic Regression, Naïve Bayes, Decision Tree, KNN machine learning algorithms, PCA, Python, Cloudera, HDFS, Hive, Sqoop, Spark



HCL      
Dallas, TX
Jan' 14 to Sep' 15
Data Scientist

Responsibilities:
* Played key role in extracting the features based on users behavioural, demographic and social data.
* Identified the factors that increase churn risk using tree based machine learning model and visualize the plot to see the importance to feature.
* Train the model using boosting algorithm, not only just one algorithm trained the model using various machine learning algorithms like Random Forest, KNN, SVM, Regression and compare the accuracy of the models.
* Cross validated the model using sampling techniques , plot roc curve and created confusion matrix to evaluate the accuracy.
* Calculated customer value metric to priorities customer based on probability and value.
* Created Dashboards using Tableau to exposed the analysis in Dashboards.

Environment: Python, Machine Learning, MySql, Tableau



Home Depot    
Atlanta GA
Jul' 11 to Dec' 13
Data Scientist

Responsibilities:
* Performed Explanatory Data Analysis that included Data Profiling on descriptive statistics (unknown response values, imbalanced data), Feature Engineering and data pre-processing functions like transformations, imputation of missing data, capping skewed values, binning, duplicates using R
* Performed advanced SQL operations that included advanced filtering and data aggregation, window functions and preparing data for use with analytic 
* Conducted Explanatory Data Analysis and carried out visualizations with ggplot2 () function
* Performed chi-square test of independence to study the association between the categorical independent variable
* Addressed overfitting by implementing the algorithm regularization methods like L2 and L1.
* Used Principal Component Analysis in feature engineering to analyze high dimensional data.
* Involved with statistical domain experts to understand the data and ed with data management team on data quality assurance
* Provided statistical analyses in comprehensive written reports or sampling plans
* ed with division personnel to ensure the deliveryof high quality and timely statistical services necessary to achieve business goals
* Carried out predictive analysis using logistic regression, decision trees and random forests
* Carried out logistic regression with forward and backward and stepwise selection procedures
* Analyzed output results through Confusion Matrix, Sensitivity, Specificity, Accuracy and Kappa
* Validated the machine learning classifiers using Accuracy, AUC, ROC Curves and Lift Charts
* Performed random forests and analyzed graphs on training and testing errors
* Continuously interacted with Marketing Strategists and Business leaders to identify their analytic needs
* Created Tableau scorecards, dashboards using Stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality. Created dashboards to have clear view of descriptive statistics of all variables, region wise trend analysis and predicted vs actual response rate for each region. ed extensively with Advance analysis Actions, Calculations, Parameters, Background images and Maps. Effectively used data blending feature in Tableau.
     
     Environment: Oracle, SQL, R, Tableau, MS Excel and PowerPoint
     
     
Wells Fargo
San Francisco, CA
Feb' 10 to Jun' 11
Senior Analytics Consultant

Responsibilities:
* Implemented Order Management systems which included Data analysis, Data modelling and reporting for various modules
* Designed and delivered a customer churn attribution project using classification algorithms in R and built a strategy to reduce the customer churn rate that can generate an additional $2 million in annual revenue
* Developed scripts to collect transaction data from 4500 stores using Teradata to load data into data warehouse
* Decided the sourcing and scheduling rules for various clients implementing neural net algorithm in Python.
* Developed operational performance analysis adhoc, dynamic reports on IBM Cognos, Tableau, excel and SQL
* Increased conversation rates of customers by 5% using predictive analytics and market basket analysis targeting promotions
* Developed a tool from scratch for automation of Integration systems using JAVA & JMeter reducing the lead time by 80%



Microsoft
Seattle WA
Oct' 08 to Jan' 10
Data Scientist

Responsibilities:
* Responsible for enabling analysis through producing information products and is involved in the research and development efforts. Traditional programming (SAS, SQL) and business intelligence (i.e. Spotfire, Tableau, or Qlik) experience
* Ability to support the creation of sophisticated, value-added analytic systems that support revenue generation, risk management, operational efficiency, regulatory compliance, portfolio management, and research.
* Deployment of advanced techniques (e.g., text mining, statistical analysis, etc.) to deliver insights
* Possesses a degree in hard science or another heavy quantitative business or social discipline
*  closely with traders, quantitative & analytics group, risk, research group to integrate pricing and risk models for new and structured trades
* Design and develop fixed income trade blotter using C#, Windows forms, Infragistics UI components, SQL Server, TIBCO messaging and Click Once deployment strategy
* Develop and Migrate CDS trades and CDS spreads with EM pricing sheets to strategic systems using Excel VBA & C#


Education:
Bachelor of commerce, Gujrat University, Jun 1994  May 1997",Data Scientist,resume,"  : * 10+ years of  IT experience and ed on various  implementations. * Experience in various industries including Healthcare, Financial, Banking and Energy. * experience in implementing data mining and statistical machine learning solutions to various business problems, performing Principal Component Analysis, SVM, implementing supervised and unsupervised machine learning models. * of experience in implementing batch and real time descriptive analytical solutions using big data, visualizations and reporting . * Built predictive analytics models to generate actionable insights. * experience in Python, R and Tableau. * Expertise with the  in Hadoop ecosystem including HDFS, Spark, Spark SQL, HBase, Hive, and Sqoop. * Used Agile Methodology of Data Warehouse development using Kanbanize. * Experience with leveraging Hadoop ecosystem components including Hive for Data Analysis, Sqoop for Data Migration between RDBMS systems and Hadoop. * Design and implement Hadoop based data ingestion solutions and implement the capabilities and offerings of the enterprise Hadoop platform. * Experience in Business Intelligence with SQL Server SSIS, SSAS, SSRS packages * Experience in MySQL and MS SQL server administering and creating stored procedures. * Proficient in analyzing and translating business requirements to  requirements and architecture. * Developed and implemented data cleansing, data security, data profiling and datamonitoring processes. * Developed Data Mapping, Data Governance, and Transformation and cleansing rules for the Master Data Management (MDM) Architecture involving OLTP, ODS. * Extensive exposure to Functional as well as  aspects.  Authorized to  in United States for any employer    Doozer.com, GA (Remote) Mar' 17 to Present Data Scientist   Responsibilities: * Reports creation and analysis at various levels for example Currency level report(CLR), Account Level Report(ALR) and other reports * Developing fraud detection models using different machine learning techniques * Defining the data streams and analyzing the data * Developed the ETL process in Hadoop Platform * Prepared High Level Logical Data Models using Erwin, and later translated the model into the physical model using the Forward Engineering technique. * Loaded semi structured data into Hadoop File System (HDFS). * Involved in writing T-SQL ing on SSIS, SSRS, SSAS, Data Cleansing, Data Scrubbing, and Data Migration. involved in writing stored procedures and involved in writing ad-jacqueries for the datamining. * Developed the Sqoop and Hive scripts for data transfer and data analysis.  Environment: Isolation Forest, Logistic Regression machine learning algorithms, Python, Cloudera, HDFS, Hive, Sqoop    VBGN  Inc., Seattle, WA Oct' 15 to Feb' 17 Data Scientist  Responsibilities: * Perform extensive data studies to understand the data behaviour * Perform Principal Component Analysis using different PCA techniques * Developing predictive models * Applied machine learning algorithms like clustering and segmentation methods for product offerings * Created Physical Data Analyst from the Logical Data Analyst using Compare and Merge Utility in ERStudio and ed with the naming standards utility. * Developed classification models to predict the products and offers subscriptions * Designed Python frames for machine learning models * Involved from design to implementation of machine learning models * Performed Import and export of data into HDFS and Hive using Sqoop and managed data coming from different sources * Defining the requirements for data lakes/pipe lines * ed on Data Analysis, Data profiling, and Data Modeling, data governance identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats. * Analyzed and fixed the data related issues with help of Hive and Sqoop. * Creating Hive tables, loading data and writing Hive queries. * Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats * Modeled various hive tables and optimized the access by designing partitions and bucketing. * Involved in code review and bug fixing for improving the performance * ed on the core and Spark SQL modules of Spark using programming languages like Python  Environment: K-means clustering, Hierarchical clustering, Logistic Regression, Naïve Bayes, Decision Tree, KNN machine learning algorithms, PCA, Python, Cloudera, HDFS, Hive, Sqoop, Spark    HCL       Dallas, TX Jan' 14 to Sep' 15 Data Scientist  Responsibilities: * Played key role in extracting the features based on users behavioural, demographic and social data. * Identified the factors that increase churn risk using tree based machine learning model and visualize the plot to see the importance to feature. * Train the model using boosting algorithm, not only just one algorithm trained the model using various machine learning algorithms like Random Forest, KNN, SVM, Regression and compare the accuracy of the models. * Cross validated the model using sampling techniques , plot roc curve and created confusion matrix to evaluate the accuracy. * Calculated customer value metric to priorities customer based on probability and value. * Created Dashboards using Tableau to exposed the analysis in Dashboards.  Environment: Python, Machine Learning, MySql, Tableau    Home Depot     Atlanta GA Jul' 11 to Dec' 13 Data Scientist  Responsibilities: * Performed Explanatory Data Analysis that included Data Profiling on descriptive statistics (unknown response values, imbalanced data), Feature Engineering and data pre-processing functions like transformations, imputation of missing data, capping skewed values, binning, duplicates using R * Performed advanced SQL operations that included advanced filtering and data aggregation, window functions and preparing data for use with analytic  * Conducted Explanatory Data Analysis and carried out visualizations with ggplot2 () function * Performed chi-square test of independence to study the association between the categorical independent variable * Addressed overfitting by implementing the algorithm regularization methods like L2 and L1. * Used Principal Component Analysis in feature engineering to analyze high dimensional data. * Involved with statistical domain experts to understand the data and ed with data management team on data quality assurance * Provided statistical analyses in comprehensive written reports or sampling plans * ed with division personnel to ensure the deliveryof high quality and timely statistical services necessary to achieve business goals * Carried out predictive analysis using logistic regression, decision trees and random forests * Carried out logistic regression with forward and backward and stepwise selection procedures * Analyzed output results through Confusion Matrix, Sensitivity, Specificity, Accuracy and Kappa * Validated the machine learning classifiers using Accuracy, AUC, ROC Curves and Lift Charts * Performed random forests and analyzed graphs on training and testing errors * Continuously interacted with Marketing Strategists and Business leaders to identify their analytic needs * Created Tableau scorecards, dashboards using Stack bars, bar graphs, scattered plots, geographical maps, Gantt charts using show me functionality. Created dashboards to have clear view of descriptive statistics of all variables, region wise trend analysis and predicted vs actual response rate for each region. ed extensively with Advance analysis Actions, Calculations, Parameters, Background images and Maps. Effectively used data blending feature in Tableau.            Environment: Oracle, SQL, R, Tableau, MS Excel and PowerPoint             Wells Fargo San Francisco, CA Feb' 10 to Jun' 11 Senior Analytics Consultant  Responsibilities: * Implemented Order Management systems which included Data analysis, Data modelling and reporting for various modules * Designed and delivered a customer churn attribution project using classification algorithms in R and built a strategy to reduce the customer churn rate that can generate an additional $2 million in annual revenue * Developed scripts to collect transaction data from 4500 stores using Teradata to load data into data warehouse * Decided the sourcing and scheduling rules for various clients implementing neural net algorithm in Python. * Developed operational performance analysis adhoc, dynamic reports on IBM Cognos, Tableau, excel and SQL * Increased conversation rates of customers by 5% using predictive analytics and market basket analysis targeting promotions * Developed a tool from scratch for automation of Integration systems using JAVA & JMeter reducing the lead time by 80%    Microsoft Seattle WA Oct' 08 to Jan' 10 Data Scientist  Responsibilities: * Responsible for enabling analysis through producing information products and is involved in the research and development efforts. Traditional programming (SAS, SQL) and business intelligence (i.e. Spotfire, Tableau, or Qlik) experience * Ability to support the creation of sophisticated, value-added analytic systems that support revenue generation, risk management, operational efficiency, regulatory compliance, portfolio management, and research. * Deployment of advanced techniques (e.g., text mining, statistical analysis, etc.) to deliver insights * Possesses a degree in hard science or another heavy quantitative business or social discipline *  closely with traders, quantitative & analytics group, risk, research group to integrate pricing and risk models for new and structured trades * Design and develop fixed income trade blotter using C#, Windows forms, Infragistics UI components, SQL Server, TIBCO messaging and Click Once deployment strategy * Develop and Migrate CDS trades and CDS spreads with EM pricing sheets to strategic systems using Excel VBA & C#   Education: Bachelor of commerce, Gujrat University, Jun 1994  May 1997"
" Over 8+ years of Experience on Machine Learning, Statistical Modelling, Predictive Modelling, Data
Analytics, Data Modelling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural
Language Processing (NLP) 
 Proficient in gathering and analyzing the Business Requirements with experience in documenting
System Requirement Specifications (SRS) and Functional Requirement Specifications (FRS) 
 Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data
Mining solutions to various business problems and generating data visualizations using R, Python, and Tableau. 
 Experience in Extracting data for creating Value Added Datasets using Python, R, SAS, Azure and SQLto analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project s. 
 Good experience of software development in Python (libraries used: Beautiful Soup, NumPy, SciPy,
Matplotlib, Python-Twitter, Pandas data frame, net, Urllib2, MySQL) 
 Experience on Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision
Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin. 
 Extensive experienced on business intelligence (and BI )  such as OLAP, Datawarehousing, reporting and querying , Data mining and Spreadsheets. 
 Efficient in developing Logical and Physical Data model and organizing data as per the businessrequirements using Sybase Power Designer, Erwin, ER Studio in both OLTP and OLAP applications 
 Strong understanding of when to use an ODS or data mart or data warehousing. 
 Experienced in employing RProgramming, MATLAB, SAS, Tableau and SQL for datacleaning, datavisualization, risk analysis and predictive analytics 
 Collaborated with the lead Data Architect to model the Data warehouse in accordance to FSLDMsubject areas, 3NF format, Snow flake schema. 
 Skilled in E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specificfeatures. 
 Experience on advanced SAS programming techniques, such as PROC SQL (JOIN/ UNION), PROC
APPEND, PROC DATASETS, and PROC TRANSPOSE. 
 Hands on experienced with Machine Learning, Regression Analysis, Clustering, Boosting,
Classification, Principal Component Analysis and Data Visualization  
 Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in theinfrastructure to provide data summarization and data manipulation using Linux Commands. 
  Knowledge on designing and implementing a fully operational production grade largescale data solution on Snowflake Data Warehouse 
 Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information
Management 
 Experienced in Database using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data andNoSQL. 
 ed closely with other data scientists to create data driven products. 
 Strong experienced in Statistical Modeling/Machine Learning and Visualization  
 Proficient in Hadoop, HDFS, Hive, MapReduce, Pig and NOSQL databases like MongoDB, HBase,
Cassandra and expertise in applying data mining techniques and optimization techniques in B2B and
B2C industries and proficient Machine Learning, Data/Text Mining, Statistical Analysis & Predictive Modeling. 
 Experienced in Data Modeling & Data Analysis experience using Dimensional DataModeling andRelational Data Modeling, Star Schema/Snowflake Modeling, FACT& Dimensions tables, Physical & Logical Data Modeling.
Willing to relocate: Anywhere
 Experience

Data Scientist
Century link - Littleton, CO August 2018 to Present
Description: CenturyLink is the second largest U.S. communications provider to global enterprise customers. With customers in more than 60 countries and an intense focus on the customer experience, CenturyLink strives to be the world's best neting company by solving customers' increased demand for reliable and secure connections.It also serves as its customers' trusted partner, helping them manage increased net and IT complexity and providing managed net and cyber security solutions that help protect their business. 
 
Responsibilities: 
 Built data pipelines for reporting, alerting, and data mining. Experienced with table design and datamanagement using HDFS, Hive, Sqoop, MySQL. 
 ed with statistical models for data analysis, predictive modeling, machine learning approachesand recommendation and optimization algorithms. 
 ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and MetadataManagement Services. 
 ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiplepurposes. 
 Analyzing Business Intelligence Reporting requirements and translating them into data sourcingand modeling requirements including Dimensional & Normalized data models, Facts, Dimensions,
Snowflake Schemas. 
 Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETLprocesses for Oracle database. 
 ed with Big Data  such Hadoop, Hive, Map Reduce 
 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoopon AWS. Implemented a Python-based distributed random forest via Python streaming. 
 Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure. 
 A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping,
Machine Learning, Python programming, SQL, GIT, Linux Commands, NoSQL, MongoDB, Hadoop. 
 Performed scoring and financial forecasting for collection priorities using Python, R and
SASmachinelearning algorithms. 
 Handled importing data from various data sources, performed transformations using Hive,
MapReduce, and loaded data into HDFS and batch processing with Linux 
 Managed existing team members lead the recruiting and on boarding of a larger DataScience teamthat addresses analytical knowledge requirements. 
 Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscriptmapping. 
 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. 
 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for clientbusiness data management. 
 Above scoring models resulted in millions of dollars of added revenue to the company and a changein priorities of the entire company. 
 
Environment: R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.
Data Scientist
Verizon - Richardson, TX
May 2017 to July 2018
Description: Verizon Communications Inc. is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The Company, through its subsidiaries, provides communications, information and entertainment products and services to consumers, businesses and governmental agencies. Its segments include Wireless and Wireline. The Wireless segment offers communications products and services, including wireless voice and data services and equipment sales, to consumer, business and government customers across the United States. 
 
Responsibilities: 
 Responsible for performing Machine-learning techniques regression/classification to predict theoutcomes. 
 Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for Modeling. 
 Identifying and executing process improvements, hands-on in various  such as Oracle,
Informatica, and Business Objects. 
 Designing and implementing data warehouses and data marts using components of KimballMethodology, like Data Warehouse Bus, Conformed Facts & Dimensions, Slowly Changing Dimensions in Snowflake Schema 
 Develop and implement innovative AI and machine learning  that will be used in the Risk. 
 Performed the feature engineering of supervised and unsupervised machine learning models. 
 Implemented SparkMLLib utilities such as including classification, regression, clustering, collaborativefiltering, and dimensionality reductions 
 Utilized Convolution Neural Nets to implement a machine learning image recognitioncomponentusing TensorFlow. 
 Strong in ETL and data integration experience in developing ETL mappings and scripts usingInformatica 
 Interaction with Business Analyst, SME and other Data Architects to understand Business needs andfunctionality for various project solutions. 
 Designed the prototype of the Data mart and documented possible outcome from it for end-user. 
 Involved in business process Modeling using UML. 
 Handled importing data from various data sources, performed transformations using Hive, Map
Reduce, and loaded data into HDFS. 
 ed on Spark tool collaborating with ML libraries in eliminating a shotgun approach to understandcustomer buying patterns. 
 Responsible for handling Hive queries using Spark SQL that integrates with Spark environment. 
 Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, andPL/SQL. 
 Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data
Definitions, and Data Formats 
 Performance tuning of the database, which includes indexes, and optimizing SQL statements,monitoring the server. 
 Updated Pythonscripts to match training data with our database stored in AWS Cloud Search, sothatwe would be able to assign each document a response label for further classification. 
 Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for seniormanagers. 
 Created PL/SQL packages and Database Triggers and developed user procedures and prepared usermanuals for the new programs. 
 
Environment: Python, MDM, MLLib, PL/SQL, Tableau, Git, NLP, SQL Server, MLLib, Scala NLP, SSMS, ERP, CRM, Netezza, Cassandra, SQL, PL/SQL, SSRS, Informatica, Spark, Azure, R Studio, MongoDB, JAVA, HIVE.
Data Scientist
Direct Energy - Houston, TX
January 2016 to April 2017
Description: Direct Energy is a North American retailer of energy and energy services. The company was founded in 1986. The company has more than six million customers in Canada and the United States, making it the largest energy retailer in North America. 
 
Responsibilities: 
 Involved in extensive hoc reporting, routine operational reporting, and data manipulation to produceroutine metrics and dashboards for management 
 Created parameters, action filters and calculated sets for preparing dashboards and sheets inTableau. 
 Interacting with other datascientists and architects, custom solutions for data visualization using like a tableau and Packages in Python. 
 Involved in running Map Reduce jobs for processing millions of records. 
 Written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc. 
 The building, publishing customized interactive reports, report scheduling and dashboards usingTableauserver. 
 Developed in Python programs for manipulating the data reading from various Teradata and convertthem as one CSV Files. 
 Performing statistical data analysis and data visualization using Python. 
 ed on creating filters and calculated sets for preparing dashboards and sheets in Tableau.  Created data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements. 
 Created new scripts for Splunk scripted input for the system, collecting CPU and OS data. 
 Implemented data refreshes on Tableau Server for biweekly and monthly increments based ona business change to ensure that the views and dashboards were displaying the changed data accurately. 
 Developed normalized Logical and Physical database models for designing an OLTP application. 
 Knowledgeable in AWS Environment for loading data files from on prim to Redshift cluster. 
 Performed SQL Testing on AWSRedshift databases. 
 Developed TeradataSQLscripts using OLAP functions like rank and rank over to improve the queryperformance while pulling the data from large tables. 
 Developed and implemented SSIS, SSRS and SSAS application solutions for various business unitsacross the organization. 
 Designed the Data Marts in dimensional data modelling using star and snowflake schemas. 
 Analyzed DataSet with SASprogramming, R and Excel. 
 Publish Interactive dashboards and schedule auto-data refreshes 
 Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS
Grid, Access and SQL queries. 
 Created Hive queries that helped market analysts spot emerging trends by comparing incrementaldata with Teradata reference tables and historical metrics. 
 Design and development of ETL processes using InformaticaETL  for dimension and fact filecreation. 
 Develop and automate solutions for a new billing and membership Enterprise data Warehouseincluding ETL routines, tables, maps, materialized views, and stored procedures incorporating
Informatica and Oracle PL/SQL ets. 
 Performed analysis of implementing Spark uses Scala and wrote spark sample programs usingPySpark. 
 
Environment:SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, R connector, Python, R, Tableau 9.2
Data Scientist/R Developer
Becton Dickinson - Franklin Lakes, NJ March 2014 to December 2015
Description: BD is a global medical technology company that is advancing the world of health by improving medical discovery, diagnostics and the delivery of care. BD leads inpatient and healthcare er safety and the  that enable medical research and clinical laboratories. 
 
Responsibilities: 
 The conducted analysis in assessing customer consuming behaviors and discover the value ofcustomers with RMF analysis, applied customer segmentation with clustering algorithms such as K-
Means Clustering and HierarchicalClustering. 
 Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries toperform data extraction and merging from Oracle. 
 Involved in managing backup and restoring data in the live Cassandra Cluster. 
 Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.  Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python. 
 Developed personalized product recommendation with Machine learning algorithms, includingGradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers. 
 Used Python and Spark to implement different machine learning algorithms, including Generalized
Linear Model, RandomForest, SVM, Boosting and Neural Net. 
 Evaluated parameters with K-Fold Cross Validation and optimized performance of models. 
 ed on benchmarking Cassandra Cluster using the Cassandra stress tool. 
 A highly immersive Data Science program involving Data Manipulation and Visualization, Web
Scraping, Machine Learning, GIT, SQL, LINUX Commands, Python programming, No SQL. 
 ed on data cleaning, data preparation, and feature engineering with Python, including Numpy,
Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn. 
 Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms. 
 Determined customer satisfaction and helped enhance customer using NLP. 
 Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and
HDFS, while maintaining data integrity. 
 Performed datavisualization and Designeddashboards with Tableau and D3.js and providedcomplexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders. 
 
Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering,
Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.
Data Analyst
ZETA Interactive
December 2012 to February 2014
Description: We're Zeta Global, a data-driven marketing technology company that helps brands acquire more customers, keep the ones they have for longer and grow their value. We create personalized customer experiences by leveraging strategy, analytics, machine learning, and awardwinning creative that ignite a perpetual dialogue between brands and their customers. 
 
Responsibilities: 
 Used SAS Proc SQLpass-throughfacility to connect to Oracle tables and created SAS datasets usingvarious SQL joins such as left join, right join, inner join and full join. 
 Performing data validation, transforming data from RDBMS oracle to SAS datasets. 
 Produce quality customized reports by using PROC TABULATE, PROC REPORT Styles, and ODS RTFand provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE. 
 Developed SAS macros for data cleaning, reporting and to support routing processing. 
 Performed advanced querying using SAS Enterprise Guide, calculating computed columns, usingafilter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets. 
 Involved in Developing, Debugging, and validating the project-specific SAS programs to generatederived SAS datasets, summary tables, and data listings according to study documents. 
 Created datasets as per the approved specification collaborated with project teams to completescientific reports and review reports to ensure accuracy and clarity. 
 Experienced in ing with data modelers to translate business rules/requirements into conceptual/logical dimensional models and ed with complex de-normalized and normalized data models 
 Performed different calculations like Quick table calculations, Date Calculations, Aggregate
Calculations, String and Number Calculations. 
 Created action filters, user filters, parameters and calculated sets for preparing dashboards andsheets in Tableau. 
 Used the dynamic SQL to perform some pre-and post-session task required while performing
Extraction, transformation, and loading. 
 Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracledatabase 
 Expertise in Agile Scrum Methodology to implement project life cycles of reports design anddevelopment 
 Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actionsetc. and published them on the web. 
 Gathering business requirements, creating business requirement documents (BRD /FRD) 
  closely with business leaders and users to define and design the data sources requirementsand data access Code, test, identify, implement and document technical solutions utilizing JavaScript, PHP&MySQL. 
 ing with themanager to prioritize requirements and preparing reports on theweekly and monthlybasis. 
 
Environment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio.
Data Analyst
Karvy Global Services
January 2011 to November 2012
Description:: Karvy Global Services is a specialist knowledge process outsourcing services firm and an arm of the Karvy Group, one of India's largest integrated financial services companies Responsibilities: 
 Participated in requirement gathering sessions with business stakeholders to understand the projectgoals and documented the business requirement documents (BRD) 
 Studied the Requirements Specifications, Use Cases and analyzed the data needs of the Businessusers. 
 Implemented conceptual and logical data models using Erwin 7.2 by adopting agile methodologies asper organization standards. 
 Redesigned some of the previous models by adding some new entities and attributes as per thebusiness requirements. 
 Converted the Logical data models to Physical data models to generate DDL scripts. 
 Reverse Engineered existed data models for analyzing and comparing the business process. 
 Expertise in the Forward Engineering of logical models to generate the physical model using Erwin.  Created the Logical data models using Erwin 7.2 and ensured that it follows the normalization rules and have the best possible traceability pattern. 
 Migrated several models from Erwin 4.1/7.1 to ERWIN 7.2 and updated the previous namingstandards. 
 Extensively ed with enterprise data warehouse development by building data marts, staging,and restaging. 
 Scheduled reports for daily, weekly, monthly reports for executives, Business analyst and customerrepresentatives for various categories and regions based on business needs using SQL Server
Reporting Services (SSRS) 
 ed with business users to understand metric definitions, presentation, and user needs. Environment: Erwin, Informatica, Cognos, Oracle 9i, SQL Server 2003, SQL, MS Office, Windows 2003.
Education

Bachelors of Science in Computer science Engineering
GITAM University 2011
Skills / IT Skills

SQL (8 years), DATABASE (6 years), INFORMATICA (6 years), SAS (5 years), Serial Attached SCSI (5 years)
Additional Information

TECHNICAL SKILLS: 
 
Languages HTML5, DHTML, WSDL, CSS3, C, C++, XML, R/R Studio, SAS Enterprise Guide, SAS, R
(Caret, Weka, ggplot), python 
Software/Libraries Keras, Caffe, TensorFlow, OpenCV, Scikit-learn, Pandas, NumPy, Microsoft Visual Studio, Microsoft Office. 
Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans. Packages ggplot2, caret, dplyr, RWeka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, Matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy. 
 
Machine Learning Algorithms 
Neural Nets, Decision trees, Support Vector Machines, Random forest, Convolutional Neural Nets, Logistic Regression, PCA, K- means, KNN. 
 
Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall 
Database SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase,
Teradata, Netezza, Mongo DB, Cassandra. 
Reporting  MS Office (Word/Excel/PowerPoint/ Visio/Outlook), Crystal Reports XI, SSRS, Cognos 7.0/6.0. 
Big Data  Hadoop, Hive, HDFS, Map Reduce, Pig. 
BI  
Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1. 
 
Database Design  and Data Modeling 
MS Visio, ERWIN 4.5/4.0, Star Schema/Snowflake Schema modeling, Fact & Dimensions tables, physical & logical data modeling, Normalization and De-normalization techniques, Kimball &Inmon
Methodologies",Data Scientist,resume," Over 8+ years of Experience on Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Modelling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP)   Proficient in gathering and analyzing the Business Requirements with experience in documenting System Requirement Specifications (SRS) and Functional Requirement Specifications (FRS)   Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data Mining solutions to various business problems and generating data visualizations using R, Python, and Tableau.   Experience in Extracting data for creating Value Added Datasets using Python, R, SAS, Azure and SQLto analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project s.   Good experience of software development in Python (libraries used: Beautiful Soup, NumPy, SciPy, Matplotlib, Python-Twitter, Pandas data frame, net, Urllib2, MySQL)   Experience on Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin.   Extensive experienced on business intelligence (and BI )  such as OLAP, Datawarehousing, reporting and querying , Data mining and Spreadsheets.   Efficient in developing Logical and Physical Data model and organizing data as per the businessrequirements using Sybase Power Designer, Erwin, ER Studio in both OLTP and OLAP applications   Strong understanding of when to use an ODS or data mart or data warehousing.   Experienced in employing RProgramming, MATLAB, SAS, Tableau and SQL for datacleaning, datavisualization, risk analysis and predictive analytics   Collaborated with the lead Data Architect to model the Data warehouse in accordance to FSLDMsubject areas, 3NF format, Snow flake schema.   Skilled in E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specificfeatures.   Experience on advanced SAS programming techniques, such as PROC SQL (JOIN/ UNION), PROC APPEND, PROC DATASETS, and PROC TRANSPOSE.   Hands on experienced with Machine Learning, Regression Analysis, Clustering, Boosting, Classification, Principal Component Analysis and Data Visualization    Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in theinfrastructure to provide data summarization and data manipulation using Linux Commands.    Knowledge on designing and implementing a fully operational production grade largescale data solution on Snowflake Data Warehouse   Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information Management   Experienced in Database using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data andNoSQL.   ed closely with other data scientists to create data driven products.   Strong experienced in Statistical Modeling/Machine Learning and Visualization    Proficient in Hadoop, HDFS, Hive, MapReduce, Pig and NOSQL databases like MongoDB, HBase, Cassandra and expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient Machine Learning, Data/Text Mining, Statistical Analysis & Predictive Modeling.   Experienced in Data Modeling & Data Analysis experience using Dimensional DataModeling andRelational Data Modeling, Star Schema/Snowflake Modeling, FACT& Dimensions tables, Physical & Logical Data Modeling. Willing to relocate: Anywhere  Experience  Data Scientist Century link - Littleton, CO August 2018 to Present Description: CenturyLink is the second largest U.S. communications provider to global enterprise customers. With customers in more than 60 countries and an intense focus on the customer experience, CenturyLink strives to be the world's best neting company by solving customers' increased demand for reliable and secure connections.It also serves as its customers' trusted partner, helping them manage increased net and IT complexity and providing managed net and cyber security solutions that help protect their business.    Responsibilities:   Built data pipelines for reporting, alerting, and data mining. Experienced with table design and datamanagement using HDFS, Hive, Sqoop, MySQL.   ed with statistical models for data analysis, predictive modeling, machine learning approachesand recommendation and optimization algorithms.   ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and MetadataManagement Services.   ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiplepurposes.   Analyzing Business Intelligence Reporting requirements and translating them into data sourcingand modeling requirements including Dimensional & Normalized data models, Facts, Dimensions, Snowflake Schemas.   Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETLprocesses for Oracle database.   ed with Big Data  such Hadoop, Hive, Map Reduce   Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoopon AWS. Implemented a Python-based distributed random forest via Python streaming.   Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.   A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Linux Commands, NoSQL, MongoDB, Hadoop.   Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms.   Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS and batch processing with Linux   Managed existing team members lead the recruiting and on boarding of a larger DataScience teamthat addresses analytical knowledge requirements.   Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscriptmapping.   Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.   Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for clientbusiness data management.   Above scoring models resulted in millions of dollars of added revenue to the company and a changein priorities of the entire company.    Environment: R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib. Data Scientist Verizon - Richardson, TX May 2017 to July 2018 Description: Verizon Communications Inc. is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The Company, through its subsidiaries, provides communications, information and entertainment products and services to consumers, businesses and governmental agencies. Its segments include Wireless and Wireline. The Wireless segment offers communications products and services, including wireless voice and data services and equipment sales, to consumer, business and government customers across the United States.    Responsibilities:   Responsible for performing Machine-learning techniques regression/classification to predict theoutcomes.   Responsible for design and development of advanced R/Python programs to prepare to transformand harmonize data sets in preparation for Modeling.   Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, and Business Objects.   Designing and implementing data warehouses and data marts using components of KimballMethodology, like Data Warehouse Bus, Conformed Facts & Dimensions, Slowly Changing Dimensions in Snowflake Schema   Develop and implement innovative AI and machine learning  that will be used in the Risk.   Performed the feature engineering of supervised and unsupervised machine learning models.   Implemented SparkMLLib utilities such as including classification, regression, clustering, collaborativefiltering, and dimensionality reductions   Utilized Convolution Neural Nets to implement a machine learning image recognitioncomponentusing TensorFlow.   Strong in ETL and data integration experience in developing ETL mappings and scripts usingInformatica   Interaction with Business Analyst, SME and other Data Architects to understand Business needs andfunctionality for various project solutions.   Designed the prototype of the Data mart and documented possible outcome from it for end-user.   Involved in business process Modeling using UML.   Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.   ed on Spark tool collaborating with ML libraries in eliminating a shotgun approach to understandcustomer buying patterns.   Responsible for handling Hive queries using Spark SQL that integrates with Spark environment.   Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, andPL/SQL.   Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions, and Data Formats   Performance tuning of the database, which includes indexes, and optimizing SQL statements,monitoring the server.   Updated Pythonscripts to match training data with our database stored in AWS Cloud Search, sothatwe would be able to assign each document a response label for further classification.   Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for seniormanagers.   Created PL/SQL packages and Database Triggers and developed user procedures and prepared usermanuals for the new programs.    Environment: Python, MDM, MLLib, PL/SQL, Tableau, Git, NLP, SQL Server, MLLib, Scala NLP, SSMS, ERP, CRM, Netezza, Cassandra, SQL, PL/SQL, SSRS, Informatica, Spark, Azure, R Studio, MongoDB, JAVA, HIVE. Data Scientist Direct Energy - Houston, TX January 2016 to April 2017 Description: Direct Energy is a North American retailer of energy and energy services. The company was founded in 1986. The company has more than six million customers in Canada and the United States, making it the largest energy retailer in North America.    Responsibilities:   Involved in extensive hoc reporting, routine operational reporting, and data manipulation to produceroutine metrics and dashboards for management   Created parameters, action filters and calculated sets for preparing dashboards and sheets inTableau.   Interacting with other datascientists and architects, custom solutions for data visualization using like a tableau and Packages in Python.   Involved in running Map Reduce jobs for processing millions of records.   Written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc.   The building, publishing customized interactive reports, report scheduling and dashboards usingTableauserver.   Developed in Python programs for manipulating the data reading from various Teradata and convertthem as one CSV Files.   Performing statistical data analysis and data visualization using Python.   ed on creating filters and calculated sets for preparing dashboards and sheets in Tableau.  Created data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements.   Created new scripts for Splunk scripted input for the system, collecting CPU and OS data.   Implemented data refreshes on Tableau Server for biweekly and monthly increments based ona business change to ensure that the views and dashboards were displaying the changed data accurately.   Developed normalized Logical and Physical database models for designing an OLTP application.   Knowledgeable in AWS Environment for loading data files from on prim to Redshift cluster.   Performed SQL Testing on AWSRedshift databases.   Developed TeradataSQLscripts using OLAP functions like rank and rank over to improve the queryperformance while pulling the data from large tables.   Developed and implemented SSIS, SSRS and SSAS application solutions for various business unitsacross the organization.   Designed the Data Marts in dimensional data modelling using star and snowflake schemas.   Analyzed DataSet with SASprogramming, R and Excel.   Publish Interactive dashboards and schedule auto-data refreshes   Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries.   Created Hive queries that helped market analysts spot emerging trends by comparing incrementaldata with Teradata reference tables and historical metrics.   Design and development of ETL processes using InformaticaETL  for dimension and fact filecreation.   Develop and automate solutions for a new billing and membership Enterprise data Warehouseincluding ETL routines, tables, maps, materialized views, and stored procedures incorporating Informatica and Oracle PL/SQL ets.   Performed analysis of implementing Spark uses Scala and wrote spark sample programs usingPySpark.    Environment:SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, R connector, Python, R, Tableau 9.2 Data Scientist/R Developer Becton Dickinson - Franklin Lakes, NJ March 2014 to December 2015 Description: BD is a global medical technology company that is advancing the world of health by improving medical discovery, diagnostics and the delivery of care. BD leads inpatient and healthcare er safety and the  that enable medical research and clinical laboratories.    Responsibilities:   The conducted analysis in assessing customer consuming behaviors and discover the value ofcustomers with RMF analysis, applied customer segmentation with clustering algorithms such as K- Means Clustering and HierarchicalClustering.   Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries toperform data extraction and merging from Oracle.   Involved in managing backup and restoring data in the live Cassandra Cluster.   Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.  Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.   Developed personalized product recommendation with Machine learning algorithms, includingGradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers.   Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net.   Evaluated parameters with K-Fold Cross Validation and optimized performance of models.   ed on benchmarking Cassandra Cluster using the Cassandra stress tool.   A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, LINUX Commands, Python programming, No SQL.   ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.   Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms.   Determined customer satisfaction and helped enhance customer using NLP.   Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.   Performed datavisualization and Designeddashboards with Tableau and D3.js and providedcomplexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders.    Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce. Data Analyst ZETA Interactive December 2012 to February 2014 Description: We're Zeta Global, a data-driven marketing technology company that helps brands acquire more customers, keep the ones they have for longer and grow their value. We create personalized customer experiences by leveraging strategy, analytics, machine learning, and awardwinning creative that ignite a perpetual dialogue between brands and their customers.    Responsibilities:   Used SAS Proc SQLpass-throughfacility to connect to Oracle tables and created SAS datasets usingvarious SQL joins such as left join, right join, inner join and full join.   Performing data validation, transforming data from RDBMS oracle to SAS datasets.   Produce quality customized reports by using PROC TABULATE, PROC REPORT Styles, and ODS RTFand provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE.   Developed SAS macros for data cleaning, reporting and to support routing processing.   Performed advanced querying using SAS Enterprise Guide, calculating computed columns, usingafilter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets.   Involved in Developing, Debugging, and validating the project-specific SAS programs to generatederived SAS datasets, summary tables, and data listings according to study documents.   Created datasets as per the approved specification collaborated with project teams to completescientific reports and review reports to ensure accuracy and clarity.   Experienced in ing with data modelers to translate business rules/requirements into conceptual/logical dimensional models and ed with complex de-normalized and normalized data models   Performed different calculations like Quick table calculations, Date Calculations, Aggregate Calculations, String and Number Calculations.   Created action filters, user filters, parameters and calculated sets for preparing dashboards andsheets in Tableau.   Used the dynamic SQL to perform some pre-and post-session task required while performing Extraction, transformation, and loading.   Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracledatabase   Expertise in Agile Scrum Methodology to implement project life cycles of reports design anddevelopment   Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actionsetc. and published them on the web.   Gathering business requirements, creating business requirement documents (BRD /FRD)    closely with business leaders and users to define and design the data sources requirementsand data access Code, test, identify, implement and document technical solutions utilizing JavaScript, PHP&MySQL.   ing with themanager to prioritize requirements and preparing reports on theweekly and monthlybasis.    Environment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio. Data Analyst Karvy Global Services January 2011 to November 2012 Description:: Karvy Global Services is a specialist knowledge process outsourcing services firm and an arm of the Karvy Group, one of India's largest integrated financial services companies Responsibilities:   Participated in requirement gathering sessions with business stakeholders to understand the projectgoals and documented the business requirement documents (BRD)   Studied the Requirements Specifications, Use Cases and analyzed the data needs of the Businessusers.   Implemented conceptual and logical data models using Erwin 7.2 by adopting agile methodologies asper organization standards.   Redesigned some of the previous models by adding some new entities and attributes as per thebusiness requirements.   Converted the Logical data models to Physical data models to generate DDL scripts.   Reverse Engineered existed data models for analyzing and comparing the business process.   Expertise in the Forward Engineering of logical models to generate the physical model using Erwin.  Created the Logical data models using Erwin 7.2 and ensured that it follows the normalization rules and have the best possible traceability pattern.   Migrated several models from Erwin 4.1/7.1 to ERWIN 7.2 and updated the previous namingstandards.   Extensively ed with enterprise data warehouse development by building data marts, staging,and restaging.   Scheduled reports for daily, weekly, monthly reports for executives, Business analyst and customerrepresentatives for various categories and regions based on business needs using SQL Server Reporting Services (SSRS)   ed with business users to understand metric definitions, presentation, and user needs. Environment: Erwin, Informatica, Cognos, Oracle 9i, SQL Server 2003, SQL, MS Office, Windows 2003. Education  Bachelors of Science in Computer science Engineering GITAM University 2011 Skills / IT Skills  SQL (8 years), DATABASE (6 years), INFORMATICA (6 years), SAS (5 years), Serial Attached SCSI (5 years) Additional Information  TECHNICAL SKILLS:    Languages HTML5, DHTML, WSDL, CSS3, C, C++, XML, R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot), python  Software/Libraries Keras, Caffe, TensorFlow, OpenCV, Scikit-learn, Pandas, NumPy, Microsoft Visual Studio, Microsoft Office.  Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans. Packages ggplot2, caret, dplyr, RWeka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, Matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy.    Machine Learning Algorithms  Neural Nets, Decision trees, Support Vector Machines, Random forest, Convolutional Neural Nets, Logistic Regression, PCA, K- means, KNN.    Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall  Database SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase, Teradata, Netezza, Mongo DB, Cassandra.  Reporting  MS Office (Word/Excel/PowerPoint/ Visio/Outlook), Crystal Reports XI, SSRS, Cognos 7.0/6.0.  Big Data  Hadoop, Hive, HDFS, Map Reduce, Pig.  BI   Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1.    Database Design  and Data Modeling  MS Visio, ERWIN 4.5/4.0, Star Schema/Snowflake Schema modeling, Fact & Dimensions tables, physical & logical data modeling, Normalization and De-normalization techniques, Kimball &Inmon Methodologies"
"
 :
* Around 4+ years of experience as Data scientist with strong  expertise in implementing advanced Machine Learning and Natural Language Processing algorithms upon data from diverse domains and building highly efficient models to derive actionable insights for business environments leveraging exploratory data analysis, feature engineering, statistical modelling and predictive analytics
* Experience with Statistical Analysis, Data Mining and Machine Learning  using Python and SQL
* Deep sighted knowledge in Data Structures & Algorithms, Statistics, Pattern recognition and Predictive modeling
* Strong mathematical knowledge and hands on experience in implementing Machine Learning algorithms like K-Nearest Neighbors, Logistic Regression, Linear regression, Naïve Bayes, Support Vector Machines, Decision Trees, Random Forests, Gradient Boosted Decision Trees, Stacking Models.
* Expert knowledge in breadth of Machine Learning algorithms with an ability to evaluate and choose best suited algorithm, perform feature selection and optimize machine learning model
* Experience in using various packages in R and python like ggplot2, caret, dplyr, Rweka, gmodels, RCurl, tm, C50, twitteR, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, BeautifulSoup, Rpy2. 
* ed on various applications using Python integrated IDEs such as Anaconda and Py Charm.
* Valuable experience ing with large datasets and Deep Learning algorithms with Tensor Flow.
* Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
* Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of BigDataEco-system. 
* Strong experience and knowledge in Data Visualization with Tableau creating Line and scatterplots, BarCharts, Histograms, Piechart, Dot charts, Boxplots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc. 
* Created data visualizations with Tableau for publishing and presenting dashboards, Storyline on web and desktop platforms.
* Skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, Remap, merge, subset, reindex, melt and reshape.
* Experienced with tuning parameters for different machine learning models to improve performance.
* Excellent communication . Successfully ing in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner

 and :

Languages
C, java Script, R, Python, Matlab

Databases
MS SQL Server,  Oracle, HBase, Amazon Redshift, MS SQL
Statistical Methods:
Hypothetical Testing, Exploratory Data Analysis (EDA), Confidence Intervals, Bayes Law, Principal Component Analysis (PCA), Dimensionality Reduction, Cross-Validation, Auto-correlation
Machine Learning
Regression analysis, Naïve Bayes, Decision Tree, Random Forests, Support Vector Machine, Neural Net, Sentiment Analysis, Collaborative Filtering, K-Means Clustering, KNN, CNN, RNN and Ada Boosting.
Data Visualization
Tableau, MatPlotLib, Seaborn, ggplot2,d3.js
Packages
ggplot2, caret, dplyr, Rweka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numPy, seaborn, sciPy, matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy.
Hadoop Ecosystem
Hadoop 2.x, Spark 2.x, MapReduce, Hive, HDFS, Pig
Cloud Services
Amazon Web Services (AWS) EC2/S3/Redshift
Reporting 
Tableau Suite of  10.x, Server and Online, Server Reporting Services(SSRS),  MS Office (Word/Excel/Power Point/ Visio)
Version Control 
SVM, GitHub.
Operating Systems
Microsoft Windows, Linux (Ubuntu), Microsoft Office Suite (Word, PowerPoint, Excel)

:


Client: ZenQ LLC (Dallas, TX)
March2018-Current
Role: Data Scientist
Description: ZenQ is one of the fastest growing digital, business consulting &  service firms. They serve clients across the US, UK, Canada, Australia and Newzealand. This project is for a client related to financial services which includes predicting customer lifetime value modelling (CLV) and sorting the customers of different levels having different credit ranges and increasing their credit limit based on their credit history and usage of credit card. This also includes fraud detection, customer analytics, NLP tasks, Ticket routing techniques, etc.
Responsibilities:
* Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends.
* Personalization, Target Marketing, Customer Segmentation and profiling.
* Performed Data Cleaning, features scaling, featurization, features engineering.
* Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN. 
* Customer segmentation based on their behaviour or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behaviour patterns. 
* The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.
* Analyzed and implemented few research proofs of concept models for Real time fraud detection over credit card and online banking purchases.
* ed with Credit Analysis, Risk modeling algorithms to implement in customer acquisition strategies into the real time business.
* Studied and implemented Fraud detection models to monitor the unconventional purchases from customer bases and alert them with updates.
* Performed Clustering with historical, demographic and behavioral data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device.
* Evaluated models using Cross validation, Log loss function used to measure the performance and used ROC curves and AUC for feature selection.
* Used Principal Component Analysis and t-SNE in feature engineering to analyze high dimensional data.
* Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by using L1 and L2 Regularization.
* Used Spark's Machine learning library to build and evaluate different models.

Environment: Python 2.x,Linux, Spark, AWS, Tableau Desktop, SQL Server 2012,NLP, Cluster Analysis, Random Forest, Microsoft Excel, Spark SQL, PySpark, Teradata.



Client: METMOX (Schaumburg, IL)
Sep2017-Feb2018

Role: Data Scientist

Description: METMOX builds custom tailored solutions. Whether you need security, net operations, server management, SAP, or software development. The company has several clients with businesses around in 59 countries. The company operates with respect to the business requirements and timeline of clients. The project aimed at assisting the Fraud Investigations team to improve the accuracy of identifying fraudulent transactions and the Loan Compliance team to identify fraudulent loan applications using advanced data analytic approaches and machine learning models.

Responsibilities:
* Extensively involved in all phases of data acquisition, data collection, data cleaning, model development, model validation, and visualization to deliver data science solutions.
* Built machine learning models to identify fraudulent applications for loan pre-approvals and to identify fraudulent credit card transactions using the history of customer transactions with supervised learning methods.
* Extracted data from database, copied into HDFS File system and used Hadoop  such as Hive and Pig Latin to retrieve the data required for building models.
* ed on data cleaning and ensured data quality, consistency, integrity using Pandas, NumPy.
* Tackled highly imbalanced Fraud dataset using sampling techniques like down-sampling, up-sampling and SMOTE (Synthetic Minority Over-Sampling Technique) using Python Scikit-learn.
* Used PCA and other feature engineering techniques to reduce the high dimensional data, feature normalization techniques and label encoding with Scikit-learn library in Python.
* Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models such as Logistic regression, Gradient Boost Decision Tree and Neural Net.
* Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting.
* Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods.
* Implemented a Python-based distributed random forest via PySpark and MLlib.
* Used AWS S3, DynamoDB, AWS lambda, AWS EC2 for data storage and models' deployment.
* Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau.

Environment: Python 2.x, CDH5, ML, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, MATLAB, Spark SQL, PySpark.



Client: TA Digital (Hyderabad, Telangana)
May2015-july2017

Role: Data Scientist
Description: TA Digital is a provider of full-service Digital Transformation solutions and services for enterprise companies with businesses across Asia, United States, Canada and the United Kingdom. It provides a variety of services in Information , Financial Services, Transportation, Retail, Automotive, Manufacturing, Utilities, Chemical, Healthcare, Pharmaceuticals, & Biotech. I ed for various in-house projects which handles customer analytics, NLP tasks, OCR models, Visualizations etc.

Responsibilities:
 Initial  started as a Mainframe Developer which changed designing the t Data Architecture.
 Involved in designing the Physical Data Architecture model of Machine to Machine Model based on Consumer Model.
 Improved the efficiency of processing to achieve a defect-free bulk order for New Multiple Connections, under VISION System.
 ed with consumers and different teams to gain insights about the data concepts behind their business.
 Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements.
 Involved in initial data pattern recognition and data cleaning using dplyr package in R.
 Developed Tabulation datasets and Analysis datasets as per the specifications.
 Coordinate with the team members to reach the target successfully.
 Identified, reviewed and documented the business requirements for pricing calculation and billing processing.
 Responsible for periodic Reporting using Reports and Graphs in TABLEAU and EXCEL.
 Compared actual results to expected results and recorded test results.
 Involved in coordinating testing activities with different testing and development teams.
 Implemented Checkpoints for Back-end Testing
 Created several efficacy tables like Summaries of Best Response etc. and determined survival analysis by using proc Life Test.
 Determined the missing data, outlier and invalid data and applied appropriate data management techniques.
 Analyzed different trends and market segmentation based on historical data using K means clustering, Classification techniques.
 Created dashboard and stories for senior managers.
 Involved in creating dashboards and reports in Tableau 8.1.1 and Maintaining server activities, user activity, and customized views on Server Analysis.
 Created Rich Graphic visualization/dashboards in Tableau to enable a fast read on claims and key business drivers and to direct attention to key area
Environment: Python, R, R Studio, DB2, OLAP, OLTP, Multi-dimensional modelling, Data Warehousing, SQL, Microsoft Office, Tableau 8.1",Data Scientist,resume,"  : * Around 4+ years of experience as Data scientist with strong  expertise in implementing advanced Machine Learning and Natural Language Processing algorithms upon data from diverse domains and building highly efficient models to derive actionable insights for business environments leveraging exploratory data analysis, feature engineering, statistical modelling and predictive analytics * Experience with Statistical Analysis, Data Mining and Machine Learning  using Python and SQL * Deep sighted knowledge in Data Structures & Algorithms, Statistics, Pattern recognition and Predictive modeling * Strong mathematical knowledge and hands on experience in implementing Machine Learning algorithms like K-Nearest Neighbors, Logistic Regression, Linear regression, Naïve Bayes, Support Vector Machines, Decision Trees, Random Forests, Gradient Boosted Decision Trees, Stacking Models. * Expert knowledge in breadth of Machine Learning algorithms with an ability to evaluate and choose best suited algorithm, perform feature selection and optimize machine learning model * Experience in using various packages in R and python like ggplot2, caret, dplyr, Rweka, gmodels, RCurl, tm, C50, twitteR, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, BeautifulSoup, Rpy2.  * ed on various applications using Python integrated IDEs such as Anaconda and Py Charm. * Valuable experience ing with large datasets and Deep Learning algorithms with Tensor Flow. * Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian, XGBoost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression-based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.  * Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of BigDataEco-system.  * Strong experience and knowledge in Data Visualization with Tableau creating Line and scatterplots, BarCharts, Histograms, Piechart, Dot charts, Boxplots, Time series, Error Bars, Multiple Charts types, Multiple Axes, subplots etc.  * Created data visualizations with Tableau for publishing and presenting dashboards, Storyline on web and desktop platforms. * Skilled in performing data parsing, data manipulation and data preparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, Remap, merge, subset, reindex, melt and reshape. * Experienced with tuning parameters for different machine learning models to improve performance. * Excellent communication . Successfully ing in fast-paced multitasking environment both independently and in collaborative team, a self-motivated enthusiastic learner   and :  Languages C, java Script, R, Python, Matlab  Databases MS SQL Server,  Oracle, HBase, Amazon Redshift, MS SQL Statistical Methods: Hypothetical Testing, Exploratory Data Analysis (EDA), Confidence Intervals, Bayes Law, Principal Component Analysis (PCA), Dimensionality Reduction, Cross-Validation, Auto-correlation Machine Learning Regression analysis, Naïve Bayes, Decision Tree, Random Forests, Support Vector Machine, Neural Net, Sentiment Analysis, Collaborative Filtering, K-Means Clustering, KNN, CNN, RNN and Ada Boosting. Data Visualization Tableau, MatPlotLib, Seaborn, ggplot2,d3.js Packages ggplot2, caret, dplyr, Rweka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numPy, seaborn, sciPy, matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy. Hadoop Ecosystem Hadoop 2.x, Spark 2.x, MapReduce, Hive, HDFS, Pig Cloud Services Amazon Web Services (AWS) EC2/S3/Redshift Reporting  Tableau Suite of  10.x, Server and Online, Server Reporting Services(SSRS),  MS Office (Word/Excel/Power Point/ Visio) Version Control  SVM, GitHub. Operating Systems Microsoft Windows, Linux (Ubuntu), Microsoft Office Suite (Word, PowerPoint, Excel)  :   Client: ZenQ LLC (Dallas, TX) March2018-Current Role: Data Scientist Description: ZenQ is one of the fastest growing digital, business consulting &  service firms. They serve clients across the US, UK, Canada, Australia and Newzealand. This project is for a client related to financial services which includes predicting customer lifetime value modelling (CLV) and sorting the customers of different levels having different credit ranges and increasing their credit limit based on their credit history and usage of credit card. This also includes fraud detection, customer analytics, NLP tasks, Ticket routing techniques, etc. Responsibilities: * Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends. * Personalization, Target Marketing, Customer Segmentation and profiling. * Performed Data Cleaning, features scaling, featurization, features engineering. * Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN.  * Customer segmentation based on their behaviour or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behaviour patterns.  * The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers. * Analyzed and implemented few research proofs of concept models for Real time fraud detection over credit card and online banking purchases. * ed with Credit Analysis, Risk modeling algorithms to implement in customer acquisition strategies into the real time business. * Studied and implemented Fraud detection models to monitor the unconventional purchases from customer bases and alert them with updates. * Performed Clustering with historical, demographic and behavioral data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device. * Evaluated models using Cross validation, Log loss function used to measure the performance and used ROC curves and AUC for feature selection. * Used Principal Component Analysis and t-SNE in feature engineering to analyze high dimensional data. * Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by using L1 and L2 Regularization. * Used Spark's Machine learning library to build and evaluate different models.  Environment: Python 2.x,Linux, Spark, AWS, Tableau Desktop, SQL Server 2012,NLP, Cluster Analysis, Random Forest, Microsoft Excel, Spark SQL, PySpark, Teradata.    Client: METMOX (Schaumburg, IL) Sep2017-Feb2018  Role: Data Scientist  Description: METMOX builds custom tailored solutions. Whether you need security, net operations, server management, SAP, or software development. The company has several clients with businesses around in 59 countries. The company operates with respect to the business requirements and timeline of clients. The project aimed at assisting the Fraud Investigations team to improve the accuracy of identifying fraudulent transactions and the Loan Compliance team to identify fraudulent loan applications using advanced data analytic approaches and machine learning models.  Responsibilities: * Extensively involved in all phases of data acquisition, data collection, data cleaning, model development, model validation, and visualization to deliver data science solutions. * Built machine learning models to identify fraudulent applications for loan pre-approvals and to identify fraudulent credit card transactions using the history of customer transactions with supervised learning methods. * Extracted data from database, copied into HDFS File system and used Hadoop  such as Hive and Pig Latin to retrieve the data required for building models. * ed on data cleaning and ensured data quality, consistency, integrity using Pandas, NumPy. * Tackled highly imbalanced Fraud dataset using sampling techniques like down-sampling, up-sampling and SMOTE (Synthetic Minority Over-Sampling Technique) using Python Scikit-learn. * Used PCA and other feature engineering techniques to reduce the high dimensional data, feature normalization techniques and label encoding with Scikit-learn library in Python. * Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models such as Logistic regression, Gradient Boost Decision Tree and Neural Net. * Used cross-validation to test the models with different batches of data to optimize the models and prevent overfitting. * Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging and Boosting methods. * Implemented a Python-based distributed random forest via PySpark and MLlib. * Used AWS S3, DynamoDB, AWS lambda, AWS EC2 for data storage and models' deployment. * Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau.  Environment: Python 2.x, CDH5, ML, HDFS, Hadoop 2.3, Hive, Impala, Linux, Spark, Tableau Desktop, SQL Server 2012, Microsoft Excel, MATLAB, Spark SQL, PySpark.    Client: TA Digital (Hyderabad, Telangana) May2015-july2017  Role: Data Scientist Description: TA Digital is a provider of full-service Digital Transformation solutions and services for enterprise companies with businesses across Asia, United States, Canada and the United Kingdom. It provides a variety of services in Information , Financial Services, Transportation, Retail, Automotive, Manufacturing, Utilities, Chemical, Healthcare, Pharmaceuticals, & Biotech. I ed for various in-house projects which handles customer analytics, NLP tasks, OCR models, Visualizations etc.  Responsibilities:  Initial  started as a Mainframe Developer which changed designing the t Data Architecture.  Involved in designing the Physical Data Architecture model of Machine to Machine Model based on Consumer Model.  Improved the efficiency of processing to achieve a defect-free bulk order for New Multiple Connections, under VISION System.  ed with consumers and different teams to gain insights about the data concepts behind their business.  Analyzed business requirements, system requirements, data mapping requirement specifications, and responsible for documenting functional requirements and supplementary requirements.  Involved in initial data pattern recognition and data cleaning using dplyr package in R.  Developed Tabulation datasets and Analysis datasets as per the specifications.  Coordinate with the team members to reach the target successfully.  Identified, reviewed and documented the business requirements for pricing calculation and billing processing.  Responsible for periodic Reporting using Reports and Graphs in TABLEAU and EXCEL.  Compared actual results to expected results and recorded test results.  Involved in coordinating testing activities with different testing and development teams.  Implemented Checkpoints for Back-end Testing  Created several efficacy tables like Summaries of Best Response etc. and determined survival analysis by using proc Life Test.  Determined the missing data, outlier and invalid data and applied appropriate data management techniques.  Analyzed different trends and market segmentation based on historical data using K means clustering, Classification techniques.  Created dashboard and stories for senior managers.  Involved in creating dashboards and reports in Tableau 8.1.1 and Maintaining server activities, user activity, and customized views on Server Analysis.  Created Rich Graphic visualization/dashboards in Tableau to enable a fast read on claims and key business drivers and to direct attention to key area Environment: Python, R, R Studio, DB2, OLAP, OLTP, Multi-dimensional modelling, Data Warehousing, SQL, Microsoft Office, Tableau 8.1"
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Vice President
UConn Consulting Group
September 2018 to Present
 Establishes and maintains relationships with internal and external clients and service provider  Partners with the executive team in formulating and implementing action plans for the clubs enhancement and sustainability  
 Drives UGC's strategic planning initiatives 
 Manages relationships with the UGC and UConn alumni  
 Facilitates the smooth running of logistics  
 Help in laying out the rules of the club in consultation with the entire leadership team 
 
Data Analytics Consultant
Feel Good Lab - Hartford, CT
September 2018 to Present
 Tracking and reporting key performance metrics, traffic behaviors and campaign performance usingGoogle Analytics 
 Strategizing the product line by market analysis and Maintaining client databases and dashboards tomonitor sales pipeline
Sr. Data Analyst
Khel Academy
September 2017 to July 2018
 Optimized the marketing campaigns by building/evaluating/eliminating segments of frequentlytargeted and suppressed populations(participants) for tournaments 
 Improved e-commerce website traffic through data analysis using Google Analytics, A/B testing by60% 
 Performed customer segmentation using Machine learning algorithms on data coming from differentmarketing campaigns, increasing participants engagement in various sports tournaments by 32%
Analyst
TEKSystems Global Services
July 2015 to June 2016
 Automated analytical reports using SQL and business intelligence  (OBIEE, ODI), performedregression analysis for end to end process for prediction & forecasting of sales, resulting in revenue boost by 5% 
 Generated and analyzed metrics to track daily status of various  integrating data dumpsthrough Tableau-based dashboard, achieving a man-hour saving of 10 hours/week, garnered appreciation from the higher management


Master of Science in Business Analytics in Analytics & Project Management
University of Connecticut School of Business December 2019
Bachelor's in Electronics and Communication Engineering
National Institute of  Surathkal
July 2011 to May 2015


Python (2 years), SQL (2 years), Javascript (3 years), Java, R (2 years)
Awards

 Certificate, Awards & Achievements
Received Best Employee of Quarter (highest employee recognition award) for outstanding performance in Khel Academy
Certifications/Licenses

 Certificate, Awards & Achievements
Additional Information

 ing as Vice President - Strategy and Operation at UCONN Graduate Consulting Group at UCONN- School of Business 
 Received Best Employee of Quarter (highest employee recognition award) for outstandingperformance in Khel Academy 
 Initiated and organized various CSR activities at TEKSystems Global Services 
 Secured 99.2 percentile (out of 1.1 Million participants) in All India Engineering Entrance Exam 2011",Data Scientist,resume,"Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Vice President UConn Consulting Group September 2018 to Present  Establishes and maintains relationships with internal and external clients and service provider  Partners with the executive team in formulating and implementing action plans for the clubs enhancement and sustainability    Drives UGC's strategic planning initiatives   Manages relationships with the UGC and UConn alumni    Facilitates the smooth running of logistics    Help in laying out the rules of the club in consultation with the entire leadership team    Data Analytics Consultant Feel Good Lab - Hartford, CT September 2018 to Present  Tracking and reporting key performance metrics, traffic behaviors and campaign performance usingGoogle Analytics   Strategizing the product line by market analysis and Maintaining client databases and dashboards tomonitor sales pipeline Sr. Data Analyst Khel Academy September 2017 to July 2018  Optimized the marketing campaigns by building/evaluating/eliminating segments of frequentlytargeted and suppressed populations(participants) for tournaments   Improved e-commerce website traffic through data analysis using Google Analytics, A/B testing by60%   Performed customer segmentation using Machine learning algorithms on data coming from differentmarketing campaigns, increasing participants engagement in various sports tournaments by 32% Analyst TEKSystems Global Services July 2015 to June 2016  Automated analytical reports using SQL and business intelligence  (OBIEE, ODI), performedregression analysis for end to end process for prediction & forecasting of sales, resulting in revenue boost by 5%   Generated and analyzed metrics to track daily status of various  integrating data dumpsthrough Tableau-based dashboard, achieving a man-hour saving of 10 hours/week, garnered appreciation from the higher management   Master of Science in Business Analytics in Analytics & Project Management University of Connecticut School of Business December 2019 Bachelor's in Electronics and Communication Engineering National Institute of  Surathkal July 2011 to May 2015   Python (2 years), SQL (2 years), Javascript (3 years), Java, R (2 years) Awards   Certificate, Awards & Achievements Received Best Employee of Quarter (highest employee recognition award) for outstanding performance in Khel Academy Certifications/Licenses   Certificate, Awards & Achievements Additional Information   ing as Vice President - Strategy and Operation at UCONN Graduate Consulting Group at UCONN- School of Business   Received Best Employee of Quarter (highest employee recognition award) for outstandingperformance in Khel Academy   Initiated and organized various CSR activities at TEKSystems Global Services   Secured 99.2 percentile (out of 1.1 Million participants) in All India Engineering Entrance Exam 2011"
":
 11 years of hands on experience and comprehensive industry knowledge in Data science/Analyst in the areas of -- Data Mining, Machine Learning, Statistical Modeling, Data Modeling, Business Intelligence, Data Visualization, solving real-world practical business problems. 
 ed with various stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions. 
 Mined and analyzed data from company databases to drive optimization to day to day business processes, product development and business strategies.
  ing experience in Machine Learning algorithms such as linear regression, segmentation and modelling, logistic regression, Tree Algorithms - Decision Trees, Random Forest, Gradient Boosting, XG Boost, Clustering, Neural Nets - RNN, LSTM, CNN, Text Analytics - TF-IDF, LDA, Time Series Analysis - ARIMA, ARCH, GARCH, Survival Analysis, Sentimental Analysis, ANOVA, Bayesian Statistics, Reinforcement Learning, Deep Learning, Dimension Reduction - PCA (Principal Compoonent Analysis)
 Experience in using Python & R libraries like Numpy, SciPy, Pandas, Matplotlib, Scikit-learn, Beautiful Soup, Caret, forecast, xgboost, tidy text, dplyr, ggplot2, Shiny, bokeh, Seaborn, SparkContext, Tensor- Flow, genism Microsoft Azure
 Deep understanding of Software Development Life Cycle (SDLC) as well as Agile/Scrum methodology to accelerate Software Development iteration.
 Proficient in SAS, MATLAB and Hands on experience in writing queries in SQL (Teradata, MySQL) and Informatica to extract, transform and load (ETL) and Visualization using Tableau
 Used predictive modeling to optimize current processes and improve corrective action timeliness. Co-ordinated with different functional teams to implement models and monitor outcomes.
 Experience in designing visualizations using Tableau and Power BI software and Storyline on web and desktop platforms, publishing and presenting dashboards. 
  expertise and business acumen necessary to translate business requirements and s into scalable, highly resilient and successful system solutions. 
 Proficient in managing end to end data science life cycle and building end to end data pipelines of Machine Learning models including Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Statistical Modeling, Testing and Validation, Visualization and Reporting the insights. 
 Strong knowledge of Statistical methodologies such as Hypothesis Testing, T-tests, ANOVA, Monto Carlo Sampling, Time Series Analysis. Developed Machine Learning/Statistical models in R and Python using various Supervised and Unsupervised Machine Learning algorithms  Regression, Classification, Clustering, Dimension Reduction, Association Rule Learning, and Natural Language Processing.
 Proficient in Machine Learning techniques -- Decision Trees, Linear/Logistic Regression, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.
 Expertise in Data Integration, Data Cleaning, Data Analysis and Profiling, Data Import and Export using multiple ETL  such as SQL Server, SSIS and SSAS. 
 Expertise in using Microsoft Office including using Microsoft Excel to build Pivot Tables and Visualizations. 
 Strong knowledge in all phases of SDLC (Software Development life cycle) from Analysis, Design, Development, Testing, Implementation and Maintenance. 
 Great exposure in interacting with end-users to gather and document the requirements, project planning and scheduling.  Actively participated in creation and implementation of Test plans. 
 Strong business sense and abilities to communicate data insights to both  and non clients. ed in both Agile and Waterfall implementations.


 

Machine Learning
Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Random Forest, K-Nearest Neighbor, Naïve Bayes, Support Vector Machines, Gradient Boosting, Bayesian models, Ensemble Methods, Principal Component Analysis, Association Rules, Factor Analysis, Cluster analysis - K-means / Hierarchical, Market Basket Analysis.
Statistics
Statistical tests: T-tests, Chi-square analysis, Correlation tests, A/B testing, Normality tests, Residual diagnostics, ANOVA
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural 
Programming
R(caret, glmnet, xgboost, dplyr, survival, rpart, ggplot, dply)
Python (numpy, pandas, scikit-learn, scipy, matplotlib, seaborn, tensor-flow, Keras)
SQL, Spark, Scala, Java
Data Visualization
Tableau, PowerBI, R (ggplot2), Python (Matplotlib, Seaborn)
Database / ETL Querying
SQL Server, Postgres, MySQL, SSIS, SSAS
DDL/DML statements, Subqueries, Joins, Normalization, Entity Relationship Diagrams, Star schema / Snow flake schema (Fact and Dimension tables) 
Cloud 
Azure
Others
Git, Software Development Life Cycle, Agile, IT Project Management, Prototyping, Gathering Functional and Non-Functional Requirements identification and analysis, SWOT Analysis, Root cause analysis, Dataflow diagrams, Use case diagrams,  Documentation,


:







Client: Discover                                                                                                                   Riverwoods, IL 
Data Scientist                                                                                                                                                Apr 2018-Present                                                                                                                                                    

Credit Card Fraud detection:		
 Designed and developed features related to fraudulent transactions. Built models to estimate the likelihood of credit card transactions being fraudulent by training artificial neural nets and other tree-based methods
 Deployed the models in production systems and a developed visualization to monitor several fraud KPIs, model accuracy charts like - Precision-Recall curve, confusion matrix, etc.

No Pay on Loan Prediction:
 Built Predictive models that analyzed large volumes of data regarding historical payments and then accurately determine which customers are unlikely to make their next payment.
 This valuable information can be used to prioritize bank staff to focus on high-risk customers, 
prepare the client for upcoming defaults and as an early-indicator for identifying potential issues with the underwriting process.

Credit default rates Prediction:
 Operationalized real time loan assessments by building models that predicted the likelihood of future defaults.
 Utilized past information of borrowers default rates to predict the likelihood of default for future borrowers. This made real time loan approval process easy allowing business to scale up and expand their loan portfolios.    
                                                                                                 
Market Mix Modeling:
? Developed and refined complex marketing mix statistical models in a team environment and ed with diverse functional groups with over $100M in annual marketing spend
? Responsible for all stages in the modeling process, from collecting, verifying, & cleaning data to visualizing model results, presenting results, and making client recommendations

Credit Risk Scoring :
? Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex  analysis into easily digestible reports for top executives in the company.
? Developed several interactive dashboards in Tableau to visualize nearly 2 Terabytes of credit data by designing a scalable data cube structure.    



Client: United Health Care Ltd                                                                                                     Mountain View, CA               
Data Scientist                                                                                                                                            May 2016- Apr 2018                                                                                                                                                                                       

Predicting Fraudulent claims:
 Developed a classification model that predicted the likelihood that a claim is fraudulent. Created a rank ordered queue of claims for fraud units to investigate with results from the model. 
 Fraud units are used to create a databased queue, investigating only those incidents that likely to require it.
 Used historical data with previous fraudulent for training the machine learning models using Algorithm like Random forest and Xgboost algorithms and used various statistical techniques like SMOTE Analysis to overcome class imbalance problem.
Claim Amount Prediction:
 Forecasted the ultimate claim amount based on the claim characteristics at the time the claim was filed. Built extremely accurate predictive models that lead to a better understanding of how much a claim will ultimately cost.
 This in turn helped the business to have an over view of how much to reserve for incurred but not reported loss amounts.
 The predicted developed loss for each claim was further used as a dependent variable for developing a pricing model.
Modelling claim Autopayment:
 Determined which claims need to be manually inspected and which claims can be auto payed using machine learning to build extremely accurate models.
 By auto paying claims where manual inspection provides little or no value helped business to close the claims quicker, driving their costs down and enhanced customer satisfaction.      
Risk Assessment:
 Used of predictive modeling to proactively identify potential customers who are at highest risk of poor health outcomes and will benefit most from intervention is one solution believed to improve risk management for providers transitioning to value-based payment.
 Created risk scores based on lab testing, biometric data, claims data, patient-generated health data, and the social determinants of health can give healthcare providers insight into which individuals might benefit from enhanced services or wellness activities.



Client: Kroger                                                                                                                                                Cincinnati, OH                                                                                                   
Data Scientist/Analyst                                                                                                                            Oct 2014- April 2016

Product Recommendation:
? Recommending Visually Similar Products Using Content Based Features using several factors like title similarity, image similarity, product description.  
? Implemented image similarity using VGG-16 architecture and sci-kit image packages in python to improve and personalize the customer experience.
 Carton Prediction: 
? Designed Logistics ML Carton Prediction algorithm to predict cartons arriving in a warehouse 10 days in advance. These predictions can be used in warehouses to allocate and plan staffing days in advance. 
? This methodology also standardized the way inbound estimates are calculated versus different methods at different warehouses right now
Price Optimization:
? Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, Kroger made selective and cautious price cuts for certain licensing categories.
Market Basket Analysis:
? Implemented market basket algorithms from transactional data, which helped identify products ordered together frequently. 
? Discovering frequent product sets helped unearth Cross sell and Upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams.
Product Review Analysis:
 Extracted the customer reviews on Kroger products from Yelp and build a sentiment analysis model that extracts the negative reviews from the customer.
 Used NLTK and Sci-kit Learn Libraries to develop the text classifier


Alexion Pharmaceuticals                                                                                                                      NewHaven, CT
Data Analyst                                                                                                                                    May 2012 to Sep 2014
Responsibilities:
 Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality.
 Utilized SAS and SQL to extract data from statewide databases for analysis.
 Acquired data from primary or secondary data sources and maintain databases/ data systems.
 Performed data Analysis using visualization  such as Tableau, Spotfire, and SharePoint to provide insights into the data.
 Configured Azure platform offerings for web applications, business intelligence using Power BI, Azure Data Factory etc.
 Data flow check with source to target mapping of the data.
 Data matrix creation for mapping the with the business requirements.
 Data profiling to cleanse the data in the data base and raise the data issues found.
 Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.
 Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.
 Assisted in mining data from the SQL database that was used in several significant presentations.
 Involved in SSIS packages to extract data from different sources like SQL server, MS Excel, MS Access, transform and then load into Dimension and Fact tables in Data Warehouse using SSIS.


ARIZONA DEPARTMENT OF EDUCATION,						Phoenix, AZ                                                                                                                                            
Role: QA Engineer                                                                  					Sep2007-May2009
Responsibilities: 
 Identified data streams and reviewed data models for testing strategies 
 Written test scripts for back-end validations and Validated mappings of correct and con- ducted data 
 Validating the load process of ETL, the target tables are populated according the data mapping provided that satisfies the transformation rules 
 Writing complex SQL queries using Case Logic, Intersect, Minus, Sub Queries, Inline Views, and Union in Oracle 
 Validating the Archive process to purge the data that meet the defined business rules. 
 Create stored procedures for Incremental extract based on time stamp, version date and record flag 
 Wrote UNIX Shell Scripts and command line utility to interact with Informatica Server 
 Implemented SCD I, II extracts based on the business requirement and client expectations 


TCS, India                                                                                       		Client: XEROX CORPORATION                                                                                                                                                                      Role: QA TESTER     								Jul 2005  Aug 2007                                                       
Responsibilities:  
 Planning and executing test cases, Reported defects in A R Tool 
 Composing and reporting daily and weekly reports 
 Giving creative suggestions to the customer and receives customer input and instructions. 
 Developed Automated Test Scripts Using DAML


Education:
Masters of  (2002-2004), VIT, India.
Bachelors of  (1998- 2002), SKIT, India.
 






",Data Scientist,resume,":  11 years of hands on experience and comprehensive industry knowledge in Data science/Analyst in the areas of -- Data Mining, Machine Learning, Statistical Modeling, Data Modeling, Business Intelligence, Data Visualization, solving real-world practical business problems.   ed with various stakeholders throughout the organization to identify opportunities for leveraging company data to drive business solutions.   Mined and analyzed data from company databases to drive optimization to day to day business processes, product development and business strategies.   ing experience in Machine Learning algorithms such as linear regression, segmentation and modelling, logistic regression, Tree Algorithms - Decision Trees, Random Forest, Gradient Boosting, XG Boost, Clustering, Neural Nets - RNN, LSTM, CNN, Text Analytics - TF-IDF, LDA, Time Series Analysis - ARIMA, ARCH, GARCH, Survival Analysis, Sentimental Analysis, ANOVA, Bayesian Statistics, Reinforcement Learning, Deep Learning, Dimension Reduction - PCA (Principal Compoonent Analysis)  Experience in using Python & R libraries like Numpy, SciPy, Pandas, Matplotlib, Scikit-learn, Beautiful Soup, Caret, forecast, xgboost, tidy text, dplyr, ggplot2, Shiny, bokeh, Seaborn, SparkContext, Tensor- Flow, genism Microsoft Azure  Deep understanding of Software Development Life Cycle (SDLC) as well as Agile/Scrum methodology to accelerate Software Development iteration.  Proficient in SAS, MATLAB and Hands on experience in writing queries in SQL (Teradata, MySQL) and Informatica to extract, transform and load (ETL) and Visualization using Tableau  Used predictive modeling to optimize current processes and improve corrective action timeliness. Co-ordinated with different functional teams to implement models and monitor outcomes.  Experience in designing visualizations using Tableau and Power BI software and Storyline on web and desktop platforms, publishing and presenting dashboards.    expertise and business acumen necessary to translate business requirements and s into scalable, highly resilient and successful system solutions.   Proficient in managing end to end data science life cycle and building end to end data pipelines of Machine Learning models including Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Statistical Modeling, Testing and Validation, Visualization and Reporting the insights.   Strong knowledge of Statistical methodologies such as Hypothesis Testing, T-tests, ANOVA, Monto Carlo Sampling, Time Series Analysis. Developed Machine Learning/Statistical models in R and Python using various Supervised and Unsupervised Machine Learning algorithms  Regression, Classification, Clustering, Dimension Reduction, Association Rule Learning, and Natural Language Processing.  Proficient in Machine Learning techniques -- Decision Trees, Linear/Logistic Regression, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.  Expertise in Data Integration, Data Cleaning, Data Analysis and Profiling, Data Import and Export using multiple ETL  such as SQL Server, SSIS and SSAS.   Expertise in using Microsoft Office including using Microsoft Excel to build Pivot Tables and Visualizations.   Strong knowledge in all phases of SDLC (Software Development life cycle) from Analysis, Design, Development, Testing, Implementation and Maintenance.   Great exposure in interacting with end-users to gather and document the requirements, project planning and scheduling.  Actively participated in creation and implementation of Test plans.   Strong business sense and abilities to communicate data insights to both  and non clients. ed in both Agile and Waterfall implementations.      Machine Learning Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Random Forest, K-Nearest Neighbor, Naïve Bayes, Support Vector Machines, Gradient Boosting, Bayesian models, Ensemble Methods, Principal Component Analysis, Association Rules, Factor Analysis, Cluster analysis - K-means / Hierarchical, Market Basket Analysis. Statistics Statistical tests: T-tests, Chi-square analysis, Correlation tests, A/B testing, Normality tests, Residual diagnostics, ANOVA Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural  Programming R(caret, glmnet, xgboost, dplyr, survival, rpart, ggplot, dply) Python (numpy, pandas, scikit-learn, scipy, matplotlib, seaborn, tensor-flow, Keras) SQL, Spark, Scala, Java Data Visualization Tableau, PowerBI, R (ggplot2), Python (Matplotlib, Seaborn) Database / ETL Querying SQL Server, Postgres, MySQL, SSIS, SSAS DDL/DML statements, Subqueries, Joins, Normalization, Entity Relationship Diagrams, Star schema / Snow flake schema (Fact and Dimension tables)  Cloud  Azure Others Git, Software Development Life Cycle, Agile, IT Project Management, Prototyping, Gathering Functional and Non-Functional Requirements identification and analysis, SWOT Analysis, Root cause analysis, Dataflow diagrams, Use case diagrams,  Documentation,   :        Client: Discover                                                                                                                   Riverwoods, IL  Data Scientist                                                                                                                                                Apr 2018-Present                                                                                                                                                      Credit Card Fraud detection:		  Designed and developed features related to fraudulent transactions. Built models to estimate the likelihood of credit card transactions being fraudulent by training artificial neural nets and other tree-based methods  Deployed the models in production systems and a developed visualization to monitor several fraud KPIs, model accuracy charts like - Precision-Recall curve, confusion matrix, etc.  No Pay on Loan Prediction:  Built Predictive models that analyzed large volumes of data regarding historical payments and then accurately determine which customers are unlikely to make their next payment.  This valuable information can be used to prioritize bank staff to focus on high-risk customers,  prepare the client for upcoming defaults and as an early-indicator for identifying potential issues with the underwriting process.  Credit default rates Prediction:  Operationalized real time loan assessments by building models that predicted the likelihood of future defaults.  Utilized past information of borrowers default rates to predict the likelihood of default for future borrowers. This made real time loan approval process easy allowing business to scale up and expand their loan portfolios.                                                                                                       Market Mix Modeling: ? Developed and refined complex marketing mix statistical models in a team environment and ed with diverse functional groups with over $100M in annual marketing spend ? Responsible for all stages in the modeling process, from collecting, verifying, & cleaning data to visualizing model results, presenting results, and making client recommendations  Credit Risk Scoring : ? Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex  analysis into easily digestible reports for top executives in the company. ? Developed several interactive dashboards in Tableau to visualize nearly 2 Terabytes of credit data by designing a scalable data cube structure.        Client: United Health Care Ltd                                                                                                     Mountain View, CA                Data Scientist                                                                                                                                            May 2016- Apr 2018                                                                                                                                                                                         Predicting Fraudulent claims:  Developed a classification model that predicted the likelihood that a claim is fraudulent. Created a rank ordered queue of claims for fraud units to investigate with results from the model.   Fraud units are used to create a databased queue, investigating only those incidents that likely to require it.  Used historical data with previous fraudulent for training the machine learning models using Algorithm like Random forest and Xgboost algorithms and used various statistical techniques like SMOTE Analysis to overcome class imbalance problem. Claim Amount Prediction:  Forecasted the ultimate claim amount based on the claim characteristics at the time the claim was filed. Built extremely accurate predictive models that lead to a better understanding of how much a claim will ultimately cost.  This in turn helped the business to have an over view of how much to reserve for incurred but not reported loss amounts.  The predicted developed loss for each claim was further used as a dependent variable for developing a pricing model. Modelling claim Autopayment:  Determined which claims need to be manually inspected and which claims can be auto payed using machine learning to build extremely accurate models.  By auto paying claims where manual inspection provides little or no value helped business to close the claims quicker, driving their costs down and enhanced customer satisfaction.       Risk Assessment:  Used of predictive modeling to proactively identify potential customers who are at highest risk of poor health outcomes and will benefit most from intervention is one solution believed to improve risk management for providers transitioning to value-based payment.  Created risk scores based on lab testing, biometric data, claims data, patient-generated health data, and the social determinants of health can give healthcare providers insight into which individuals might benefit from enhanced services or wellness activities.    Client: Kroger                                                                                                                                                Cincinnati, OH                                                                                                    Data Scientist/Analyst                                                                                                                            Oct 2014- April 2016  Product Recommendation: ? Recommending Visually Similar Products Using Content Based Features using several factors like title similarity, image similarity, product description.   ? Implemented image similarity using VGG-16 architecture and sci-kit image packages in python to improve and personalize the customer experience.  Carton Prediction:  ? Designed Logistics ML Carton Prediction algorithm to predict cartons arriving in a warehouse 10 days in advance. These predictions can be used in warehouses to allocate and plan staffing days in advance.  ? This methodology also standardized the way inbound estimates are calculated versus different methods at different warehouses right now Price Optimization: ? Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, Kroger made selective and cautious price cuts for certain licensing categories. Market Basket Analysis: ? Implemented market basket algorithms from transactional data, which helped identify products ordered together frequently.  ? Discovering frequent product sets helped unearth Cross sell and Upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams. Product Review Analysis:  Extracted the customer reviews on Kroger products from Yelp and build a sentiment analysis model that extracts the negative reviews from the customer.  Used NLTK and Sci-kit Learn Libraries to develop the text classifier   Alexion Pharmaceuticals                                                                                                                      NewHaven, CT Data Analyst                                                                                                                                    May 2012 to Sep 2014 Responsibilities:  Develop and implement databases, data collection systems, data analytics and other strategies that optimize statistical efficiency and quality.  Utilized SAS and SQL to extract data from statewide databases for analysis.  Acquired data from primary or secondary data sources and maintain databases/ data systems.  Performed data Analysis using visualization  such as Tableau, Spotfire, and SharePoint to provide insights into the data.  Configured Azure platform offerings for web applications, business intelligence using Power BI, Azure Data Factory etc.  Data flow check with source to target mapping of the data.  Data matrix creation for mapping the with the business requirements.  Data profiling to cleanse the data in the data base and raise the data issues found.  Performed data analysis and data profiling using complex SQL on various sources systems including Oracle and Teradata.  Involved with data profiling for multiple sources and answered complex business questions by providing data to business users.  Assisted in mining data from the SQL database that was used in several significant presentations.  Involved in SSIS packages to extract data from different sources like SQL server, MS Excel, MS Access, transform and then load into Dimension and Fact tables in Data Warehouse using SSIS.   ARIZONA DEPARTMENT OF EDUCATION,						Phoenix, AZ                                                                                                                                             Role: QA Engineer                                                                  					Sep2007-May2009 Responsibilities:   Identified data streams and reviewed data models for testing strategies   Written test scripts for back-end validations and Validated mappings of correct and con- ducted data   Validating the load process of ETL, the target tables are populated according the data mapping provided that satisfies the transformation rules   Writing complex SQL queries using Case Logic, Intersect, Minus, Sub Queries, Inline Views, and Union in Oracle   Validating the Archive process to purge the data that meet the defined business rules.   Create stored procedures for Incremental extract based on time stamp, version date and record flag   Wrote UNIX Shell Scripts and command line utility to interact with Informatica Server   Implemented SCD I, II extracts based on the business requirement and client expectations    TCS, India                                                                                       		Client: XEROX CORPORATION                                                                                                                                                                      Role: QA TESTER     								Jul 2005  Aug 2007                                                        Responsibilities:    Planning and executing test cases, Reported defects in A R Tool   Composing and reporting daily and weekly reports   Giving creative suggestions to the customer and receives customer input and instructions.   Developed Automated Test Scripts Using DAML   Education: Masters of  (2002-2004), VIT, India. Bachelors of  (1998- 2002), SKIT, India.         "
"
Clearance: T3 Secret DOD Clearance (Pending Eligible)

:

* Over 10+ years of experience as Big Data Engineer /Data Engineer and Data Analyst including designing, developing and implementation of data models for enterprise-level applications and systems.
*  IT experience this includes recent experience in Big Data/Hadoop Ecosystem Competence in using various Hadoop components such as MapReduce (MR1),YARN(MR2), HDFS, Pig, Hive, HBase, ZooKeeper, Oozie, Hue Experience in building highly reliable, scalable Big data solutions on Hadoop distributions Cloudera, Horton s, AWS EMR.
* Good experienced in Data Modeling and Data Analysis as a Proficient in gathering business requirements and handling requirements management.
* Pleasant experience ing in Agile/Scrum development environment participated in  discussions with client and contributed to project analysis and development specs.
* Experience in transferring the data using Informatica tool from AWS S3 to AWS Redshift Hands on experience in Normalization (1NF, 2NF, 3NF and BCNF) Denormalization techniques for effective and optimum performance in OLTP and OLAP environments.
* Expertise in moving structured schema data between Pig and Hive using HCatalog.
* Excellent ing experience in Scrum / Agile frame and Waterfall project execution methodologies.
* Experience in migrating the data using Sqoop from HDFS and Hive to Relational Database System and vice-versa according to client's requirement.
* Experience in SQL and good knowledge in PL/SQL programming and developed Stored Procedures and Triggers and Data Stage, DB2, Unix, Cognos, MDM, Hadoop, Pig.
* Experience with RDBMS like SQL Server, MySQL, Oracle and data warehouses like Teradata and Netezza.
* Proficient knowledge and hands on experience in writing shell scripts in Linux.
* Good Experience on importing and exporting the data from HDFS and Hive into Relational Database Systems like MySQL and vice versa using Sqoop.
* Good knowledge on NoSQL Databases including HBase, MongoDB, MapR-DB.
* Installation, configuration and administration experience in Big Data platforms Cloudera Manager of Cloudera, MCS of MapR.
* Strong experience and knowledge of NoSQL databases such as MongoDB and Cassandra.
* Strong Knowledge of Data Warehouse Architecture and Star Schema, Snow flake Schema, FACT and Dimensional Tables.
* Experience in using PL/SQL to write Stored Procedures, Functions and Triggers.
* Excellent  and analytical  with clear understanding of design goals of ER modeling for OLTP and dimension modeling for OLAP.
* Experience ing with Relational Database Management Systems (RDMS) Capable of processing large sets of structured, semi-structured and unstructured data and supporting systems application architecture Good understanding of service-oriented architecture (SOA) and web services like XML and SOAP.
* Experience in object-oriented analysis and design (OOAD), used modelling language (UML) and design patterns.
* Expertise in SQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS) .
* Involve in writing SQL queries, PL/SQL programming and created new packages and procedures and modified and tuned existing procedure and queries using TOAD.
* Good Understanding and experience in Data Mining Techniques like Classification, Clustering, Regression and Optimization.
* Experience in complete project life cycle (design, development, testing and implementation) of Client Server and Web applications.

ADEXEC Austin, TX (Remote)	 Mar' 17 to Present
Lead Data Scientist

Responsibilities: 

* Designed and developed software applications, testing, and building automation .
* Involved in start to end process of Hadoop jobs that used various  such as Sqoop, PIG, Hive, MapReduce, Spark and Shells scripts (for scheduling of few jobs).
* Involved in importing and exporting data between RDBMS and HDFS using Sqoop.
* Performed querying of both managed and external tables created by Hive using Impala.
* Designed/developed tables, views, various SQL queries, stored procedures, functions.
* Involved in PL/SQL code review and modification for the development of new requirements.
* Extracted data from existing data source and performed ad-hoc queries.
* Utilized SAS and SQL extensively for collecting, validating and analyzing the raw data received from the client.
* Executed data extraction programs/data profiling and analyzing data for accuracy and quality.
* Analyzed the data using advanced excel functions like Pivot tables, VLOOK up, visualizations to get the descriptive analysis of the data.
* Created Schema objects like Indexes, Views, and Sequences, triggers, grants, roles, Snapshots.
* Used advanced Microsoft Excel to create pivot tables, and other excel functions to prepare reports and dashboard with user data.
* Maintained numerous monthly scripts, executed on monthly basis, produces reports and submitted on time for business review.
* Developed ad-hoc reports using Crystal reports for performance analysis by business users.
* Implemented the Big Data solution using Hadoop, and hive to pull/load the data into the HDFS system.
* Installed and configured Hadoop and responsible for maintaining cluster and managing and reviewing Hadoop log files.
* Implemented and configured flows using Oozie to automate jobs.
* Migrated the needed data from MySQL in to HDFS using Sqoop and importing various formats of flat files into HDFS.
* As a Sr. Big Data Engineer, provided  expertise and aptitude to Hadoop  as they related to the development of analytics.
* Expertise in writing Hadoop Jobs to analyze data using MapReduce, Hive, and Pig.
* Experience in designing, building and implementing complete Hadoop ecosystem comprising of MapReduce, HDFS, Hive, Pig, HBase, MongoDB, and Spark.
* ed experience in Scrum / Agile frame and Waterfall project execution methodologies.
* Managed data from various file system to HDFS using UNIX command line utilities.
* ed in Azure environment for development and deployment of Custom Hadoop Applications.
* Managed and support of enterprise Data Warehouse operation, big data advanced predictive application development using Cloudera &.
* Extensively ed on Shell scripts for running SAS programs in batch mode on UNIX.
                                                                             
Oracle4u LLC, Dallas, TX.           	Sep' 15 to Jan' 17
Lead Data Scientist

Responsibilities: 

* Participated in requirements sessions to gather requirements along with business analysts and product owners.
* Involved in Agile development methodology active member in scrum meetings.
* Involvement in design, development and testing phases of Software Development Life Cycle (SDLC).
* Installed and configured Hive and also written Hive UDFs and Cluster coordination services through Zookeeper.
* Architected, Designed and Developed Business applications and Data marts for reporting.
* Involved in different phases of Development life including Analysis, Design, Coding, Unit Testing, Integration Testing, Review and Release as per the business requirements.
* Developed Big Data solutions focused on pattern matching and predictive modeling
*  of this project is to build a data lake as a cloud based solution in AWS using Apache Spark.
* Installed and configured Hadoop Ecosystem components.
* ed on implementation and maintenance of Cloudera Hadoop cluster.
* Created Hive External tables to stage data and then move the data from Staging to main tables
* Implemented the Big Data solution using Hadoop, hive and Informatica to pull/load the data into the HDFS system.
* Pulling the data from data lake (HDFS) and massaging the data with various RDD transformations.
* Experience in Server infrastructure development on Gateway, ELB, Auto Scaling, Dynamo DB, Elastic search, Virtual Private Cloud (VPC)
* Involved in Kafka and building use case relevant to our environment.
* Developed Scala scripts, UDF's using both Data frames/SQL and RDD/MapReduce in Spark for Data Aggregation, queries and writing data back into RDBMS through Sqoop.
* Developed Spark code using Scala and Spark-SQL/Streaming for faster processing of data.
* Developed Oozie flow jobs to execute hive, Sqoop and MapReduce actions.
* Provided thought leadership for architecture and the design of Big Data Analytics solutions for customers, actively drive Proof of Concept (POC) and Proof of Technology (POT) evaluations and to implement a Big Data solution.
* Responsible for developing data pipeline using flume, Sqoop and pig to extract the data from weblogs and store in HDFS.
* Imported the data from different sources like HDFS/HBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS.
* Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra.
* Documented the requirements including the available code which should be implemented using Spark, Hive, HDFS, HBase and Elastic Search.
* Developed Spark code using Scala for faster testing and processing of data.
* Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case.
* Apache Hadoop installation & configuration of multiple nodes on AWS EC2 system
* Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is fed to AWS S3.
* Collaborated with Business users for requirement gathering for building Tableau reports per business needs.
* Developed continuous flow of data into HDFS from social feeds using Apache Storm Spouts and Bolts.
* Involved in loading data from Unix file system to HDFS.

Environment: Hadoop 3.0, 3NF, flume 1.8, Sqoop 1.4, pig 0.17, YARN, HDFS, HBase 1.2, Kafka, Scala 2.12, NoSQL, Cassandra 3.11, Elastic Search, MLlib, Teradata 15, Sqoop, MapReduce, UNIX, Zookeeper 3.4

PWC, Irvine, CA	 Jan' 15 to Aug' 15
Sr. Data Scientist

Responsibilities: 

* Installed and configured Apache Hadoop, Hive and Pig environment on the prototype server.
* Configured SQL database to store Hive metadata.
* Loaded unstructured data into Hadoop File System (HDFS).
* Created ETL jobs to load Twitter JSON data and server data into MongoDB and transported MongoDB into the Data Warehouse.
* Created reports and dashboards using structured and unstructured data.
* Performed SQL tuning Advisor for tuning of SQL queries and database systems.
* Created User accounts, Roles and granting required access permissions and privileges to the users.
* Partitioning and re-organization of table and indexes.
* Responsible in configuring and backing up database using RMAN, Hot backups, Cold backups and Logical backups.
* Written documentation for backups and cloning of database for future reference.
* Migrated databases from HP-UX to IBM AIX platforms.
* Continuously monitored database performance during batch jobs running on the database using AWR report and OEM.
* Used Data pump to take the logical backups of databases.
* ed on ASM instances for 11g databases.
* Installed and configured 12c software and database.
* ed on pluggable and container 12c database model.
* Performed other DBA activities including space management and performance monitoring.

Environment: HP-UX, Big Data Database Version 10g/11g, RMAN, Data guard, RAC, ASM, Oracle Enterprise Manager Grid Control 12c, Opatch, Shell Scripting.

Ford Inc, Detroit MI ( Remote)	Jul' 14 to Dec' 14
Sr. Hadoop Developer

Responsibilities:

* Cluster capacity planning along with operations team and management team and Cluster maintenance as well as creation and removal of nodes, HDFS support and maintenance.
* Manage and review Hadoop log files, File system management and monitoring.
* Involved in Cluster upgrade and required jobs are modified.
* Involved in implementing security on Hadoop Cluster with Kerberos by ing along with operations team to move non secured cluster to secured cluster.
* Data migration from RDMS to hadoop using sqoop for analysis and implemented Oozie jobs for automatic data imports from source.
* Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries.
* To analyze data migrated to HDFS, used Hive data warehouse tool and developed Hive queries.
* Created external tables with proper partitions for efficiency and loaded the structured data in HDFS resulted from MR jobs.
* Implemented Hive UDF for comprehensive data analysis.
* Responsible for troubleshooting MapReduce jobs by reviewing the log files.
* Involved in importing the real time data to hadoop using Kafka and implemented the Oozie job for daily imports.
* Involved with various teams on and offshore for understanding of the data that is imported from their source.
* Developed generic Shell scripts to automate Sqoop job by passing parameters for data imports.
* Involved in data visualization and provided the files required for the team by analysing the data in hive and developed Pig scripts for advanced analytics on the data.
* As a part of POC used the Amazon AWS S3 as an underlying file system for the Hadoop and implemented the elastic Map-Reduce jobs on the data in S3 buckets.
* Participated with operations team for Spark Installation on Secured cluster.
* Provided updates in daily SCRUM and Self planning on start of sprint and provided the planned task using JIRA. In sync up with team in order to pick priority task

Environment: Hadoop, HDFS, Pig, Sqoop, Spark, MapReduce, Cloudera, Snappy, Zookeeper, NoSQL, HBase, Shell Scripting, Ubuntu, Linux Red Hat.

Wal-Mart, San Jose, CA 	March' 13 to Jun' 14
Sr. Hadoop Developer

Responsibilities: 

* Developed Big Data Solutions that enabled the business and technology teams to make data-driven decisions on the best ways to acquire customers and provide them business solutions.
* Involved in installing, configuring and managing Hadoop Ecosystem components like Hive, Pig, Sqoop and Flume.
* Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data.
* ed on Importing and exporting data from different databases like MySQL, Oracle into HDFS and Hive using Sqoop.
* ed on Writing Hive queries for data analysis to meet the business requirements.
* Responsible for loading unstructured and semi-structured data into Hadoop cluster coming from different sources using Flume and managing.
* Developed MapReduce programs to cleanse and parse data in HDFS obtained from various data sources and to perform joins on the Map side using distributed cache.
* Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries.
* Created internal and external tables with properly defined static and dynamic partitions for efficiency.
* Implemented Hive custom UDF's to achieve comprehensive data analysis.
* Used the RegEx, JSON and Avro SerDe's for serialization and de-serialization packaged with Hive to parse the contents of streamed log data.
* Developed Pig scripts for advanced analytics on the data for recommendations.
* Experience in writing Pig UDF's and macros.
* Exported the business required information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data.
* Developed generic Shell scripts to automate Sqoop job by passing parameters for data imports.
* Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data.
* Implemented daily flow for extraction, processing and analysis of data with Oozie.
* Responsible for troubleshooting MapReduce jobs by reviewing the log files.

Environment: Hadoop, MapReduce, Hive, Oozie, Sqoop, Flume, JAVA, LINUX, CentOS



Mindseeker Inc, Laurel, MD	 Nov' 11 to feb 13                                                                                                                
Hadoop Developer

Responsibilities: 

* Involved in analyzing the requirements and establish development capabilities to support future opportunities.
* Involved in Design and Development of  specifications using Hadoop technology.
* Handled importing of data from various data sources, performed transformations using PIG, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using SQOOP.
* ed on streaming the analyzed data to the existing relational databases using SQOOP by making it available for visualization and report generation to the BI team.
* Created Pig Latin scripts to sort, group, join and filter the enterprise wise data.
* Analyzed Web server log data using Apache Flume.
* Implemented schedulers on the Job tracker to share resources of the cluster for the MapReduce jobs given by cluster.
* Used Sqoop to import and export the data from HDFS.
* Moved data from HDFS to Cassandra using MapReduce and Bulk Output Format class.
* Participated with the admin team in designing and migrating the cluster from Cloudera to HDP.
* Developed some helper class for abstracting Cassandra cluster connection act as core toolkit.
* Involved in Agile methodologies, daily Scrum meetings, Sprint planning.
* Wrote Query Mappers and MQ Experience in Junit test Cases.
* Involved in designing the next generation data architecture for the unstructured and semi structured data.

Environment: HDFS, MapReduce, Cassandra, Pig, Hive, Sqoop, Maven, Log4j, Junit, Tableau.


Kforce Inc, Reston, VA 	Feb' 10 to Oct' 11
Hadoop Developer

Responsibilities:

* Knowledge on the real-time message processing systems (Storm, S4)
* Collected the business requirements from the Business Partners and Experts.
* Involved in installing Hadoop Ecosystem components.
* Responsible to manage data coming from different sources.
* Used Apache flume to ingest log data from multiple sources directly into HDFS.
* Customized flume to enrich data with LDAP lookups and GOIP lookups.
* Involved in writing Map Reduce Programs which are running on the cluster.
* Involved in HDFS maintenance and loading of structured and unstructured data.
* Installed and configured Pig and also written PigLatin scripts.
* Wrote MapReduce job using Java API.
* Wrote MapReduce job using Pig Latin.
* Imported data from MySQL to HDFS by using Sqoop to load data.
* Developed Scripts and Batch Job to schedule various Hadoop Program.
* Wrote Hive queries for data analysis to meet the business requirements and generated reports.
* Created Hive tables by using Hive QL and ed on them.
* Wrote Hive UDF for frequently used HiveQL queries.
* Utilized Agile Scrum Methodology to help manage and organize a team of 4 developers with regular code review sessions.
* Regular meetings with  teams and active participation in code review sessions with other developers.
* Used Continuum for integration testing and JUnit for unit testing.

Environment: Hadoop, HDFS, MapReduce, Unix, Flume, Python, Pig, MySQL , MySQL bench Hive
Java , Hbase, Storm, Flume, Zookeeper, Putty, Eclipse , Cloudera, Eclipse , Linux.

Bank of America, Charlotte,NC 	Apr' 08 to Jan' 2010                                                                                           
Java Developer

Responsibilities:

* Involved in the design, development and analysis of the application. Participated in users' meetings along with the Business Analyst for requirements gathering and analysis
* Designed the UML class diagrams and sequence diagrams using Rational Rose
* Used HTML, XML, CSS, AJAX and JavaScript for developing front end pages and client side validations
* Developed the application using Spring MVC Frame integrated with Hibernate
* Used Spring IOC, Spring ORM, Spring MVC modules for developing the application
* ed on Struts Tiles Frame and written modules for internationalization using i18N concept in the front end
* Involved in designing of LDAP-backed authentication system for employee secure login
* Widely used Design Patterns like DAO, Singleton, Factory Pattern, DTO in the process of system designing and development
* Communicating with the BA's for any requirements changes, attending meetings with them regarding design reviews and code reviews
* Agile delivery of software using SCRUM methodology
* Implemented persistence layer using Hibernate that use the POJO classes to represent the persistence database tables
* Used Oracle as a backend database and was responsible to configure and write stored procedures to create, insert, delete and modify data in the database
* Coding JUnit test scripts of the SG, SSG, Role Management, Event Notification and Authorization modules
* Used MAVEN for creating and deploying the .war files
* Developed the J2EE components in the IDE tool, RAD
* Code Review for Roster, System, Node and Product modules
* Development, enhancements and bug fixes in the application

Environment: JDK 1.4/1.5, J2EE, JSP, Servlets, Spring MVC, Hibernate 3.x, Struts Tiles and Validator Frames, i18n, JUnit, RAD 6, WebSphere Application Server, HTML/DHTML, AJAX, CSS, XML, XSLT, JavaScript, Rational Rose, Agile(Extreme Programming), Oracle, IPlanet Directory Server, Web Services, Apache Axis, Maven, CVS


Capgemini, El Paso TX 	June 05 to Mar 08
Java Developer

Responsibilities: 

* Interacted with business users to gather check-in online module requirement.
* Develop the sequence and class diagrams and get it approved from the client
* Designing classes using design pattern methodologies such as Singleton, Service Locator, and DAO factory and session façade patterns.
* Developed Hibernate DAO classes and with the Spring frame  manager classes retrieve and save the data
* Involved in Performance tuning.
* Created the front end using JSP, HTML, XML, and advanced JavaScript.
* Used Oracle as backend
* Performed Unit Cases for components using JUnit
* Configuration management - Clear case used.
* Provided support for user acceptance testing & performance testing.

Environment: Java1.6, JSP, Servlets, JSTL, Struts, Hibernate, Multi-threading, JAXB2.0, SOAP, HTML, CSS, XML, PL/SQL, TOAD, Rational Rose, JavaScript, Weblogic 8.1, eclipse 3.0, Maven Java Messaging services(JMS), MQ and Apollo mainframe server, Content management application (CMA).

Education

* M.S., Johns Hopkins University, Baltimore, MD -2004
* Bs Cs ., Johns Hopkins University, Baltimore, MD -2001
",Data Scientist,resume," Clearance: T3 Secret DOD Clearance (Pending Eligible)  :  * Over 10+ years of experience as Big Data Engineer /Data Engineer and Data Analyst including designing, developing and implementation of data models for enterprise-level applications and systems. *  IT experience this includes recent experience in Big Data/Hadoop Ecosystem Competence in using various Hadoop components such as MapReduce (MR1),YARN(MR2), HDFS, Pig, Hive, HBase, ZooKeeper, Oozie, Hue Experience in building highly reliable, scalable Big data solutions on Hadoop distributions Cloudera, Horton s, AWS EMR. * Good experienced in Data Modeling and Data Analysis as a Proficient in gathering business requirements and handling requirements management. * Pleasant experience ing in Agile/Scrum development environment participated in  discussions with client and contributed to project analysis and development specs. * Experience in transferring the data using Informatica tool from AWS S3 to AWS Redshift Hands on experience in Normalization (1NF, 2NF, 3NF and BCNF) Denormalization techniques for effective and optimum performance in OLTP and OLAP environments. * Expertise in moving structured schema data between Pig and Hive using HCatalog. * Excellent ing experience in Scrum / Agile frame and Waterfall project execution methodologies. * Experience in migrating the data using Sqoop from HDFS and Hive to Relational Database System and vice-versa according to client's requirement. * Experience in SQL and good knowledge in PL/SQL programming and developed Stored Procedures and Triggers and Data Stage, DB2, Unix, Cognos, MDM, Hadoop, Pig. * Experience with RDBMS like SQL Server, MySQL, Oracle and data warehouses like Teradata and Netezza. * Proficient knowledge and hands on experience in writing shell scripts in Linux. * Good Experience on importing and exporting the data from HDFS and Hive into Relational Database Systems like MySQL and vice versa using Sqoop. * Good knowledge on NoSQL Databases including HBase, MongoDB, MapR-DB. * Installation, configuration and administration experience in Big Data platforms Cloudera Manager of Cloudera, MCS of MapR. * Strong experience and knowledge of NoSQL databases such as MongoDB and Cassandra. * Strong Knowledge of Data Warehouse Architecture and Star Schema, Snow flake Schema, FACT and Dimensional Tables. * Experience in using PL/SQL to write Stored Procedures, Functions and Triggers. * Excellent  and analytical  with clear understanding of design goals of ER modeling for OLTP and dimension modeling for OLAP. * Experience ing with Relational Database Management Systems (RDMS) Capable of processing large sets of structured, semi-structured and unstructured data and supporting systems application architecture Good understanding of service-oriented architecture (SOA) and web services like XML and SOAP. * Experience in object-oriented analysis and design (OOAD), used modelling language (UML) and design patterns. * Expertise in SQL Server Analysis Services (SSAS) and SQL Server Reporting Services (SSRS) . * Involve in writing SQL queries, PL/SQL programming and created new packages and procedures and modified and tuned existing procedure and queries using TOAD. * Good Understanding and experience in Data Mining Techniques like Classification, Clustering, Regression and Optimization. * Experience in complete project life cycle (design, development, testing and implementation) of Client Server and Web applications.  ADEXEC Austin, TX (Remote)	 Mar' 17 to Present Lead Data Scientist  Responsibilities:   * Designed and developed software applications, testing, and building automation . * Involved in start to end process of Hadoop jobs that used various  such as Sqoop, PIG, Hive, MapReduce, Spark and Shells scripts (for scheduling of few jobs). * Involved in importing and exporting data between RDBMS and HDFS using Sqoop. * Performed querying of both managed and external tables created by Hive using Impala. * Designed/developed tables, views, various SQL queries, stored procedures, functions. * Involved in PL/SQL code review and modification for the development of new requirements. * Extracted data from existing data source and performed ad-hoc queries. * Utilized SAS and SQL extensively for collecting, validating and analyzing the raw data received from the client. * Executed data extraction programs/data profiling and analyzing data for accuracy and quality. * Analyzed the data using advanced excel functions like Pivot tables, VLOOK up, visualizations to get the descriptive analysis of the data. * Created Schema objects like Indexes, Views, and Sequences, triggers, grants, roles, Snapshots. * Used advanced Microsoft Excel to create pivot tables, and other excel functions to prepare reports and dashboard with user data. * Maintained numerous monthly scripts, executed on monthly basis, produces reports and submitted on time for business review. * Developed ad-hoc reports using Crystal reports for performance analysis by business users. * Implemented the Big Data solution using Hadoop, and hive to pull/load the data into the HDFS system. * Installed and configured Hadoop and responsible for maintaining cluster and managing and reviewing Hadoop log files. * Implemented and configured flows using Oozie to automate jobs. * Migrated the needed data from MySQL in to HDFS using Sqoop and importing various formats of flat files into HDFS. * As a Sr. Big Data Engineer, provided  expertise and aptitude to Hadoop  as they related to the development of analytics. * Expertise in writing Hadoop Jobs to analyze data using MapReduce, Hive, and Pig. * Experience in designing, building and implementing complete Hadoop ecosystem comprising of MapReduce, HDFS, Hive, Pig, HBase, MongoDB, and Spark. * ed experience in Scrum / Agile frame and Waterfall project execution methodologies. * Managed data from various file system to HDFS using UNIX command line utilities. * ed in Azure environment for development and deployment of Custom Hadoop Applications. * Managed and support of enterprise Data Warehouse operation, big data advanced predictive application development using Cloudera &. * Extensively ed on Shell scripts for running SAS programs in batch mode on UNIX.                                                                               Oracle4u LLC, Dallas, TX.           	Sep' 15 to Jan' 17 Lead Data Scientist  Responsibilities:   * Participated in requirements sessions to gather requirements along with business analysts and product owners. * Involved in Agile development methodology active member in scrum meetings. * Involvement in design, development and testing phases of Software Development Life Cycle (SDLC). * Installed and configured Hive and also written Hive UDFs and Cluster coordination services through Zookeeper. * Architected, Designed and Developed Business applications and Data marts for reporting. * Involved in different phases of Development life including Analysis, Design, Coding, Unit Testing, Integration Testing, Review and Release as per the business requirements. * Developed Big Data solutions focused on pattern matching and predictive modeling *  of this project is to build a data lake as a cloud based solution in AWS using Apache Spark. * Installed and configured Hadoop Ecosystem components. * ed on implementation and maintenance of Cloudera Hadoop cluster. * Created Hive External tables to stage data and then move the data from Staging to main tables * Implemented the Big Data solution using Hadoop, hive and Informatica to pull/load the data into the HDFS system. * Pulling the data from data lake (HDFS) and massaging the data with various RDD transformations. * Experience in Server infrastructure development on Gateway, ELB, Auto Scaling, Dynamo DB, Elastic search, Virtual Private Cloud (VPC) * Involved in Kafka and building use case relevant to our environment. * Developed Scala scripts, UDF's using both Data frames/SQL and RDD/MapReduce in Spark for Data Aggregation, queries and writing data back into RDBMS through Sqoop. * Developed Spark code using Scala and Spark-SQL/Streaming for faster processing of data. * Developed Oozie flow jobs to execute hive, Sqoop and MapReduce actions. * Provided thought leadership for architecture and the design of Big Data Analytics solutions for customers, actively drive Proof of Concept (POC) and Proof of Technology (POT) evaluations and to implement a Big Data solution. * Responsible for developing data pipeline using flume, Sqoop and pig to extract the data from weblogs and store in HDFS. * Imported the data from different sources like HDFS/HBase into Spark RDD and developed a data pipeline using Kafka and Storm to store data into HDFS. * Used Spark streaming to receive real time data from the Kafka and store the stream data to HDFS using Scala and NoSQL databases such as HBase and Cassandra. * Documented the requirements including the available code which should be implemented using Spark, Hive, HDFS, HBase and Elastic Search. * Developed Spark code using Scala for faster testing and processing of data. * Explored MLlib algorithms in Spark to understand the possible Machine Learning functionalities that can be used for our use case. * Apache Hadoop installation & configuration of multiple nodes on AWS EC2 system * Developed Pig Latin scripts for replacing the existing legacy process to the Hadoop and the data is fed to AWS S3. * Collaborated with Business users for requirement gathering for building Tableau reports per business needs. * Developed continuous flow of data into HDFS from social feeds using Apache Storm Spouts and Bolts. * Involved in loading data from Unix file system to HDFS.  Environment: Hadoop 3.0, 3NF, flume 1.8, Sqoop 1.4, pig 0.17, YARN, HDFS, HBase 1.2, Kafka, Scala 2.12, NoSQL, Cassandra 3.11, Elastic Search, MLlib, Teradata 15, Sqoop, MapReduce, UNIX, Zookeeper 3.4  PWC, Irvine, CA	 Jan' 15 to Aug' 15 Sr. Data Scientist  Responsibilities:   * Installed and configured Apache Hadoop, Hive and Pig environment on the prototype server. * Configured SQL database to store Hive metadata. * Loaded unstructured data into Hadoop File System (HDFS). * Created ETL jobs to load Twitter JSON data and server data into MongoDB and transported MongoDB into the Data Warehouse. * Created reports and dashboards using structured and unstructured data. * Performed SQL tuning Advisor for tuning of SQL queries and database systems. * Created User accounts, Roles and granting required access permissions and privileges to the users. * Partitioning and re-organization of table and indexes. * Responsible in configuring and backing up database using RMAN, Hot backups, Cold backups and Logical backups. * Written documentation for backups and cloning of database for future reference. * Migrated databases from HP-UX to IBM AIX platforms. * Continuously monitored database performance during batch jobs running on the database using AWR report and OEM. * Used Data pump to take the logical backups of databases. * ed on ASM instances for 11g databases. * Installed and configured 12c software and database. * ed on pluggable and container 12c database model. * Performed other DBA activities including space management and performance monitoring.  Environment: HP-UX, Big Data Database Version 10g/11g, RMAN, Data guard, RAC, ASM, Oracle Enterprise Manager Grid Control 12c, Opatch, Shell Scripting.  Ford Inc, Detroit MI ( Remote)	Jul' 14 to Dec' 14 Sr. Hadoop Developer  Responsibilities:  * Cluster capacity planning along with operations team and management team and Cluster maintenance as well as creation and removal of nodes, HDFS support and maintenance. * Manage and review Hadoop log files, File system management and monitoring. * Involved in Cluster upgrade and required jobs are modified. * Involved in implementing security on Hadoop Cluster with Kerberos by ing along with operations team to move non secured cluster to secured cluster. * Data migration from RDMS to hadoop using sqoop for analysis and implemented Oozie jobs for automatic data imports from source. * Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries. * To analyze data migrated to HDFS, used Hive data warehouse tool and developed Hive queries. * Created external tables with proper partitions for efficiency and loaded the structured data in HDFS resulted from MR jobs. * Implemented Hive UDF for comprehensive data analysis. * Responsible for troubleshooting MapReduce jobs by reviewing the log files. * Involved in importing the real time data to hadoop using Kafka and implemented the Oozie job for daily imports. * Involved with various teams on and offshore for understanding of the data that is imported from their source. * Developed generic Shell scripts to automate Sqoop job by passing parameters for data imports. * Involved in data visualization and provided the files required for the team by analysing the data in hive and developed Pig scripts for advanced analytics on the data. * As a part of POC used the Amazon AWS S3 as an underlying file system for the Hadoop and implemented the elastic Map-Reduce jobs on the data in S3 buckets. * Participated with operations team for Spark Installation on Secured cluster. * Provided updates in daily SCRUM and Self planning on start of sprint and provided the planned task using JIRA. In sync up with team in order to pick priority task  Environment: Hadoop, HDFS, Pig, Sqoop, Spark, MapReduce, Cloudera, Snappy, Zookeeper, NoSQL, HBase, Shell Scripting, Ubuntu, Linux Red Hat.  Wal-Mart, San Jose, CA 	March' 13 to Jun' 14 Sr. Hadoop Developer  Responsibilities:   * Developed Big Data Solutions that enabled the business and technology teams to make data-driven decisions on the best ways to acquire customers and provide them business solutions. * Involved in installing, configuring and managing Hadoop Ecosystem components like Hive, Pig, Sqoop and Flume. * Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data. * ed on Importing and exporting data from different databases like MySQL, Oracle into HDFS and Hive using Sqoop. * ed on Writing Hive queries for data analysis to meet the business requirements. * Responsible for loading unstructured and semi-structured data into Hadoop cluster coming from different sources using Flume and managing. * Developed MapReduce programs to cleanse and parse data in HDFS obtained from various data sources and to perform joins on the Map side using distributed cache. * Used Hive data warehouse tool to analyze the data in HDFS and developed Hive queries. * Created internal and external tables with properly defined static and dynamic partitions for efficiency. * Implemented Hive custom UDF's to achieve comprehensive data analysis. * Used the RegEx, JSON and Avro SerDe's for serialization and de-serialization packaged with Hive to parse the contents of streamed log data. * Developed Pig scripts for advanced analytics on the data for recommendations. * Experience in writing Pig UDF's and macros. * Exported the business required information to RDBMS using Sqoop to make the data available for BI team to generate reports based on data. * Developed generic Shell scripts to automate Sqoop job by passing parameters for data imports. * Migrated the existing data to Hadoop from RDBMS (SQL Server and Oracle) using Sqoop for processing the data. * Implemented daily flow for extraction, processing and analysis of data with Oozie. * Responsible for troubleshooting MapReduce jobs by reviewing the log files.  Environment: Hadoop, MapReduce, Hive, Oozie, Sqoop, Flume, JAVA, LINUX, CentOS    Mindseeker Inc, Laurel, MD	 Nov' 11 to feb 13                                                                                                                 Hadoop Developer  Responsibilities:   * Involved in analyzing the requirements and establish development capabilities to support future opportunities. * Involved in Design and Development of  specifications using Hadoop technology. * Handled importing of data from various data sources, performed transformations using PIG, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using SQOOP. * ed on streaming the analyzed data to the existing relational databases using SQOOP by making it available for visualization and report generation to the BI team. * Created Pig Latin scripts to sort, group, join and filter the enterprise wise data. * Analyzed Web server log data using Apache Flume. * Implemented schedulers on the Job tracker to share resources of the cluster for the MapReduce jobs given by cluster. * Used Sqoop to import and export the data from HDFS. * Moved data from HDFS to Cassandra using MapReduce and Bulk Output Format class. * Participated with the admin team in designing and migrating the cluster from Cloudera to HDP. * Developed some helper class for abstracting Cassandra cluster connection act as core toolkit. * Involved in Agile methodologies, daily Scrum meetings, Sprint planning. * Wrote Query Mappers and MQ Experience in Junit test Cases. * Involved in designing the next generation data architecture for the unstructured and semi structured data.  Environment: HDFS, MapReduce, Cassandra, Pig, Hive, Sqoop, Maven, Log4j, Junit, Tableau.   Kforce Inc, Reston, VA 	Feb' 10 to Oct' 11 Hadoop Developer  Responsibilities:  * Knowledge on the real-time message processing systems (Storm, S4) * Collected the business requirements from the Business Partners and Experts. * Involved in installing Hadoop Ecosystem components. * Responsible to manage data coming from different sources. * Used Apache flume to ingest log data from multiple sources directly into HDFS. * Customized flume to enrich data with LDAP lookups and GOIP lookups. * Involved in writing Map Reduce Programs which are running on the cluster. * Involved in HDFS maintenance and loading of structured and unstructured data. * Installed and configured Pig and also written PigLatin scripts. * Wrote MapReduce job using Java API. * Wrote MapReduce job using Pig Latin. * Imported data from MySQL to HDFS by using Sqoop to load data. * Developed Scripts and Batch Job to schedule various Hadoop Program. * Wrote Hive queries for data analysis to meet the business requirements and generated reports. * Created Hive tables by using Hive QL and ed on them. * Wrote Hive UDF for frequently used HiveQL queries. * Utilized Agile Scrum Methodology to help manage and organize a team of 4 developers with regular code review sessions. * Regular meetings with  teams and active participation in code review sessions with other developers. * Used Continuum for integration testing and JUnit for unit testing.  Environment: Hadoop, HDFS, MapReduce, Unix, Flume, Python, Pig, MySQL , MySQL bench Hive Java , Hbase, Storm, Flume, Zookeeper, Putty, Eclipse , Cloudera, Eclipse , Linux.  Bank of America, Charlotte,NC 	Apr' 08 to Jan' 2010                                                                                            Java Developer  Responsibilities:  * Involved in the design, development and analysis of the application. Participated in users' meetings along with the Business Analyst for requirements gathering and analysis * Designed the UML class diagrams and sequence diagrams using Rational Rose * Used HTML, XML, CSS, AJAX and JavaScript for developing front end pages and client side validations * Developed the application using Spring MVC Frame integrated with Hibernate * Used Spring IOC, Spring ORM, Spring MVC modules for developing the application * ed on Struts Tiles Frame and written modules for internationalization using i18N concept in the front end * Involved in designing of LDAP-backed authentication system for employee secure login * Widely used Design Patterns like DAO, Singleton, Factory Pattern, DTO in the process of system designing and development * Communicating with the BA's for any requirements changes, attending meetings with them regarding design reviews and code reviews * Agile delivery of software using SCRUM methodology * Implemented persistence layer using Hibernate that use the POJO classes to represent the persistence database tables * Used Oracle as a backend database and was responsible to configure and write stored procedures to create, insert, delete and modify data in the database * Coding JUnit test scripts of the SG, SSG, Role Management, Event Notification and Authorization modules * Used MAVEN for creating and deploying the .war files * Developed the J2EE components in the IDE tool, RAD * Code Review for Roster, System, Node and Product modules * Development, enhancements and bug fixes in the application  Environment: JDK 1.4/1.5, J2EE, JSP, Servlets, Spring MVC, Hibernate 3.x, Struts Tiles and Validator Frames, i18n, JUnit, RAD 6, WebSphere Application Server, HTML/DHTML, AJAX, CSS, XML, XSLT, JavaScript, Rational Rose, Agile(Extreme Programming), Oracle, IPlanet Directory Server, Web Services, Apache Axis, Maven, CVS   Capgemini, El Paso TX 	June 05 to Mar 08 Java Developer  Responsibilities:   * Interacted with business users to gather check-in online module requirement. * Develop the sequence and class diagrams and get it approved from the client * Designing classes using design pattern methodologies such as Singleton, Service Locator, and DAO factory and session façade patterns. * Developed Hibernate DAO classes and with the Spring frame  manager classes retrieve and save the data * Involved in Performance tuning. * Created the front end using JSP, HTML, XML, and advanced JavaScript. * Used Oracle as backend * Performed Unit Cases for components using JUnit * Configuration management - Clear case used. * Provided support for user acceptance testing & performance testing.  Environment: Java1.6, JSP, Servlets, JSTL, Struts, Hibernate, Multi-threading, JAXB2.0, SOAP, HTML, CSS, XML, PL/SQL, TOAD, Rational Rose, JavaScript, Weblogic 8.1, eclipse 3.0, Maven Java Messaging services(JMS), MQ and Apollo mainframe server, Content management application (CMA).  Education  * M.S., Johns Hopkins University, Baltimore, MD -2004 * Bs Cs ., Johns Hopkins University, Baltimore, MD -2001 "
"
 

	? 	A Mathematical Science major senior currently looking for a full-time data analyst job after graduation. 
 

University of California, Santa Barbara, CA                                                                                           Expected June 2020  
Major: Mathematical Science B.S.       Minor: Statistical Science 
Course: multivariate calculus, linear algebra, ODE, PDE, linear regression, complex analysis 
 

Program Language  
    * python, R, SQL, SAS Data Analysis 
    * pandas, scipy, numpy, dplyr, ggplot2, matplotlib, excel 
 EXPERIENCE 

Food Bank of Santa Barbara, CA                                                                                                    March 2019-August 2019 
Program Evaluation Intern 
* Tracked data for specific modules and based on the project requirements 
* Entered survey data into spreadsheets and produce  statistic using R 
* Create charts as visual representation of results to provide data support for operational strategy 
Disabled Student Program, UCSB, CA                                                                                                  January 2018-Present Notetaker  
* Produced accurate, legible and clearly stated notes and uploaded in a timely manner 
* Maintained report form to protect student confidentiality and  relationships 
PROJECT EXPERIENCE 

Multiple Linear Regression Analysis                                                                                                                     Spring 2019 
* Built a regression model in R to predict the determining factors of the housing price and produced statistical tests and data visualization to optimize the model 
* Participated in conducting multiple linear regression analysis and outlier test in R to examine the determining factors of the cement density 
* Delivered a formal report based on the analysis by R markdown 
Research on the effect of GDP recession on house prices                                                                                 Summer 2019 
* Independently analyzed data from Zillow, Wikipedia and US Department of Commerce and completed data cleaning, data wrangling, querying, hypothesis testing by using the pandas, scipy and numpy library in Python 
* Verified the housing prices in university towns and that were less affected by the recession 
Research on the weather pattern near Ann Arbor, Michigan                                                                          Summer 2019 
* Conducted data visualization by pandas, matplotlib in Python 
* Collected climate data near Ann Arbor, Michigan through NCEI 
* Completed data cleaning and wrangling by pandas, plotted a line graph by matplotlib 
* Recorded high and low temperatures by day of the year (2005-2014), shaded region between the recorded high and low temperature, overlaid a scatter of the broken high and low temperatures in 2015.      ",Data Scientist,resume,"    	? 	A Mathematical Science major senior currently looking for a full-time data analyst job after graduation.     University of California, Santa Barbara, CA                                                                                           Expected June 2020   Major: Mathematical Science B.S.       Minor: Statistical Science  Course: multivariate calculus, linear algebra, ODE, PDE, linear regression, complex analysis     Program Language       * python, R, SQL, SAS Data Analysis      * pandas, scipy, numpy, dplyr, ggplot2, matplotlib, excel   EXPERIENCE   Food Bank of Santa Barbara, CA                                                                                                    March 2019-August 2019  Program Evaluation Intern  * Tracked data for specific modules and based on the project requirements  * Entered survey data into spreadsheets and produce  statistic using R  * Create charts as visual representation of results to provide data support for operational strategy  Disabled Student Program, UCSB, CA                                                                                                  January 2018-Present Notetaker   * Produced accurate, legible and clearly stated notes and uploaded in a timely manner  * Maintained report form to protect student confidentiality and  relationships  PROJECT EXPERIENCE   Multiple Linear Regression Analysis                                                                                                                     Spring 2019  * Built a regression model in R to predict the determining factors of the housing price and produced statistical tests and data visualization to optimize the model  * Participated in conducting multiple linear regression analysis and outlier test in R to examine the determining factors of the cement density  * Delivered a formal report based on the analysis by R markdown  Research on the effect of GDP recession on house prices                                                                                 Summer 2019  * Independently analyzed data from Zillow, Wikipedia and US Department of Commerce and completed data cleaning, data wrangling, querying, hypothesis testing by using the pandas, scipy and numpy library in Python  * Verified the housing prices in university towns and that were less affected by the recession  Research on the weather pattern near Ann Arbor, Michigan                                                                          Summer 2019  * Conducted data visualization by pandas, matplotlib in Python  * Collected climate data near Ann Arbor, Michigan through NCEI  * Completed data cleaning and wrangling by pandas, plotted a line graph by matplotlib  * Recorded high and low temperatures by day of the year (2005-2014), shaded region between the recorded high and low temperature, overlaid a scatter of the broken high and low temperatures in 2015.      "
" :									

* 7 years of experience as Data Scientist/Engineer with strong  expertise, business experience, and communication  to drive high-impact business outcomes through data-driven innovations and decisions.
* Skilled in data cleansing, analyzing, interpreting results to business users with statistical modelling and visualization with Python, R, Alteryx
*  in business, analytics, database modeling and product design and development of applications
* Proficient in Feature Engineering of variables and Machine Learning algorithms (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian) in Predictive Analytics
* Designed SQL queries for marketing and sales teams and implemented dashboards and reports to prepare weekly and monthly metrics to Directors and VPs of departments
* Expertise in Big data  such as Spark, Hadoop, HiveQL, Pig on AWS cloud environment to create HDFS files from Unstructured data
* Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards, Stories on web and desktop platforms. 
* Extensive experience in machine learning and statistics to draw meaningful insights from data. I am good at communication and storytelling with data. 
* Expertise in all aspects of Agile SDLC from requirement analysis, Design, Development Coding, Testing, Implementation, and maintenance
* Delivered enterprise level  under budget and on time to Fortune 500 clients with global cross functional teams
* Extensive hands-on experience and high proficiency with structures, semi-structured and unstructured data, using a broad range of data science programming languages and big data  including R, Python, Spark, SQL, ScikitLearn, Hadoop MapReduce 
* Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
* Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features. 
* Utilize analytical applications/libraries like Plotly D3JS to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into marketing strategies that drive value. 
* Strong knowledge of statistical methods (regression, time series, hypothesis testing, randomized experiment), machine learning, algorithms, data structures and data infrastructure. 
* Expertise in  proficiency in Designing, Data Modeling Lead for Architecting Data Warehouse/Business Intelligence Applications. 
* Defining job flows in Hadoop environment-using  like Oozie for data scrubbing and processing. 
* Experience in Data migration from existing data stores to Hadoop. 
* Developed MapReduce programs to perform Data Transformation and analysis.


:

Masters of Science, Business Analytics, Silicon Valley University, San Jose, CA 
Bachelors of , Jawaharlal Nehru Technological University, Hyderabad, India





 AND :
Databases/ETL/Query
Teradata, SQL Server, Postgres and Hadoop (Map Reduce); SQL, Hive, Pig and Alteryx
Visualization
Tableau, ggplot2 and R-Shiny
Statistics
Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, Neural Nets
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Market Basket Analysis
Machine Learning
R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot
Python: pandas, numpy, scikit-learn, scipy, stats models, matplotlib,tensorflow
SAS: Forecast server, SAS Procedures and Data Steps
Spark: MLlib, GraphX
SQL: Subqueries, joins, DDL/DML statements
:	

  Client: Pacific Life, Newport Beach, CA
                                                             May 2018  Present
 Role: Senior Data Scientist  Marketing/Insurance
Responsibilities:
* Analyzed product plans in competitive markets using A/B testing and recommended metrics to increase operational efficiency with model development using DataRobot
* Implemented Data pipelines for big data processing using Spark Clusters in Amazon EMR, Python and performed customer segmentation
* Implemented business processing models using predictive & prescriptive analytics on transactional data with regression, classification
* Designed flows using SQL queries with Alteryx for data preprocessing such as cleansing and transformation of data to implement & automate KPI dashboards
* Implemented Logistic, Random forests with Python packages to decide insurance purchase by a USAA member
* Implemented HDFS clusters from unstructured marketing data platforms with HiveQL, Spark , Hadoop 
* Collaborated with business owners of products for understanding business needs and automated business processes and data storytelling in Tableau
* Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn in Python for developing various machine learning algorithms  
* Executed process improvements in data flows using Alteryx processing engine
* ed as Data Architects and IT Architects to understand the movement of data and its storage
* Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects and SmartView. 
* Implemented Agile Methodology for building the applications and data frame development 
* Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.  
* Delivered various complex OLAP databases/cubes, scorecards, dashboards and reports. 
* Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS. 
* Researched, evaluated, architected, and deployed new , frames, and patterns to build sustainable Big Data platforms for the clients 

Environment: R, Python, TensorFlow, Machine Learning, Tableau, Bigdata, Alteryx, Hive, OLAP, DB2, Metadata, MS Excel, DataRobot

  Client: Info-Matrix, Camp Hill, PA
                                                          Dec 2016  April 2018
 Role: Data Scientist - /Marketing
Responsibilities:
* Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn in Python for developing various machine learning algorithms 
* Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis. 
* Implemented Logistic regression, TensorFlow with R packages  dplyr, mice, rpart 
* ed as Data Architects and IT Architects to understand the movement of data and its storage
* Data Manipulation and Aggregation froma different source using Nexus, Toad, Business Objects, PowerBI and SmartView. 
* Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.  
* Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification. 
* Data transformation from various resources, data organization, features extraction from raw and stored. 
* Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS. 
* Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions 
* Researched, evaluated, architected, and deployed new , frames, and patterns to built sustainable Big Data platforms for the clients 
* Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, Business Objects. 


Environment: Python, TensorFlow, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, H20.ai

Client: Accion International, Boston, MA			                                                           Dec 2014  Nov 2016 
Role: Financial Data Analyst

      Responsibilities:
* ed with several R packages including knitr, dplyr, SparkR, Causal Infer, space-time. 
* Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization  using R, Mahout, Hadoop, andMongoDB. 
* Gathering all the data that is required from multiple data sources and creating datasets that will be used in theanalysis. 
* Extracted data using SQL from data sources and performed Exploratory Data Analysis (EDA) and Data Visualizations using R, and Tableau. 
* Implemented Uni-variate and Bi-variate analysis to understand the intrinsic effect/combined effects. 
* ed with Data Governance, Data quality, data lineage, Data architect to design various models and processes. 
* Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop. 
* Designed data models and data flow diagrams using MS Visio. 
* As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation. 
* Developed, Implemented &Maintained the Conceptual, Logical&PhysicalDataModels using Erwin for forwarding/Reverse Engineered Databases.  
* Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team 
* Performed data cleaning and imputation of missing values using R. 
* Take up ad-hoc requests based on different departments and locations 
* Used Hive to store the data and perform datacleaning steps for huge datasets. 
* Created dash boards and visualization on regular basis using ggplot2 and Tableau.
* Creating customized business reports and sharing insights to the management. 
* ed with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata. 
* Interacted with the other departments to understand and identify data needs and requirements and  with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.

Environment: R, SQL, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc.


Client: Axis Bank, India      					                                                 Sep 2012  Nov 2014
Role: Marketing Data Analyst

Responsibilities:
* Analyzed survey response data to determine consumer preferences on client products and proposed recommendations
* Improved efficiency of business processes by 10% through implementation of data management procedures
* Automated the computations to determine market metric information on consumer demographic information
* Implemented predictive modeling techniques to increase long-term growth by 12% for products in US regions
* Developed a scoring mechanism using SAS based on customer segmentation to increase sales by 20%
* Performed Map Reduce Programs those are running on the cluster. 
* Developed multiple MapReduce jobs in java for data cleaning and preprocessing. 
* Analyzed the partitioned and bucketed data and compute various metrics for reporting. 
* Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume. 
* ed on loading the data from MySQL to HBase where necessary using Sqoop. 
* Developed Hive queries for Analysis across different banners.
* Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded tothe database. 
* Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications.
* Exported the result set from Hive to MySQL using Sqoop after processing the data. 
* Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior. 
* Have hands-on experience ing withSequence files, AVRO, HAR file formats and compression. 
* Used Hive to partition and bucket data. 
* Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data. 
* Created HBase tables to store various data formats of data coming from different portfolios. 
* ed on improvingthe performance of existing Pig and Hive Queries. 

Environment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Python, MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS.
Client: Persistent Systems Limited, Hyderabad, India		                                               Oct 2011  Aug 2012
Role: Senior Data Analyst	

  Responsibilities:
* Extracted and validated financial data from external data source like Quandl to generate reports to C-level executives
* Designed a data story frame and new financial benchmark metrics on Costs and departmental expenditures
* Implemented charts, graphs and distribution of revenues through visualization  in Tableau for CFOs
* Reduced 500 man-hours by auto cleaning of data with validations using Python and R to improve efficiency
* Predicted revenue based on R&D and Sales expenses using financial econometric models
* ed with large amounts of structured and unstructured data.
* Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.)
* ed in Business Intelligence  and visualization  such as Business Objects, ChartIO, etc.
* Configured the project on WebSphere 6.1 application servers 
* Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC 
* Conducted Design reviews and  reviews with other project stakeholders. 
* Was a part of the complete life cycle of the project from the requirements to the production support
* Created test plan documents for all back-end database modules 

Environment: MDM, Tableau, Statistical modeling, PL/SQL, HDFS, Teradata, Python, JSON, HADOOP (HDFS), MapReduce, PIG, R Studio, MAHOUT, JAVA, HIVE, AWS.",Data Scientist,resume," :									  * 7 years of experience as Data Scientist/Engineer with strong  expertise, business experience, and communication  to drive high-impact business outcomes through data-driven innovations and decisions. * Skilled in data cleansing, analyzing, interpreting results to business users with statistical modelling and visualization with Python, R, Alteryx *  in business, analytics, database modeling and product design and development of applications * Proficient in Feature Engineering of variables and Machine Learning algorithms (Linear, Logistics, Decision Trees, Random Forest, SVM, K-Nearest Neighbors, Bayesian) in Predictive Analytics * Designed SQL queries for marketing and sales teams and implemented dashboards and reports to prepare weekly and monthly metrics to Directors and VPs of departments * Expertise in Big data  such as Spark, Hadoop, HiveQL, Pig on AWS cloud environment to create HDFS files from Unstructured data * Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards, Stories on web and desktop platforms.  * Extensive experience in machine learning and statistics to draw meaningful insights from data. I am good at communication and storytelling with data.  * Expertise in all aspects of Agile SDLC from requirement analysis, Design, Development Coding, Testing, Implementation, and maintenance * Delivered enterprise level  under budget and on time to Fortune 500 clients with global cross functional teams * Extensive hands-on experience and high proficiency with structures, semi-structured and unstructured data, using a broad range of data science programming languages and big data  including R, Python, Spark, SQL, ScikitLearn, Hadoop MapReduce  * Regularly accessing JIRA tool and other internal issue trackers for the Project development.  * Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features.  * Utilize analytical applications/libraries like Plotly D3JS to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into marketing strategies that drive value.  * Strong knowledge of statistical methods (regression, time series, hypothesis testing, randomized experiment), machine learning, algorithms, data structures and data infrastructure.  * Expertise in  proficiency in Designing, Data Modeling Lead for Architecting Data Warehouse/Business Intelligence Applications.  * Defining job flows in Hadoop environment-using  like Oozie for data scrubbing and processing.  * Experience in Data migration from existing data stores to Hadoop.  * Developed MapReduce programs to perform Data Transformation and analysis.   :  Masters of Science, Business Analytics, Silicon Valley University, San Jose, CA  Bachelors of , Jawaharlal Nehru Technological University, Hyderabad, India       AND : Databases/ETL/Query Teradata, SQL Server, Postgres and Hadoop (Map Reduce); SQL, Hive, Pig and Alteryx Visualization Tableau, ggplot2 and R-Shiny Statistics Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau  Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, Neural Nets Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Market Basket Analysis Machine Learning R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot Python: pandas, numpy, scikit-learn, scipy, stats models, matplotlib,tensorflow SAS: Forecast server, SAS Procedures and Data Steps Spark: MLlib, GraphX SQL: Subqueries, joins, DDL/DML statements :	    Client: Pacific Life, Newport Beach, CA                                                              May 2018  Present  Role: Senior Data Scientist  Marketing/Insurance Responsibilities: * Analyzed product plans in competitive markets using A/B testing and recommended metrics to increase operational efficiency with model development using DataRobot * Implemented Data pipelines for big data processing using Spark Clusters in Amazon EMR, Python and performed customer segmentation * Implemented business processing models using predictive & prescriptive analytics on transactional data with regression, classification * Designed flows using SQL queries with Alteryx for data preprocessing such as cleansing and transformation of data to implement & automate KPI dashboards * Implemented Logistic, Random forests with Python packages to decide insurance purchase by a USAA member * Implemented HDFS clusters from unstructured marketing data platforms with HiveQL, Spark , Hadoop  * Collaborated with business owners of products for understanding business needs and automated business processes and data storytelling in Tableau * Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn in Python for developing various machine learning algorithms   * Executed process improvements in data flows using Alteryx processing engine * ed as Data Architects and IT Architects to understand the movement of data and its storage * Data Manipulation and Aggregation from different source using Nexus, Toad, Business Objects and SmartView.  * Implemented Agile Methodology for building the applications and data frame development  * Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.   * Delivered various complex OLAP databases/cubes, scorecards, dashboards and reports.  * Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.  * Researched, evaluated, architected, and deployed new , frames, and patterns to build sustainable Big Data platforms for the clients   Environment: R, Python, TensorFlow, Machine Learning, Tableau, Bigdata, Alteryx, Hive, OLAP, DB2, Metadata, MS Excel, DataRobot    Client: Info-Matrix, Camp Hill, PA                                                           Dec 2016  April 2018  Role: Data Scientist - /Marketing Responsibilities: * Used pandas, numpy, Seaborn, scipy, matplotlib, sci-kit-learn in Python for developing various machine learning algorithms  * Participated in all phases of data mining; data collection, data cleaning, developing models, validation, visualization and performed Gap analysis.  * Implemented Logistic regression, TensorFlow with R packages  dplyr, mice, rpart  * ed as Data Architects and IT Architects to understand the movement of data and its storage * Data Manipulation and Aggregation froma different source using Nexus, Toad, Business Objects, PowerBI and SmartView.  * Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.   * Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.  * Data transformation from various resources, data organization, features extraction from raw and stored.  * Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS.  * Interaction with Business Analyst, SMEs, and other Data Architects to understand Business needs and functionality for various project solutions  * Researched, evaluated, architected, and deployed new , frames, and patterns to built sustainable Big Data platforms for the clients  * Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, Business Objects.    Environment: Python, TensorFlow, Informatica 9.0, ODS, OLTP, Bigdata, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, H20.ai  Client: Accion International, Boston, MA			                                                           Dec 2014  Nov 2016  Role: Financial Data Analyst        Responsibilities: * ed with several R packages including knitr, dplyr, SparkR, Causal Infer, space-time.  * Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization  using R, Mahout, Hadoop, andMongoDB.  * Gathering all the data that is required from multiple data sources and creating datasets that will be used in theanalysis.  * Extracted data using SQL from data sources and performed Exploratory Data Analysis (EDA) and Data Visualizations using R, and Tableau.  * Implemented Uni-variate and Bi-variate analysis to understand the intrinsic effect/combined effects.  * ed with Data Governance, Data quality, data lineage, Data architect to design various models and processes.  * Independently coded new programs and designed Tables to load and test the program effectively for the given POC's using with Big Data/Hadoop.  * Designed data models and data flow diagrams using MS Visio.  * As an Architect implemented MDM hub to provide clean, consistent data for anSOA implementation.  * Developed, Implemented &Maintained the Conceptual, Logical&PhysicalDataModels using Erwin for forwarding/Reverse Engineered Databases.   * Lead the development and presentation of a dataanalytics data-hub prototype with the help of the other members of the emerging solutions team  * Performed data cleaning and imputation of missing values using R.  * Take up ad-hoc requests based on different departments and locations  * Used Hive to store the data and perform datacleaning steps for huge datasets.  * Created dash boards and visualization on regular basis using ggplot2 and Tableau. * Creating customized business reports and sharing insights to the management.  * ed with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.  * Interacted with the other departments to understand and identify data needs and requirements and  with other members of the IT organization to deliver data visualization and reporting solutions to address those needs.  Environment: R, SQL, Informatica, ODS, OLTP, Oracle 10g, Hive, OLAP, DB2, Metadata, MS Excel, Mainframes MS Visio, Rational Rose, and Requisite Pro, Hadoop, PL/SQL, etc.   Client: Axis Bank, India      					                                                 Sep 2012  Nov 2014 Role: Marketing Data Analyst  Responsibilities: * Analyzed survey response data to determine consumer preferences on client products and proposed recommendations * Improved efficiency of business processes by 10% through implementation of data management procedures * Automated the computations to determine market metric information on consumer demographic information * Implemented predictive modeling techniques to increase long-term growth by 12% for products in US regions * Developed a scoring mechanism using SAS based on customer segmentation to increase sales by 20% * Performed Map Reduce Programs those are running on the cluster.  * Developed multiple MapReduce jobs in java for data cleaning and preprocessing.  * Analyzed the partitioned and bucketed data and compute various metrics for reporting.  * Involved in loading data from RDBMS and weblogs into HDFS using Sqoop and Flume.  * ed on loading the data from MySQL to HBase where necessary using Sqoop.  * Developed Hive queries for Analysis across different banners. * Extracted data from Twitter using Java and Twitter API. Parsed JSON formatted twitter data and uploaded tothe database.  * Launching Amazon EC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuring launched instances with respect to specific applications. * Exported the result set from Hive to MySQL using Sqoop after processing the data.  * Analyzed the data by performing Hive queries and running Pig scripts to study customer behavior.  * Have hands-on experience ing withSequence files, AVRO, HAR file formats and compression.  * Used Hive to partition and bucket data.  * Experience in writing MapReduce programs with Java API to cleanse Structured and unstructured data.  * Created HBase tables to store various data formats of data coming from different portfolios.  * ed on improvingthe performance of existing Pig and Hive Queries.   Environment: SQL/Server, Oracle, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, HDFS, Teradata 14.1, JSON, HADOOP (HDFS), Python, MapReduce, PIG, Spark, R Studio, MAHOUT, JAVA, HIVE, AWS. Client: Persistent Systems Limited, Hyderabad, India		                                               Oct 2011  Aug 2012 Role: Senior Data Analyst	    Responsibilities: * Extracted and validated financial data from external data source like Quandl to generate reports to C-level executives * Designed a data story frame and new financial benchmark metrics on Costs and departmental expenditures * Implemented charts, graphs and distribution of revenues through visualization  in Tableau for CFOs * Reduced 500 man-hours by auto cleaning of data with validations using Python and R to improve efficiency * Predicted revenue based on R&D and Sales expenses using financial econometric models * ed with large amounts of structured and unstructured data. * Knowledge of Machine Learning concepts (Generalized Linear models, Regularization, Random Forest, Time Series models, etc.) * ed in Business Intelligence  and visualization  such as Business Objects, ChartIO, etc. * Configured the project on WebSphere 6.1 application servers  * Communicated with other Health Care info by using Web Services with the help of SOAP, WSDL JAX-RPC  * Conducted Design reviews and  reviews with other project stakeholders.  * Was a part of the complete life cycle of the project from the requirements to the production support * Created test plan documents for all back-end database modules   Environment: MDM, Tableau, Statistical modeling, PL/SQL, HDFS, Teradata, Python, JSON, HADOOP (HDFS), MapReduce, PIG, R Studio, MAHOUT, JAVA, HIVE, AWS."
"Information Systems graduate and Data Analytics aficionado with 2 years of experience and certification in Data Science, Visualization and Machine Learning. Passionate about solving real-world problems with data using available resources at hand. Critical thinker and innovative solution explorer.   
Databases: MS SQL Server, MS Access, Teradata 
BI / Visualization: Microsoft Power BI, Tableau, Visio, ggplot2 
Methodologies: Data Mining, Data Discovery & Cleaning, Regular Expressions, Regression and Classification Model Building & Tuning, Clustering, Feature Engineering, Hyperparameter Tuning, Hypothesis Testing, NLP, Results Analysis & Presentation 
Algorithms: GLM, KNN, K-Means, SVM, Neural Net, Boosted Decision Trees, Bagging and Cross Validation, Naïve 
Bayes, Random Forest, Linear Programming 
IDEs:  RStudio, Jupyter Notebook, H2O Driverless AI 
Business System Analysis: Data Flow Diagrams, Use Cases, Context Diagram, Data Modeling, Software 
Design/Requirements Specification gathering, Software Development Plan Languages:  R, T-SQL, Python, C, HTML 
 
 
	DecisionLogic, San Diego, CA 	 	 	 	        	 	             	       Sept 2018  Jul 2019 
Data and Business Analyst, IT Dept 
 Model Based Bank Transaction Categorization:  
- Set up a predictive web service using Azure ML Studio to categorize incoming bank transactions, achieving a Test AUC of over 95% and less than 1.5 sec web service response time 
- Built the categorization model in R utilizing fastText library for superfast training 
 Loan Default Score: 
- Built the Proof of Concept for detecting first time loan defaults using clients loan performance data, resulting in an AUC of 85% in the small dollar personal loan market 
- Completed milestones like Data Gathering, Wrangling, Visualizing, Model Building/Tuning and Reporting 
- Constructed the binomial classification model on an ensemble of LightGBM, XGBoost and GLM using H2Os Driverless AI  
 Income Stability Score: 
- Effectively drove the conceptualization, development and productionizing of Income Stability Score, ranging from 1-100, was built on 3 major attributes namely borrowers payroll frequency, payroll amount and payroll source to give an estimate of financial stability 
- Successfully deployed the score as a web service using Plumber API in R and scalable docker containers in Ubuntu 
 KPI and Performance Reporting: 
- Built and published numerous dashboards in Microsoft Power BI to monitor KPIs related to Client Success rates, Data Aggregators performance, Financial and Sales stats by implementing complex cross DB joins in 
SQL Server 
 
	San Diego Housing Commission, San Diego, CA 	 	 	 	 	      	     July 2017  May 2018 
Information Systems Intern, IT Dept 
 Demonstrated problem solving abilities by manipulating data in Salesforce, SQL Server (Joins, SSAS), and Excel (Pivot Tables, V-Lookup) to create a centralized data source to build 3 Tableau dashboards leveraging business intelligence 
 Undertook requirements gathering and analysis for 2  and developed Business flows 
 Successfully performed bulk geo-coding of tenant data stored in SQL Server using QGIS for business departments 
 
	San Diego State University, Fowler College of Business, San Diego, CA 	 	    	   June 2017  April 2018 
	Statistical Research Assistant, Management Information Systems Dept 	 
 Research assistant to the Management Information Systems department chair in the field of statistical analysis using R 
 Successfully reduced data processing time of analytical algorithm by more than 60% by building efficient R code 
 ed on building optimization functions, regression models and linear programming functions in R 
	Accenture Services PVT LTD, Pune, India 	 	 	      	 	  	        	        Nov 2013  Jul 2016 
Database Analyst, Data Quality Department 
 Demonstrated SQL  by performing data mining and analysis by using complex joins for and slicing and dicing the data by employing Stored Procedures, SSRS and SSAS services 
 Performed database, ETL, Functional and A/B testing on healthcare data in accordance with SDLC, STLC and agile methodology to verify quality and validate the completeness of data during data warehouse migration 
 ed on data requirements analysis for 2  with data mapping and lookup transformations  Certifications 
         Microsoft  Program for Data Science:  	 	 	 	 	 	 	     2019 
 Covers 11 Microsoft certified courses/ including Data Analysis, Visualization, Insights and Storytelling, Machine Learning, Statistics, etc. 
 
Relevant Academic  
	Toxic Online Comments Identification  	 	 	 	 	 	 	        	       Spring 2018 
 Text analysis project to classify online comments based on the level of toxicity using ML techniques in R and RShiny 
	Lending Club Loan Data Analysis Project 	 	 	 	 	 	 	                          Fall 2017 
 Classification of a given customer into predefined credit grades using classification, clustering and ML techniques in R 
          US Births Data Analysis 	 	 	 	 	 	 	 	 	                          Fall 2017 
 Census data analysis to compare, analyze and graph the Male and Female birth statistics using Python  like NLP, ""NumPy"", Matplotlib, ""Pandas"" and Jupyter notebooks 
          Gardening Catalog Response Mailing Dataset 	 	 	 	 	 	        	       Spring 2017 
 Marketing data analysis and statistical model building as a part of FICOs Academic Engagement Program using R and SQL 
          Alumni Outreach Database System 	 	 	 	 	 	 	                           Fall 2016 
 Database design project specifying required database model, normalization, and creation with user documentation by utilizing SSRS, SSAS and SQL Stored procedures in SQL Server Management Studio 
 
 
          San Diego State University | MS in Information Systems  GPA 3.6                         San Diego, CA | May 2018             Course: Statistical Analysis, Business Analytics, Enterprise Data Management, Big Data Analytics 
	- 	Volunteer Master Builder at Build IT Lab at SDSU Library 	 
 	 
          University of Pune | Bachelor of Engineering in Computer Science  	 	 	 Pune, India | June 2013 - 	In charge of IT dept. under engineering student body 
2 
 ",Data Scientist,resume,"Information Systems graduate and Data Analytics aficionado with 2 years of experience and certification in Data Science, Visualization and Machine Learning. Passionate about solving real-world problems with data using available resources at hand. Critical thinker and innovative solution explorer.    Databases: MS SQL Server, MS Access, Teradata  BI / Visualization: Microsoft Power BI, Tableau, Visio, ggplot2  Methodologies: Data Mining, Data Discovery & Cleaning, Regular Expressions, Regression and Classification Model Building & Tuning, Clustering, Feature Engineering, Hyperparameter Tuning, Hypothesis Testing, NLP, Results Analysis & Presentation  Algorithms: GLM, KNN, K-Means, SVM, Neural Net, Boosted Decision Trees, Bagging and Cross Validation, Naïve  Bayes, Random Forest, Linear Programming  IDEs:  RStudio, Jupyter Notebook, H2O Driverless AI  Business System Analysis: Data Flow Diagrams, Use Cases, Context Diagram, Data Modeling, Software  Design/Requirements Specification gathering, Software Development Plan Languages:  R, T-SQL, Python, C, HTML      	DecisionLogic, San Diego, CA 	 	 	 	        	 	             	       Sept 2018  Jul 2019  Data and Business Analyst, IT Dept   Model Based Bank Transaction Categorization:   - Set up a predictive web service using Azure ML Studio to categorize incoming bank transactions, achieving a Test AUC of over 95% and less than 1.5 sec web service response time  - Built the categorization model in R utilizing fastText library for superfast training   Loan Default Score:  - Built the Proof of Concept for detecting first time loan defaults using clients loan performance data, resulting in an AUC of 85% in the small dollar personal loan market  - Completed milestones like Data Gathering, Wrangling, Visualizing, Model Building/Tuning and Reporting  - Constructed the binomial classification model on an ensemble of LightGBM, XGBoost and GLM using H2Os Driverless AI    Income Stability Score:  - Effectively drove the conceptualization, development and productionizing of Income Stability Score, ranging from 1-100, was built on 3 major attributes namely borrowers payroll frequency, payroll amount and payroll source to give an estimate of financial stability  - Successfully deployed the score as a web service using Plumber API in R and scalable docker containers in Ubuntu   KPI and Performance Reporting:  - Built and published numerous dashboards in Microsoft Power BI to monitor KPIs related to Client Success rates, Data Aggregators performance, Financial and Sales stats by implementing complex cross DB joins in  SQL Server    	San Diego Housing Commission, San Diego, CA 	 	 	 	 	      	     July 2017  May 2018  Information Systems Intern, IT Dept   Demonstrated problem solving abilities by manipulating data in Salesforce, SQL Server (Joins, SSAS), and Excel (Pivot Tables, V-Lookup) to create a centralized data source to build 3 Tableau dashboards leveraging business intelligence   Undertook requirements gathering and analysis for 2  and developed Business flows   Successfully performed bulk geo-coding of tenant data stored in SQL Server using QGIS for business departments    	San Diego State University, Fowler College of Business, San Diego, CA 	 	    	   June 2017  April 2018  	Statistical Research Assistant, Management Information Systems Dept 	   Research assistant to the Management Information Systems department chair in the field of statistical analysis using R   Successfully reduced data processing time of analytical algorithm by more than 60% by building efficient R code   ed on building optimization functions, regression models and linear programming functions in R  	Accenture Services PVT LTD, Pune, India 	 	 	      	 	  	        	        Nov 2013  Jul 2016  Database Analyst, Data Quality Department   Demonstrated SQL  by performing data mining and analysis by using complex joins for and slicing and dicing the data by employing Stored Procedures, SSRS and SSAS services   Performed database, ETL, Functional and A/B testing on healthcare data in accordance with SDLC, STLC and agile methodology to verify quality and validate the completeness of data during data warehouse migration   ed on data requirements analysis for 2  with data mapping and lookup transformations  Certifications           Microsoft  Program for Data Science:  	 	 	 	 	 	 	     2019   Covers 11 Microsoft certified courses/ including Data Analysis, Visualization, Insights and Storytelling, Machine Learning, Statistics, etc.    Relevant Academic   	Toxic Online Comments Identification  	 	 	 	 	 	 	        	       Spring 2018   Text analysis project to classify online comments based on the level of toxicity using ML techniques in R and RShiny  	Lending Club Loan Data Analysis Project 	 	 	 	 	 	 	                          Fall 2017   Classification of a given customer into predefined credit grades using classification, clustering and ML techniques in R            US Births Data Analysis 	 	 	 	 	 	 	 	 	                          Fall 2017   Census data analysis to compare, analyze and graph the Male and Female birth statistics using Python  like NLP, ""NumPy"", Matplotlib, ""Pandas"" and Jupyter notebooks            Gardening Catalog Response Mailing Dataset 	 	 	 	 	 	        	       Spring 2017   Marketing data analysis and statistical model building as a part of FICOs Academic Engagement Program using R and SQL            Alumni Outreach Database System 	 	 	 	 	 	 	                           Fall 2016   Database design project specifying required database model, normalization, and creation with user documentation by utilizing SSRS, SSAS and SQL Stored procedures in SQL Server Management Studio                San Diego State University | MS in Information Systems  GPA 3.6                         San Diego, CA | May 2018             Course: Statistical Analysis, Business Analytics, Enterprise Data Management, Big Data Analytics  	- 	Volunteer Master Builder at Build IT Lab at SDSU Library 	   	            University of Pune | Bachelor of Engineering in Computer Science  	 	 	 Pune, India | June 2013 - 	In charge of IT dept. under engineering student body  2   "
" 

* Over 8+ years of Experience on Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Modelling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP)
* Proficient in gathering and analyzing the Business Requirements with experience in documenting System Requirement Specifications (SRS) and Functional Requirement Specifications (FRS).
* Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data Mining solutions to various business problems and generating data visualizations using R, Python, and Tableau.
* Experience in Extracting data for creating Value Added Datasets using Python, R, SAS, Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project s.
* Good experience of software development in Python (libraries used: Beautiful Soup, NumPy, SciPy, Matplotlib, Python-Twitter, Pandas data frame, net, Urllib2, MySQL)
* Experience on Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin.
* Extensive experienced on business intelligence (and BI )  such as OLAP, Data warehousing, reporting and querying , Data mining and Spreadsheets.
* Efficient in developing Logical and Physical Data model and organizing data as per the business requirements using Sybase Power Designer, Erwin, ER Studio in both OLTP and OLAP applications
* Strong understanding of when to use an ODS or data mart or data warehousing.
* Experienced in employing RProgramming, MATLAB, SAS, Tableau and SQL for datacleaning, data visualization, risk analysis and predictive analytics
* Collaborated with the lead Data Architect to model the Data warehouse in accordance to FSLDM subject areas, 3NF format, Snow flake schema.
* Skilled in E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features.
* Experience on advanced SAS programming techniques, such as PROC SQL (JOIN/ UNION),PROC APPEND, PROC DATASETS, and PROC TRANSPOSE.
* Hands on experienced with Machine Learning, Regression Analysis, Clustering, Boosting, Classification, Principal Component Analysis and Data Visualization 
* Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in the infrastructure to provide data summarization and data manipulation using Linux Commands.
*  Knowledge on designing and implementing a fully operational production grade large scale data solution on Snowflake Data Warehouse
* Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information Management
* Experienced in Database using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data and NoSQL.
* ed closely with other data scientists to create data driven products.
* Strong experienced in Statistical Modeling/Machine Learning and Visualization 
* Proficient in Hadoop, HDFS, Hive, MapReduce, Pig and NOSQL databases like MongoDB, HBase, Cassandra and expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient Machine Learning, Data/Text Mining, Statistical Analysis & Predictive Modeling.
* Experienced in Data Modeling & Data Analysis experience using Dimensional DataModeling and Relational Data Modeling, Star Schema/Snowflake Modeling, FACT& Dimensions tables, Physical & Logical Data Modeling.
	
 :

Languages
HTML5, DHTML, WSDL, CSS3, C, C++, XML, R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot) ,  python 
Software/Libraries
Keras, Caffe, TensorFlow, OpenCV, Scikit-learn, Pandas, NumPy, Microsoft Visual Studio, Microsoft Office.
Development 
Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans.
Packages
ggplot2, caret, dplyr, RWeka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, Matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy.
Machine Learning Algorithms
Neural Nets, Decision trees, Support Vector Machines, Random forest, Convolutional Neural Nets, Logistic Regression, PCA, K- means, KNN.
Development Methodologies
Agile/Scrum, UML, Design Patterns, Waterfall

Database
SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase, Teradata, Netezza, Mongo DB, Cassandra.
Reporting 
MS Office (Word/Excel/PowerPoint/ Visio/Outlook), Crystal Reports XI, SSRS, Cognos 7.0/6.0.
Big Data 
Hadoop, Hive, HDFS, Map Reduce, Pig.
BI 
Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1.
Database Design  and Data Modeling
MS Visio, ERWIN 4.5/4.0, Star Schema/Snowflake Schema modeling, Fact & Dimensions tables, physical & logical data modeling, Normalization and De-normalization techniques, Kimball &Inmon Methodologies:

Bachelors of Science, Computer science Engineering                                                       2011
GITAM University

:	

Client: Century link, Littleton, CO						             Aug 2018 - Till Date
Role: Data Scientist.

 Description:  CenturyLink is the second largest U.S. communications provider to global enterprise customers. With customers in more than 60 countries and an intense focus on the customer experience, CenturyLink strives to be the world's best neting company by solving customers'? increased demand for reliable and secure connections.It also serves as its customers'? trusted partner, helping them manage increased net and IT complexity and providing managed net and cyber security solutions that help protect their business.

Responsibilities:
* Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Sqoop, MySQL.
* ed with statistical models for data analysis, predictive modeling, machine learning approaches and recommendation and optimization algorithms.
* ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services.
* ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiple purposes.
* Analyzing Business Intelligence Reporting requirements and translating them into data sourcing and modeling requirements including Dimensional & Normalized data models, Facts, Dimensions, Snowflake Schemas. 
* Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database.
* ed with Big Data  such Hadoop, Hive, Map Reduce
* Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming.
* Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.
* A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Linux Commands, NoSQL, MongoDB, Hadoop.
* Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms.
* Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS and batch processing with Linux
* Managed existing team members lead the recruiting and on boarding of a larger DataScience team that addresses analytical knowledge requirements.
* Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscript mapping.
* Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.
* Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.
* Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.

Environment : R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.



Client: Verizon, Richardson, TX							May 2017 - Jul 2018
Role: Data Scientist.	
            Description:  Verizon Communications Inc. is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The Company, through its subsidiaries, provides communications, information and entertainment products and services to consumers, businesses and governmental agencies. Its segments include Wireless and Wireline. The Wireless segment offers communications products and services, including wireless voice and data services and equipment sales, to consumer, business and government customers across the United States.

           Responsibilities:
* Responsible for performing Machine-learning techniques regression/classification to predict the outcomes.
* Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for Modeling.
* Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, and Business Objects.
* Designing and implementing data warehouses and data marts using components of Kimball Methodology, like Data Warehouse Bus, Conformed Facts & Dimensions, Slowly Changing Dimensions in Snowflake Schema
* Develop and implement innovative AI and machine learning  that will be used in the Risk.
* Performed the feature engineering of supervised and unsupervised machine learning models.
* Implemented SparkMLLib utilities such as including classification, regression, clustering, collaborative filtering,and dimensionality reductions
* Utilized Convolution Neural Nets to implement a machine learning image recognition componentusing TensorFlow.
* Strong in ETL and data integration experience in developing ETL mappings and scripts using Informatica 
* Interaction with Business Analyst, SME and other Data Architects to understand Business needs and functionality for various project solutions.
* Designed the prototype of the Data mart and documented possible outcome from it for end-user.
* Involved in business process Modeling using UML.
* Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS.
* ed on Spark tool collaborating with ML libraries in eliminating a shotgun approach to understand customer buying patterns.
* Responsible for handling Hive queries using Spark SQL that integrates with Spark environment.
* Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, andPL/SQL.
* Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions, and Data Formats
* Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server.
* Updated Pythonscripts to match training data with our database stored in AWS Cloud Search, sothat we would be able to assign each document a response label for further classification.
* Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for senior managers.
* Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.

Environment: Python, MDM, MLLib, PL/SQL, Tableau, Git,NLP,SQL Server, MLLib, Scala NLP, SSMS, ERP,    CRM, Netezza, Cassandra, SQL, PL/SQL, SSRS, Informatica, Spark, Azure, R Studio, MongoDB, JAVA, HIVE.

 Client:  Direct Energy - Houston, TX                                                                     Jan 2016 - Apr2017          
Role: Data Scientist.	

 Description:  Direct Energy is a North American retailer of energy and energy services. The company was founded in 1986. The company has more than six million customers in Canada and the United States, making it the largest energy retailer in North America.

Responsibilities:

* Involved in extensive hoc reporting, routine operational reporting, and data manipulation to produce routine metrics and dashboards for management
* Created parameters, action filters and calculated sets for preparing dashboards and sheets in Tableau.
* Interacting with other datascientists and architects, custom solutions for data visualization using  like a tableau and Packages in Python.
* Involved in running Map Reduce jobs for processing millions of records.
* Written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc.
* The building, publishing customized interactive reports, report scheduling and dashboards using Tableauserver.
* Developed in Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files.
* Performing statistical data analysis and data visualization using Python.
* ed on creating filters and calculated sets for preparing dashboards and sheets in Tableau.
* Created data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements.
* Created new scripts for Splunk scripted input for the system, collecting CPU and OS data.
* Implemented data refreshes on Tableau Server for biweekly and monthly increments based on a business change to ensure that the views and dashboards were displaying the changed data accurately.
* Developed normalized Logical and Physical database models for designing an OLTP application.
* Knowledgeable in AWS Environment for loading data files from on prim to Redshift cluster.
* Performed SQL Testing on AWSRedshift databases.
* Developed TeradataSQLscripts using OLAP functions like rank and rank over to improve the query performance while pulling the data from large tables.
* Developed and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization.
* Designed the Data Marts in dimensional data modelling using star and snowflake schemas.
* Analyzed DataSet with SASprogramming, R and Excel.
* Publish Interactive dashboards and schedule auto-data refreshes
* Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries.
* Created Hive queries that helped market analysts spot emerging trends by comparing incremental data with Teradata reference tables and historical metrics.
* Design and development of ETL processes using InformaticaETL  for dimension and fact file creation.
* Develop and automate solutions for a new billing and membership Enterprise data Warehouse including ETL routines, tables, maps, materialized views, and stored procedures incorporating Informatica and Oracle PL/SQL ets.
* Performed analysis of implementing Spark uses Scala and wrote spark sample programs using PySpark.

Environment:SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, R connector, Python, R, Tableau 9.2
 Client:  Becton Dickinson - Franklin Lakes, NJ                                                                                      Mar 2014 - Dec 2015
Role:  Data Scientist/R Developer.	

 Description:  BD is a global medical technology company that is advancing the world of health by improving medical discovery, diagnostics and the delivery of care. BD leads inpatient and healthcare er safety and the  that enable medical research and clinical laboratories.

Responsibilities:

* The conducted analysis in assessing customer consuming behaviors and discover the value of customers with RMF analysis, applied customer segmentation with clustering algorithms such as K-Means Clustering and HierarchicalClustering.
* Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle.
* Involved in managing backup and restoring data in the live Cassandra Cluster.
* Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.
* Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.
* Developed personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers.
* Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net.
* Evaluated parameters with K-Fold Cross Validation and optimized performance of models.
* ed on benchmarking Cassandra Cluster using the Cassandra stress tool.
* A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, LINUX Commands, Python programming, No SQL.
* ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn.
* Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms.
* Determined customer satisfaction and helped enhance customer using NLP.
* Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity.
* Performed datavisualization and Designeddashboards with Tableau and D3.js and provided complexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders.


Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.

 Client:  ZETA Interactive, India                                                                           Dec 2012 - Feb 2014
Role: Data Analyst.

Description: We're Zeta Global, a data-driven marketing technology company that helps brands acquire more customers, keep the ones they have for longer and grow their value. We create personalized customer experiences by leveraging strategy, analytics, machine learning, and award-winning creative that ignite a perpetual dialogue between brands and their customers. 
  

Responsibilities:

* Used SAS Proc SQLpass-throughfacility to connect to Oracle tables and created SAS datasets using various SQL joins such as left join, right join, inner join and full join. 
* Performing data validation, transforming data from RDBMS oracle to SAS datasets. 
* Produce quality customized reports by using PROC TABULATE, PROC REPORT Styles, and ODS RTF and provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE. 
* Developed SAS macros for data cleaning, reporting and to support routing processing. 
* Performed advanced querying using SAS Enterprise Guide, calculating computed columns, using afilter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets. 
* Involved in Developing, Debugging, and validating the project-specific SAS programs to generate derived SAS datasets,  tables, and data listings according to study documents. 
* Created datasets as per the approved specification collaborated with project teams to complete scientific reports and review reports to ensure accuracy and clarity.
* Experienced in ing with data modelers to translate business rules/requirements into conceptual/logical dimensional models and ed with complex de-normalized and normalized data models
* Performed different calculations like Quick table calculations, Date Calculations, Aggregate Calculations, String and Number Calculations.
* Created action filters, user filters, parameters and calculated sets for preparing dashboards and sheets in Tableau. 
* Used the dynamic SQL to perform some pre-and post-session task required while performing Extraction, transformation, and loading.
* Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracle database
* Expertise in Agile Scrum Methodology to implement project life cycles of reports design and development 
* Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actions etc. and published them on the web.
* Gathering business requirements, creating business requirement documents (BRD /FRD). 
*  closely with business leaders and users to define and design the data sources requirements and data access Code, test, identify, implement and document  solutions utilizing JavaScript, PHP&MySQL.
* ing with themanager to prioritize requirements and preparing reports on theweekly and monthly basis. 

Environment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio.

Client:  Karvy Global Services - IN                                					Jan 2011 - Nov 2012
Role: Data Analyst.	

Description: : Karvy Global Services is a specialist knowledge process outsourcing services firm and an arm of the Karvy Group, one of India's largest integrated financial services companies
Responsibilities:
* Participated in requirement gathering sessions with business stakeholders to understand the project goals and documented the business requirement documents(BRD)
* Studied the Requirements Specifications, Use Cases and analyzed the data needs of the Business users.
* Implemented conceptual and logical data models using Erwin 7.2 by adopting agile methodologies as per organization standards.
* Redesigned some of the previous models by adding some new entities and attributes as per the business requirements.
* Converted the Logical data models to Physical data models to generate DDL scripts.
* Reverse Engineered existed data models for analyzing and comparing the business process.
* Expertise in the Forward Engineering of logical models to generate the physical model using Erwin.
* Created the Logical data models using Erwin 7.2 and ensured that it follows the normalization rules and have the best possible traceability pattern.
* Migrated several models from Erwin 4.1/7.1 to ERWIN 7.2 and updated the previous naming standards.
* Extensively ed with enterprise data warehouse development by building data marts, staging, and restaging.
* Scheduled reports for daily, weekly, monthly reports for executives, Business analyst and customer representatives for various categories and regions based on business needs using SQL Server Reporting Services (SSRS)
* ed with business users to understand metric definitions, presentation, and user needs.


Environment: Erwin, Informatica, Cognos, Oracle 9i, SQL Server 2003, SQL, MS Office, Windows 2003.








",Data Scientist,resume,"   * Over 8+ years of Experience on Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Modelling, Data Architecture, Data Analysis, Data Mining, Text Mining & Natural Language Processing (NLP) * Proficient in gathering and analyzing the Business Requirements with experience in documenting System Requirement Specifications (SRS) and Functional Requirement Specifications (FRS). * Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data Mining solutions to various business problems and generating data visualizations using R, Python, and Tableau. * Experience in Extracting data for creating Value Added Datasets using Python, R, SAS, Azure and SQL to analyze the behavior to target a specific set of customers to obtain hidden insights within the data to effectively implement the project s. * Good experience of software development in Python (libraries used: Beautiful Soup, NumPy, SciPy, Matplotlib, Python-Twitter, Pandas data frame, net, Urllib2, MySQL) * Experience on Artificial Intelligence algorithms, Business Intelligence, Analytics Models (like Decision Trees, Linear & Logistic Regression, Hadoop (Hive, PIG), R, Python, Spark, Scala, MS Excel, SQL and Postgre SQL, Erwin. * Extensive experienced on business intelligence (and BI )  such as OLAP, Data warehousing, reporting and querying , Data mining and Spreadsheets. * Efficient in developing Logical and Physical Data model and organizing data as per the business requirements using Sybase Power Designer, Erwin, ER Studio in both OLTP and OLAP applications * Strong understanding of when to use an ODS or data mart or data warehousing. * Experienced in employing RProgramming, MATLAB, SAS, Tableau and SQL for datacleaning, data visualization, risk analysis and predictive analytics * Collaborated with the lead Data Architect to model the Data warehouse in accordance to FSLDM subject areas, 3NF format, Snow flake schema. * Skilled in E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features. * Experience on advanced SAS programming techniques, such as PROC SQL (JOIN/ UNION),PROC APPEND, PROC DATASETS, and PROC TRANSPOSE. * Hands on experienced with Machine Learning, Regression Analysis, Clustering, Boosting, Classification, Principal Component Analysis and Data Visualization  * Highly skilled in using Hadoop (pig and Hive) for basic analysis and extraction of data in the infrastructure to provide data summarization and data manipulation using Linux Commands. *  Knowledge on designing and implementing a fully operational production grade large scale data solution on Snowflake Data Warehouse * Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information Management * Experienced in Database using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data and NoSQL. * ed closely with other data scientists to create data driven products. * Strong experienced in Statistical Modeling/Machine Learning and Visualization  * Proficient in Hadoop, HDFS, Hive, MapReduce, Pig and NOSQL databases like MongoDB, HBase, Cassandra and expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient Machine Learning, Data/Text Mining, Statistical Analysis & Predictive Modeling. * Experienced in Data Modeling & Data Analysis experience using Dimensional DataModeling and Relational Data Modeling, Star Schema/Snowflake Modeling, FACT& Dimensions tables, Physical & Logical Data Modeling. 	  :  Languages HTML5, DHTML, WSDL, CSS3, C, C++, XML, R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot) ,  python  Software/Libraries Keras, Caffe, TensorFlow, OpenCV, Scikit-learn, Pandas, NumPy, Microsoft Visual Studio, Microsoft Office. Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans. Packages ggplot2, caret, dplyr, RWeka, gmodels, RCurl, tm, C50, twitter, NLP, Reshape2, rjson, plyr, pandas, numpy, seaborn, scipy, Matplot lib, scikit-learn, Beautiful Soup, Rpy2, sqlalchemy. Machine Learning Algorithms Neural Nets, Decision trees, Support Vector Machines, Random forest, Convolutional Neural Nets, Logistic Regression, PCA, K- means, KNN. Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall  Database SQL, Hive, Impala, Pig, Spark SQL, Databases SQL-Server, My SQL, MS Access, HDFS, HBase, Teradata, Netezza, Mongo DB, Cassandra. Reporting  MS Office (Word/Excel/PowerPoint/ Visio/Outlook), Crystal Reports XI, SSRS, Cognos 7.0/6.0. Big Data  Hadoop, Hive, HDFS, Map Reduce, Pig. BI  Microsoft Power BI, Tableau, SSIS, SSRS, SSAS, Business Intelligence Development Studio (BIDS), Visual Studio, Crystal Reports, Informatica 6.1. Database Design  and Data Modeling MS Visio, ERWIN 4.5/4.0, Star Schema/Snowflake Schema modeling, Fact & Dimensions tables, physical & logical data modeling, Normalization and De-normalization techniques, Kimball &Inmon Methodologies:  Bachelors of Science, Computer science Engineering                                                       2011 GITAM University  :	  Client: Century link, Littleton, CO						             Aug 2018 - Till Date Role: Data Scientist.   Description:  CenturyLink is the second largest U.S. communications provider to global enterprise customers. With customers in more than 60 countries and an intense focus on the customer experience, CenturyLink strives to be the world's best neting company by solving customers'? increased demand for reliable and secure connections.It also serves as its customers'? trusted partner, helping them manage increased net and IT complexity and providing managed net and cyber security solutions that help protect their business.  Responsibilities: * Built data pipelines for reporting, alerting, and data mining. Experienced with table design and data management using HDFS, Hive, Sqoop, MySQL. * ed with statistical models for data analysis, predictive modeling, machine learning approaches and recommendation and optimization algorithms. * ing in Business and Data Analysis, Data Profiling, Data Migration, Data Integration and Metadata Management Services. * ed extensively on Databases preferably Oracle 11g/12c and writing PL/SQL scripts for multiple purposes. * Analyzing Business Intelligence Reporting requirements and translating them into data sourcing and modeling requirements including Dimensional & Normalized data models, Facts, Dimensions, Snowflake Schemas.  * Design, coding, unit testing of ETL package source marts and subject marts using Informatica ETL processes for Oracle database. * ed with Big Data  such Hadoop, Hive, Map Reduce * Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. Implemented a Python-based distributed random forest via Python streaming. * Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure. * A highly immersive Data Science program involving Data Manipulation & Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, Linux Commands, NoSQL, MongoDB, Hadoop. * Performed scoring and financial forecasting for collection priorities using Python, R and SASmachinelearning algorithms. * Handled importing data from various data sources, performed transformations using Hive, MapReduce, and loaded data into HDFS and batch processing with Linux * Managed existing team members lead the recruiting and on boarding of a larger DataScience team that addresses analytical knowledge requirements. * Created SQL scripts and analyzed the data in MS Access/Excel and ed on SQL and SASscript mapping. * Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. * Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management. * Above scoring models resulted in millions of dollars of added revenue to the company and a change in priorities of the entire company.  Environment : R, SQL, Python 2.7.x, SQL Server 2014, regression, logistic regression, random forest, neural nets, Topic Modeling, NLTK, SVM (Support Vector Machine), JSON, XML, HIVE, HADOOP, PIG, Sklearn, SciPy, Graph Lab, No SQL, SAS, SPSS, Spark, Hadoop, Kafka, HBase, MLib.    Client: Verizon, Richardson, TX							May 2017 - Jul 2018 Role: Data Scientist.	             Description:  Verizon Communications Inc. is an American multinational telecommunications conglomerate and a corporate component of the Dow Jones Industrial Average. The Company, through its subsidiaries, provides communications, information and entertainment products and services to consumers, businesses and governmental agencies. Its segments include Wireless and Wireline. The Wireless segment offers communications products and services, including wireless voice and data services and equipment sales, to consumer, business and government customers across the United States.             Responsibilities: * Responsible for performing Machine-learning techniques regression/classification to predict the outcomes. * Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for Modeling. * Identifying and executing process improvements, hands-on in various  such as Oracle, Informatica, and Business Objects. * Designing and implementing data warehouses and data marts using components of Kimball Methodology, like Data Warehouse Bus, Conformed Facts & Dimensions, Slowly Changing Dimensions in Snowflake Schema * Develop and implement innovative AI and machine learning  that will be used in the Risk. * Performed the feature engineering of supervised and unsupervised machine learning models. * Implemented SparkMLLib utilities such as including classification, regression, clustering, collaborative filtering,and dimensionality reductions * Utilized Convolution Neural Nets to implement a machine learning image recognition componentusing TensorFlow. * Strong in ETL and data integration experience in developing ETL mappings and scripts using Informatica  * Interaction with Business Analyst, SME and other Data Architects to understand Business needs and functionality for various project solutions. * Designed the prototype of the Data mart and documented possible outcome from it for end-user. * Involved in business process Modeling using UML. * Handled importing data from various data sources, performed transformations using Hive, Map Reduce, and loaded data into HDFS. * ed on Spark tool collaborating with ML libraries in eliminating a shotgun approach to understand customer buying patterns. * Responsible for handling Hive queries using Spark SQL that integrates with Spark environment. * Created SQL tables with referential integrity and developed queries using SQL, SQL*PLUS, andPL/SQL. * Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions, and Data Formats * Performance tuning of the database, which includes indexes, and optimizing SQL statements, monitoring the server. * Updated Pythonscripts to match training data with our database stored in AWS Cloud Search, sothat we would be able to assign each document a response label for further classification. * Wrote simple and advanced SQL queries and scripts to create standard and Adhoc reports for senior managers. * Created PL/SQL packages and Database Triggers and developed user procedures and prepared user manuals for the new programs.  Environment: Python, MDM, MLLib, PL/SQL, Tableau, Git,NLP,SQL Server, MLLib, Scala NLP, SSMS, ERP,    CRM, Netezza, Cassandra, SQL, PL/SQL, SSRS, Informatica, Spark, Azure, R Studio, MongoDB, JAVA, HIVE.   Client:  Direct Energy - Houston, TX                                                                     Jan 2016 - Apr2017           Role: Data Scientist.	   Description:  Direct Energy is a North American retailer of energy and energy services. The company was founded in 1986. The company has more than six million customers in Canada and the United States, making it the largest energy retailer in North America.  Responsibilities:  * Involved in extensive hoc reporting, routine operational reporting, and data manipulation to produce routine metrics and dashboards for management * Created parameters, action filters and calculated sets for preparing dashboards and sheets in Tableau. * Interacting with other datascientists and architects, custom solutions for data visualization using  like a tableau and Packages in Python. * Involved in running Map Reduce jobs for processing millions of records. * Written complex SQL queries using joins and OLAP functions like Count, CSUM, and Rank etc. * The building, publishing customized interactive reports, report scheduling and dashboards using Tableauserver. * Developed in Python programs for manipulating the data reading from various Teradata and convert them as one CSV Files. * Performing statistical data analysis and data visualization using Python. * ed on creating filters and calculated sets for preparing dashboards and sheets in Tableau. * Created data models in Splunk using pivot tables by analyzing the vast amount of data and extracting key information to suit various business requirements. * Created new scripts for Splunk scripted input for the system, collecting CPU and OS data. * Implemented data refreshes on Tableau Server for biweekly and monthly increments based on a business change to ensure that the views and dashboards were displaying the changed data accurately. * Developed normalized Logical and Physical database models for designing an OLTP application. * Knowledgeable in AWS Environment for loading data files from on prim to Redshift cluster. * Performed SQL Testing on AWSRedshift databases. * Developed TeradataSQLscripts using OLAP functions like rank and rank over to improve the query performance while pulling the data from large tables. * Developed and implemented SSIS, SSRS and SSAS application solutions for various business units across the organization. * Designed the Data Marts in dimensional data modelling using star and snowflake schemas. * Analyzed DataSet with SASprogramming, R and Excel. * Publish Interactive dashboards and schedule auto-data refreshes * Maintenance of large data sets, combining data from various sources by Excel, Enterprise, and SAS Grid, Access and SQL queries. * Created Hive queries that helped market analysts spot emerging trends by comparing incremental data with Teradata reference tables and historical metrics. * Design and development of ETL processes using InformaticaETL  for dimension and fact file creation. * Develop and automate solutions for a new billing and membership Enterprise data Warehouse including ETL routines, tables, maps, materialized views, and stored procedures incorporating Informatica and Oracle PL/SQL ets. * Performed analysis of implementing Spark uses Scala and wrote spark sample programs using PySpark.  Environment:SQL/Server, Oracle 10g/11g, MS-Office, Teradata, Informatica, ER Studio, XML, R connector, Python, R, Tableau 9.2  Client:  Becton Dickinson - Franklin Lakes, NJ                                                                                      Mar 2014 - Dec 2015 Role:  Data Scientist/R Developer.	   Description:  BD is a global medical technology company that is advancing the world of health by improving medical discovery, diagnostics and the delivery of care. BD leads inpatient and healthcare er safety and the  that enable medical research and clinical laboratories.  Responsibilities:  * The conducted analysis in assessing customer consuming behaviors and discover the value of customers with RMF analysis, applied customer segmentation with clustering algorithms such as K-Means Clustering and HierarchicalClustering. * Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle. * Involved in managing backup and restoring data in the live Cassandra Cluster. * Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes. * Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python. * Developed personalized product recommendation with Machine learning algorithms, including Gradient Boosting Tree and Collaborative filtering to better meet the needs of existing customers and acquire new customers. * Used Python and Spark to implement different machine learning algorithms, including Generalized Linear Model, RandomForest, SVM, Boosting and Neural Net. * Evaluated parameters with K-Fold Cross Validation and optimized performance of models. * ed on benchmarking Cassandra Cluster using the Cassandra stress tool. * A highly immersive Data Science program involving Data Manipulation and Visualization, Web Scraping, Machine Learning, GIT, SQL, LINUX Commands, Python programming, No SQL. * ed on data cleaning, data preparation, and feature engineering with Python, including Numpy, Scipy, Matplotlib, Seaborn, Pandas, and Scikit-learn. * Identified risk level and eligibility of new insurance applicants with MachineLearning algorithms. * Determined customer satisfaction and helped enhance customer using NLP. * Utilized SQL and HiveQL to query, manipulate data from variety data sources including Oracle and HDFS, while maintaining data integrity. * Performed datavisualization and Designeddashboards with Tableau and D3.js and provided complexreports, includingcharts, summaries, and graphs to interpret the findings to the team and stakeholders.   Environment: R, MATLAB, MongoDB, exploratory analysis, feature engineering, K-Means Clustering, Hierarchical Clustering, Machine Learning), Python, Spark (MLlib, PY Spark), Tableau, Micro Strategy, SAS, Tensor Flow, regression, logistic regression, Hadoop 2.7, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.   Client:  ZETA Interactive, India                                                                           Dec 2012 - Feb 2014 Role: Data Analyst.  Description: We're Zeta Global, a data-driven marketing technology company that helps brands acquire more customers, keep the ones they have for longer and grow their value. We create personalized customer experiences by leveraging strategy, analytics, machine learning, and award-winning creative that ignite a perpetual dialogue between brands and their customers.      Responsibilities:  * Used SAS Proc SQLpass-throughfacility to connect to Oracle tables and created SAS datasets using various SQL joins such as left join, right join, inner join and full join.  * Performing data validation, transforming data from RDBMS oracle to SAS datasets.  * Produce quality customized reports by using PROC TABULATE, PROC REPORT Styles, and ODS RTF and provide descriptive statistics using PROC MEANS, PROC FREQ, and PROC UNIVARIATE.  * Developed SAS macros for data cleaning, reporting and to support routing processing.  * Performed advanced querying using SAS Enterprise Guide, calculating computed columns, using afilter, manipulate and prepare data for Reporting, Graphing, and Summarization, statistical analysis, finally generating SAS datasets.  * Involved in Developing, Debugging, and validating the project-specific SAS programs to generate derived SAS datasets,  tables, and data listings according to study documents.  * Created datasets as per the approved specification collaborated with project teams to complete scientific reports and review reports to ensure accuracy and clarity. * Experienced in ing with data modelers to translate business rules/requirements into conceptual/logical dimensional models and ed with complex de-normalized and normalized data models * Performed different calculations like Quick table calculations, Date Calculations, Aggregate Calculations, String and Number Calculations. * Created action filters, user filters, parameters and calculated sets for preparing dashboards and sheets in Tableau.  * Used the dynamic SQL to perform some pre-and post-session task required while performing Extraction, transformation, and loading. * Designing the ETL process using Informatica to populate the Data Mart using the flat files to Oracle database * Expertise in Agile Scrum Methodology to implement project life cycles of reports design and development  * Combined Tableau visualizations into Interactive Dashboards using filter actions, highlight actions etc. and published them on the web. * Gathering business requirements, creating business requirement documents (BRD /FRD).  *  closely with business leaders and users to define and design the data sources requirements and data access Code, test, identify, implement and document  solutions utilizing JavaScript, PHP&MySQL. * ing with themanager to prioritize requirements and preparing reports on theweekly and monthly basis.   Environment: SQL Server, Oracle 11g/10g, MS Office Suite, PowerPivot, Power Point, SAS Base, SAS Enterprise Guide, SAS/MACRO, SAS/SQL, SAS/ODS, SQL, PL/SQL, Visio.  Client:  Karvy Global Services - IN                                					Jan 2011 - Nov 2012 Role: Data Analyst.	  Description: : Karvy Global Services is a specialist knowledge process outsourcing services firm and an arm of the Karvy Group, one of India's largest integrated financial services companies Responsibilities: * Participated in requirement gathering sessions with business stakeholders to understand the project goals and documented the business requirement documents(BRD) * Studied the Requirements Specifications, Use Cases and analyzed the data needs of the Business users. * Implemented conceptual and logical data models using Erwin 7.2 by adopting agile methodologies as per organization standards. * Redesigned some of the previous models by adding some new entities and attributes as per the business requirements. * Converted the Logical data models to Physical data models to generate DDL scripts. * Reverse Engineered existed data models for analyzing and comparing the business process. * Expertise in the Forward Engineering of logical models to generate the physical model using Erwin. * Created the Logical data models using Erwin 7.2 and ensured that it follows the normalization rules and have the best possible traceability pattern. * Migrated several models from Erwin 4.1/7.1 to ERWIN 7.2 and updated the previous naming standards. * Extensively ed with enterprise data warehouse development by building data marts, staging, and restaging. * Scheduled reports for daily, weekly, monthly reports for executives, Business analyst and customer representatives for various categories and regions based on business needs using SQL Server Reporting Services (SSRS) * ed with business users to understand metric definitions, presentation, and user needs.   Environment: Erwin, Informatica, Cognos, Oracle 9i, SQL Server 2003, SQL, MS Office, Windows 2003.         "
"US Citizen 
-------------------------------------------------------------------------------------------------------------------------------

Energetic, harding and dedicated computer science graduate seeking a full-time position as an entry level Web Developer.
:
Princess Sumaya University of 
Amman- Jordan
Bachelor of Science in Information , June 2016

	
* Good Knowledge of Java and Python
* Knowledge of Java Script, J Query, PHP, HTML, CSS
* Good experience in using Black Box test methodologies and Functional Testing
* Excellent communication 
* Team player with multi-tasking  
* Very good leadership 
* Fast learner and critical thinker
* Strong self-motivation; proactive and willing to take on new challenges



INTERNSHIP:
   Microsoft Innovation Center				Amman - Jordan
   Computer Program Intern				June 2015  Sep. 2015	

* Developed a Windows Phone application using C# and XAML.
* Created Database on Azure (Microsoft's public cloud computing platform) and connected it to my Windows Phone application.
* Trained to  within a team and to think creatively and out of the box.
* My application was selected and presented to potential investor out of more than 50 applications sponsored by MS Innovation Center and USAID.


EXPERINECE:
English Talent School					Amman-Jordan
First Grade Teacher						Aug 2017  July 2019

* Teaching a full range of subject areas (English, math, integrated studies etc.)
* Collaborate with staff members in developing school activities and joint lessons.
* Commended for ability to redirect students exhibiting behavior problems. 
* Preparing daily and long-term lesson plans according to the curriculum guidelines.
* Setting and conducting testing.
* Established open-door policy, improving communication and trust with students and parent
* Served on school committees and taskforces focused on curriculum development and textbook review.



",Data Scientist,resume,"US Citizen  -------------------------------------------------------------------------------------------------------------------------------  Energetic, harding and dedicated computer science graduate seeking a full-time position as an entry level Web Developer. : Princess Sumaya University of  Amman- Jordan Bachelor of Science in Information , June 2016  	 * Good Knowledge of Java and Python * Knowledge of Java Script, J Query, PHP, HTML, CSS * Good experience in using Black Box test methodologies and Functional Testing * Excellent communication  * Team player with multi-tasking   * Very good leadership  * Fast learner and critical thinker * Strong self-motivation; proactive and willing to take on new challenges    INTERNSHIP:    Microsoft Innovation Center				Amman - Jordan    Computer Program Intern				June 2015  Sep. 2015	  * Developed a Windows Phone application using C# and XAML. * Created Database on Azure (Microsoft's public cloud computing platform) and connected it to my Windows Phone application. * Trained to  within a team and to think creatively and out of the box. * My application was selected and presented to potential investor out of more than 50 applications sponsored by MS Innovation Center and USAID.   EXPERINECE: English Talent School					Amman-Jordan First Grade Teacher						Aug 2017  July 2019  * Teaching a full range of subject areas (English, math, integrated studies etc.) * Collaborate with staff members in developing school activities and joint lessons. * Commended for ability to redirect students exhibiting behavior problems.  * Preparing daily and long-term lesson plans according to the curriculum guidelines. * Setting and conducting testing. * Established open-door policy, improving communication and trust with students and parent * Served on school committees and taskforces focused on curriculum development and textbook review.    "
"
 
* 6+ years of experience solving complex business problems across multiple industry verticals by combining business strategy and tactical implementation.
* Hands-on experience and comprehensive industry knowledge of Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Mining, Text Mining and Natural Language Processing (NLP).
* ed with several Python packages like Pandas, NumPy, Scikit-learn, Keras etc. 
* ed with several R packages like ggplot2, dplyr, plyr, data. tables etc.
* Strong statistical  such as Hypothesis Testing, Principle Component Analysis (PCA) etc.
* Expertise in using several Machine Learning Models including Linear Regression, Logistic Regression, Regularization, k Nearest Neighbor, Decision Trees, Random Forests, Boosting, Neural Nets.
* Experience in Natural Language Processing (NLP) using Python libraries such as NLTK and spacy 
* Experience with Deep Learning Models (ANN, CNN, RNN)
* Good experience in writing SQL queries and implementing functions, tables, views etc.
* Strong knowledge in Statistical methodologies such as Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions, ANOVA, Chi-Square tests, Time Series, Factor Analysis, Discriminant Analysis
* Experienced in utilizing analytical applications like R and Python to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into decision making and marketing strategies that drive value. 
* Expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis, Sentiment Analysis and Predictive Modeling. 


 
Programming 
Python, R, SQL
Databases
MySQL
Statistical Software
SAS, R, Python
ETL/BI  
Tableau, MS Excel, SQL 
Statistical Methods
Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions, ANOVA, Chi-Square tests, Time Series, Factor Analysis, Discriminant Analysis
Big Data
Hadoop, Hive, Sqoop, Spark(Scala)
Machine Learning Models
Linear Regression, Logistic Regression, Regularization, Support Vector Machines, Neural Nets, Decision Trees, Ensemble Methods like Random Forests, Gradient Boost etc., Deep Learning





SunRay Enterprise
Town Sports International, Princeton, NJ
Data Scientist                                                                                                                                          July 2018 - Present                                     

Responsibilities: 
* Developed customer re-engagement strategies and win back strategies
* Executed predictive analysis using Python on 100,000 data points to identify top customers more likely to churn next month 
* ed with several Python packages like Pandas, NumPy, Scikit-learn etc.
* Various Classification where used and tested. Gradient Boosting was finalized based the feasibility and accuracy of results.
* Achieved 92% accuracy in prediction of customer churn using Gradient Boosting Algorithm
* Responsible for data aggregation, data pre-processing, missing value imputation and descriptive and inferential analysis
* Data elements validation using exploratory data analysis (univariate, bi-variate, multi-variate analysis).
* Missing value treatment, outlier capping and anomalies treatment using statistical methods, deriving customized key metrics.
* Dummy variables where created for certain datasets to into the regression. 
* Had to use SMOTE sampling technique to balance the dataset by over-sampling the minority label class
* ed closely with subject-matter experts and business analysts and investigating statistical and predictive and prescriptive patterns in the data to build business solutions. 
* Used Correlation analysis to identify relation between variables, patterns, outliers and causal factors.
* Provided actionable insights for Fitness Center Management to define new business strategies geared towards improving their programs, creating effective marketing campaigns and offering personalized rewards to members
* Utilized statistical techniques to understand the data, perform descriptive statistics (mean, median, mode, density distributions, box plots etc.), inferential statistics (t-test, ANOVA, Chi square etc.) and hypothesis testing.
* Created, analyzed and presented various performance parameters to quickly spot customer behavioral aspects and preferences using Tableau.
* Data Visualization extensively performed using TABLEAU 8.3. 
* Improved visitor experience to ultimately drive higher volumes of customer leads using Google Analytics

Environment: MS Excel, Tableau, Google Analytics, Python's Pandas, NumPy, Sklearn, Seaborn, Machine Learning 







AmerisourceBergen, Dallas, Texas                                            
Data Analyst                                                                                                                                      Jan 2017  June 2018

Responsibilities: 
* Developed financial data analysis leveraging MS Excel and Apptio TBM software to demonstrate the financial impacts of business decisions to over 500 internal clients
* Involved in the complete life cycle of the project performing various tasks like Data Understanding by performing Exploratory Data Analysis, Data Cleansing, Modeling, Evaluating and Deploying. 
* Performed Data Manipulation and Aggregation as required on data collected from Apptio. 
* Identified patterns, data quality issues through Exploratory Data Analysis. 
* Optimized IT service costing platform by introducing new services to be billed, improving transparency to stakeholders
* Collaborated with IT Executives, leadership and other stakeholders on IT Total Cost of Ownership, financial metrics, and cost transparency initiatives
* Provided support to the stakeholders through  expertise on data analytics, driving process and data improvement
* Enhanced organizations expenditure forecasting ability including assessment of trends, identification of variance drivers and feasible action plans

Environment: MS Excel, Apptio, Tableau

Amazon Web Services, New Delhi, India                                                       
Data Analyst                                                                                                                                         Nov 2015  Aug 2016 

Responsibilities: 
* Produced digestible business intelligence and actionable information leading to revenue acceleration
* Captured and surfaced the best data and information to make optimal decisions, driving a rapid expansion of its world-wide sales team
* Spearheaded resource planning and analytical support to the sales team leading to better customer service and cloud product sales
* Defined, built, and scaled metrics and analytical insights to measure the success and drive the day-to-day behavior for the AWS Business Development Team 
* Provided ad-hoc analysis and reports to Executive level management team using Excel and Tableau

Environment: MS Excel, Tableau

AnalytixLabs, Gurgaon, India                                                                                
Data Analyst                                                                                                                                           Apr 2015  Oct 2015                                                                                                                           

Responsibilities:
* Architected strategy for expansion of an ice-cream company by tapping into the customer behavior and characteristics
* Gathering, leaning present and historical data in preparation for data mining;
* Interpreted complex simulation data using statistical methods as per requirements.
* Architected and implemented analytics and visualization components for data analysis.
* Performed predictive modelling to understand thel behavior and preferences of potential customers.
* ed on customer segmentation using an unsupervised learning technique - clustering. 
* Performed K-means clustering on behavioral data to segment and identify the most profitable customers
* Built logistic regression model with 89% accuracy to detect key demographic variables that help discriminate between more profitable and less profitable customers

Environment: Python, SQL, Oracle 12c, R, Tableau, Cluster analysis


Cognizant, Chennai, India
Programmer Analyst - Big Data Division                                                                                          Jun 2014  Mar 2015

Responsibilities: 
* Performed data analysis and data profiling using complex SQL on various sources systems. 
* Created SQL scripts to find data quality issues and to identify keys, data anomalies, and data validation issues.
* Involved in defining the source to target data mappings, business rules, and data definitions.
* Involved in identifying the source data from different systems and map the data into the warehouse.
* Created HBase tables to store variable data formats of input data coming from different portfolios.
* Managed excel spreadsheets, resolved discrepancies associated with metadata.
* Strong experience in importing the metadata from various applications and build end-to-end Data Lineage.
* Importing and exporting data into HDFS and Hive from Teradata using Sqoop.
* Involved in creating Hive tables, loading with data and writing hive queries, which will run internally in MapReduce
* Data preparation including data sanitization, imputing missing values, dealing with outliers/anomalous data.
* Perform Text Analytics on the social media websites like Twitter to capture customer sentiment and improve customer satisfaction index by 7%  
* Gathered business requirements and prepared Software Requirement Specification (SRS) document. Created Visio charts for the flow architecture of the system. 
* Collaborated with one team member in design, analysis, coding.
* Used Team Studio and Build Manager  to develop applications and promote the new design to test environment. 
* Coordinated with the business users on the User Acceptance Tests (UAT) and to get the approval from a business on the design changes.

Environment: Python, R, SQL, BigData Technologies (Sqoop, Spark, Hive), Text Analysis 
				
Paytm, Noida, India
Data Analyst                                                                                                                                 Jan 2013   May 2014
  
Responsibilities: 
* Extracted relevant information from large databases using SQL queries to detect fraud patterns
* Conducted data analysis and identified fraud patterns related to chargebacks
* Identified, evaluated, and documented potential data sources in support of project requirements.
* Extracted, transformed and loaded present and historical data in preparation for data mining.
* Prototyped predictive models of user fraud activity using machine learning;
* Analyzed data to throw insights on effectiveness of campaigns running on Paytm marketplace
* Built content-based recommender from scratch to help users to choose products based on their previous selections.
* Recommendation system, built using Python, provided similar products based on keywords and bio

Environment: SQL, Google Analytics, Python, Machine Learning Models. 

Additional Projects:Web Analytics using Google Analytics 	 
* Designed, built and tuned google analytics reporting to understand user behavior and enhanced e-commerce metrics, increasing google merchandise stores revenue by 15% 
Tableau Data visualization for Cinemark 
* Optimized marketing strategy for Cinemark by visualizing consumer behavior based on the impact of video demand services on movie watching industry
American Heart Association (AHA), Dallas		 
* Recognized characteristics of top donors and potential markets to well align resources and generate more revenue for AHA
Machine Learning: Churn Analysis                                                      		 
* Implemented Logistic regression and Support Vector Machine algorithms to predict parameters influencing customer churn
Machine Learning: Breast Cancer Detection                     		 
* Devised Artificial Neural Net model and decision tree algorithms with 83% accuracy to detect breast cancer
Big Data Analytics		 
* Launched natural language processing (NLP) project to capture trending hashtags and conducted sentimental analysis on twitter feed related to United States presidential election using R
* Analyzed craft brewery data to determine most brewed beer styles and their properties via Spark and Sqoop
Marketing Predictive Analytics using SAS					 
* Evaluated yogurt retail market to find profitable customer segments using hierarchical clustering based on RFM scores
* Deployed logit model to understand the effect of various factors on the utility of our chosen yogurt brand



Education
* Masters, Business Analytics, University of Texas at Dallas, 2018
* Bachelor of Technology, Computer Science Engineering, VIT University-India-2014.









",Data Scientist,resume,"   * 6+ years of experience solving complex business problems across multiple industry verticals by combining business strategy and tactical implementation. * Hands-on experience and comprehensive industry knowledge of Machine Learning, Statistical Modelling, Predictive Modelling, Data Analytics, Data Mining, Text Mining and Natural Language Processing (NLP). * ed with several Python packages like Pandas, NumPy, Scikit-learn, Keras etc.  * ed with several R packages like ggplot2, dplyr, plyr, data. tables etc. * Strong statistical  such as Hypothesis Testing, Principle Component Analysis (PCA) etc. * Expertise in using several Machine Learning Models including Linear Regression, Logistic Regression, Regularization, k Nearest Neighbor, Decision Trees, Random Forests, Boosting, Neural Nets. * Experience in Natural Language Processing (NLP) using Python libraries such as NLTK and spacy  * Experience with Deep Learning Models (ANN, CNN, RNN) * Good experience in writing SQL queries and implementing functions, tables, views etc. * Strong knowledge in Statistical methodologies such as Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions, ANOVA, Chi-Square tests, Time Series, Factor Analysis, Discriminant Analysis * Experienced in utilizing analytical applications like R and Python to identify trends and relationships between different pieces of data, draw appropriate conclusions and translate analytical findings into decision making and marketing strategies that drive value.  * Expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis, Sentiment Analysis and Predictive Modeling.      Programming  Python, R, SQL Databases MySQL Statistical Software SAS, R, Python ETL/BI   Tableau, MS Excel, SQL  Statistical Methods Hypothesis Testing, Principal Component Analysis (PCA), Sampling Distributions, ANOVA, Chi-Square tests, Time Series, Factor Analysis, Discriminant Analysis Big Data Hadoop, Hive, Sqoop, Spark(Scala) Machine Learning Models Linear Regression, Logistic Regression, Regularization, Support Vector Machines, Neural Nets, Decision Trees, Ensemble Methods like Random Forests, Gradient Boost etc., Deep Learning      SunRay Enterprise Town Sports International, Princeton, NJ Data Scientist                                                                                                                                          July 2018 - Present                                       Responsibilities:  * Developed customer re-engagement strategies and win back strategies * Executed predictive analysis using Python on 100,000 data points to identify top customers more likely to churn next month  * ed with several Python packages like Pandas, NumPy, Scikit-learn etc. * Various Classification where used and tested. Gradient Boosting was finalized based the feasibility and accuracy of results. * Achieved 92% accuracy in prediction of customer churn using Gradient Boosting Algorithm * Responsible for data aggregation, data pre-processing, missing value imputation and descriptive and inferential analysis * Data elements validation using exploratory data analysis (univariate, bi-variate, multi-variate analysis). * Missing value treatment, outlier capping and anomalies treatment using statistical methods, deriving customized key metrics. * Dummy variables where created for certain datasets to into the regression.  * Had to use SMOTE sampling technique to balance the dataset by over-sampling the minority label class * ed closely with subject-matter experts and business analysts and investigating statistical and predictive and prescriptive patterns in the data to build business solutions.  * Used Correlation analysis to identify relation between variables, patterns, outliers and causal factors. * Provided actionable insights for Fitness Center Management to define new business strategies geared towards improving their programs, creating effective marketing campaigns and offering personalized rewards to members * Utilized statistical techniques to understand the data, perform descriptive statistics (mean, median, mode, density distributions, box plots etc.), inferential statistics (t-test, ANOVA, Chi square etc.) and hypothesis testing. * Created, analyzed and presented various performance parameters to quickly spot customer behavioral aspects and preferences using Tableau. * Data Visualization extensively performed using TABLEAU 8.3.  * Improved visitor experience to ultimately drive higher volumes of customer leads using Google Analytics  Environment: MS Excel, Tableau, Google Analytics, Python's Pandas, NumPy, Sklearn, Seaborn, Machine Learning         AmerisourceBergen, Dallas, Texas                                             Data Analyst                                                                                                                                      Jan 2017  June 2018  Responsibilities:  * Developed financial data analysis leveraging MS Excel and Apptio TBM software to demonstrate the financial impacts of business decisions to over 500 internal clients * Involved in the complete life cycle of the project performing various tasks like Data Understanding by performing Exploratory Data Analysis, Data Cleansing, Modeling, Evaluating and Deploying.  * Performed Data Manipulation and Aggregation as required on data collected from Apptio.  * Identified patterns, data quality issues through Exploratory Data Analysis.  * Optimized IT service costing platform by introducing new services to be billed, improving transparency to stakeholders * Collaborated with IT Executives, leadership and other stakeholders on IT Total Cost of Ownership, financial metrics, and cost transparency initiatives * Provided support to the stakeholders through  expertise on data analytics, driving process and data improvement * Enhanced organizations expenditure forecasting ability including assessment of trends, identification of variance drivers and feasible action plans  Environment: MS Excel, Apptio, Tableau  Amazon Web Services, New Delhi, India                                                        Data Analyst                                                                                                                                         Nov 2015  Aug 2016   Responsibilities:  * Produced digestible business intelligence and actionable information leading to revenue acceleration * Captured and surfaced the best data and information to make optimal decisions, driving a rapid expansion of its world-wide sales team * Spearheaded resource planning and analytical support to the sales team leading to better customer service and cloud product sales * Defined, built, and scaled metrics and analytical insights to measure the success and drive the day-to-day behavior for the AWS Business Development Team  * Provided ad-hoc analysis and reports to Executive level management team using Excel and Tableau  Environment: MS Excel, Tableau  AnalytixLabs, Gurgaon, India                                                                                 Data Analyst                                                                                                                                           Apr 2015  Oct 2015                                                                                                                             Responsibilities: * Architected strategy for expansion of an ice-cream company by tapping into the customer behavior and characteristics * Gathering, leaning present and historical data in preparation for data mining; * Interpreted complex simulation data using statistical methods as per requirements. * Architected and implemented analytics and visualization components for data analysis. * Performed predictive modelling to understand thel behavior and preferences of potential customers. * ed on customer segmentation using an unsupervised learning technique - clustering.  * Performed K-means clustering on behavioral data to segment and identify the most profitable customers * Built logistic regression model with 89% accuracy to detect key demographic variables that help discriminate between more profitable and less profitable customers  Environment: Python, SQL, Oracle 12c, R, Tableau, Cluster analysis   Cognizant, Chennai, India Programmer Analyst - Big Data Division                                                                                          Jun 2014  Mar 2015  Responsibilities:  * Performed data analysis and data profiling using complex SQL on various sources systems.  * Created SQL scripts to find data quality issues and to identify keys, data anomalies, and data validation issues. * Involved in defining the source to target data mappings, business rules, and data definitions. * Involved in identifying the source data from different systems and map the data into the warehouse. * Created HBase tables to store variable data formats of input data coming from different portfolios. * Managed excel spreadsheets, resolved discrepancies associated with metadata. * Strong experience in importing the metadata from various applications and build end-to-end Data Lineage. * Importing and exporting data into HDFS and Hive from Teradata using Sqoop. * Involved in creating Hive tables, loading with data and writing hive queries, which will run internally in MapReduce * Data preparation including data sanitization, imputing missing values, dealing with outliers/anomalous data. * Perform Text Analytics on the social media websites like Twitter to capture customer sentiment and improve customer satisfaction index by 7%   * Gathered business requirements and prepared Software Requirement Specification (SRS) document. Created Visio charts for the flow architecture of the system.  * Collaborated with one team member in design, analysis, coding. * Used Team Studio and Build Manager  to develop applications and promote the new design to test environment.  * Coordinated with the business users on the User Acceptance Tests (UAT) and to get the approval from a business on the design changes.  Environment: Python, R, SQL, BigData Technologies (Sqoop, Spark, Hive), Text Analysis  				 Paytm, Noida, India Data Analyst                                                                                                                                 Jan 2013   May 2014    Responsibilities:  * Extracted relevant information from large databases using SQL queries to detect fraud patterns * Conducted data analysis and identified fraud patterns related to chargebacks * Identified, evaluated, and documented potential data sources in support of project requirements. * Extracted, transformed and loaded present and historical data in preparation for data mining. * Prototyped predictive models of user fraud activity using machine learning; * Analyzed data to throw insights on effectiveness of campaigns running on Paytm marketplace * Built content-based recommender from scratch to help users to choose products based on their previous selections. * Recommendation system, built using Python, provided similar products based on keywords and bio  Environment: SQL, Google Analytics, Python, Machine Learning Models.   Additional Projects:Web Analytics using Google Analytics 	  * Designed, built and tuned google analytics reporting to understand user behavior and enhanced e-commerce metrics, increasing google merchandise stores revenue by 15%  Tableau Data visualization for Cinemark  * Optimized marketing strategy for Cinemark by visualizing consumer behavior based on the impact of video demand services on movie watching industry American Heart Association (AHA), Dallas		  * Recognized characteristics of top donors and potential markets to well align resources and generate more revenue for AHA Machine Learning: Churn Analysis                                                      		  * Implemented Logistic regression and Support Vector Machine algorithms to predict parameters influencing customer churn Machine Learning: Breast Cancer Detection                     		  * Devised Artificial Neural Net model and decision tree algorithms with 83% accuracy to detect breast cancer Big Data Analytics		  * Launched natural language processing (NLP) project to capture trending hashtags and conducted sentimental analysis on twitter feed related to United States presidential election using R * Analyzed craft brewery data to determine most brewed beer styles and their properties via Spark and Sqoop Marketing Predictive Analytics using SAS					  * Evaluated yogurt retail market to find profitable customer segments using hierarchical clustering based on RFM scores * Deployed logit model to understand the effect of various factors on the utility of our chosen yogurt brand    Education * Masters, Business Analytics, University of Texas at Dallas, 2018 * Bachelor of Technology, Computer Science Engineering, VIT University-India-2014.          "
" Extensive programming  in analytical and statistical programming language R, python and SQL.
 
 Experience in querying and extracting the data from various RDBMS databases and NoSQL Databaseand creating data sets for Analysis. 
 Extensive programming  in analytical and statistical programming l anguage R, python and SQL.
 
 Experience in querying and extracting the data from various RDBMS databases and NoSQL Databaseand creating data sets for Analysis. 
 Experience in writing complex SQL queries to retrieve and prepare the data for analysis. 
 Hands-on experience with discovering, analyzing, and identifying key elements in the data therebycommunicating the insights, trends and future forecasts that impact consumer behavior.  Experience ing on data preprocessing steps like exploration, aggregation, missing data imputation, sampling, feature selection, dimensionality reduction, outlier detection in both R and Python. 
 Well versed in machine learning algorithms such as Linear, Logistic and other general linear models,Decision Trees, Random Forest, Support Vector Machines, K nearest neighbors and regularization methods. 
 Experience in designing and developing several predictive models for various financial and non -financial institutions in both R and Python. 
 Significant Experience and knowledge in using python's machine learning toolbox Scikit-Learn and
Natural Language processing toolbox NLTK. 
 Experience in ing with visualization  ggplot2, Matplotlib, Seaborn, Plotly and Tableau. 
 Significant knowledge and experience in Hadoop, MapR, Hive, Impala, Spark Python API (PySpark), 
 SparkSQL and Spark's machine learning library MLLib. 
 A self-motivated and inquisitive individual with strong  ethics who thrives ing independently& in teams.
 Experience

Data Science Analyst
Bowling Green State University
August 2018 to Present
 Developed pricing model to optimize various membership charges in wellness center, resulted in
20% growth in customer base. 
 Analyzed requirements, generated reports and identified business opportunities using Tableaudashboard. 
 Created survey templates across all membership groups to capture customer experience.  Enhanced data collection procedures to include relevant information to build and continuously optimize the analytic systems. 
 Collaborated with I.T. to continuously optimize business performance. 
 Processed, cleansed and verified the integrity of data from various sources used for analysis andreporting. 
 Used predictive modeling to increase and optimize customer experiences, revenue generation, adtargeting and other business outcomes. 
 Implemented various statistical techniques to manipulate the data like missing data imputation,principle component analysis, sampling and t-SNE for visualizing high dimensional data. 
 ed on Text Analytics, developing different Statistical Machine Learning, Data Mining solutions tovarious business problems and generating Data Visualizations using R and Python. 
 Used statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regressionand Time Series Analysis to analyze data for further Model Building. 
 Leveraged the latest data visualization  and techniques to present and communicate analysis tothe leadership team utilizing data management, analytics modeling, and business analysis. 
 Advised leadership team and stakeholders with data-driven solutions and recommended strategiesthat address business challenges. 
Environment: 
Python (NumPy, Pandas, PySpark, Scikit-learn, Matplotlib, NLTK), TSQL, MS SQL Server, XML, R Studio, Spyder, MATLAB, ETL, Machine Learning, Shiny, AWS, Redshift, Java, Tableau.
Data Scientist
Nissan North America - Franklin, TN
November 2017 to July 2018
 Performed sentimental analysis on tweets made on twitter using text mining in R Studio.  Converted unstructured pure text consumer comments data to structured dataset using NLP techniques and feature engineering. 
 Used common NLP techniques, such as pre-processing (tokenization, part-of-speech tagging, parsing,stemming) 
 Built predictive models including support Vector Machine, Decision tree, Naive Bayes Classifier,Neural Net plus ensemble methods of the models to evaluate how the likelihood to recommend of customer groups would change in different set of service by using python scikit-learn. 
 Created insightful dashboards, identified market opportunities across the United States usingTableau. 
 Provided insights for building advertising strategy, leading to 8% increase in sales. 
 Designed and developed Ad-hoc reports as per business requirements. 
 ed on Data Verifications and Validations to evaluate the data generated according to therequirements is appropriate and consistent. 
 Used libraries like BeautifulSoup, pandas, and NumPy in python to scrape the important data fromwebsites. 
 Maintained project plan and weekly status documents to keep track of project activities & timelines. 
 
Environment: 
Python, R, SQL, Pyspark, SparkR, SparkSQL, Hadoop, HDFS, Databricks, DBFS, NLP, Tableau, Web Scrapping.
Data Analyst
Accenture Solutions Pvt Ltd
February 2016 to July 2017
 Played a vital role in developing retail store Loyalty and Sales strategy, increased the sales by 15%. 
 Written SQL queries to fetch complex data from different tables in databases. 
 ed with various customer analytics such as Customer targeting, campaign sales analysis, KPIanalysis, forecasting sales, NLP models. 
 ed on Personalized marketing models to implement simplicity and targeted marketing forspecific customers. 
 ed with Clustering algorithms to target specific group of customers to generate profitablerevenue. 
 Used market basket analysis, association rules analysis to identified patterns, data quality issues andleveraged insights. 
 Designed and analyzed test campaign and recommended future incentives. 
 Derived core insights from the data, developed a hypothesis for A/B testing and performedmultivariate tests for the campaign optimization. 
 Wrote automation processes using Python and the AWS Lambda service 
 Created insightful dashboard on performance of personalization campaigns using Excel & Tableau.  ed extensively in documenting the Source to Target Mapping documents with data transformation logic. 
 Designed KPI dashboards in Google Analytics to measure the effectiveness of ad campaigns. Environment: 
AWS, Lambda, Redshift, Python, TSQL, MS SQL Server, XML, R Studio, Spyder, Jupyter, Docker, Machine Learning, Shiny, Java, Tableau, NLP, Google Analytics
Statistical Analyst
Aakash Engineers Inc
August 2014 to January 2016
 Responsible for gathering business requirements, collect data and perform data preprocessing.  Analyzed customer trends using time series decomposition method and forecast the demand using trend fit analysis. 
 Prepared comprehensive presentations and provided insights to leadership. 
 Communicated effectively in both a verbal and written manner to client team. 
 Completed documentation on all assigned systems and databases, including business rules, logic,and processes. 
 Created Test data and Test Cases documentation for regression and performance. 
 Designed, built, and implemented relational databases. 
 Determined changes in physical database by studying project requirements. 
 Developed intermediate business knowledge of the functional area and processed to understand theapplication of data information to support business function. 
 Facilitated gathering moderately complex business requirements by defining the business problem. 
 Utilized SPSS statistical software to track and analyze data. 
 Optimized data collection procedures and generated reports on a weekly, monthly, and quarterlybasis. 
 Used advanced Microsoft Excel to create pivot tables, used VLOOKUP and other Excel functions. 
 Successfully interpreted data to draw conclusions for managerial action and strategy. 
 Created Data chart presentations and coded variables from original data, conducted statisticalanalysis as and when required and provided summaries of analysis. 
 Maintained the data integrity during extraction, manipulation, processing, analysis and storage. Environment: 
Oracle, Erwin, Informatica, Data Warehousing, SQL, Tableau, MS Excel, ETL 
 
PROJECT PROFILE 
Analysis of Amazon Reviews (Electronic Category) - Big Data Concept: Spark 
 Built product recommendation engine using ALS model in PySpark, predicted average ratings of aproduct using K- Nearest neighbors, segmented items using K-means clustering in Python.  Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model. 
 Performed preprocessing and exploratory data analysis using PySpark. 
 
Housing Sale Price Prediction - Regression Analysis 
 Built multiple regression model to predict sales price of residential properties in Ames, Iowa.  Used statistical techniques such as transformation of variables, check for regression assumptions, variable selection based on AIC, step wise regression, correlation and verified assumptions using constant variance and normality test. 
 
Recommendation Systems - Virtual sales man 
 Built a recommendation system that suggests artists to users according to their musical taste basedon user-based and artist-based filtering using Python. 
 Performed data cleaning, necessary variable transformation, used collaborative filtering technique tofind similarities. 
 Used SFrames functionality from Graphlab package to handle large datasets. 
 
User Interface and Database Design - Using SQL Server 
 Collected data and designed a best secure database for an University facing problem in storing thealumni data. 
 Drawn data models and ERD's and used them to create databases and schemes both in SQL Serverand MY SQL. 
 
 Filtered the data, normalized the relations, assigned specific data types, domains, keys, andconstraints to all fields.
Education

Master of Science in Applied Statistics
Bowling Green State University - Bowling Green, OH
 / IT 

PYTHON (4 years), SQL (4 years), CLUSTERING (4 years), MACHINE LEARNING (4 years), DATABASES (2 years)
Online Profile

https://www.linkedin.com/in/samitha-gillala
Additional Information
 
 
Languages R, Python, SQL, SAS 
AWS EC2, S3, RedShift, Glue 
Big Data Hadoop, Map Reduce, Hive, Impala, Spark (Pyspark, SparkSQL, MLLib) 
Analysis Supervised and Unsupervised Techniques, Time Series Analysis 
Databases Oracle, MySQL, MongoDB, Teradata 
 R Studio, Jupyter, Docker 
Visualization  Tableau, ggplot2 (R), matplotlib (Python), Seaborn, Plotly 
 
MACHINE LEARNING 
 
Classification Logistic Regression, LDA, KNN, SVM, Naïve Bayes, Decision Tree, Random forests, Neural
Nets 
Regression 
Multiple Regression, Ridge Regression, Lasso 
Regression, Regression Trees 
 
Unsupervised learning PCA, k-Means clustering, Hierarchical clustering 
Recommendation Engines Market Basket Analysis, Collaborative filtering, Content based filtering 
Boosting ADA Boost, XGBoost 
Time Series Moving Average, ARIMA 
Text Analytics Text Pre-Processing, Classification, Topic Modeling, Clustering, Sentiment Analysis, Word cloud 
Python Libraries Numpy, Pandas, Scikit-Learn, NLTK, PyMongo",Data Scientist,resume," Extensive programming  in analytical and statistical programming language R, python and SQL.    Experience in querying and extracting the data from various RDBMS databases and NoSQL Databaseand creating data sets for Analysis.   Extensive programming  in analytical and statistical programming l anguage R, python and SQL.    Experience in querying and extracting the data from various RDBMS databases and NoSQL Databaseand creating data sets for Analysis.   Experience in writing complex SQL queries to retrieve and prepare the data for analysis.   Hands-on experience with discovering, analyzing, and identifying key elements in the data therebycommunicating the insights, trends and future forecasts that impact consumer behavior.  Experience ing on data preprocessing steps like exploration, aggregation, missing data imputation, sampling, feature selection, dimensionality reduction, outlier detection in both R and Python.   Well versed in machine learning algorithms such as Linear, Logistic and other general linear models,Decision Trees, Random Forest, Support Vector Machines, K nearest neighbors and regularization methods.   Experience in designing and developing several predictive models for various financial and non -financial institutions in both R and Python.   Significant Experience and knowledge in using python's machine learning toolbox Scikit-Learn and Natural Language processing toolbox NLTK.   Experience in ing with visualization  ggplot2, Matplotlib, Seaborn, Plotly and Tableau.   Significant knowledge and experience in Hadoop, MapR, Hive, Impala, Spark Python API (PySpark),   SparkSQL and Spark's machine learning library MLLib.   A self-motivated and inquisitive individual with strong  ethics who thrives ing independently& in teams.  Experience  Data Science Analyst Bowling Green State University August 2018 to Present  Developed pricing model to optimize various membership charges in wellness center, resulted in 20% growth in customer base.   Analyzed requirements, generated reports and identified business opportunities using Tableaudashboard.   Created survey templates across all membership groups to capture customer experience.  Enhanced data collection procedures to include relevant information to build and continuously optimize the analytic systems.   Collaborated with I.T. to continuously optimize business performance.   Processed, cleansed and verified the integrity of data from various sources used for analysis andreporting.   Used predictive modeling to increase and optimize customer experiences, revenue generation, adtargeting and other business outcomes.   Implemented various statistical techniques to manipulate the data like missing data imputation,principle component analysis, sampling and t-SNE for visualizing high dimensional data.   ed on Text Analytics, developing different Statistical Machine Learning, Data Mining solutions tovarious business problems and generating Data Visualizations using R and Python.   Used statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regressionand Time Series Analysis to analyze data for further Model Building.   Leveraged the latest data visualization  and techniques to present and communicate analysis tothe leadership team utilizing data management, analytics modeling, and business analysis.   Advised leadership team and stakeholders with data-driven solutions and recommended strategiesthat address business challenges.  Environment:  Python (NumPy, Pandas, PySpark, Scikit-learn, Matplotlib, NLTK), TSQL, MS SQL Server, XML, R Studio, Spyder, MATLAB, ETL, Machine Learning, Shiny, AWS, Redshift, Java, Tableau. Data Scientist Nissan North America - Franklin, TN November 2017 to July 2018  Performed sentimental analysis on tweets made on twitter using text mining in R Studio.  Converted unstructured pure text consumer comments data to structured dataset using NLP techniques and feature engineering.   Used common NLP techniques, such as pre-processing (tokenization, part-of-speech tagging, parsing,stemming)   Built predictive models including support Vector Machine, Decision tree, Naive Bayes Classifier,Neural Net plus ensemble methods of the models to evaluate how the likelihood to recommend of customer groups would change in different set of service by using python scikit-learn.   Created insightful dashboards, identified market opportunities across the United States usingTableau.   Provided insights for building advertising strategy, leading to 8% increase in sales.   Designed and developed Ad-hoc reports as per business requirements.   ed on Data Verifications and Validations to evaluate the data generated according to therequirements is appropriate and consistent.   Used libraries like BeautifulSoup, pandas, and NumPy in python to scrape the important data fromwebsites.   Maintained project plan and weekly status documents to keep track of project activities & timelines.    Environment:  Python, R, SQL, Pyspark, SparkR, SparkSQL, Hadoop, HDFS, Databricks, DBFS, NLP, Tableau, Web Scrapping. Data Analyst Accenture Solutions Pvt Ltd February 2016 to July 2017  Played a vital role in developing retail store Loyalty and Sales strategy, increased the sales by 15%.   Written SQL queries to fetch complex data from different tables in databases.   ed with various customer analytics such as Customer targeting, campaign sales analysis, KPIanalysis, forecasting sales, NLP models.   ed on Personalized marketing models to implement simplicity and targeted marketing forspecific customers.   ed with Clustering algorithms to target specific group of customers to generate profitablerevenue.   Used market basket analysis, association rules analysis to identified patterns, data quality issues andleveraged insights.   Designed and analyzed test campaign and recommended future incentives.   Derived core insights from the data, developed a hypothesis for A/B testing and performedmultivariate tests for the campaign optimization.   Wrote automation processes using Python and the AWS Lambda service   Created insightful dashboard on performance of personalization campaigns using Excel & Tableau.  ed extensively in documenting the Source to Target Mapping documents with data transformation logic.   Designed KPI dashboards in Google Analytics to measure the effectiveness of ad campaigns. Environment:  AWS, Lambda, Redshift, Python, TSQL, MS SQL Server, XML, R Studio, Spyder, Jupyter, Docker, Machine Learning, Shiny, Java, Tableau, NLP, Google Analytics Statistical Analyst Aakash Engineers Inc August 2014 to January 2016  Responsible for gathering business requirements, collect data and perform data preprocessing.  Analyzed customer trends using time series decomposition method and forecast the demand using trend fit analysis.   Prepared comprehensive presentations and provided insights to leadership.   Communicated effectively in both a verbal and written manner to client team.   Completed documentation on all assigned systems and databases, including business rules, logic,and processes.   Created Test data and Test Cases documentation for regression and performance.   Designed, built, and implemented relational databases.   Determined changes in physical database by studying project requirements.   Developed intermediate business knowledge of the functional area and processed to understand theapplication of data information to support business function.   Facilitated gathering moderately complex business requirements by defining the business problem.   Utilized SPSS statistical software to track and analyze data.   Optimized data collection procedures and generated reports on a weekly, monthly, and quarterlybasis.   Used advanced Microsoft Excel to create pivot tables, used VLOOKUP and other Excel functions.   Successfully interpreted data to draw conclusions for managerial action and strategy.   Created Data chart presentations and coded variables from original data, conducted statisticalanalysis as and when required and provided summaries of analysis.   Maintained the data integrity during extraction, manipulation, processing, analysis and storage. Environment:  Oracle, Erwin, Informatica, Data Warehousing, SQL, Tableau, MS Excel, ETL    PROJECT PROFILE  Analysis of Amazon Reviews (Electronic Category) - Big Data Concept: Spark   Built product recommendation engine using ALS model in PySpark, predicted average ratings of aproduct using K- Nearest neighbors, segmented items using K-means clustering in Python.  Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model.   Performed preprocessing and exploratory data analysis using PySpark.    Housing Sale Price Prediction - Regression Analysis   Built multiple regression model to predict sales price of residential properties in Ames, Iowa.  Used statistical techniques such as transformation of variables, check for regression assumptions, variable selection based on AIC, step wise regression, correlation and verified assumptions using constant variance and normality test.    Recommendation Systems - Virtual sales man   Built a recommendation system that suggests artists to users according to their musical taste basedon user-based and artist-based filtering using Python.   Performed data cleaning, necessary variable transformation, used collaborative filtering technique tofind similarities.   Used SFrames functionality from Graphlab package to handle large datasets.    User Interface and Database Design - Using SQL Server   Collected data and designed a best secure database for an University facing problem in storing thealumni data.   Drawn data models and ERD's and used them to create databases and schemes both in SQL Serverand MY SQL.     Filtered the data, normalized the relations, assigned specific data types, domains, keys, andconstraints to all fields. Education  Master of Science in Applied Statistics Bowling Green State University - Bowling Green, OH  / IT   PYTHON (4 years), SQL (4 years), CLUSTERING (4 years), MACHINE LEARNING (4 years), DATABASES (2 years) Online Profile  https://www.linkedin.com/in/samitha-gillala Additional Information     Languages R, Python, SQL, SAS  AWS EC2, S3, RedShift, Glue  Big Data Hadoop, Map Reduce, Hive, Impala, Spark (Pyspark, SparkSQL, MLLib)  Analysis Supervised and Unsupervised Techniques, Time Series Analysis  Databases Oracle, MySQL, MongoDB, Teradata   R Studio, Jupyter, Docker  Visualization  Tableau, ggplot2 (R), matplotlib (Python), Seaborn, Plotly    MACHINE LEARNING    Classification Logistic Regression, LDA, KNN, SVM, Naïve Bayes, Decision Tree, Random forests, Neural Nets  Regression  Multiple Regression, Ridge Regression, Lasso  Regression, Regression Trees    Unsupervised learning PCA, k-Means clustering, Hierarchical clustering  Recommendation Engines Market Basket Analysis, Collaborative filtering, Content based filtering  Boosting ADA Boost, XGBoost  Time Series Moving Average, ARIMA  Text Analytics Text Pre-Processing, Classification, Topic Modeling, Clustering, Sentiment Analysis, Word cloud  Python Libraries Numpy, Pandas, Scikit-Learn, NLTK, PyMongo"
" A Passionate, team-oriented Data Scientist with experience in Data Extraction, Data Modelling,
Statistical Modeling, Data Mining, Machine Learning and Data Visualization. 
 Expertise in transforming business resources and tasks into regularized data and analytical models,designing algorithms, developing data mining and reporting solutions across a massive volume of structured and unstructured data. 
 Involved in entire data science project life cycle, including Data Acquisition, Data Cleansing, Data
Manipulation, Feature Engineering, Modelling, Evaluation, Optimization, Testing and Deployment. 
 Experienced in building various machine learning predictive models using algorithms such as Linear
Regression, Logistic Regression, Naïve Bayes Classifier, Support Vector Machines (SVM), Neural
Nets, KNN, K-means Clustering, Decision Trees, Ensemble methods (Random Forest, AdaBoost,
Gradient Boosting, and Bagging). 
 Proficient with Python 3.x including NumPy, Scikit-learn, NLP, Pandas, Matplotlib and Seaborn.  Extensive experience in RDBMS such as SQL server 2012, Oracle 9i/10g and non-relational database such as MongoDB 3.x. 
 Hand on experience on Hadoop 2.x ecosystem and Apache Spark 2.x frame such as Hive, Pig,and PySpark. 
 Proficient at data visualization  such as Tableau, R ggplot, Python Matplotlib and Seaborn. 
 Experienced designing and developing T-SQL queries, ETL packages and business reports using SQL
Server Management Studio (SSMS) and BI Suite (SSIS/SSRS). 
 Adept in developing and debugging Stored Procedures, User-defined Functions (UDFs), Triggers,
Indexes, Constraints, Transactions and Queries using Transact-SQL (T-SQL). 
 Knowledge and experience ing in Waterfall as well as Agile environments including the Scrumprocess and using Project Management  like ProjectLibre, Jira/Confluence and version control  such as Github. 
 Self-motivated, Fast Learner, good team lead and player, strong managing and communication 
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data Scientist
AmeriHealth Caritas, PA
September 2018 to Present
AmeriHealth has been providing managed care, primarily HMO, products to more than 265,000 members in Delaware, New Jersey, and Pennsylvania. As a subsidiary of Independence Blue Cross, the managed care provider operates as two entities, AmeriHealth HMO and AmeriHealth Insurance, though it prefers one unified name, AmeriHealth. 
 
Responsibilities: 
 Communicated and coordinated with end client for collecting data and performed ETL to define theuniform standard format. 
 Queried and retrieved data from SQL Server database to get the sample dataset. 
 In preprocessing phase, used Pandas to clean all the missing data, datatype casting and merging orgrouping tables for EDA process. 
 Used PCA and other feature engineering, feature normalization and label encoding Scikit-learnpreprocessing techniques to reduce the high dimensional data (>150 features). 
 In data exploration stage used correlation analysis and graphical techniques in Matplotlib and
Seaborn to get some insights about the patient admission and discharge data. 
 Experimented with predictive models including Logistic Regression, Support Vector Machine (SVC),Random Forest provided by Scikit-learn, XGBoost, LightGBM and Neural net by Keras to predict showing probability and visiting counts. 
 Designed and implemented Cross-validation and statistical tests including k-fold, stratified k-fold,hold-out scheme to test and verify the models' significance. 
 Implemented, tuned and tested the model on AWS Lambda with the best performing algorithm andparameters. 
 Implemented Hypothesis testing kit for sparse sample data by wring R packages. 
 Collected the feedback after deployment, retrained the model to improve the performance. 
 Communicated and coordinated with end client for collecting data and performed ETL to define theuniform standard format. 
 Queried and retrieved data from SQL Server database to get the sample dataset. 
 In preprocessing phase, used Pandas to clean all the missing data, datatype casting and merging orgrouping tables for EDA process. 
 Used PCA and other feature engineering, feature normalization and label encoding Scikit-learnpreprocessing techniques to reduce the high dimensional data (>150 features). 
 In data exploration stage used correlation analysis and graphical techniques in Matplotlib and
Seaborn to get some insights about the patient admission and discharge data. 
 Experimented with predictive models including Logistic Regression, Support Vector Machine (SVC),Random Forest provided by Scikit-learn, XGBoost, LightGBM and Neural net by Keras to predict showing probability and visiting counts. 
 Designed and implemented Cross-validation and statistical tests including k-fold, stratified k-fold,hold-out scheme to test and verify the models' significance. 
 Implemented, tuned and tested the model on AWS Lambda with the best performing algorithm andparameters. 
 Implemented Hypothesis testing kit for sparse sample data by wring R packages. 
 Collected the feedback after deployment, retrained the model to improve the performance. 
 Designed, developed and maintained daily and monthly , trending and benchmark reportsin Tableau Desktop. 
 
Environment: 
R 3.x, gmodels, Python3.x, Scikit-Learn, Web Crawling, ETL, Root Cause Analysis, Factor Analysis, PCA,
KNN, Statistical Tests, Ensemble Model, Regularized Linear Models, Lasso Model, Random Forests, Attribution Models, Tableau 9.x
Data Quality Analyst
City of New York-Fire Department - New York, NY
June 2018 to September 2018
The Fire Department of the City of New York (FDNY) is the largest Fire Department in the United States and universally is recognized as the world's busiest and most highly skilled emergency response agency. Management Analysis and Planning (MAP) department is responsible for providing ongoing analytical support to critical Department initiatives through quantitative modeling, reporting and forecasting. 
 
Responsibilities: 
 Collaborated in a team of four to data mine FDNY records and determine the changes in racialdiversity of firefighter recruitment over the course of fourteen years. 
 Extracted, processed, and analyzed EMS travel data throughout the course of the day in New York
City to reduce response time. 
 Recommended demand forecasting models directly to the Deputy Commissioner of Management of
Analysis and Planning for EMS deployment to improve accuracy and reduce response time.  Created action filters, parameters, and calculated fields to prepare interactive dashboards and sheets in Tableau and identify trends for other colleagues. 
 Implemented advanced geographic mapping techniques in R to visualize the location of inactive firealarm boxes in all five boroughs of New York City. 
 Proposed a cost cutting solution for the diversity recruitment project by delivering a presentation tothe department supervisors. 
 
Environment: 
SQL Server 2012/2014, AWS EC2, AWS Lambda, AWS S3, AWS EMR, Linux, Python3.x (Scikit-Learn, NumPy, Pandas, Matplotlib), R, Machine Learning algorithms, Tableau.
Data Scientist
Hexistech Inc - North Brunswick, NJ October 2017 to May 2018
Hexistech was founded by s from Bell Labs, with a vision of providing quality and costeffective IT services. Which ed relentlessly to build a mature and thriving IT services firm, helping clients develop and deploy meaningful IT solutions enhancing their strategic goals. 
 
Responsibilities: 
 Developed and applied methods to identify, collect, process, and analyze large volumes of data tobuild and enhance products, processes, and systems. 
 Conducted data mining and retrieval, and apply statistical and mathematical analyses to identifytrends, solve analytical problems, optimize performance, and gather intelligence. 
 Visualized information using a range of  (e.g., GIS , RStudio), develop scripts andalgorithms, create explanatory analysis and predictive models, and perform comparative analyses to address complex problems. 
 Responsible for reporting of findings that will use gathered metrics to infer and draw logicalconclusions from past and future behavior. 
 Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route. 
 Used Principal Component Analysis & Factor Analysis in feature engineering to analyze highdimensional data in MATLAB. 
 Performed data analysis by using SQL to retrieve the data from Hadoop cluster. 
 Used R machine learning library to build and evaluate different models. 
 
Environment: 
SQL Server 2012/2014, AWS EC2, AWS Lambda, AWS S3, AWS EMR, Linux, Python3.x (Scikit-Learn, NumPy, Pandas, Matplotlib), R, Machine Learning algorithms, Tableau.
Data Analyst (Python/R)
I-Next Technology
January 2015 to July 2017
I-Next Technology is a market research and analytics start-up company. The company offers market research, but there is also market analysis, competitive intelligence, product intelligence and, most importantly, the expertise to combine these elements in an expert synthesis that generates insight and recommendation rooted in cast-iron fact. 
 
Responsibilities: 
 Participated in requirement gathering, analyzed business needs for modeling and analytics andprovided roadmaps for statistical analysis. 
 Performed data visualization, data cleaning, feature engineering (categorical feature encoding,feature conjunction, normalization), model selection (Deep neural nets, gradient boosted regressor and random forests), and model ensemble (stacking). 
 Implemented ETL process and Data Cleaning for both the internal and external data sources through
Python Pandas and NumPy. 
 Identified and selected the effective features by using Principal Components Analysis and KNN byusing Python SciPy. 
 Built predictive models including Regularized Linear Models, Lasso Model, Random Forests to predictfuture claim severity by using Python Scikit-Learn. 
 Developed Ensemble Model using R gmodels to combine multiple predictive models and theirpredictions for improving the prediction accuracy. 
 Designed and implemented cross-validation and statistical tests including Hypothetical Testing,
ANOVA, Autocorrelation to verify each predictive model. 
 Evaluated and recommended the optimized time frequency and time duration for email advertisingcampaigns. 
 Used Tableau 9.x, R to create detail level  reports and dashboards to technical and businessstakeholders, by using KPI's and visualized trend analysis. 
 
Environment: 
R 3.x, gmodels, Python3.x, Scikit-Learn, Web Crawling, ETL, Root Cause Analysis, Factor Analysis, PCA,
KNN, Statistical Tests, Ensemble Model, Regularized Linear Models, Lasso Model, Random Forests, Attribution Models, Tableau 9.x
Education

Master's in Information Systems in Information Systems
Pace University - New York, NY


Anova, Boosting, Decision trees, K-means, Lda, Logistic regression, Machine learning, Naïve bayes,
Neural nets, Principal component analysis, Pca, Random forest, Support vector machine, Svm, C+ +, Git, Hadoop, Hbase, Hdfs, Hive, Business Intelligence, Excel, access, testing, SQL
Additional Information

TECHNICAL  
Databases MS SQL Server 2008/2008R2/2012/2014/2016, MongoDB 3.x, MySQL 5.x, Oracle, HBase,
Amazon Redshift, Teradata 
Statistical Methods 
Hypothetical Testing, ANOVA, Time Series, Confidence Intervals, Bayes Law, Principal Component Analysis (PCA), Chi-square test, Chebyshev's inequality 
 
Machine Learning 
Linear Regressions, Logistic Regression, Naïve Bayes, Decision Trees, Random Forest, Support Vector
Machine (SVM), Neural Nets, Sentiment Analysis, K-Means Clustering, K-nearest Neighbors (KNN), Ensemble Methods, Gradient Boosting Trees, Ada Boosting, PCA, LDA 
 
Hadoop Ecosystem Hadoop 2.x, Spark 2.x, MapReduce, Hive QL, HDFS, Sqoop, Pig Latin 
BI Reporting  Tableau 10.x / 9.x, MS SQL Server Integration Service and Reporting Service (SSIS/ SSRS), Power BI 
Data Visualization Tableau, Python (Matplotlib, Seaborn), R(ggplot2), Looker, Power BI, QlikView Languages 
Python 2.x/3.x (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn), R (dplyr, ggplot2, rpart, caret,
Random Forest, gbm, neuralnet), SQL (T-SQL, MySQL), C++, MATLAB, Octave 
 
Operating Systems UNIX/UNIX Shell Scripting (via PuTTY client), Linux and Windows XP/7/8/10, Mac OS
 
Other  and technologies 
Azure ML Studio, Google TensorFlow, Apache Tomcat Webserver, MS Office Suite, Lucid Chart, Stat
, ProjectLibre, Google Analytics, Google Tag Manager, Salesforce, MS SharePoint, Trello, JIRA,
Confluence, GitHub/Git, AWS - (EC2/S3/Redshift/EMR/Lambda)",Data Scientist,resume," A Passionate, team-oriented Data Scientist with experience in Data Extraction, Data Modelling, Statistical Modeling, Data Mining, Machine Learning and Data Visualization.   Expertise in transforming business resources and tasks into regularized data and analytical models,designing algorithms, developing data mining and reporting solutions across a massive volume of structured and unstructured data.   Involved in entire data science project life cycle, including Data Acquisition, Data Cleansing, Data Manipulation, Feature Engineering, Modelling, Evaluation, Optimization, Testing and Deployment.   Experienced in building various machine learning predictive models using algorithms such as Linear Regression, Logistic Regression, Naïve Bayes Classifier, Support Vector Machines (SVM), Neural Nets, KNN, K-means Clustering, Decision Trees, Ensemble methods (Random Forest, AdaBoost, Gradient Boosting, and Bagging).   Proficient with Python 3.x including NumPy, Scikit-learn, NLP, Pandas, Matplotlib and Seaborn.  Extensive experience in RDBMS such as SQL server 2012, Oracle 9i/10g and non-relational database such as MongoDB 3.x.   Hand on experience on Hadoop 2.x ecosystem and Apache Spark 2.x frame such as Hive, Pig,and PySpark.   Proficient at data visualization  such as Tableau, R ggplot, Python Matplotlib and Seaborn.   Experienced designing and developing T-SQL queries, ETL packages and business reports using SQL Server Management Studio (SSMS) and BI Suite (SSIS/SSRS).   Adept in developing and debugging Stored Procedures, User-defined Functions (UDFs), Triggers, Indexes, Constraints, Transactions and Queries using Transact-SQL (T-SQL).   Knowledge and experience ing in Waterfall as well as Agile environments including the Scrumprocess and using Project Management  like ProjectLibre, Jira/Confluence and version control  such as Github.   Self-motivated, Fast Learner, good team lead and player, strong managing and communication  Willing to relocate: Anywhere Sponsorship required to  in the US  Experience  Data Scientist AmeriHealth Caritas, PA September 2018 to Present AmeriHealth has been providing managed care, primarily HMO, products to more than 265,000 members in Delaware, New Jersey, and Pennsylvania. As a subsidiary of Independence Blue Cross, the managed care provider operates as two entities, AmeriHealth HMO and AmeriHealth Insurance, though it prefers one unified name, AmeriHealth.    Responsibilities:   Communicated and coordinated with end client for collecting data and performed ETL to define theuniform standard format.   Queried and retrieved data from SQL Server database to get the sample dataset.   In preprocessing phase, used Pandas to clean all the missing data, datatype casting and merging orgrouping tables for EDA process.   Used PCA and other feature engineering, feature normalization and label encoding Scikit-learnpreprocessing techniques to reduce the high dimensional data (>150 features).   In data exploration stage used correlation analysis and graphical techniques in Matplotlib and Seaborn to get some insights about the patient admission and discharge data.   Experimented with predictive models including Logistic Regression, Support Vector Machine (SVC),Random Forest provided by Scikit-learn, XGBoost, LightGBM and Neural net by Keras to predict showing probability and visiting counts.   Designed and implemented Cross-validation and statistical tests including k-fold, stratified k-fold,hold-out scheme to test and verify the models' significance.   Implemented, tuned and tested the model on AWS Lambda with the best performing algorithm andparameters.   Implemented Hypothesis testing kit for sparse sample data by wring R packages.   Collected the feedback after deployment, retrained the model to improve the performance.   Communicated and coordinated with end client for collecting data and performed ETL to define theuniform standard format.   Queried and retrieved data from SQL Server database to get the sample dataset.   In preprocessing phase, used Pandas to clean all the missing data, datatype casting and merging orgrouping tables for EDA process.   Used PCA and other feature engineering, feature normalization and label encoding Scikit-learnpreprocessing techniques to reduce the high dimensional data (>150 features).   In data exploration stage used correlation analysis and graphical techniques in Matplotlib and Seaborn to get some insights about the patient admission and discharge data.   Experimented with predictive models including Logistic Regression, Support Vector Machine (SVC),Random Forest provided by Scikit-learn, XGBoost, LightGBM and Neural net by Keras to predict showing probability and visiting counts.   Designed and implemented Cross-validation and statistical tests including k-fold, stratified k-fold,hold-out scheme to test and verify the models' significance.   Implemented, tuned and tested the model on AWS Lambda with the best performing algorithm andparameters.   Implemented Hypothesis testing kit for sparse sample data by wring R packages.   Collected the feedback after deployment, retrained the model to improve the performance.   Designed, developed and maintained daily and monthly , trending and benchmark reportsin Tableau Desktop.    Environment:  R 3.x, gmodels, Python3.x, Scikit-Learn, Web Crawling, ETL, Root Cause Analysis, Factor Analysis, PCA, KNN, Statistical Tests, Ensemble Model, Regularized Linear Models, Lasso Model, Random Forests, Attribution Models, Tableau 9.x Data Quality Analyst City of New York-Fire Department - New York, NY June 2018 to September 2018 The Fire Department of the City of New York (FDNY) is the largest Fire Department in the United States and universally is recognized as the world's busiest and most highly skilled emergency response agency. Management Analysis and Planning (MAP) department is responsible for providing ongoing analytical support to critical Department initiatives through quantitative modeling, reporting and forecasting.    Responsibilities:   Collaborated in a team of four to data mine FDNY records and determine the changes in racialdiversity of firefighter recruitment over the course of fourteen years.   Extracted, processed, and analyzed EMS travel data throughout the course of the day in New York City to reduce response time.   Recommended demand forecasting models directly to the Deputy Commissioner of Management of Analysis and Planning for EMS deployment to improve accuracy and reduce response time.  Created action filters, parameters, and calculated fields to prepare interactive dashboards and sheets in Tableau and identify trends for other colleagues.   Implemented advanced geographic mapping techniques in R to visualize the location of inactive firealarm boxes in all five boroughs of New York City.   Proposed a cost cutting solution for the diversity recruitment project by delivering a presentation tothe department supervisors.    Environment:  SQL Server 2012/2014, AWS EC2, AWS Lambda, AWS S3, AWS EMR, Linux, Python3.x (Scikit-Learn, NumPy, Pandas, Matplotlib), R, Machine Learning algorithms, Tableau. Data Scientist Hexistech Inc - North Brunswick, NJ October 2017 to May 2018 Hexistech was founded by s from Bell Labs, with a vision of providing quality and costeffective IT services. Which ed relentlessly to build a mature and thriving IT services firm, helping clients develop and deploy meaningful IT solutions enhancing their strategic goals.    Responsibilities:   Developed and applied methods to identify, collect, process, and analyze large volumes of data tobuild and enhance products, processes, and systems.   Conducted data mining and retrieval, and apply statistical and mathematical analyses to identifytrends, solve analytical problems, optimize performance, and gather intelligence.   Visualized information using a range of  (e.g., GIS , RStudio), develop scripts andalgorithms, create explanatory analysis and predictive models, and perform comparative analyses to address complex problems.   Responsible for reporting of findings that will use gathered metrics to infer and draw logicalconclusions from past and future behavior.   Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route.   Used Principal Component Analysis & Factor Analysis in feature engineering to analyze highdimensional data in MATLAB.   Performed data analysis by using SQL to retrieve the data from Hadoop cluster.   Used R machine learning library to build and evaluate different models.    Environment:  SQL Server 2012/2014, AWS EC2, AWS Lambda, AWS S3, AWS EMR, Linux, Python3.x (Scikit-Learn, NumPy, Pandas, Matplotlib), R, Machine Learning algorithms, Tableau. Data Analyst (Python/R) I-Next Technology January 2015 to July 2017 I-Next Technology is a market research and analytics start-up company. The company offers market research, but there is also market analysis, competitive intelligence, product intelligence and, most importantly, the expertise to combine these elements in an expert synthesis that generates insight and recommendation rooted in cast-iron fact.    Responsibilities:   Participated in requirement gathering, analyzed business needs for modeling and analytics andprovided roadmaps for statistical analysis.   Performed data visualization, data cleaning, feature engineering (categorical feature encoding,feature conjunction, normalization), model selection (Deep neural nets, gradient boosted regressor and random forests), and model ensemble (stacking).   Implemented ETL process and Data Cleaning for both the internal and external data sources through Python Pandas and NumPy.   Identified and selected the effective features by using Principal Components Analysis and KNN byusing Python SciPy.   Built predictive models including Regularized Linear Models, Lasso Model, Random Forests to predictfuture claim severity by using Python Scikit-Learn.   Developed Ensemble Model using R gmodels to combine multiple predictive models and theirpredictions for improving the prediction accuracy.   Designed and implemented cross-validation and statistical tests including Hypothetical Testing, ANOVA, Autocorrelation to verify each predictive model.   Evaluated and recommended the optimized time frequency and time duration for email advertisingcampaigns.   Used Tableau 9.x, R to create detail level  reports and dashboards to technical and businessstakeholders, by using KPI's and visualized trend analysis.    Environment:  R 3.x, gmodels, Python3.x, Scikit-Learn, Web Crawling, ETL, Root Cause Analysis, Factor Analysis, PCA, KNN, Statistical Tests, Ensemble Model, Regularized Linear Models, Lasso Model, Random Forests, Attribution Models, Tableau 9.x Education  Master's in Information Systems in Information Systems Pace University - New York, NY   Anova, Boosting, Decision trees, K-means, Lda, Logistic regression, Machine learning, Naïve bayes, Neural nets, Principal component analysis, Pca, Random forest, Support vector machine, Svm, C+ +, Git, Hadoop, Hbase, Hdfs, Hive, Business Intelligence, Excel, access, testing, SQL Additional Information  TECHNICAL   Databases MS SQL Server 2008/2008R2/2012/2014/2016, MongoDB 3.x, MySQL 5.x, Oracle, HBase, Amazon Redshift, Teradata  Statistical Methods  Hypothetical Testing, ANOVA, Time Series, Confidence Intervals, Bayes Law, Principal Component Analysis (PCA), Chi-square test, Chebyshev's inequality    Machine Learning  Linear Regressions, Logistic Regression, Naïve Bayes, Decision Trees, Random Forest, Support Vector Machine (SVM), Neural Nets, Sentiment Analysis, K-Means Clustering, K-nearest Neighbors (KNN), Ensemble Methods, Gradient Boosting Trees, Ada Boosting, PCA, LDA    Hadoop Ecosystem Hadoop 2.x, Spark 2.x, MapReduce, Hive QL, HDFS, Sqoop, Pig Latin  BI Reporting  Tableau 10.x / 9.x, MS SQL Server Integration Service and Reporting Service (SSIS/ SSRS), Power BI  Data Visualization Tableau, Python (Matplotlib, Seaborn), R(ggplot2), Looker, Power BI, QlikView Languages  Python 2.x/3.x (NumPy, Pandas, Scikit-learn, Matplotlib, Seaborn), R (dplyr, ggplot2, rpart, caret, Random Forest, gbm, neuralnet), SQL (T-SQL, MySQL), C++, MATLAB, Octave    Operating Systems UNIX/UNIX Shell Scripting (via PuTTY client), Linux and Windows XP/7/8/10, Mac OS   Other  and technologies  Azure ML Studio, Google TensorFlow, Apache Tomcat Webserver, MS Office Suite, Lucid Chart, Stat , ProjectLibre, Google Analytics, Google Tag Manager, Salesforce, MS SharePoint, Trello, JIRA, Confluence, GitHub/Git, AWS - (EC2/S3/Redshift/EMR/Lambda)"
"  qualified Data Scientist/Data Analyst with around 8+ years of experience in Data
Science and Analytics including Data Mining, Deep Learning/Machine Learning and Statistical Analysis  Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionanddata visualization with large data sets of structured and unstructured data, created ER diagrams and schema. 
 Experienced with machine learning algorithm such as logistic regression, KNN, SVM, random forest,neural net, linear regression, lasso regression and k - means 
 Implemented Bagging and Boosting to enhance the model performance. 
 Experience in implementing data analysis with various analytic , such as Anaconda 4.0 Jupiter
Notebook 4.X, R 3.0 (ggplot2,, dplyr, Caret) and Excel 
 Solid ability to write and optimize diverse SQL queries, ing knowledge of RDBMS like SQL Server
2008/2010/2012 , NoSql databases like MongoDB 3.2 
 Excellent understanding Agile and Scrum development methodology 
 Experienced the full software life cycle in SDLC, Agile, DevOps and Scrum methodologies includingcreating requirements, test plans. 
 Skilled in Advanced Regression Modeling, Correlation, Multivariate Analysis, Model Building, Business
Intelligence  and application of Statistical Concepts. 
 Developed predictive models using Decision Tree, Naive Bayes, Logistic Regression, Random Forest,
Social Net Analysis, Cluster Analysis, and Neural Nets. 
 Experienced in Machine Learning and Statistical Analysis with Python Scikit-Learn. 
 Experienced in Python to manipulate data for data loading and extraction and ed with pythonlibraries like Matplotlib, Scipy, Numpy and Pandas for data analysis. 
 ed with complex applications such as R,R Shiny, SAS, Plotly, ArcGIS, Matlaband SPSS to developneural net, cluster analysis. 
 Strong SQL programming , with experience in ing with functions, packages and triggers.  Expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data. 
 Skilled in performing data parsing, data manipulation, data architecture, data ingestion and datapreparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, Remap, subset, reindex, melt and reshape. 
 Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research.Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system. ed with NoSQLDatabase including Hbase, Cassandra and
MongoDB. 
 Good Knowledge and experience in deep learning algorithms such as Artificial Neural net (ANN),Convolutional Neural Net (CNN) and Recurrent Neural Net (RNN), LSTM and RNN based speech recognition using Tensor Flow. 
 Experienced in Big Data with Hadoop, MapReduce, HDFS and Spark. 
 Experienced in Data Integration Validation and Data Quality controls for ETL process and Data
Warehousing using MS Visual Studio, SSAS, SSISandSSRS. 
 Proficient in Tableau and R-Shiny data visualization  to analyze and obtain insights into largedatasets, create visually powerful and actionable interactive reports and dashboards. 
 Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau. 
 ed in development environment like Git and VM. 
 Excellent communication . Successfully ing in fast-paced multitasking environment bothindependently and in collaborative team, a self-motivated enthusiastic learner.
Willing to relocate to: vr,md,wato
Authorized to  in the US for any employer
 Experience

Data Scientist/ Machine Learning Engineer
BBA Aviation - Orlando, FL
August 2018 to Present
Description: Our people are the foundation of our success. Their service  and their functional, operational and engineering expertise are the core of our business. We are committed to investing in and empowering our employees and providing a safe  environment so that they can have rewarding careers. 
 
Responsibilities: 
 Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra,Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.
 
 Utilized the engine to increase user lifetime by 45% and triple user conversations for targetcategories. 
 Application of various machine learning algorithms and statistical modeling like decision trees, textanalytics, natural language processing (NLP), supervised and unsupervised, regression models, social net analysis, neural nets, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab. 
 ed onanalyzing data from Google Analytics, AdWords, Facebook etc. 
 Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for featureselection and elastic  like ElasticSearch, Kibana. 
 Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package isgoing to deliver on time for the new route. 
 Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python. 
 Performed data cleaning and feature selection using MLlib package in PySpark and ing withdeep learning frames such as Caffe, Neon. 
 Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hiveenvironment with Linux/Windows for big data resources. 
 Used clustering technique K-Means to identify outliers and to classify un-labeled data. 
 Tracking operations using sensors until certain criteria is met using AirFlow. 
 Responsible for different Data mapping activities from Source systems to Teradata using utilities like
TPump, FEXP,BTEQ, MLOAD, FLOADetc 
 Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2. 
 Used Principal Component Analysis in feature engineering to analyze high dimensional data. 
 Used MLlib, Spark's Machine learning library to build and evaluate different models. 
 Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behavior. 
 Developed MapReduce pipeline for feature extraction using Hive and Pig. 
 Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Created various types of data visualizations using Python and Tableau. 
 
Environment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark.
Sr.Data Scientist/ Machine Learning Engineer
USAID - Washington, DC
May 2017 to July 2018
Description: USAID leads international development and humanitarian efforts to save lives, reduce poverty, strengthen democratic governance and help people progress beyond assistance. 
 
Responsibilities: 
 Extracted data from HDFS and prepared data for exploratory analysis using data munging  Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest. 
 Participated in all phases of data mining, data cleaning, data collection, developing models,validation, and visualizationand performed Gap analysis. 
 A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping,
Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop. 
 Setup storage and data analysis  in AWS cloud computing infrastructure. 
 Installed and used CaffeDeep Learning Frame 
 ed on different data formats such as JSON, XML and performed machine learning algorithms inPython. 
 ed as Data Architects and IT Architects to understand the movement of data and its storage andER Studio 9.7 
 Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing variousmachine learning algorithms. 
 Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, PowerBI and Smart View. 
 Implemented Agile Methodology for building an internal application. 
 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition ofIdentity Systems. 
 Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra andsamples to reduce unnecessary procedures and costs. 
 Programmed a utility in Python that used multiple packages (numpy, scipy, pandas) 
 Implemented Classification using supervised algorithms like Logistic Regression, Decision trees,Naive Bayes, KNN. 
 As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports. 
 Updated Python scripts to match training data with our database stored in AWS Cloud Search, so thatwe would be able to assign each document a response label for further classification. 
 Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETLfrom OLTP Source Systems to OLAP Target Systems 
 Data transformation from various resources, data organization, features extraction from raw andstored. 
 Validated the machine learning classifiers using ROC Curves and Lift Charts. 
 
Environment: Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce.
Data Scientist
Bytemark, Inc - New York, NY
January 2016 to April 2017
Description:Bytemark team is a talented group of people dedicated to building a mobile payment system to serve both consumers and merchants while maintaining the highest standards of security. Built several Business Intelligence applications with their specialties in Data Science and Machine Learning. I  to integrate ML solutions into the existing business processes to improve decision making and ROI. 
 
Responsibilities: 
 Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce,Pig and others 
 Converted time lag problems in order fulfilment into Data mining tasks 
 Performed Data Profiling to assess data quality using SQL through complex internal database 
 Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas in Python 
 Built Data warehouse to support end-user queries with Oracle and MS Visual Studio 
 Designed and implemented Dimensional DataModelling for order fulfilment process 
 Deployed SSIS packages to complete ETL and Data Mapping process 
 Transformed data through methods like Aggregation, Slowly Changing Dimension, Splitting 
 Derived business intelligence report for order fulfilment using MS SSAS and SSRS 
 Determined regression model predictors using Correlation matrix for Factor analysis in R 
 Built Regression model to understand order fulfilment time lag issue using Scikit-learn in Python 
 Optimized predictive model by reducing insignificant variables using Stepwise Regression 
 Empowered decision makers with data analysis dashboards using Tableau and Power BI 
 Interface with other  teams to extract, transform, and load (ETL) data from a wide varietyof data sources 
 Own the functional and non-functional scaling of software systems in your ownership area. 
 Provides input and recommendations on technical issues to BIEngineers, Business&DataAnalysts andData Scientists. 
 Outstanding analytical and problem-solving  are essential. 
 
Environment: - Python, Hive, C/C++, C#, Java or Python, Bash, HTML5, PERL, Processing, Python and J Query, SOAPUI, WCF, WPF, VSO, TFS, GIT,XML, XSD, SQL Server 2008, Oracle 10/11g,.
Data Scientist
Ameriprise Financial, Inc - New York, NY
March 2014 to December 2015
Description:Ameriprise Financial, Inc. is an American diversified financial services company to help people feel confident about their financial future. It provides financial planning, products and services, including wealth management, asset management, insurance, annuities and estate planning. 
 
Responsibilities: 
 Supported MapReduce Programs running on the cluster. 
 Configured Hadoop cluster with Name node and slaves and formatted HDFS. 
 Used Oozie flow engine to run multiple Hive and Pig jobs. 
 Performed MapReduce Programs those are running on the cluster. 
 Developed multiple MapReduce jobs in java for data cleaning and pre-processing. 
 Analyzed the partitioned and bucketed data and compute various metrics for reporting. 
 Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume. 
 ed on loading the data from MySQL to HBase where necessary using Sqoop. 
 Developed Hive queries for Analysis across different banners. 
 Extracted data from Twitter using Java and Twitter API.ParsedJSON formatted twitter data anduploaded to database. 
 Launching AmazonEC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuringlaunched instances with respect to specific applications. 
 Exported the result set from Hive to MySQL using Sqoop after processing the data. 
 Analyzed the data by performing Hive queries and running Pigscripts to study customer behavior. 
 Have hands on experience ing on Sequence files, AVRO, HAR file formats and compression. 
 Used Hive to partition and bucketdata. 
 Experience in writing MapReduce programs with JavaAPI to cleanse Structured and unstructureddata. 
 Wrote Pig Scripts to perform ETL procedures on the data in HDFS. 
 Created HBase tables to store various data formats of data coming from different portfolios.  ed on improving performance of existing Pig and HiveQueries. 
 
Environment: -SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, Deep learning approaches
Data Analyst/Modeler
Suditi Soft  Pvt Ltd - Hyderabad, Telangana December 2012 to February 2014
Description:Suditi Soft  is a software solutions company. ed as a Data Engineer, involved in various activities such as Data Exploration, Data Synchronization, Data Transformation, Data Governance, Data Model Creation and Performance Optimization. Involved in all phases of the SDLC of the project, starting from requirement gathering till development of data models. Created status reports and delivered action plans. 
 
Responsibilities: 
 Participated in JAD sessions, gathered information from Business Analysts, end users and otherstakeholders to determine the requirements. 
 Developed the logical data models and physical data models that confine existing condition/potentialstatus data fundamentals and data flows using ER Studio. 
 Created Data warehousing methodologies/Dimensional Data modeling techniques such as Star/
Snowflake schema using ERWIN9.1. 
 Extensively used AginityNetezzabench to perform various DDL, DML etc. operations on Netezzadatabase. 
 Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models.  Performed Daily Monitoring of Oracle instances using Oracle Enterprise Manager, ADDM, TOAD, monitor users, table spaces, memory structures, rollback segments, logs and alerts. 
 Involved in Teradata SQL Development, Unit Testing and Performance Tuning and to ensure testingissues are resolved on the basis of using defect reports. 
 Customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC. 
 Translate business and data requirements into Logical data models in support of Enterprise
DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems. 
 ed on database testing, wrote complex SQL queries to verify the transactions and business logiclike identifying the duplicate rows by using SQL Developer and PL/SQL Developer. 
 Used Teradata SQL Assistant, Teradata Administrator, PMON and data load/export utilities like BTEQ,FastLoad, Multi Load, Fast Export, TPumpon UNIX/Windows environments and running the batch process for Teradata. 
 Hands on Data warehouse concepts like Data warehouse Architecture, Star schema, Snowflakeschema, and Data Marts, Dimension and Fact tables. 
 Developed SQL Queries to fetch complex data from different tables in remote databases using joins,database links and Bulk collects. 
 Migrated database from legacy systems, SQL server to Oracle and Netezza. 
 ed on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) andSSRS (Reporting Services). 
 
Environment: -ER Studio, OBIEE 11.1.1.6, Teradata13.1, SQL, PL/SQL, BTEQ, DB2, Oracle, MDM, Netezza, ETL, RTF UNIX, SQL Server2010, Informatica, SSRS, SSIS, SSAS, SAS, Aginity.
Data Analyst
Systopic Laboratories Pvt. Ltd - Hyderabad, Telangana January 2011 to November 2012
Description:Systopic Laboratories Pvt. Ltd. was incorporated in the year 1984 with an objective to provide quality innovative therapeutic solutions. The guiding principle at Systopic is pursuit of
""Excellence through People & Innovation"". This dictum forms the basis of all thinking & action at Systopic. 
 
Responsibilities: 
 Analyze business information requirements and model class diagrams and/or conceptual domainmodels. 
 Gather & Review Customer Information Requirements for OLAP and building the data mart. 
 Performed document analysis involving creation of Use Cases and Use Case narrations using
Microsoft Visio, in order to present the efficiency of the gathered requirements. 
 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using
Microsoft Access and Oracle SQL. 
 Analyzed business process flows and assisted in the development of ETL procedures formapping data from source to target systems. 
 ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. 
 Enterprise Metadata Library with any changes or updates. 
 Document data quality and traceability documents for each source interface. 
 Establish standards of procedures. 
 Generate weekly and monthly asset inventory reports. 
 Managed the project requirements, documents and use cases by IBM Rational RequisitePro. 
 Assisted in building an Integrated LogicalDataDesign, propose physical database design for buildingthe data mart. 
 Document all data mapping and transformation processes in the Functional Design documents basedon the business requirements. 
 
Environment: -SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.
Education

Bachelor's


Db2, Microsoft sql server, Microsoft sql server 2008, Sql server, Sql server 2008, Mysql, Oracle, Sql,
Cassandra, Hdfs, Impala, Mapreduce, Oozie, Sqoop, Hbase, Kafka, Flume, Hadoop, Mongodb, Splunk
Additional Information

TECHNICAL  
 
BigData/Hadoop  Hadoop, HDFS, YARN, MapReduce, Hive, Pig, Impala, Sqoop, Flume,
Spark, Kafka, Storm, Drill, Zookeeper and Oozie 
Languages 
HTML5,DHTML, WSDL, CSS3, C, C++, XML,R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot), Perl, MATLAB, Mathematica, , Json, Ajax, Java, Scala, Python (NumPy, SciPy, Pandas, Gensim, Keras), Java Script, Shell Scripting 
 
NO SQL Databases Cassandra, HBase, MongoDB, MariaDB 
Business Intelligence  
Tableau server, Tableau Reader, Tableau, Splunk, SAP Business Objects, OBIEE, SAP Business Intelligence, QlikView, Amazon Redshift, or Azure Data Warehouse 
 
Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans. 
Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall 
Build  Jenkins, Toad, SQL Loader, Maven, ANT, RTC, RSA, Control-M, Oziee, Hue, SOAP UI 
Reporting  MS Office (Word/Excel/Power Point/ Visio/Outlook), Crystal reports XI, SSRS, cognos 7.0/6.0. 
Databases Microsoft SQL Server 2008,2010/2012, MySQL 4.x/5.x, Oracle 11g, 12c, DB2, Teradata, Netezza 
Operating Systems All versions of Windows, UNIX, LINUX, Macintosh HD, Sun Solaris",Data Scientist,resume,"  qualified Data Scientist/Data Analyst with around 8+ years of experience in Data Science and Analytics including Data Mining, Deep Learning/Machine Learning and Statistical Analysis  Involved in the entire data science project life cycle and actively involved in all the phases including data cleaning, data extractionanddata visualization with large data sets of structured and unstructured data, created ER diagrams and schema.   Experienced with machine learning algorithm such as logistic regression, KNN, SVM, random forest,neural net, linear regression, lasso regression and k - means   Implemented Bagging and Boosting to enhance the model performance.   Experience in implementing data analysis with various analytic , such as Anaconda 4.0 Jupiter Notebook 4.X, R 3.0 (ggplot2,, dplyr, Caret) and Excel   Solid ability to write and optimize diverse SQL queries, ing knowledge of RDBMS like SQL Server 2008/2010/2012 , NoSql databases like MongoDB 3.2   Excellent understanding Agile and Scrum development methodology   Experienced the full software life cycle in SDLC, Agile, DevOps and Scrum methodologies includingcreating requirements, test plans.   Skilled in Advanced Regression Modeling, Correlation, Multivariate Analysis, Model Building, Business Intelligence  and application of Statistical Concepts.   Developed predictive models using Decision Tree, Naive Bayes, Logistic Regression, Random Forest, Social Net Analysis, Cluster Analysis, and Neural Nets.   Experienced in Machine Learning and Statistical Analysis with Python Scikit-Learn.   Experienced in Python to manipulate data for data loading and extraction and ed with pythonlibraries like Matplotlib, Scipy, Numpy and Pandas for data analysis.   ed with complex applications such as R,R Shiny, SAS, Plotly, ArcGIS, Matlaband SPSS to developneural net, cluster analysis.   Strong SQL programming , with experience in ing with functions, packages and triggers.  Expertise in transforming business requirements into designing algorithms, analytical models, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data.   Skilled in performing data parsing, data manipulation, data architecture, data ingestion and datapreparation with methods including describe data contents, compute descriptive statistics of data, regex, split and combine, merge, Remap, subset, reindex, melt and reshape.   Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research.Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system. ed with NoSQLDatabase including Hbase, Cassandra and MongoDB.   Good Knowledge and experience in deep learning algorithms such as Artificial Neural net (ANN),Convolutional Neural Net (CNN) and Recurrent Neural Net (RNN), LSTM and RNN based speech recognition using Tensor Flow.   Experienced in Big Data with Hadoop, MapReduce, HDFS and Spark.   Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSAS, SSISandSSRS.   Proficient in Tableau and R-Shiny data visualization  to analyze and obtain insights into largedatasets, create visually powerful and actionable interactive reports and dashboards.   Automated recurring reports using SQL andPython and visualized them on BI platform like Tableau.   ed in development environment like Git and VM.   Excellent communication . Successfully ing in fast-paced multitasking environment bothindependently and in collaborative team, a self-motivated enthusiastic learner. Willing to relocate to: vr,md,wato Authorized to  in the US for any employer  Experience  Data Scientist/ Machine Learning Engineer BBA Aviation - Orlando, FL August 2018 to Present Description: Our people are the foundation of our success. Their service  and their functional, operational and engineering expertise are the core of our business. We are committed to investing in and empowering our employees and providing a safe  environment so that they can have rewarding careers.    Responsibilities:   Utilized Spark, Scala, Hadoop, HQL, VQL, oozie, pySpark, Data Lake, TensorFlow, HBase, Cassandra,Redshift, MongoDB, Kafka, Kinesis, Spark Streaming, Edward, CUDA, MLLib, AWS, Python, a broad variety of machine learning methods including classifications, regressions, dimensionally reduction etc.    Utilized the engine to increase user lifetime by 45% and triple user conversations for targetcategories.   Application of various machine learning algorithms and statistical modeling like decision trees, textanalytics, natural language processing (NLP), supervised and unsupervised, regression models, social net analysis, neural nets, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.   ed onanalyzing data from Google Analytics, AdWords, Facebook etc.   Evaluated models using Cross Validation, Log loss function, ROC curves and used AUC for featureselection and elastic  like ElasticSearch, Kibana.   Performed Multinomial Logistic Regression, Decision Tree, Random forest, SVM to classify package isgoing to deliver on time for the new route.   Performed data analysis by using Hive to retrieve the data from Hadoop cluster, Sql to retrieve datafrom Oracle database and used ETL for data transformation.   Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python.   Performed data cleaning and feature selection using MLlib package in PySpark and ing withdeep learning frames such as Caffe, Neon.   Developed Spark/Scala,R Python for regular expression (regex) project in the Hadoop/Hiveenvironment with Linux/Windows for big data resources.   Used clustering technique K-Means to identify outliers and to classify un-labeled data.   Tracking operations using sensors until certain criteria is met using AirFlow.   Responsible for different Data mapping activities from Source systems to Teradata using utilities like TPump, FEXP,BTEQ, MLOAD, FLOADetc   Addressed over fitting by implementing of the algorithm regularization methods like L1 and L2.   Used Principal Component Analysis in feature engineering to analyze high dimensional data.   Used MLlib, Spark's Machine learning library to build and evaluate different models.   Created and designed reports that will use gathered metrics to infer and draw logical conclusions ofpast and future behavior.   Developed MapReduce pipeline for feature extraction using Hive and Pig.   Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of thedata. Created various types of data visualizations using Python and Tableau.    Environment: Python 2.x, CDH5, HDFS, Hadoop 2.3, Hive, Impala, AWS, Linux, Spark, Tableau Desktop, SQL Server 2014, Microsoft Excel, Matlab, Spark SQL, Pyspark. Sr.Data Scientist/ Machine Learning Engineer USAID - Washington, DC May 2017 to July 2018 Description: USAID leads international development and humanitarian efforts to save lives, reduce poverty, strengthen democratic governance and help people progress beyond assistance.    Responsibilities:   Extracted data from HDFS and prepared data for exploratory analysis using data munging  Built models using Statistical techniques like Bayesian HMM and Machine Learning classification models like XGBoost, SVM, and Random Forest.   Participated in all phases of data mining, data cleaning, data collection, developing models,validation, and visualizationand performed Gap analysis.   A highly immersive Data Science program involving Data Manipulation&Visualization, Web Scraping, Machine Learning, Python programming, SQL, GIT, MongoDB, Hadoop.   Setup storage and data analysis  in AWS cloud computing infrastructure.   Installed and used CaffeDeep Learning Frame   ed on different data formats such as JSON, XML and performed machine learning algorithms inPython.   ed as Data Architects and IT Architects to understand the movement of data and its storage andER Studio 9.7   Used pandas, numpy, seaborn, matplotlib, scikit-learn, scipy, NLTK in Python for developing variousmachine learning algorithms.   Data Manipulation and Aggregation from different source using Nexus, Business Objects, Toad, PowerBI and Smart View.   Implemented Agile Methodology for building an internal application.   Focus on integration overlap and Informatica newer commitment to MDM with the acquisition ofIdentity Systems.   Coded proprietary packages to analyze and visualize SPCfile data to identify bad spectra andsamples to reduce unnecessary procedures and costs.   Programmed a utility in Python that used multiple packages (numpy, scipy, pandas)   Implemented Classification using supervised algorithms like Logistic Regression, Decision trees,Naive Bayes, KNN.   As Architect delivered various complex OLAPdatabases/cubes, scorecards, dashboards and reports.   Updated Python scripts to match training data with our database stored in AWS Cloud Search, so thatwe would be able to assign each document a response label for further classification.   Used Teradata utilities such as Fast Export, MLOAD for handling various tasks data migration/ETLfrom OLTP Source Systems to OLAP Target Systems   Data transformation from various resources, data organization, features extraction from raw andstored.   Validated the machine learning classifiers using ROC Curves and Lift Charts.    Environment: Unix, Python 3.5.2, MLLib, SAS, regression, logistic regression, Hadoop 2.7.4, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML and MapReduce. Data Scientist Bytemark, Inc - New York, NY January 2016 to April 2017 Description:Bytemark team is a talented group of people dedicated to building a mobile payment system to serve both consumers and merchants while maintaining the highest standards of security. Built several Business Intelligence applications with their specialties in Data Science and Machine Learning. I  to integrate ML solutions into the existing business processes to improve decision making and ROI.    Responsibilities:   Utilize a broad variety of statistical packages like SAS, R, MLIB, Graphs, Hadoop, Spark, MapReduce,Pig and others   Converted time lag problems in order fulfilment into Data mining tasks   Performed Data Profiling to assess data quality using SQL through complex internal database   Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas in Python   Built Data warehouse to support end-user queries with Oracle and MS Visual Studio   Designed and implemented Dimensional DataModelling for order fulfilment process   Deployed SSIS packages to complete ETL and Data Mapping process   Transformed data through methods like Aggregation, Slowly Changing Dimension, Splitting   Derived business intelligence report for order fulfilment using MS SSAS and SSRS   Determined regression model predictors using Correlation matrix for Factor analysis in R   Built Regression model to understand order fulfilment time lag issue using Scikit-learn in Python   Optimized predictive model by reducing insignificant variables using Stepwise Regression   Empowered decision makers with data analysis dashboards using Tableau and Power BI   Interface with other  teams to extract, transform, and load (ETL) data from a wide varietyof data sources   Own the functional and non-functional scaling of software systems in your ownership area.   Provides input and recommendations on technical issues to BIEngineers, Business&DataAnalysts andData Scientists.   Outstanding analytical and problem-solving  are essential.    Environment: - Python, Hive, C/C++, C#, Java or Python, Bash, HTML5, PERL, Processing, Python and J Query, SOAPUI, WCF, WPF, VSO, TFS, GIT,XML, XSD, SQL Server 2008, Oracle 10/11g,. Data Scientist Ameriprise Financial, Inc - New York, NY March 2014 to December 2015 Description:Ameriprise Financial, Inc. is an American diversified financial services company to help people feel confident about their financial future. It provides financial planning, products and services, including wealth management, asset management, insurance, annuities and estate planning.    Responsibilities:   Supported MapReduce Programs running on the cluster.   Configured Hadoop cluster with Name node and slaves and formatted HDFS.   Used Oozie flow engine to run multiple Hive and Pig jobs.   Performed MapReduce Programs those are running on the cluster.   Developed multiple MapReduce jobs in java for data cleaning and pre-processing.   Analyzed the partitioned and bucketed data and compute various metrics for reporting.   Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume.   ed on loading the data from MySQL to HBase where necessary using Sqoop.   Developed Hive queries for Analysis across different banners.   Extracted data from Twitter using Java and Twitter API.ParsedJSON formatted twitter data anduploaded to database.   Launching AmazonEC2 Cloud Instances using Amazon Images (Linux/ Ubuntu) and Configuringlaunched instances with respect to specific applications.   Exported the result set from Hive to MySQL using Sqoop after processing the data.   Analyzed the data by performing Hive queries and running Pigscripts to study customer behavior.   Have hands on experience ing on Sequence files, AVRO, HAR file formats and compression.   Used Hive to partition and bucketdata.   Experience in writing MapReduce programs with JavaAPI to cleanse Structured and unstructureddata.   Wrote Pig Scripts to perform ETL procedures on the data in HDFS.   Created HBase tables to store various data formats of data coming from different portfolios.  ed on improving performance of existing Pig and HiveQueries.    Environment: -SQL/Server, Oracle 9i, MS-Office, Teradata, Informatica, ER Studio, XML, Business Objects, Deep learning approaches Data Analyst/Modeler Suditi Soft  Pvt Ltd - Hyderabad, Telangana December 2012 to February 2014 Description:Suditi Soft  is a software solutions company. ed as a Data Engineer, involved in various activities such as Data Exploration, Data Synchronization, Data Transformation, Data Governance, Data Model Creation and Performance Optimization. Involved in all phases of the SDLC of the project, starting from requirement gathering till development of data models. Created status reports and delivered action plans.    Responsibilities:   Participated in JAD sessions, gathered information from Business Analysts, end users and otherstakeholders to determine the requirements.   Developed the logical data models and physical data models that confine existing condition/potentialstatus data fundamentals and data flows using ER Studio.   Created Data warehousing methodologies/Dimensional Data modeling techniques such as Star/ Snowflake schema using ERWIN9.1.   Extensively used AginityNetezzabench to perform various DDL, DML etc. operations on Netezzadatabase.   Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models.  Performed Daily Monitoring of Oracle instances using Oracle Enterprise Manager, ADDM, TOAD, monitor users, table spaces, memory structures, rollback segments, logs and alerts.   Involved in Teradata SQL Development, Unit Testing and Performance Tuning and to ensure testingissues are resolved on the basis of using defect reports.   Customized reports using SAS/MACRO facility, PROC REPORT, PROC TABULATE and PROC.   Translate business and data requirements into Logical data models in support of Enterprise DataModels, ODS, OLAP, OLTP, Operational Data Structures and Analytical systems.   ed on database testing, wrote complex SQL queries to verify the transactions and business logiclike identifying the duplicate rows by using SQL Developer and PL/SQL Developer.   Used Teradata SQL Assistant, Teradata Administrator, PMON and data load/export utilities like BTEQ,FastLoad, Multi Load, Fast Export, TPumpon UNIX/Windows environments and running the batch process for Teradata.   Hands on Data warehouse concepts like Data warehouse Architecture, Star schema, Snowflakeschema, and Data Marts, Dimension and Fact tables.   Developed SQL Queries to fetch complex data from different tables in remote databases using joins,database links and Bulk collects.   Migrated database from legacy systems, SQL server to Oracle and Netezza.   ed on SQL Server concepts SSIS (SQL Server Integration Services), SSAS (Analysis Services) andSSRS (Reporting Services).    Environment: -ER Studio, OBIEE 11.1.1.6, Teradata13.1, SQL, PL/SQL, BTEQ, DB2, Oracle, MDM, Netezza, ETL, RTF UNIX, SQL Server2010, Informatica, SSRS, SSIS, SSAS, SAS, Aginity. Data Analyst Systopic Laboratories Pvt. Ltd - Hyderabad, Telangana January 2011 to November 2012 Description:Systopic Laboratories Pvt. Ltd. was incorporated in the year 1984 with an objective to provide quality innovative therapeutic solutions. The guiding principle at Systopic is pursuit of ""Excellence through People & Innovation"". This dictum forms the basis of all thinking & action at Systopic.    Responsibilities:   Analyze business information requirements and model class diagrams and/or conceptual domainmodels.   Gather & Review Customer Information Requirements for OLAP and building the data mart.   Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.   Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.   Analyzed business process flows and assisted in the development of ETL procedures formapping data from source to target systems.   ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.   Enterprise Metadata Library with any changes or updates.   Document data quality and traceability documents for each source interface.   Establish standards of procedures.   Generate weekly and monthly asset inventory reports.   Managed the project requirements, documents and use cases by IBM Rational RequisitePro.   Assisted in building an Integrated LogicalDataDesign, propose physical database design for buildingthe data mart.   Document all data mapping and transformation processes in the Functional Design documents basedon the business requirements.    Environment: -SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer. Education  Bachelor's   Db2, Microsoft sql server, Microsoft sql server 2008, Sql server, Sql server 2008, Mysql, Oracle, Sql, Cassandra, Hdfs, Impala, Mapreduce, Oozie, Sqoop, Hbase, Kafka, Flume, Hadoop, Mongodb, Splunk Additional Information  TECHNICAL     BigData/Hadoop  Hadoop, HDFS, YARN, MapReduce, Hive, Pig, Impala, Sqoop, Flume, Spark, Kafka, Storm, Drill, Zookeeper and Oozie  Languages  HTML5,DHTML, WSDL, CSS3, C, C++, XML,R/R Studio, SAS Enterprise Guide, SAS, R (Caret, Weka, ggplot), Perl, MATLAB, Mathematica, , Json, Ajax, Java, Scala, Python (NumPy, SciPy, Pandas, Gensim, Keras), Java Script, Shell Scripting    NO SQL Databases Cassandra, HBase, MongoDB, MariaDB  Business Intelligence   Tableau server, Tableau Reader, Tableau, Splunk, SAP Business Objects, OBIEE, SAP Business Intelligence, QlikView, Amazon Redshift, or Azure Data Warehouse    Development  Microsoft SQL Studio, IntelliJ, Eclipse, NetBeans.  Development Methodologies Agile/Scrum, UML, Design Patterns, Waterfall  Build  Jenkins, Toad, SQL Loader, Maven, ANT, RTC, RSA, Control-M, Oziee, Hue, SOAP UI  Reporting  MS Office (Word/Excel/Power Point/ Visio/Outlook), Crystal reports XI, SSRS, cognos 7.0/6.0.  Databases Microsoft SQL Server 2008,2010/2012, MySQL 4.x/5.x, Oracle 11g, 12c, DB2, Teradata, Netezza  Operating Systems All versions of Windows, UNIX, LINUX, Macintosh HD, Sun Solaris"
"  
MS in Business Analytics graduate student with experience in providing Audit and 
Consumer products sector 
Possess  in R, Tableau, Python, SQL & Big Data  
 
Advisory services in Industries, Infrastructure and
University of Illinois at Chicago  MS in Business Analytics 	          	          	                           
                              Aug 2018  Dec 2019 
Uttar Pradesh  University  B.Tech in Electronics and Communication Eng.    	              
                         Aug 2010  May 2014 
Statistical/Analytical 	R Studio, SPSS, RapidMiner,  	 Database 
 	MS Excel, Julia, SAS 
Visualization and 	Tableau, Google Analytics, 	 Big Data  
Reporting 	ggplot(R) 	 Languages/Programming 
Productivity 	Microsoft Office Suite 	 
 
MS SQL Server, Oracle, MySQL, 
MongoDB, MS Access, SAP HANA 
Database 
Hadoop & Spark Ecosystems 
Python, R, C, C++ 
 
  
 
 
 
 
	Consultant  IT Risk and Assurance  Ernst & Young, LLP (India)   	 	 	                                       June 2014  July 2018 
 Performed audits related to Program Risk Management, Financial Audit IT Integration, Application Risk and Controls Identification, 
Internal Audit - IT, Service Organization Control Reports (SOCR) in accordance with the attestation standards (ISAE 3402, SSAE 16 and SSAE 18) 
 Performed Fraud Investigation using Data Analytics (using Tableau, MS excel & SQL) and Software Compliance Management 
 Performed internal audits pertaining to vendor risk management, scrap sales, material management (purchase cycle), financial transaction and asset management 
 Performed Marketing collateral planning and execution, business development and contributing to EYs knowledge repository Key Assignments: 
 IT External Audit for the largest Indian Multinational Conglomerate Company 
- Devised audit plan and calendar for quarterly limited reviews and annual audit which included sub-domains like IT General Controls Identification and testing, IT Application Controls testing 
 Performed Data Analytics for Journal Entries (using MS excel) and Data Analytics on transactional data for investigating fraud (using Tableau & python (package  numpy, pandas & matplotlib)) 
 Service Organization Control Review for a Global Business Process Management Company 
- Inspected the description of IT general Controls System for the period under inspection and the suitability of the design and operating effectiveness of controls described therein to achieve the related control s stated in the Description of System (DOS) 
- Conducted examination in accordance with the attestation standards (SSAE 16) established by the American Institute of Certified Public Accountants (AICPA) 
	ACADEMIC  	 	 	 	 	 	 	 	 	 	 	 

	Spam & Ham classifier using NLP (Python  NLTK, Numpy, Pandas & Scikit learn)   	 	                       May 2019  June 2019 
 Used TF-IDF and count vectorization methods for vectoring the data 
 Developed random forest and gradient boosting model for predictions 
 Chose random forest model in model selection due to its high precision as compared to gradient boosting 
	Donor Prediction in Marketing Campaign Analysis (RapidMiner) 	 	 	 	 	        Sept 2018  Dec 2018 
 Sample data from Paralyzed Veterans of America (PVA)s training dataset has been used to predict donors, and then built and compared predictive models like decision trees, and boosted trees, random forest, etc. for identifying donors 
	Credit Score Analysis (R, RapidMiner) 	 	 	 	 	 	 	 	        Sept 2018  Dec 2018  
 Gathered past credit applicants data from a credit dataset. Explained various reasons for good/bad credit score for an applicant and obtained a decision tree-based model to determine if new credit card applicants will present a good or bad credit risk 
	Google Analytics Customer Revenue Prediction (R)  	 	 	 	                                       Sept 2018  Dec 2018 
 Predicted the natural log of the total revenue per unique users basis the exploration of data. The process included the classification as well as regression, as we first needed to fetch patterns to classify users as unique or regular and then regression for the predictions of total revenue per unique users 
	Text Mining & Sentiment Analysis (R, RapidMiner) 	 	 	 	 	 	                        Sept 2018  Dec 2018  
 Used Bag of Words (BoW) approach of tokenizing, normalizing and filtering the text to create the document term matrix 
 Used 3 dictionaries of positive and negative words to obtain aggregated sentiment scores for each movie review 
 Developed models like Lasso Logistic Regression, KNN, Random Forest and SVM to predict the review sentiment (positive or negative)  ",Data Scientist,resume,"   MS in Business Analytics graduate student with experience in providing Audit and  Consumer products sector  Possess  in R, Tableau, Python, SQL & Big Data     Advisory services in Industries, Infrastructure and University of Illinois at Chicago  MS in Business Analytics 	          	          	                                                          Aug 2018  Dec 2019  Uttar Pradesh  University  B.Tech in Electronics and Communication Eng.    	                                        Aug 2010  May 2014  Statistical/Analytical 	R Studio, SPSS, RapidMiner,  	 Database   	MS Excel, Julia, SAS  Visualization and 	Tableau, Google Analytics, 	 Big Data   Reporting 	ggplot(R) 	 Languages/Programming  Productivity 	Microsoft Office Suite 	    MS SQL Server, Oracle, MySQL,  MongoDB, MS Access, SAP HANA  Database  Hadoop & Spark Ecosystems  Python, R, C, C++               	Consultant  IT Risk and Assurance  Ernst & Young, LLP (India)   	 	 	                                       June 2014  July 2018   Performed audits related to Program Risk Management, Financial Audit IT Integration, Application Risk and Controls Identification,  Internal Audit - IT, Service Organization Control Reports (SOCR) in accordance with the attestation standards (ISAE 3402, SSAE 16 and SSAE 18)   Performed Fraud Investigation using Data Analytics (using Tableau, MS excel & SQL) and Software Compliance Management   Performed internal audits pertaining to vendor risk management, scrap sales, material management (purchase cycle), financial transaction and asset management   Performed Marketing collateral planning and execution, business development and contributing to EYs knowledge repository Key Assignments:   IT External Audit for the largest Indian Multinational Conglomerate Company  - Devised audit plan and calendar for quarterly limited reviews and annual audit which included sub-domains like IT General Controls Identification and testing, IT Application Controls testing   Performed Data Analytics for Journal Entries (using MS excel) and Data Analytics on transactional data for investigating fraud (using Tableau & python (package  numpy, pandas & matplotlib))   Service Organization Control Review for a Global Business Process Management Company  - Inspected the description of IT general Controls System for the period under inspection and the suitability of the design and operating effectiveness of controls described therein to achieve the related control s stated in the Description of System (DOS)  - Conducted examination in accordance with the attestation standards (SSAE 16) established by the American Institute of Certified Public Accountants (AICPA)  	ACADEMIC  	 	 	 	 	 	 	 	 	 	 	   	Spam & Ham classifier using NLP (Python  NLTK, Numpy, Pandas & Scikit learn)   	 	                       May 2019  June 2019   Used TF-IDF and count vectorization methods for vectoring the data   Developed random forest and gradient boosting model for predictions   Chose random forest model in model selection due to its high precision as compared to gradient boosting  	Donor Prediction in Marketing Campaign Analysis (RapidMiner) 	 	 	 	 	        Sept 2018  Dec 2018   Sample data from Paralyzed Veterans of America (PVA)s training dataset has been used to predict donors, and then built and compared predictive models like decision trees, and boosted trees, random forest, etc. for identifying donors  	Credit Score Analysis (R, RapidMiner) 	 	 	 	 	 	 	 	        Sept 2018  Dec 2018    Gathered past credit applicants data from a credit dataset. Explained various reasons for good/bad credit score for an applicant and obtained a decision tree-based model to determine if new credit card applicants will present a good or bad credit risk  	Google Analytics Customer Revenue Prediction (R)  	 	 	 	                                       Sept 2018  Dec 2018   Predicted the natural log of the total revenue per unique users basis the exploration of data. The process included the classification as well as regression, as we first needed to fetch patterns to classify users as unique or regular and then regression for the predictions of total revenue per unique users  	Text Mining & Sentiment Analysis (R, RapidMiner) 	 	 	 	 	 	                        Sept 2018  Dec 2018    Used Bag of Words (BoW) approach of tokenizing, normalizing and filtering the text to create the document term matrix   Used 3 dictionaries of positive and negative words to obtain aggregated sentiment scores for each movie review   Developed models like Lasso Logistic Regression, KNN, Random Forest and SVM to predict the review sentiment (positive or negative)  "
"Graduate in Computer Science with 2+ years experience solving challenging problems in data science and machine learning, with a focus on python programming  
 
The University of Texas at San Antonio (UTSA) 	               May 2019?	 
Masters of Science in Computer Science  
Courses - Cloud Computing, Computers and Net Security, Machine Learning, Introduction to Data Science, Database 
Management Systems, Algorithms, Data Structures	 	 
BMS College of Engineering, Bangalore, India 	May 2016 
Bachelor of Engineering in Information Science and Engineering 	 	 
  
Languages:  Python, C, C++ ?	| Analytics :?	 Pandas, SQL, NumPy, Scikit Learn,  Spark, Spacy, Jupyter Notebook?	 
Visualization : Matplotlib, Seaborn, Plotly ?	|? ? Web Scraping Frames:?	 Beautiful Soup ?	|? ? OS:?	 Linux, Windows?	 
Experience 
Data Science Fellow | The Data Incubator	 	       April 2019  May 2019?	 
 Trained a Decision Tree Classifier to predict and analyze the important features responsible for the attrition rates of a company using the data scraped from anonymous Glassdoor reviews. The training model had an accuracy of 71%, which can be used by companies to effectively address their attrition rates.  
Graduate Research Assistant | UTSA 	 	 	 	 	        Jan 2018  Mar 2019 
 Project HuddleUp : HuddleUp, a social media tool used in the university amongst students for interaction. From the data gathered, there were about 55 attributes that were divided into Social, Technological and User factors. These factors were ranked by training the Extra Trees Classifier Machine Learning Model 
 Ushahidi project : ed on collecting various datasets to train a Machine Learning model on crowdsourced data to enable efficient identification of natural disasters. 
? Email classification : Accumulated various email datasets which were used to train a Machine Learning classification model to classify emails as spam, legit and phishing 
Graduate Research Assistant | Open Cloud Institute 	 	                        Jan 2017  Dec 2017 
 Trained a k-Nearest Neighbors model using the Pima Indian Diabetes data which contained attributes that affect diabetes like Age, BMI, Pregnancy, Glucose Levels and Blood Pressure. Trained the kNN algorithm to determine the probability of a person having a risk of diabetes in the future. 
 Face emotion recognition Project: Trained a Convolution Neural Net and used the tensorflow library to detect emotions with the FER dataset 
Software Engineer | Clef Software, Bangalore 	 	 	 	                        June 2015 - June 2016 
 Researched about Drone Software modules such as Ground Control Software and Flight Control Software. Performed simulation using the Pixhawk board, Radio transmitter-receiver and Mission planner software to customize drone software. 
 Developed the companys websit?	e ? using Drupal to code and customize, leading to a user friendly website. 
 
Net Based Classification of Breast Cancer Metastasis | UTSA                                                                     June 2018 - March 2019 
 Analyzed 30 breast cancer gene based datasets and built models for different Classification algorithms. Area under the Curve was chosen as the metric to determine which algorithm performed better for each dataset. 
 Analyzed Lung, Breast and Cervical cancer datasets using statistical techniques. Supervised Machine Learning algorithms were devised to determine accuracy of each dataset. 
HR analytics | UTSA	          Aug 2018 - Dec 2018 
 Analysis of the companys attrition rates and trends using datasets from Kaggl? e using ML techniques like kNN, Linear Regression and Random forest. 
Implementation of Security Protocols in a Client-Server Model | UTSA	                         Jan 2017 - May 2017 
 To allow various clients access the server without breaking the confidentiality and integrity of the client information. This was achieved by socket programming and RSA algorithm implemented in Python. ",Data Scientist,resume,"Graduate in Computer Science with 2+ years experience solving challenging problems in data science and machine learning, with a focus on python programming     The University of Texas at San Antonio (UTSA) 	               May 2019?	  Masters of Science in Computer Science   Courses - Cloud Computing, Computers and Net Security, Machine Learning, Introduction to Data Science, Database  Management Systems, Algorithms, Data Structures	 	  BMS College of Engineering, Bangalore, India 	May 2016  Bachelor of Engineering in Information Science and Engineering 	 	     Languages:  Python, C, C++ ?	| Analytics :?	 Pandas, SQL, NumPy, Scikit Learn,  Spark, Spacy, Jupyter Notebook?	  Visualization : Matplotlib, Seaborn, Plotly ?	|? ? Web Scraping Frames:?	 Beautiful Soup ?	|? ? OS:?	 Linux, Windows?	  Experience  Data Science Fellow | The Data Incubator	 	       April 2019  May 2019?	   Trained a Decision Tree Classifier to predict and analyze the important features responsible for the attrition rates of a company using the data scraped from anonymous Glassdoor reviews. The training model had an accuracy of 71%, which can be used by companies to effectively address their attrition rates.   Graduate Research Assistant | UTSA 	 	 	 	 	        Jan 2018  Mar 2019   Project HuddleUp : HuddleUp, a social media tool used in the university amongst students for interaction. From the data gathered, there were about 55 attributes that were divided into Social, Technological and User factors. These factors were ranked by training the Extra Trees Classifier Machine Learning Model   Ushahidi project : ed on collecting various datasets to train a Machine Learning model on crowdsourced data to enable efficient identification of natural disasters.  ? Email classification : Accumulated various email datasets which were used to train a Machine Learning classification model to classify emails as spam, legit and phishing  Graduate Research Assistant | Open Cloud Institute 	 	                        Jan 2017  Dec 2017   Trained a k-Nearest Neighbors model using the Pima Indian Diabetes data which contained attributes that affect diabetes like Age, BMI, Pregnancy, Glucose Levels and Blood Pressure. Trained the kNN algorithm to determine the probability of a person having a risk of diabetes in the future.   Face emotion recognition Project: Trained a Convolution Neural Net and used the tensorflow library to detect emotions with the FER dataset  Software Engineer | Clef Software, Bangalore 	 	 	 	                        June 2015 - June 2016   Researched about Drone Software modules such as Ground Control Software and Flight Control Software. Performed simulation using the Pixhawk board, Radio transmitter-receiver and Mission planner software to customize drone software.   Developed the companys websit?	e ? using Drupal to code and customize, leading to a user friendly website.    Net Based Classification of Breast Cancer Metastasis | UTSA                                                                     June 2018 - March 2019   Analyzed 30 breast cancer gene based datasets and built models for different Classification algorithms. Area under the Curve was chosen as the metric to determine which algorithm performed better for each dataset.   Analyzed Lung, Breast and Cervical cancer datasets using statistical techniques. Supervised Machine Learning algorithms were devised to determine accuracy of each dataset.  HR analytics | UTSA	          Aug 2018 - Dec 2018   Analysis of the companys attrition rates and trends using datasets from Kaggl? e using ML techniques like kNN, Linear Regression and Random forest.  Implementation of Security Protocols in a Client-Server Model | UTSA	                         Jan 2017 - May 2017   To allow various clients access the server without breaking the confidentiality and integrity of the client information. This was achieved by socket programming and RSA algorithm implemented in Python. "
" 
IBM certified data scientist with 3.5+ years of experience using predictive modelling, data processing, and data mining algorithms to deliver insights and implement action-oriented solution to challenging business problems. Beat 700+ teams of deep learning scientists in a Kaggle competition, by building models that fit the best.  

* Hands-on experience in analysis like data extraction, data preprocessing, feature engineering, building predictive models, visualization and communicating insights and trends. 
* Well versed in data preprocessing steps such as exploration, aggregation, missing data imputation, sampling, feature selection, dimensionality reduction and outlier detection. 
* Proficient in machine learning algorithms such as linear regression, Gradient Boost, XGboost, Decision Trees, Random Forest, Support Vector Machines, K nearest neighbors, Neural Nets.   
* ed with big data (200GB+) in creating predictive model using Artificial neural nets (ANN) like CNN, and LSTM which are currently being implemented in small scale environment. 
* ed with various Python libraries such as NumPy, SciPy for mathematical calculations, Pandas for data preprocessing/wrangling, Matplotlib, Seaborn for data visualization, Sklearn for machine learning, PyTorch for deep learning and NLTK for NLP.  
* Developing statistical models in R using various supervised and unsupervised machine learning algorithms such as linear and logistic regression, Decision Trees, Ensemble methods, KNN, linear SVM, Naive Bayes, K-Means on structured and unstructured data. 
* Statistics methodologies such as hypothesis testing, ANOVA, principle component analysis (PCA), time series analysis. 
* Model validation and optimization with model selection, parameter tuning, and K-fold cross-validation. 
* Loading and analyzing real world datasets with Hadoop frame such as MapReduce, HDFS and SPARK. 
  
Programming 	 	 
: 
Python, R, SAS, and SPSS. 
Machine learning algorithms  
:  
Decision trees, random forest, XGBoost, SVM, KNN, and K-Mean.  
Deep learning algorithms 
:  
CNN, RNN, and LSTM.  
Big data  
:  
PySpark, Map reduce, fastai and Google Cloud Platform (GCP). 
Visualization 	 	 
:  
Tableau, Power BI, and RShiny.  
Web  	 
: 
JavaScript, jQuery, PHP, CSS, HTML, XML, JSON, and REST.  
Databases 	 	 
: 
SQL Server, MySQL, and NoSQL. 
Version Control 	 
:  
Jira, and Git/GitHub 
Machine Learning Libraries 
: 
TensorFlow, PyTorch, Scikit-learn, Pandas, NumPy, Scipy, Matplotlib,  
 	 	 	 
 
Seaborn, plotly, ggplot, dplyr, tidyverse, and NLTK. 
OS and Other softwares  
:  
Windows, Linux, MS Office (Excel, Access, Word, PPT). DATA SCIENTIST, SENSEONICS Inc., MD 
 

 
 
 
 
      March 2019 - Present 
 Created SQL database warehouse from database for data analytics team. 
 Extracting data from SQL Server and performing data wrangling in Python.  
 Kaplan-Meier survival analysis to see glucose sensor survival rates among various countries. 
 Glucose sensors life has been increased by 6% after identifying shelf-life and S0 to be the reason for early sensor retirement using K-means clustering.  
 From CGM data, built predictive algorithm using XGboost with 85% accuracy that is currently being tested in a small group of patients.  
 Using data from the SQL database, custom R scripts were written within PowerBI to manipulate data and generate automated monthly and weekly reports.  
 
DATA ANALYST INTERN, COVANCE, IN 
 
 
 
 
 
      May 2018  Aug 2018 
 Built predictive analytical models in support of organizations operational and business priorities.  
 Conducted a data regression analysis of the relationship between study cancels data and nature of the client, class of the drug, budget and length of the study, achieving a 10% more accurate prediction than earlier years.  
 Forecasted cancels data 15% better than previous years using ARIMA model for time series in Python.  
 Increased accessibility to this data by designing visualizations to include statistical graphs and information graphics in Tableau.  
 Imported, cleaned, merged and transformed datasets in R to build dashboards.  
 Built a KPI dashboard in RShiny which automatically sends monthly performance reports and restricts the access to different employees based on their position in the organization.   
 Created Access database and developed complex SQL queries to transform data sources into warehouses.  
RESEARCH DATA SCIENTIST, IUPUI, IN 
 
 
 
 
 
        Jan 2017  Dec 2018 
 Multiple claims like Dental claims, high cost claims and Emergency Department (ED) Medicaid insurance 
claims from 2017 Indiana Medicaid challenge were analyzed for average cost per recipient and average cost per claim among federally qualified & non-qualified clinics, primary care physicians (PCPs) and Non-PCPs.  
 Federally qualified health clinics and PCPs were found to have high cost claims than their counterparts which is due to preventive care and frequent visits in FQHCs and PCPs respectively.  
 Using demographics groups of individuals with high healthcare costs and more prevalent diseases were identified. Underlying conditions like job type, socioeconomic status and  level were correlated.   
 These claims were analyzed using various machine learning algorithms like:  o K-means clustering to identify patterns in amount & number of claims per diseases, physician specialties 
o Time series analysis to forecast claim amount, diseases per geographic region and  o Regression algorithms (Multivariate regression, Decision trees and Random forest) to identify underlying causes like , socio-economic status, drug abuse etc. in python.  
 Using the scores (gre, ielts, toefl) considered for admission into a Masters program at IUPUI, a using binary logistic regression prediction model was built in R with accuracy of 93% and F-1 score of 0.87.  
 Indiana Community Health Centers (CHCs) data was concatenated using MySQL and disease patterns were identified using descriptive, inferential statistics and were plotted in Tableau.  
 Negative pearsons correlation was identified between income and frequency of diseases. A predictive model was built using linear regression in Python with 84% accuracy. 
 Indiana State Department of Health (ISDH) data was normalized and correlated the no of active physicians with deaths per county in Indiana. Death patterns were visualized in all counties from 2011 to 2015 using ggplot2 in R.  
 Developed and maintained PL/SQL stored procedures, ETL and triggers for various academic based applications. 
 Built a web application using Python Flask to feed both human and machine results of analyzing Chest X-ray report which showed improved performance of human under machine guidance using statistical tests like ttests, one-way ANOVA, F-Score in python.  
 Experienced in Machine Learning Regression Algorithms like Simple, Multiple, Polynomial, SVR (Support Vector Regression), Decision Tree Regression, Random Forest Regression.  
 Experienced in Machine Learning Classification Algorithms like Logistic Regression, K-NN, SVM, Kernel SVM, Naive Bayes, Decision Tree & Random Forest classification. 
 Expertise in employing techniques for Supervised and Unsupervised (Clustering, Classification, PCA, Decision trees, KNN, SVM) learning, Predictive Analytics, Optimization methods, high-dimensional data analysis and Time Series Analysis. 
 Proficient in various statistical models like MANCOVA, two-way ANOVA, chi-square etc. 
 Performed various parametric tests like t-tests, one-way ANOVA and nonparametric statistical tests like spearmans correlation, Mann Whitney U test, Friedman test, Kruskall-Wallis test and Ad-hoc analysis.  
DATA ANALYST, DIC, KVSRSCOPS 
 
 
 
 
 
                   Oct 2015  Nov 2016 
 Slashed staffing costs 10% by predicting seasonality in patient visits per specialty, and effectively utilizing nursing and junior doctor staff without jeopardizing quality.   
 Predictive analytics were performed using time series healthcare data (patient vitals, ECG, EMR, ventilator and other clinical data) with various machine learning algorithms like support vector machine (SVM), decision trees, Naive Bayes classifier to predict adverse clinical event, health severity progression and mortality rates.  
 Collaborated with stakeholders from clinical, operations and product teams to identify analytics opportunities and leverage solutions, maintaining monitoring system in Tableau. 
 Creating queries for data extraction using SAS and reports generation using SQL, and Tableau.  

RSNA pneumonia detection challenge (Kaggle) 
* A deep learning algorithm was built to detect pneumonia with lung opacities from Chest X-rays. Along with pneumonia detection, bounding boxes were generated to identify the location of the opacities.  
* Our algorithm performed better than 700 teams participated in the competition.  
*  used: Convolutional Neural Nets (CNN), fastai, Python.  
Identification of diagnosis from discharge notes using RNN 
* Natural Language Processing (NLP) using Recurrent Neural Nets (RNN) was applied on MIMIC-III Note-events data to identify diagnosis from the discharge notes.  
* Diagnosis table was merged with note-events table; our model was trained on this table and could predict the diagnosis with 82% accuracy. 
*  used: Recurrent Neural Nets, LSTM, fastai, Python, Google Cloud Platform (GCP). 
Fraud detection in Medicare claims data from CMS 
* Medicare data (25 GB) was dumped from CMS and concatenated per each category like Part B Prescriber, Part D Prescriber, Inpatient, Outpatient, Nursing facilities etc. 
* Merging all these datasets to a single dataset, a fraud detection algorithm was created using machine learning algorithms like random forest, gradient boost descent and logistic regression with 80% accuracy. 
*  used: PySpark, SparkSQL, MLlib. 
Analysis of Substance Abuse Trends in UNITED STATES: AN EPIDEMIOLOGIC STUDY 
? Extracted 10 Million records, cleaned for missing values and preprocessing using R ? Data was analyzed by performing predictive analysis ?  used:  o R: Boruta algorithm for variable selection 
o Python: Recursive feature elimination; Logistic regression & Support Vector Machines  o Graphical representation: Choropleth using Plotly in Python 
Development of mobile application to monitor baby care and teach healthcare ers 
* A hybrid mobile application was built using Essential Care for Every Baby (ECEB) action plan and currently being tested.  
*  used: jQuery, JavaScript, CSS, HTML, Cordova, Ionic, and Frame7. 

	Masters in Health Informatics 	 	 	 	 	 	 	              Jan 2017 - Dec 2018 
	Indiana University Purdue University, Indianapolis   	 	 	 	 	 
	Doctor of Pharmacy (Pharm D) 	 	 	 	 	 	 	            Oct 2010  Aug 2016 
	Krishna University, India   	 	 	 	 	 	 	 	 

* Evaluating the implementation of deep learning in LibreHealth Radiology on Chest X-Rays.  ? Phronesis of AI in radiology: Super human meets natural stupidity.  

* IBM data science professional. 
* Mentor, Google Code-in, 2017. 
* Knowledge of ICD Codes (ICD-10/ICD-9), SNOMED, LOINC, CPT, HL-7, CCDA, HIPAA, HITECH, CMS, 
Project lifecycle, code testing, statistical analysis and predictive analysis.  ",Data Scientist,resume,"  IBM certified data scientist with 3.5+ years of experience using predictive modelling, data processing, and data mining algorithms to deliver insights and implement action-oriented solution to challenging business problems. Beat 700+ teams of deep learning scientists in a Kaggle competition, by building models that fit the best.    * Hands-on experience in analysis like data extraction, data preprocessing, feature engineering, building predictive models, visualization and communicating insights and trends.  * Well versed in data preprocessing steps such as exploration, aggregation, missing data imputation, sampling, feature selection, dimensionality reduction and outlier detection.  * Proficient in machine learning algorithms such as linear regression, Gradient Boost, XGboost, Decision Trees, Random Forest, Support Vector Machines, K nearest neighbors, Neural Nets.    * ed with big data (200GB+) in creating predictive model using Artificial neural nets (ANN) like CNN, and LSTM which are currently being implemented in small scale environment.  * ed with various Python libraries such as NumPy, SciPy for mathematical calculations, Pandas for data preprocessing/wrangling, Matplotlib, Seaborn for data visualization, Sklearn for machine learning, PyTorch for deep learning and NLTK for NLP.   * Developing statistical models in R using various supervised and unsupervised machine learning algorithms such as linear and logistic regression, Decision Trees, Ensemble methods, KNN, linear SVM, Naive Bayes, K-Means on structured and unstructured data.  * Statistics methodologies such as hypothesis testing, ANOVA, principle component analysis (PCA), time series analysis.  * Model validation and optimization with model selection, parameter tuning, and K-fold cross-validation.  * Loading and analyzing real world datasets with Hadoop frame such as MapReduce, HDFS and SPARK.     Programming 	 	  :  Python, R, SAS, and SPSS.  Machine learning algorithms   :   Decision trees, random forest, XGBoost, SVM, KNN, and K-Mean.   Deep learning algorithms  :   CNN, RNN, and LSTM.   Big data   :   PySpark, Map reduce, fastai and Google Cloud Platform (GCP).  Visualization 	 	  :   Tableau, Power BI, and RShiny.   Web  	  :  JavaScript, jQuery, PHP, CSS, HTML, XML, JSON, and REST.   Databases 	 	  :  SQL Server, MySQL, and NoSQL.  Version Control 	  :   Jira, and Git/GitHub  Machine Learning Libraries  :  TensorFlow, PyTorch, Scikit-learn, Pandas, NumPy, Scipy, Matplotlib,    	 	 	    Seaborn, plotly, ggplot, dplyr, tidyverse, and NLTK.  OS and Other softwares   :   Windows, Linux, MS Office (Excel, Access, Word, PPT). DATA SCIENTIST, SENSEONICS Inc., MD                   March 2019 - Present   Created SQL database warehouse from database for data analytics team.   Extracting data from SQL Server and performing data wrangling in Python.    Kaplan-Meier survival analysis to see glucose sensor survival rates among various countries.   Glucose sensors life has been increased by 6% after identifying shelf-life and S0 to be the reason for early sensor retirement using K-means clustering.    From CGM data, built predictive algorithm using XGboost with 85% accuracy that is currently being tested in a small group of patients.    Using data from the SQL database, custom R scripts were written within PowerBI to manipulate data and generate automated monthly and weekly reports.     DATA ANALYST INTERN, COVANCE, IN                  May 2018  Aug 2018   Built predictive analytical models in support of organizations operational and business priorities.    Conducted a data regression analysis of the relationship between study cancels data and nature of the client, class of the drug, budget and length of the study, achieving a 10% more accurate prediction than earlier years.    Forecasted cancels data 15% better than previous years using ARIMA model for time series in Python.    Increased accessibility to this data by designing visualizations to include statistical graphs and information graphics in Tableau.    Imported, cleaned, merged and transformed datasets in R to build dashboards.    Built a KPI dashboard in RShiny which automatically sends monthly performance reports and restricts the access to different employees based on their position in the organization.     Created Access database and developed complex SQL queries to transform data sources into warehouses.   RESEARCH DATA SCIENTIST, IUPUI, IN                    Jan 2017  Dec 2018   Multiple claims like Dental claims, high cost claims and Emergency Department (ED) Medicaid insurance  claims from 2017 Indiana Medicaid challenge were analyzed for average cost per recipient and average cost per claim among federally qualified & non-qualified clinics, primary care physicians (PCPs) and Non-PCPs.    Federally qualified health clinics and PCPs were found to have high cost claims than their counterparts which is due to preventive care and frequent visits in FQHCs and PCPs respectively.    Using demographics groups of individuals with high healthcare costs and more prevalent diseases were identified. Underlying conditions like job type, socioeconomic status and  level were correlated.     These claims were analyzed using various machine learning algorithms like:  o K-means clustering to identify patterns in amount & number of claims per diseases, physician specialties  o Time series analysis to forecast claim amount, diseases per geographic region and  o Regression algorithms (Multivariate regression, Decision trees and Random forest) to identify underlying causes like , socio-economic status, drug abuse etc. in python.    Using the scores (gre, ielts, toefl) considered for admission into a Masters program at IUPUI, a using binary logistic regression prediction model was built in R with accuracy of 93% and F-1 score of 0.87.    Indiana Community Health Centers (CHCs) data was concatenated using MySQL and disease patterns were identified using descriptive, inferential statistics and were plotted in Tableau.    Negative pearsons correlation was identified between income and frequency of diseases. A predictive model was built using linear regression in Python with 84% accuracy.   Indiana State Department of Health (ISDH) data was normalized and correlated the no of active physicians with deaths per county in Indiana. Death patterns were visualized in all counties from 2011 to 2015 using ggplot2 in R.    Developed and maintained PL/SQL stored procedures, ETL and triggers for various academic based applications.   Built a web application using Python Flask to feed both human and machine results of analyzing Chest X-ray report which showed improved performance of human under machine guidance using statistical tests like ttests, one-way ANOVA, F-Score in python.    Experienced in Machine Learning Regression Algorithms like Simple, Multiple, Polynomial, SVR (Support Vector Regression), Decision Tree Regression, Random Forest Regression.    Experienced in Machine Learning Classification Algorithms like Logistic Regression, K-NN, SVM, Kernel SVM, Naive Bayes, Decision Tree & Random Forest classification.   Expertise in employing techniques for Supervised and Unsupervised (Clustering, Classification, PCA, Decision trees, KNN, SVM) learning, Predictive Analytics, Optimization methods, high-dimensional data analysis and Time Series Analysis.   Proficient in various statistical models like MANCOVA, two-way ANOVA, chi-square etc.   Performed various parametric tests like t-tests, one-way ANOVA and nonparametric statistical tests like spearmans correlation, Mann Whitney U test, Friedman test, Kruskall-Wallis test and Ad-hoc analysis.   DATA ANALYST, DIC, KVSRSCOPS                               Oct 2015  Nov 2016   Slashed staffing costs 10% by predicting seasonality in patient visits per specialty, and effectively utilizing nursing and junior doctor staff without jeopardizing quality.     Predictive analytics were performed using time series healthcare data (patient vitals, ECG, EMR, ventilator and other clinical data) with various machine learning algorithms like support vector machine (SVM), decision trees, Naive Bayes classifier to predict adverse clinical event, health severity progression and mortality rates.    Collaborated with stakeholders from clinical, operations and product teams to identify analytics opportunities and leverage solutions, maintaining monitoring system in Tableau.   Creating queries for data extraction using SAS and reports generation using SQL, and Tableau.    RSNA pneumonia detection challenge (Kaggle)  * A deep learning algorithm was built to detect pneumonia with lung opacities from Chest X-rays. Along with pneumonia detection, bounding boxes were generated to identify the location of the opacities.   * Our algorithm performed better than 700 teams participated in the competition.   *  used: Convolutional Neural Nets (CNN), fastai, Python.   Identification of diagnosis from discharge notes using RNN  * Natural Language Processing (NLP) using Recurrent Neural Nets (RNN) was applied on MIMIC-III Note-events data to identify diagnosis from the discharge notes.   * Diagnosis table was merged with note-events table; our model was trained on this table and could predict the diagnosis with 82% accuracy.  *  used: Recurrent Neural Nets, LSTM, fastai, Python, Google Cloud Platform (GCP).  Fraud detection in Medicare claims data from CMS  * Medicare data (25 GB) was dumped from CMS and concatenated per each category like Part B Prescriber, Part D Prescriber, Inpatient, Outpatient, Nursing facilities etc.  * Merging all these datasets to a single dataset, a fraud detection algorithm was created using machine learning algorithms like random forest, gradient boost descent and logistic regression with 80% accuracy.  *  used: PySpark, SparkSQL, MLlib.  Analysis of Substance Abuse Trends in UNITED STATES: AN EPIDEMIOLOGIC STUDY  ? Extracted 10 Million records, cleaned for missing values and preprocessing using R ? Data was analyzed by performing predictive analysis ?  used:  o R: Boruta algorithm for variable selection  o Python: Recursive feature elimination; Logistic regression & Support Vector Machines  o Graphical representation: Choropleth using Plotly in Python  Development of mobile application to monitor baby care and teach healthcare ers  * A hybrid mobile application was built using Essential Care for Every Baby (ECEB) action plan and currently being tested.   *  used: jQuery, JavaScript, CSS, HTML, Cordova, Ionic, and Frame7.   	Masters in Health Informatics 	 	 	 	 	 	 	              Jan 2017 - Dec 2018  	Indiana University Purdue University, Indianapolis   	 	 	 	 	  	Doctor of Pharmacy (Pharm D) 	 	 	 	 	 	 	            Oct 2010  Aug 2016  	Krishna University, India   	 	 	 	 	 	 	 	   * Evaluating the implementation of deep learning in LibreHealth Radiology on Chest X-Rays.  ? Phronesis of AI in radiology: Super human meets natural stupidity.    * IBM data science professional.  * Mentor, Google Code-in, 2017.  * Knowledge of ICD Codes (ICD-10/ICD-9), SNOMED, LOINC, CPT, HL-7, CCDA, HIPAA, HITECH, CMS,  Project lifecycle, code testing, statistical analysis and predictive analysis.  "
" 8+ years of experience building interpretable machine learning models, and building end to enddata pipelines which included extracting, transforming and combine all incoming data with the goal of discovering hidden insight, with an eye to improve business processes, address business problems or result in cost savings 
 
 Hands on experience communicating business insights by dashboarding in Tableau. Developedautomated tableau dashboards that helped evaluate and evolve existing user data strategies, which include user metrics, measurement frames, and methods to measurement. Also developed and deployed dashboards in Tableau and RShiny to identify trends and opportunities, surface actionable insights, and help teams set goals, forecasts and prioritization of initiatives 
 
 Experience in architecting and building comprehensive analytical solutions in Marketing, Salesand Operations functions across , Retail and Banking industries. ed closely with functional team leaders (in Product, Operations, Marketing, etc.) to explain analysis, findings, and recommendations 
 
 Experienced in acquiring, merging, cleaning, analyzing and mining structured, semi-structured andunstructured data sets for analysis 
 
 Strong track record of contributing to successful end-to-end analytic solutions (clarifying businesss and hypotheses, communicating project deliverables and timelines, and informing action based on findings) 
 
 Expertise writing production quality code in SQL, R, Python and Spark. Hands on experience buildingregression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments 
 
 Expert knowledge in supervised and unsupervised learning algorithms such as Ensemble Methods
(Random forests), Logistic Regression, Regularized Linear Regression, SVMs, Deep Neural Nets, Extreme Gradient Boosting, Decision Trees, KMeans, Gaussian Mixture Models, Hierarchical models, and time series models (ARIMA,GARCH, VARCH etc.) 
 
 Led independent research and experimentation of new methodologies to discover insights,improvements for problems. Delivered findings and actionable results to management team through data visualization, presentation, or training sessions. Proactively involved in roadmap discussions, data science initiatives and the optimal approach to apply the underlying algorithms 
 
 Experience building interpretable machine learning models, and building end to end data pipelineswhich included extracting, transforming and combine all incoming data with the goal of discovering hidden insight, with an eye to improve business processes, address business problems or result in cost savings 
 
 Experience ing with large data and metadata sources ; interpret and communicate insights andfindings from analysis and experiments to both  and non- audiences in ad, service, and business 
 
 Experienced in Data Modeling retaining concepts of RDBMS, Logical and Physical Data Modeling until
3NormalForm (3NF) and Multidimensional Data Modeling Schema (Star schema, Snow-Flake Modeling, Facts and dimensions). Hands on experience in optimizing the SQL Queries and database performance tuning in Oracle, SQL Server and Teradata databases
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data Scientist
VERIZON - Colorado Springs, CO
October 2018 to May 2019
 Utilizing analytical, statistical and programming  to collect, analyze and interpret large data setsto develop data driven and  solutions to difficult business problems using  such as SQL,
Python and R for Net Transformation project. 
 Through machine learning algorithms such as Linear Regression, Logistic Regression, RandomForests, Support Vector Machines, Clustering and others identify the accurate revenue sources of billing records in Net Transformation project and clearly articulating pros and cons of various techniques. 
 Supporting the operations and maintenance of AWS Advanced Analytics platform named Finalytics. 
 Developing systems, applications, and visual dashboards using Tableau. 
 Ensuring data quality and promoting process improvement. 
 Designing, developing, implementing and maintaining a database to manage data analysis efforts. 
 Designing creative approaches to uncover the biggest opportunities for cost and time savings. 
 Defining and driving the analytics strategy and modeling approaches. 
 Communicating findings to stakeholders after data analysis. 
 Using customer's data identify opportunities and optimize conversion and revenue/profitability withinVerizon's financial business unit. 
 Analyzing customer paths across all channels - Digital / Contact Centers / Retail to help improve theoverall customer experience and business outcomes. 
 Managing model development through the various cycles of the development process 
 Managing daily, weekly, and monthly report execution and distribution, highlighting Key PerformanceIndicators 
 Partnering with Finance, Marketing and other cross functional teams in Verizon to support businessinitiatives.
Data Scientist
APPLE INC - Cupertino, CA
December 2014 to September 2018


Bachelor of Engineering in Computer Science
VIT UNIVERSITY - Vellore, Tamil Nadu
August 2005 to May 2009


Clustering (Less than 1 year), data analysis (Less than 1 year), Logistic regression (Less than 1 year), machine learning (Less than 1 year), Marketing analysis (Less than 1 year), SQL
Additional Information

  
Statistics/ML 
Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation,
Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble
Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning 
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means,
Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and
Low Rank Matrix Factorization 
Feature Engineering: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods,
Wrapper Methods and Embedded Methods 
Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests,
Residual diagnostics, Partial dependence plots and Anova 
Sampling Methods: Bootstrap sampling methods and Stratified sampling 
Model Tuning/Selection: Cross Validation, AUC, Precision/Recall, Walk Forward Estimation, AIC/BIC
Criterions, Grid Search and Regularization 
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series 
 
Machine Learning / 
Deep Learning 
 
R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot 
Python: pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib, tensorflow 
SAS: Forecast server, SAS Procedures and Data Steps 
Spark: MLlib, GraphX 
SQL: Subqueries, joins, DDL/DML statements 
 
Databases/ETL/Query Teradata, SQL Server, Postgres and Hadoop (MapReduce); SQL, Hive, Pig and Alteryx 
Visualization Tableau, ggplot2 and RShiny 
Prototyping PowerPoint, RShiny and Tableau",Data Scientist,resume," 8+ years of experience building interpretable machine learning models, and building end to enddata pipelines which included extracting, transforming and combine all incoming data with the goal of discovering hidden insight, with an eye to improve business processes, address business problems or result in cost savings     Hands on experience communicating business insights by dashboarding in Tableau. Developedautomated tableau dashboards that helped evaluate and evolve existing user data strategies, which include user metrics, measurement frames, and methods to measurement. Also developed and deployed dashboards in Tableau and RShiny to identify trends and opportunities, surface actionable insights, and help teams set goals, forecasts and prioritization of initiatives     Experience in architecting and building comprehensive analytical solutions in Marketing, Salesand Operations functions across , Retail and Banking industries. ed closely with functional team leaders (in Product, Operations, Marketing, etc.) to explain analysis, findings, and recommendations     Experienced in acquiring, merging, cleaning, analyzing and mining structured, semi-structured andunstructured data sets for analysis     Strong track record of contributing to successful end-to-end analytic solutions (clarifying businesss and hypotheses, communicating project deliverables and timelines, and informing action based on findings)     Expertise writing production quality code in SQL, R, Python and Spark. Hands on experience buildingregression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments     Expert knowledge in supervised and unsupervised learning algorithms such as Ensemble Methods (Random forests), Logistic Regression, Regularized Linear Regression, SVMs, Deep Neural Nets, Extreme Gradient Boosting, Decision Trees, KMeans, Gaussian Mixture Models, Hierarchical models, and time series models (ARIMA,GARCH, VARCH etc.)     Led independent research and experimentation of new methodologies to discover insights,improvements for problems. Delivered findings and actionable results to management team through data visualization, presentation, or training sessions. Proactively involved in roadmap discussions, data science initiatives and the optimal approach to apply the underlying algorithms     Experience building interpretable machine learning models, and building end to end data pipelineswhich included extracting, transforming and combine all incoming data with the goal of discovering hidden insight, with an eye to improve business processes, address business problems or result in cost savings     Experience ing with large data and metadata sources ; interpret and communicate insights andfindings from analysis and experiments to both  and non- audiences in ad, service, and business     Experienced in Data Modeling retaining concepts of RDBMS, Logical and Physical Data Modeling until 3NormalForm (3NF) and Multidimensional Data Modeling Schema (Star schema, Snow-Flake Modeling, Facts and dimensions). Hands on experience in optimizing the SQL Queries and database performance tuning in Oracle, SQL Server and Teradata databases Willing to relocate: Anywhere Sponsorship required to  in the US  Experience  Data Scientist VERIZON - Colorado Springs, CO October 2018 to May 2019  Utilizing analytical, statistical and programming  to collect, analyze and interpret large data setsto develop data driven and  solutions to difficult business problems using  such as SQL, Python and R for Net Transformation project.   Through machine learning algorithms such as Linear Regression, Logistic Regression, RandomForests, Support Vector Machines, Clustering and others identify the accurate revenue sources of billing records in Net Transformation project and clearly articulating pros and cons of various techniques.   Supporting the operations and maintenance of AWS Advanced Analytics platform named Finalytics.   Developing systems, applications, and visual dashboards using Tableau.   Ensuring data quality and promoting process improvement.   Designing, developing, implementing and maintaining a database to manage data analysis efforts.   Designing creative approaches to uncover the biggest opportunities for cost and time savings.   Defining and driving the analytics strategy and modeling approaches.   Communicating findings to stakeholders after data analysis.   Using customer's data identify opportunities and optimize conversion and revenue/profitability withinVerizon's financial business unit.   Analyzing customer paths across all channels - Digital / Contact Centers / Retail to help improve theoverall customer experience and business outcomes.   Managing model development through the various cycles of the development process   Managing daily, weekly, and monthly report execution and distribution, highlighting Key PerformanceIndicators   Partnering with Finance, Marketing and other cross functional teams in Verizon to support businessinitiatives. Data Scientist APPLE INC - Cupertino, CA December 2014 to September 2018   Bachelor of Engineering in Computer Science VIT UNIVERSITY - Vellore, Tamil Nadu August 2005 to May 2009   Clustering (Less than 1 year), data analysis (Less than 1 year), Logistic regression (Less than 1 year), machine learning (Less than 1 year), Marketing analysis (Less than 1 year), SQL Additional Information     Statistics/ML  Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau  Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning  Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization  Feature Engineering: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods  Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova  Sampling Methods: Bootstrap sampling methods and Stratified sampling  Model Tuning/Selection: Cross Validation, AUC, Precision/Recall, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization  Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series    Machine Learning /  Deep Learning    R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot  Python: pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib, tensorflow  SAS: Forecast server, SAS Procedures and Data Steps  Spark: MLlib, GraphX  SQL: Subqueries, joins, DDL/DML statements    Databases/ETL/Query Teradata, SQL Server, Postgres and Hadoop (MapReduce); SQL, Hive, Pig and Alteryx  Visualization Tableau, ggplot2 and RShiny  Prototyping PowerPoint, RShiny and Tableau"
"Authorized to  in the US for any employer
 Experience

Freelance
Aetna
April 2015 to August 2016
Sql Consultant (freelance )---Windward Consulting 
 
AETNA Business Analyst/Data Modelor/Testing 
 
Analyzed Aetna database structure/data across several departments (sqlserver, oracle, active directory xenapp, proprietary datasources, cloud servers etc.) and create a Data Model and Data Warehouse. Provided recommendation for a Analytical Tool after evaluating  such as Tableu, Splunk etc Analysis required research on a cost effective method to build the warehouse: (SSIS(for ETL), SSAS(analytical) and SSRS(Reproting) or use an analytical tool.
SQL Server Database Administrator/Qa
Cambridge Associates, VA
March 2001 to November 2010
Cambridge Associates ( www.cambridgeassociates.com ) provides unbiased financial management, information and advice on financial and investment issues to endowed nonprofit institutions and private clients. I perform the administration of SQL Server databases (2005/200/7.0/6.5), physical database design, schema management, database development, performance tuning, data migration, backup and recovery strategies, database software installations and upgrades, troubleshooting, resolving errors and failures, capacity planning and resource utilization. 
 
Responsibilities: 
? Installing SQL Server or upgrading an existing SQL Server 
? Clustering Servers 
? Replication Across Servers 
? Resolve errors/issues resulting from upgrade from 2000 to 2005 in ssis packages, upgrading products/, researching lastest database tips 
?Qa Testing 
? Data Analyst for Factset Data for major application--download data from external website andincorporate into ssis package, required constant manipulation as tickers and currency changed. Correct bad data. 
? Design logical and physical implementation of the database on SQL Server 
? Reorganize database structures as needed to meet new requirements and to improve performance 
? Analyze and tune the database for optimal performance using a variety of tuning strategies, including use of database traces and performance monitoring  in the evaluation process, and index evaluation/reorganization, query modification. 
? Maintaining the proper use of storage by making sure that databases and transaction logs are created correctly, monitoring space requirements, and adding new storage space when required 
? Performing the Backup and Recovery of database by scripting all the objects and its attributes (permissions, logins, roles, indexes etc). 
? Maintaining Database Users and Security for each user. Assigning permissions to the employees andclients on various database objects such as tables, views, and stored procedures. 
? Responsible for maintaining Application schema, Schema changes, Schema versions on 
Production, Testing and Development Databases. 
? Compiled scripts on Production Server 
? ed with developers to resolve data, application and performance issues: ? Monitored performance, optimized database queries, assigns proper indexes, aiding them in creating triggers & stored procedures, roles, and established and maintains 
Naming Conventions and Standards. 
? Wrote DTS/SSIS Packages and jobs to perform scheduled tasks and SQL processes 
? Migrations of SQL Server from 7.0 to 2000 to 2005 
? Migrations of SQL Server from 6.5 to 7.0 
? Setting up and maintaining data replication 
? Support to the Citrix and SQL Server 2000 Project: Citrix/Active Directory test lab ? To develop and document procedures for administration of the database. To maintain records of all changes and developments to the database. 
? Researching the web for latest news in the SQL Server World and presenting it to Sr. 
DBA 
? Created in-house training presentations 
? Providing 24-hour access 
 
Environment: SQL Server 2005/2000/7.0/6.5, SQL r, Query Analyzer, DB Artisan, SQL 
Compare, RSScripter, Sql Litespeed, Erwin, Sql Blocks
Analyst & SQL Server Database Administrator
MS SQL
2000 to 2005
I am a highly motivated Analyst & SQL Server Database Administrator with over 12 years of experience performing installation, configuration and development, and administration functions of MS SQL Server databases (2008/2000/7.0/6.5).  closely and effectively with all levels of management to satisfy project and productivity requirements. Have proven track record with successful implementation of medium to large turnkey . Capable of  analysis and skilled at making quick decisions based on experience, and judgment. Excellent team player and can  under strong pressure on multiple  and have good communication .
Consultant
VOCUS, INC - Lanham, MD
November 1999 to March 2001
Vocus, Inc. supplies On-Demand Software for Corporate Communications and Public Relations. Involved in administration of SQL Server databases, schema management, backup and recovery strategies, troubleshooting, resolving errors and failures, capacity planning and resource utilization. 
 
Responsibilities: 
? Responsible for providing knowledge and  support on all Vocus software applications and their relevance to targeted  industries including the government. 
? Maintained Customer Database 
? Performed the Backup and Recovery of database by scripting all the objects and its attributes (permissions, logins, roles, indexes etc). 
? Responsible for maintaining Application schema, Schema changes, Schema versions on 
Production, Testing and Development Databases. 
? Maintained Database Users and Security for users. Assigning permissions to the employees andclients on various database objects such as tables, views, and stored procedures. 
? ed with developers to resolve data, application and performance issues: ? Monitored performance, optimized database queries, assigns proper indexes, aiding them in creating triggers & stored procedures and established and maintains Naming 
Conventions and Standards. 
? Wrote DTS Packages to import client's data in our database, which involved writing tasks for furtherdata manipulation. 
? Exported data for clients in Flat files, Excel and Access formats. 
? Created DSNs on users' machines so the software they're using are pointing to an appropriatedatabase. 
? Identification and trouble-shooting of software, environmental, and configuration problems. 
? 
 
? Support clients to help resolving advanced  issues relating to connectivity, database related problems, Windows OS configuration, printers, and complex software issues. 
? Hands on training for employees on database, Vocus software application and other MS products. 
 
Environment: SQL Server 2005/2000/7.0/6.5, SQL r, Query Analyzer, SQL Compare,
Client/Server Coordinator & SQL Server Database Administrator
Computer Associates, Inc
October 1998 to November 1999
CA is one the world's largest IT management software providers. CA's software and expertise unify and simplify complex IT environments in a secure way across the enterprise for greater business results. 
 
Responsibilities: 
Client/Server Coordinator 
? Handled business related issues, such as product invoices, renewing maintenance 
licenses, processing purchase orders, assisting clients with their federal accounts, and placing software and documentation orders, and data entry on client database. 
 
Database Administrator 
? Updated queries, entries, I/O, addresses, creating new site id's, transfer license information, and invoice transfer, cancellations and maintenance reinstatement. 
? Researched and resolved all-star issue Verify, research and update tops database. 
 
Ritters & Co, Vienna, VA Administrative Support 
 
? Responsible for handling accounts payable, payroll, filing of quarterly and year-end tax reports, and compiling tax reports as well as financial statements. 
? Other duties included answering phone calls, interacting with clients, ordering supplies, scheduling meetings, and fax machine maintenance.
Research Assistant
George Washington Psychology Dept
August 1995 to September 1996
Research Assistant 
? Compiled and analyzed data concerning minority groups. 
? Applied statistical analysis to attain results for psychology publishing group. 
 
Assistant Analyst 
? Assisted in customization of a confidential database to keep payroll and personal records
Education

Bachelor's


sql server dba (10+ years)
Certifications/Licenses

Mcdba
Additional Information

  Summary 
Operating System DBA/Development  RDBMS Data 
Modeling 
Windows XP, 2003, 2000, DB Artisan, SQL Compare, Scripter, MS SQL Server Erwin 
Windows NT 4.0, 95/98, Redgate -Litespeed, Visual Basic 6.5, 7.0, 2000, Visio 
MVS, HP Unix, SCO Unix 6.0, IIS, HTML 2005 
MS Access, Strong in database Concepts",Data Scientist,resume,"Authorized to  in the US for any employer  Experience  Freelance Aetna April 2015 to August 2016 Sql Consultant (freelance )---Windward Consulting    AETNA Business Analyst/Data Modelor/Testing    Analyzed Aetna database structure/data across several departments (sqlserver, oracle, active directory xenapp, proprietary datasources, cloud servers etc.) and create a Data Model and Data Warehouse. Provided recommendation for a Analytical Tool after evaluating  such as Tableu, Splunk etc Analysis required research on a cost effective method to build the warehouse: (SSIS(for ETL), SSAS(analytical) and SSRS(Reproting) or use an analytical tool. SQL Server Database Administrator/Qa Cambridge Associates, VA March 2001 to November 2010 Cambridge Associates ( www.cambridgeassociates.com ) provides unbiased financial management, information and advice on financial and investment issues to endowed nonprofit institutions and private clients. I perform the administration of SQL Server databases (2005/200/7.0/6.5), physical database design, schema management, database development, performance tuning, data migration, backup and recovery strategies, database software installations and upgrades, troubleshooting, resolving errors and failures, capacity planning and resource utilization.    Responsibilities:  ? Installing SQL Server or upgrading an existing SQL Server  ? Clustering Servers  ? Replication Across Servers  ? Resolve errors/issues resulting from upgrade from 2000 to 2005 in ssis packages, upgrading products/, researching lastest database tips  ?Qa Testing  ? Data Analyst for Factset Data for major application--download data from external website andincorporate into ssis package, required constant manipulation as tickers and currency changed. Correct bad data.  ? Design logical and physical implementation of the database on SQL Server  ? Reorganize database structures as needed to meet new requirements and to improve performance  ? Analyze and tune the database for optimal performance using a variety of tuning strategies, including use of database traces and performance monitoring  in the evaluation process, and index evaluation/reorganization, query modification.  ? Maintaining the proper use of storage by making sure that databases and transaction logs are created correctly, monitoring space requirements, and adding new storage space when required  ? Performing the Backup and Recovery of database by scripting all the objects and its attributes (permissions, logins, roles, indexes etc).  ? Maintaining Database Users and Security for each user. Assigning permissions to the employees andclients on various database objects such as tables, views, and stored procedures.  ? Responsible for maintaining Application schema, Schema changes, Schema versions on  Production, Testing and Development Databases.  ? Compiled scripts on Production Server  ? ed with developers to resolve data, application and performance issues: ? Monitored performance, optimized database queries, assigns proper indexes, aiding them in creating triggers & stored procedures, roles, and established and maintains  Naming Conventions and Standards.  ? Wrote DTS/SSIS Packages and jobs to perform scheduled tasks and SQL processes  ? Migrations of SQL Server from 7.0 to 2000 to 2005  ? Migrations of SQL Server from 6.5 to 7.0  ? Setting up and maintaining data replication  ? Support to the Citrix and SQL Server 2000 Project: Citrix/Active Directory test lab ? To develop and document procedures for administration of the database. To maintain records of all changes and developments to the database.  ? Researching the web for latest news in the SQL Server World and presenting it to Sr.  DBA  ? Created in-house training presentations  ? Providing 24-hour access    Environment: SQL Server 2005/2000/7.0/6.5, SQL r, Query Analyzer, DB Artisan, SQL  Compare, RSScripter, Sql Litespeed, Erwin, Sql Blocks Analyst & SQL Server Database Administrator MS SQL 2000 to 2005 I am a highly motivated Analyst & SQL Server Database Administrator with over 12 years of experience performing installation, configuration and development, and administration functions of MS SQL Server databases (2008/2000/7.0/6.5).  closely and effectively with all levels of management to satisfy project and productivity requirements. Have proven track record with successful implementation of medium to large turnkey . Capable of  analysis and skilled at making quick decisions based on experience, and judgment. Excellent team player and can  under strong pressure on multiple  and have good communication . Consultant VOCUS, INC - Lanham, MD November 1999 to March 2001 Vocus, Inc. supplies On-Demand Software for Corporate Communications and Public Relations. Involved in administration of SQL Server databases, schema management, backup and recovery strategies, troubleshooting, resolving errors and failures, capacity planning and resource utilization.    Responsibilities:  ? Responsible for providing knowledge and  support on all Vocus software applications and their relevance to targeted  industries including the government.  ? Maintained Customer Database  ? Performed the Backup and Recovery of database by scripting all the objects and its attributes (permissions, logins, roles, indexes etc).  ? Responsible for maintaining Application schema, Schema changes, Schema versions on  Production, Testing and Development Databases.  ? Maintained Database Users and Security for users. Assigning permissions to the employees andclients on various database objects such as tables, views, and stored procedures.  ? ed with developers to resolve data, application and performance issues: ? Monitored performance, optimized database queries, assigns proper indexes, aiding them in creating triggers & stored procedures and established and maintains Naming  Conventions and Standards.  ? Wrote DTS Packages to import client's data in our database, which involved writing tasks for furtherdata manipulation.  ? Exported data for clients in Flat files, Excel and Access formats.  ? Created DSNs on users' machines so the software they're using are pointing to an appropriatedatabase.  ? Identification and trouble-shooting of software, environmental, and configuration problems.  ?    ? Support clients to help resolving advanced  issues relating to connectivity, database related problems, Windows OS configuration, printers, and complex software issues.  ? Hands on training for employees on database, Vocus software application and other MS products.    Environment: SQL Server 2005/2000/7.0/6.5, SQL r, Query Analyzer, SQL Compare, Client/Server Coordinator & SQL Server Database Administrator Computer Associates, Inc October 1998 to November 1999 CA is one the world's largest IT management software providers. CA's software and expertise unify and simplify complex IT environments in a secure way across the enterprise for greater business results.    Responsibilities:  Client/Server Coordinator  ? Handled business related issues, such as product invoices, renewing maintenance  licenses, processing purchase orders, assisting clients with their federal accounts, and placing software and documentation orders, and data entry on client database.    Database Administrator  ? Updated queries, entries, I/O, addresses, creating new site id's, transfer license information, and invoice transfer, cancellations and maintenance reinstatement.  ? Researched and resolved all-star issue Verify, research and update tops database.    Ritters & Co, Vienna, VA Administrative Support    ? Responsible for handling accounts payable, payroll, filing of quarterly and year-end tax reports, and compiling tax reports as well as financial statements.  ? Other duties included answering phone calls, interacting with clients, ordering supplies, scheduling meetings, and fax machine maintenance. Research Assistant George Washington Psychology Dept August 1995 to September 1996 Research Assistant  ? Compiled and analyzed data concerning minority groups.  ? Applied statistical analysis to attain results for psychology publishing group.    Assistant Analyst  ? Assisted in customization of a confidential database to keep payroll and personal records Education  Bachelor's   sql server dba (10+ years) Certifications/Licenses  Mcdba Additional Information    Summary  Operating System DBA/Development  RDBMS Data  Modeling  Windows XP, 2003, 2000, DB Artisan, SQL Compare, Scripter, MS SQL Server Erwin  Windows NT 4.0, 95/98, Redgate -Litespeed, Visual Basic 6.5, 7.0, 2000, Visio  MVS, HP Unix, SCO Unix 6.0, IIS, HTML 2005  MS Access, Strong in database Concepts"
"
 :
* Over 10+ years of  IT experience with experience in analysis, architectural design, prototyping, development, Integration and testing of applications using Java/J2EE  and experience in Big Data Analytics as Hadoop Developer with Hadoop ecosystem .
* Delivery experience on major Hadoop ecosystem Components such as Pig, Hive, Spark Kafka, Elastic Search &Hbase and monitoring with Cloudera Manager.Extensive ing experience using Sqoop to import data into HDFS from RDBMS and vice-versa.
* In-depth experience and knowledge in developing and analyzing Map Reduce Jobs and Applications developed standalone and/or through Pig/Hive.
* Extensive experience in developing Pig Latin Scripts for transformations and using Hive Query Language for data analytics.In depth knowledge of Spark concepts and experience with Spark in Data Transformation and Processing.
* Design, implement, and maintain 3NF / dimensional data models for the data warehouse.
* Hands on experience ing on NoSQL databases including Hbase, Cassandra and its integration with Hadoop cluster.Experience in development and utilization of Apache SOLR with Data Computations and Transformation for use by Down Stream Online Applications.
* Good experience in ETL tool Informatica.Managing/maintaining the Hadoop cluster with the help of Apache Ambari 2.2.
* Expert in Ralph Kimball and Bill Inmon's Data Warehouse methodologies.
* Solid experience in developing job flows and schedules with Oozie, and IBM Tivoli
* Experience in Hadoop administration activities such as installation and configuration of clusters using Apache, Cloudera and AWS.ing on Automation using Perl, Shell,Python and Scala Scripts.Developed UDFs in Java as and when necessary.
* Training and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.Automating the jobs using Unix shell scripting and providing production support.
* Training and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.
* Good experience in Python.Developed UDFs in Java as and when necessary to use in PIG and HIVE queries.Followed best practices for preparing and maintaining Apache Hadoop in production.
* ing knowledge of database such as Oracle 8i/9i/10g, Microsoft SQL Server, DB2, Netezza.
* Good experience in Oracle Business Intelligence Enterprise Edition(OBIEE)
* Experienced in using Version Control  like SubVersion, Git.Experience in development of logging standards and mechanism based on Log4J.Good understanding and experience with Software Development methodologies like Agile and Waterfall.
* Experienced in design, development, Unit testing,integration, debugging and implementation and production support, client interaction and understanding business application, business data flow and data relations from them.
* ed on migration project from Oracle DB to Hadoop environment thus enhancing the business to next level.

 Experience:
Panacea IT, FL (Remote)                                                                         
Mar 17  Present
Lead  Hadoop Developer/ Data Scientist 	

Responsibilities:
* ed as a Sr. Big Data/Hadoop Developer with Hadoop Ecosystems components.
* Developed Big Data solutions focused on pattern matching and predictive modeling.
* ed on analyzing Hadoop cluster and different big data analytic  including Pig, HBase database and Sqoop.
* Involved in Agile methodologies, daily scrum meetings, spring planning.
* Primarily involved in Data Migration process using Azure by integrating with GitHub repository and Jenkins.
* Migrating Hive & MapReduce jobs to EMR and Qubole with Automating the flows using Airflow.
* Used Kibana, which is an open source based browser analytics and search dashboard for Elastic Search.
* I was an order picker for UDF. I ed in a freezer and picked orders.
* ed on Oozie, Airflow & SWF for complex flows. Familiarity with HUE.
* Used Java Persistence API (JPA) frame for object relational mapping which is based on POJO Classes.
* Various tasks to support data warehouse operations including data modeling, reference datamanagement, resolving user data trouble tickets, and automation scripting.
* Upgraded the Hadoop Cluster from CDH3 to CDH4, setting up High Availability Cluster and integrating Hive with existing applications.
* Designed & Developed a Flattened View (Merge and Flattened dataset) de-normalizing several Datasets in Hive/HDFS.
* ed on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop.
* Performed multiple MapReduce jobs in Pig and Hive for data cleaning and pre-processing.
* Build Hadoop solutions for big data problems using MR1 and MR2 in YARN.
* Handled importing of data from various data sources, performed transformations using Hive, Pig, and loaded data into HDFS.
* Involved in identifying job dependencies to design flow for Oozie& Yarn resource management.
* Designed solution for various system components using Microsoft Azure.
* Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context, Spark-SQL, Data Frame, pair RDD's.
* Created Hive Tables, loaded claims data from Oracle using Sqoop and loaded the processed data into target database.
* Exported data from HDFS to RDBMS via Sqoop for Business Intelligence, visualization and user report generation.
* Developed Nifi flows dealing with various kinds of data formats such as XML, JSON and Avro.
* Developed and designed data integration and migration solutions in Azure.
* ed on Proof of concept with Spark with Scala and Kafka.
* ed on visualizing the aggregated datasets in Tableau.
* ed on importing data from HDFS to MYSQL database and vice-versa using Sqoop.
* Implemented MapReduce jobs in Hive by querying the available data.
* Configured Hive Meta store with MySQL, which stores the metadata for Hive tables.
* Performed data analytics in Hive and then exported those metrics back to Oracle Database using Sqoop.
* Performance tuning of Hive queries, MapReduce programs for different applications.
* Proactively involved in ongoing maintenance, support and improvements in Hadoop cluster.
* Developed Spark code using Scala and Spark-SQL/Streaming for faster testing and processing of data.
* Used Cloudera Manager for installation and management of Hadoop Cluster.
* Developed data pipeline using Flume, Sqoop, Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis.
* ed on MongoDB, HBase databases which differ from classic relational databases
* Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming.
* Integrated Kafka-Spark streaming for high efficiency throughput and reliability
* ed on Apache Flume for collecting and aggregating huge amount of log data and stored it on HDFS for doing further analysis.
* ed in tuning Hive & Pig to improve performance and solved performance issues in both scripts.
Environment: Hadoop 3.0, Agile, Pig 0.17, Hbase 1.4.3, Jenkins 2.12, NoSQL, Sqoop 1.4, Impala 3.0.0, Hive 2.3, MapReduce, YARN, Oozie, Microsoft Azure, Nifi, Avro, MYSQL, Kafka, Scala 2.12, Spark, Apache Flume 1.8

Businessneeds Inc. (Remote)
Sr. Big Data/Hadoop Developer / Data Scientist 
Aug 16 - Feb 17

Responsibilities:
* As a Big Data/Hadoop Developer ed on Hadoop eco-systems including Hive, MongoDB, Zookeeper, Spark Streaming with MapR distribution.
* Developed multiple MapReduce jobs in Java for data cleaning and preprocessing.
* Involved in various phases of development analyzed and developed the system going through Agile Scrum methodology.
* ed on MongoDB by using CRUD (Create, Read, Update and Delete), Indexing, Replication and Sharding features.
* Involved in designing the row key in HBase to store Text and JSON as key values in HBase table and designed row key in such a way to get/scan it in a sorted order.
* Developed MapReduce (YARN) jobs for cleaning, accessing and validating the data.
* Created and ed Sqoop jobs with incremental load to populate Hive External tables.
* Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3)
* ed with Apache Nifi to Develop Custom Processors for the purpose of processing and disturbing data among cloud systems.
* Involved in development of Hadoop System and improving multi-node Hadoop Cluster performance.
* Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in MongoDB.
* Creating Hive tables and ing on them using Hive QL.
* Designed and Implemented Partitioning (Static, Dynamic) Buckets in HIVE.
* Developed multiple POCs using PySpark and deployed on the YARN cluster, compared the performance of Spark, with Hive
* Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting.
* Used Spark to create the structured data from large amount of unstructured data from various sources.
* Used Apache Spark on Yarn to have fast large scale data processing and to increase performance.
* Responsible for design & development of Spark SQL Scripts using Scala/Java based on Functional Specifications.
* ed on Cluster co-ordination services through Zookeeper.
* Monitored load, job performance and capacity planning using Cloudera Manager.
* Involved in build applications using Maven and integrated with CI servers like Jenkins to build jobs.
* Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection.
Environment: Hive 2.3, Zookeeper, Hadoop 3.0, MapReduce, MongoDB, MapReduce, Java, Apache Nifi 1.6, XML, JSON, MySQL, Apache Spark 2.3, Yarn, Scala, Kafka 1.1


Sears hoffman texas
Sr. Bigdata/Hadoop Developer								
Apr 14  Jul 16

Responsibilities:
* ed on Streamline analytics and data consolidation projects on the product Lucids Fusion.
* Integrated Kafka, Spark, Scala and HBasefor streamline analytics for creating a predictive model and implemented Machine Learning protocol.
* Automated the complex flows using the Airflow flow handler.
* Created parent/child packages for ETL using SSIS, designed patterns using variables, package containers, for loading into Data Warehouse 
* Developed Scala scripts, UDFFs using both Data frames in Spark for Data Aggregation, queries and writing data back into OLTP system through Sqoop.
* Used Solr/Lucene for indexing and querying the JSON formatted data.
* Handled cloud operations inside Rackspacefor persistence logic.
* Monitored OOTB requests with ATG, Akamai and TIBCO.
* Used REST services for handling unfinished jobs, knowing the status and creating a dataset inside a URL.
* ed on ingesting, reconciling, compacting and purging base table and incremental table data using Hive and HBaseand job scheduling through Oozie.
* Designed and implemented streaming data on UI with Scala.js
* Utilized DevOps principle components to ensure operational excellence before deploying in production.
* Operating the cluster on AWS by using EC2, Akka, EMR, S3 and cloudwatch.
* Transported data to HBase using Flume.
* Used Java UDFs for performance tuning in Hive and Pig by manually driving the MR part.
* Used Java APIs such as machine learning library functions, graph algorithms for training and predicting the linear model in spark streaming.
* Have implemented unit testing in Java for pig and hive applications.
* Developed flow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig.
* Involved in source system analysis, data analysis, and data modeling to ETL (Extract, Transform and Load).
* Written Spark programs to model data for extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV& other compressed file formats.
* ed on loading the RDBMS data onto Hadoop/HDFS, using sqoop.
* Developing the Pig scripts / UDFs to manipulate/transform the loaded data.
* Creating Hive tables, loading data into Hive using UDFs, partioning and bucketing principles.
* Migration Hive data into Hbase.
* Involved in integrations among Pig, Hive and Hbase.
* Instrumental in debugging. Created Hive External tables to store the processed data from Map Reduce
Environment: Hadoop0.20, HDFS, Hive,Pig, Sqoop, Java, Cloudera Distribution for Hadoop3(cdh3u5), Linux.

Citi Bank - New York, NY
Big Data  Expert / Hadoop Developer							
Sep 11  Mar 14

Responsibilities:
* Hands on experience in loading data from UNIX file system and Teradata to HDFS
* Experienced on loading and transforming of large sets of structured, semi structured and unstructured data from HBase through Sqoop and placed in HDFS for further processing.
* Installed and configured Flume, Hive, Pig, Sqoop and Oozie on the Hadoop cluster.
* Involved in creating Hive tables, loading data and running hive queries in those data.
* Extensive ing knowledge of partitioned table, UDFs, performance tuning, compression-related properties, thrift server in Hive.
* Built the Education Data Warehouse, applications and Adhoc Reports for schools within the Charter Net
* Involved in writing optimized Pig Script along with involved in developing and testing Pig Latin Scripts
* ing knowledge in writing Pig's Load and Store functions.
* Developed Java MapReduce programs on log data to transform into structured way to find user location, age group, spending time.
* Developed optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop.
* Collected and aggregated large amounts of web log data from different sources such as webservers, mobile and net devices using Apache Flume and stored the data into HDFS for analysis
* Monitored multiple Hadoop clusters environments using Ganglia.
* Developed PIG scripts for the analysis of semi structured data.
* Developed and involved in the industry specific UDF (user defined functions).
* Used Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and net devices and pushed to HDFS.
* Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit duration, most purchased product on website.
* Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Map-Reduce, Pig, Hive, and Sqoop) as well as system specific jobs (such as Java programs and shell scripts).
* Monitored load, job performance and capacity planning using Cloudera Manager.
* Managing and scheduling Jobs on a Hadoop cluster using Oozie.
Environment: Amazon EC2, Apache Hadoop 1.0.1, MapReduce, HDFS, CentOS 6.4, HBase, Kafka, Scala, Elastic Search, Hive, Pig, Oozie, Flume, Java (jdk 1.6), Eclipse, Sqoop, Ganglia, Hbase.

Fidelity Investments Bangalore, IN								
Big Data/Hadoop Developer								
Jul 09  Aug 11

Responsibilities:
* ed on analyzing Hadoop cluster and different big data analytic  including MapReduce, Hive and Spark.
* Involved in loading data from LINUX file system, servers, Java web services using Kafka Producers, partitions.
* Implemented KafkaCustom encoders for custom input format to load data into Kafka Partitions.
* Migrated complex map reduce programs into SparkRDD transformations, actions.
* Implemented SparkRDD transformations to map business analysis and apply actions on top of transformations.
* Automated all the jobs from pulling data from Storage to loading data into MySQL using ShellScripts
* Automated all the jobs starting from pulling the Data from different Data Sources like MySOL and pushing the result dataset to Hadoop Distributed File System and running MR jobs and PIG/Hive using Kettle and Oozie( Flow management).
* Rendered and delivered reports in desired formats by using reporting  such as Tableau.
* ed on syncing OracleRDBMS to Hadoop DB (HBase) while retaining oracle as the main data store.
* Developed shell scripts and made the process automatic to drive the process from JSON to BSON.
* Used Kafka to stream the data with twitter4j from source to Hadoop.
* Offline Analysis was performed on HDFS and sent the results to MongoDB databases to update the information on the existing table, From Hadoop to MongoDB move was done using Mapreduce, Hive/ Pigscripts by connecting with Mongo-Hadoop connectors.
* Developed the MapReduce programs to parse the raw data and store the pre Aggregated data in the portioned tables.
* Created partitioned tables in Hive, mentored analyst and test team for writing Hive Queries.
* Developed PigLatin scripts to extract the data from the web server output files to load into HDFS.
* Involved in Installing, Configuring Hadoop EcoSystem, and Cloudera Manager using CDH4 Distribution.
* Automation of all the jobs starting from pulling the Data from different Data Sources like MySQL and pushing the result dataset to Hadoop Distributed File System and runningMR, PIG, and Hive jobs using Kettle and Oozie (Flowmanagement)
* ed with NoSQL databases like HBase in creating tables to load large sets of semi structureddata coming from various sources.
* Performed ETL process with Python-SQL Server pipelines/frame to perform data analytics and visualization in Python, NumPy, SciPy, Pandas, and MATLAB stack.
* ed with moving tables from Teradata to Hadoop using Sqoop.
* Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using Sqoop
* ed with application teams to install operating system, Hadoop updates, patches, version upgrades as required
Environment: Hadoop, HBase, HDFS, Map Reduce, Teradata, SQL, Cloudera, Ganglia, Pig Latin, Sqoop, Hive, pig, MySQL, Oozie, Flume, Informatica, Zookeeper , R, and Python.


Education: Bachelor of engineering, Computer Science, Gujrat Technical University, 2006",Data Scientist,resume,"  : * Over 10+ years of  IT experience with experience in analysis, architectural design, prototyping, development, Integration and testing of applications using Java/J2EE  and experience in Big Data Analytics as Hadoop Developer with Hadoop ecosystem . * Delivery experience on major Hadoop ecosystem Components such as Pig, Hive, Spark Kafka, Elastic Search &Hbase and monitoring with Cloudera Manager.Extensive ing experience using Sqoop to import data into HDFS from RDBMS and vice-versa. * In-depth experience and knowledge in developing and analyzing Map Reduce Jobs and Applications developed standalone and/or through Pig/Hive. * Extensive experience in developing Pig Latin Scripts for transformations and using Hive Query Language for data analytics.In depth knowledge of Spark concepts and experience with Spark in Data Transformation and Processing. * Design, implement, and maintain 3NF / dimensional data models for the data warehouse. * Hands on experience ing on NoSQL databases including Hbase, Cassandra and its integration with Hadoop cluster.Experience in development and utilization of Apache SOLR with Data Computations and Transformation for use by Down Stream Online Applications. * Good experience in ETL tool Informatica.Managing/maintaining the Hadoop cluster with the help of Apache Ambari 2.2. * Expert in Ralph Kimball and Bill Inmon's Data Warehouse methodologies. * Solid experience in developing job flows and schedules with Oozie, and IBM Tivoli * Experience in Hadoop administration activities such as installation and configuration of clusters using Apache, Cloudera and AWS.ing on Automation using Perl, Shell,Python and Scala Scripts.Developed UDFs in Java as and when necessary. * Training and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection.Automating the jobs using Unix shell scripting and providing production support. * Training and Knowledge in Mahout, Spark MLlib for use in data classification, regression analysis, recommendation engines and anomaly detection. * Good experience in Python.Developed UDFs in Java as and when necessary to use in PIG and HIVE queries.Followed best practices for preparing and maintaining Apache Hadoop in production. * ing knowledge of database such as Oracle 8i/9i/10g, Microsoft SQL Server, DB2, Netezza. * Good experience in Oracle Business Intelligence Enterprise Edition(OBIEE) * Experienced in using Version Control  like SubVersion, Git.Experience in development of logging standards and mechanism based on Log4J.Good understanding and experience with Software Development methodologies like Agile and Waterfall. * Experienced in design, development, Unit testing,integration, debugging and implementation and production support, client interaction and understanding business application, business data flow and data relations from them. * ed on migration project from Oracle DB to Hadoop environment thus enhancing the business to next level.   Experience: Panacea IT, FL (Remote)                                                                          Mar 17  Present Lead  Hadoop Developer/ Data Scientist 	  Responsibilities: * ed as a Sr. Big Data/Hadoop Developer with Hadoop Ecosystems components. * Developed Big Data solutions focused on pattern matching and predictive modeling. * ed on analyzing Hadoop cluster and different big data analytic  including Pig, HBase database and Sqoop. * Involved in Agile methodologies, daily scrum meetings, spring planning. * Primarily involved in Data Migration process using Azure by integrating with GitHub repository and Jenkins. * Migrating Hive & MapReduce jobs to EMR and Qubole with Automating the flows using Airflow. * Used Kibana, which is an open source based browser analytics and search dashboard for Elastic Search. * I was an order picker for UDF. I ed in a freezer and picked orders. * ed on Oozie, Airflow & SWF for complex flows. Familiarity with HUE. * Used Java Persistence API (JPA) frame for object relational mapping which is based on POJO Classes. * Various tasks to support data warehouse operations including data modeling, reference datamanagement, resolving user data trouble tickets, and automation scripting. * Upgraded the Hadoop Cluster from CDH3 to CDH4, setting up High Availability Cluster and integrating Hive with existing applications. * Designed & Developed a Flattened View (Merge and Flattened dataset) de-normalizing several Datasets in Hive/HDFS. * ed on NoSQL support enterprise production and loading data into HBase using Impala and Sqoop. * Performed multiple MapReduce jobs in Pig and Hive for data cleaning and pre-processing. * Build Hadoop solutions for big data problems using MR1 and MR2 in YARN. * Handled importing of data from various data sources, performed transformations using Hive, Pig, and loaded data into HDFS. * Involved in identifying job dependencies to design flow for Oozie& Yarn resource management. * Designed solution for various system components using Microsoft Azure. * Exploring with Spark to improve the performance and optimization of the existing algorithms in Hadoop using Spark context, Spark-SQL, Data Frame, pair RDD's. * Created Hive Tables, loaded claims data from Oracle using Sqoop and loaded the processed data into target database. * Exported data from HDFS to RDBMS via Sqoop for Business Intelligence, visualization and user report generation. * Developed Nifi flows dealing with various kinds of data formats such as XML, JSON and Avro. * Developed and designed data integration and migration solutions in Azure. * ed on Proof of concept with Spark with Scala and Kafka. * ed on visualizing the aggregated datasets in Tableau. * ed on importing data from HDFS to MYSQL database and vice-versa using Sqoop. * Implemented MapReduce jobs in Hive by querying the available data. * Configured Hive Meta store with MySQL, which stores the metadata for Hive tables. * Performed data analytics in Hive and then exported those metrics back to Oracle Database using Sqoop. * Performance tuning of Hive queries, MapReduce programs for different applications. * Proactively involved in ongoing maintenance, support and improvements in Hadoop cluster. * Developed Spark code using Scala and Spark-SQL/Streaming for faster testing and processing of data. * Used Cloudera Manager for installation and management of Hadoop Cluster. * Developed data pipeline using Flume, Sqoop, Pig and Java MapReduce to ingest customer behavioral data and financial histories into HDFS for analysis. * ed on MongoDB, HBase databases which differ from classic relational databases * Involved in converting HiveQL into Spark transformations using Spark RDD and through Scala programming. * Integrated Kafka-Spark streaming for high efficiency throughput and reliability * ed on Apache Flume for collecting and aggregating huge amount of log data and stored it on HDFS for doing further analysis. * ed in tuning Hive & Pig to improve performance and solved performance issues in both scripts. Environment: Hadoop 3.0, Agile, Pig 0.17, Hbase 1.4.3, Jenkins 2.12, NoSQL, Sqoop 1.4, Impala 3.0.0, Hive 2.3, MapReduce, YARN, Oozie, Microsoft Azure, Nifi, Avro, MYSQL, Kafka, Scala 2.12, Spark, Apache Flume 1.8  Businessneeds Inc. (Remote) Sr. Big Data/Hadoop Developer / Data Scientist  Aug 16 - Feb 17  Responsibilities: * As a Big Data/Hadoop Developer ed on Hadoop eco-systems including Hive, MongoDB, Zookeeper, Spark Streaming with MapR distribution. * Developed multiple MapReduce jobs in Java for data cleaning and preprocessing. * Involved in various phases of development analyzed and developed the system going through Agile Scrum methodology. * ed on MongoDB by using CRUD (Create, Read, Update and Delete), Indexing, Replication and Sharding features. * Involved in designing the row key in HBase to store Text and JSON as key values in HBase table and designed row key in such a way to get/scan it in a sorted order. * Developed MapReduce (YARN) jobs for cleaning, accessing and validating the data. * Created and ed Sqoop jobs with incremental load to populate Hive External tables. * Implemented usage of Amazon EMR for processing Big Data across Hadoop Cluster of virtual servers on Amazon Elastic Compute Cloud (EC2) and Amazon Simple Storage Service (S3) * ed with Apache Nifi to Develop Custom Processors for the purpose of processing and disturbing data among cloud systems. * Involved in development of Hadoop System and improving multi-node Hadoop Cluster performance. * Responsible for developing data pipeline with Amazon AWS to extract the data from weblogs and store in MongoDB. * Creating Hive tables and ing on them using Hive QL. * Designed and Implemented Partitioning (Static, Dynamic) Buckets in HIVE. * Developed multiple POCs using PySpark and deployed on the YARN cluster, compared the performance of Spark, with Hive * Used Hive to analyze the partitioned and bucketed data and compute various metrics for reporting. * Used Spark to create the structured data from large amount of unstructured data from various sources. * Used Apache Spark on Yarn to have fast large scale data processing and to increase performance. * Responsible for design & development of Spark SQL Scripts using Scala/Java based on Functional Specifications. * ed on Cluster co-ordination services through Zookeeper. * Monitored load, job performance and capacity planning using Cloudera Manager. * Involved in build applications using Maven and integrated with CI servers like Jenkins to build jobs. * Developed Python scripts to find vulnerabilities with SQL Queries by doing SQL injection. Environment: Hive 2.3, Zookeeper, Hadoop 3.0, MapReduce, MongoDB, MapReduce, Java, Apache Nifi 1.6, XML, JSON, MySQL, Apache Spark 2.3, Yarn, Scala, Kafka 1.1   Sears hoffman texas Sr. Bigdata/Hadoop Developer								 Apr 14  Jul 16  Responsibilities: * ed on Streamline analytics and data consolidation projects on the product Lucids Fusion. * Integrated Kafka, Spark, Scala and HBasefor streamline analytics for creating a predictive model and implemented Machine Learning protocol. * Automated the complex flows using the Airflow flow handler. * Created parent/child packages for ETL using SSIS, designed patterns using variables, package containers, for loading into Data Warehouse  * Developed Scala scripts, UDFFs using both Data frames in Spark for Data Aggregation, queries and writing data back into OLTP system through Sqoop. * Used Solr/Lucene for indexing and querying the JSON formatted data. * Handled cloud operations inside Rackspacefor persistence logic. * Monitored OOTB requests with ATG, Akamai and TIBCO. * Used REST services for handling unfinished jobs, knowing the status and creating a dataset inside a URL. * ed on ingesting, reconciling, compacting and purging base table and incremental table data using Hive and HBaseand job scheduling through Oozie. * Designed and implemented streaming data on UI with Scala.js * Utilized DevOps principle components to ensure operational excellence before deploying in production. * Operating the cluster on AWS by using EC2, Akka, EMR, S3 and cloudwatch. * Transported data to HBase using Flume. * Used Java UDFs for performance tuning in Hive and Pig by manually driving the MR part. * Used Java APIs such as machine learning library functions, graph algorithms for training and predicting the linear model in spark streaming. * Have implemented unit testing in Java for pig and hive applications. * Developed flow in Oozie to automate the tasks of loading the data into HDFS and pre-processing with Pig. * Involved in source system analysis, data analysis, and data modeling to ETL (Extract, Transform and Load). * Written Spark programs to model data for extraction, transformation and aggregation from multiple file formats including XML, JSON, CSV& other compressed file formats. * ed on loading the RDBMS data onto Hadoop/HDFS, using sqoop. * Developing the Pig scripts / UDFs to manipulate/transform the loaded data. * Creating Hive tables, loading data into Hive using UDFs, partioning and bucketing principles. * Migration Hive data into Hbase. * Involved in integrations among Pig, Hive and Hbase. * Instrumental in debugging. Created Hive External tables to store the processed data from Map Reduce Environment: Hadoop0.20, HDFS, Hive,Pig, Sqoop, Java, Cloudera Distribution for Hadoop3(cdh3u5), Linux.  Citi Bank - New York, NY Big Data  Expert / Hadoop Developer							 Sep 11  Mar 14  Responsibilities: * Hands on experience in loading data from UNIX file system and Teradata to HDFS * Experienced on loading and transforming of large sets of structured, semi structured and unstructured data from HBase through Sqoop and placed in HDFS for further processing. * Installed and configured Flume, Hive, Pig, Sqoop and Oozie on the Hadoop cluster. * Involved in creating Hive tables, loading data and running hive queries in those data. * Extensive ing knowledge of partitioned table, UDFs, performance tuning, compression-related properties, thrift server in Hive. * Built the Education Data Warehouse, applications and Adhoc Reports for schools within the Charter Net * Involved in writing optimized Pig Script along with involved in developing and testing Pig Latin Scripts * ing knowledge in writing Pig's Load and Store functions. * Developed Java MapReduce programs on log data to transform into structured way to find user location, age group, spending time. * Developed optimal strategies for distributing the web log data over the cluster, importing and exporting the stored web log data into HDFS and Hive using Scoop. * Collected and aggregated large amounts of web log data from different sources such as webservers, mobile and net devices using Apache Flume and stored the data into HDFS for analysis * Monitored multiple Hadoop clusters environments using Ganglia. * Developed PIG scripts for the analysis of semi structured data. * Developed and involved in the industry specific UDF (user defined functions). * Used Flume to collect, aggregate, and store the web log data from different sources like web servers, mobile and net devices and pushed to HDFS. * Analyzed the web log data using the HiveQL to extract number of unique visitors per day, page views, visit duration, most purchased product on website. * Integrated Oozie with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Map-Reduce, Pig, Hive, and Sqoop) as well as system specific jobs (such as Java programs and shell scripts). * Monitored load, job performance and capacity planning using Cloudera Manager. * Managing and scheduling Jobs on a Hadoop cluster using Oozie. Environment: Amazon EC2, Apache Hadoop 1.0.1, MapReduce, HDFS, CentOS 6.4, HBase, Kafka, Scala, Elastic Search, Hive, Pig, Oozie, Flume, Java (jdk 1.6), Eclipse, Sqoop, Ganglia, Hbase.  Fidelity Investments Bangalore, IN								 Big Data/Hadoop Developer								 Jul 09  Aug 11  Responsibilities: * ed on analyzing Hadoop cluster and different big data analytic  including MapReduce, Hive and Spark. * Involved in loading data from LINUX file system, servers, Java web services using Kafka Producers, partitions. * Implemented KafkaCustom encoders for custom input format to load data into Kafka Partitions. * Migrated complex map reduce programs into SparkRDD transformations, actions. * Implemented SparkRDD transformations to map business analysis and apply actions on top of transformations. * Automated all the jobs from pulling data from Storage to loading data into MySQL using ShellScripts * Automated all the jobs starting from pulling the Data from different Data Sources like MySOL and pushing the result dataset to Hadoop Distributed File System and running MR jobs and PIG/Hive using Kettle and Oozie( Flow management). * Rendered and delivered reports in desired formats by using reporting  such as Tableau. * ed on syncing OracleRDBMS to Hadoop DB (HBase) while retaining oracle as the main data store. * Developed shell scripts and made the process automatic to drive the process from JSON to BSON. * Used Kafka to stream the data with twitter4j from source to Hadoop. * Offline Analysis was performed on HDFS and sent the results to MongoDB databases to update the information on the existing table, From Hadoop to MongoDB move was done using Mapreduce, Hive/ Pigscripts by connecting with Mongo-Hadoop connectors. * Developed the MapReduce programs to parse the raw data and store the pre Aggregated data in the portioned tables. * Created partitioned tables in Hive, mentored analyst and test team for writing Hive Queries. * Developed PigLatin scripts to extract the data from the web server output files to load into HDFS. * Involved in Installing, Configuring Hadoop EcoSystem, and Cloudera Manager using CDH4 Distribution. * Automation of all the jobs starting from pulling the Data from different Data Sources like MySQL and pushing the result dataset to Hadoop Distributed File System and runningMR, PIG, and Hive jobs using Kettle and Oozie (Flowmanagement) * ed with NoSQL databases like HBase in creating tables to load large sets of semi structureddata coming from various sources. * Performed ETL process with Python-SQL Server pipelines/frame to perform data analytics and visualization in Python, NumPy, SciPy, Pandas, and MATLAB stack. * ed with moving tables from Teradata to Hadoop using Sqoop. * Handled importing of data from various data sources, performed transformations using Hive, MapReduce, loaded data into HDFS and extracted the data from MySQL into HDFS using Sqoop * ed with application teams to install operating system, Hadoop updates, patches, version upgrades as required Environment: Hadoop, HBase, HDFS, Map Reduce, Teradata, SQL, Cloudera, Ganglia, Pig Latin, Sqoop, Hive, pig, MySQL, Oozie, Flume, Informatica, Zookeeper , R, and Python.   Education: Bachelor of engineering, Computer Science, Gujrat Technical University, 2006"
"

* Senior Data Scientist with 10+ years of SAS & SQL Programming expertise, Statistical Analysis, Data Modeling, Risk Analysis and reporting experience in various SAS  emphasizing on analysis, design, development, testing and implementation of various  in Financial, Health care, and Pet care industries.
* Experience ing remotely for clients such as Mastercard (100% remote), NBC Universal (100% remote), Nestle Purina (50% remote), CareSource (50% remote) and JPMorgan Chase (75% remote).
* Experience using SAS in the analysis and reporting of clinical trials data in the pharmaceutical industry 
* Experience in direct marketing processes, List Processing and marketing analytical methods.
* Expertise in creating Packages using SQL Server Integration Services (SSIS).
* Experienced in Database optimization and developing stored procedures, Triggers, Cursors, Joins, Views, Cursors and SQL on databases: MySQL, Oracle12g, OMWB tool.
* Experience in HEDIS Analytics and Reports, as well as, Gaps in Care reports and Ad-hoc reporting requests.
* Experience with Statistical Analysis & Data Modeling using JMP, MINITAB, Design Expert and Weka.
* Expertise in Transact-SQL (DDL, DML, DCL) and in Design and Normalization of the database tables.
* Experience in implementing business logic using Triggers, Indexes, Views and Stored procedures.
* Extensive experience in SAS BASE, SAS MACROS, SAS SQL, SAS/STAT, SAS/GRAPH, SAS Enterprise Miner, SAS Forecast Studio, SAS BI : SAS Enterprise Guide, SAS DI Studio, SAS Web Report Studio.
* ing knowledge of SAS High Performance Analytics (HPA) and SAS High Performance Forecasting (HPF) including PROC HP, PROC HPDMDB, PROC HPSAMPLE, PROC HPCORR and PROC HPREG.
* Hands on experience in SAS Programming, Merging SAS Data Sets, Developing SAS procedures, Macros, Preparing Data, Producing Reports, SAS Formats, SAS Functions, SAS Informats and managing dataset.
* Extensively ed on concatenating, interleaving, and merging large SAS datasets.
* Extensive industry analytics experience using statistical analysis and predictive modeling.
* Predictive modeling experience in linear and logistic regression analysis.
* Had experience in data modeling using Erwin, Star Schema Modeling, and Snowflake modeling, FACT and Dimensions tables, physical and logical modeling.
* Strong experience in Data Warehousing and ETL using Informatica Power Center and Salesforce systems.
* Experience in PROC SQL joins and PROC SQL set operators to combine tables horizontally and vertically.
* Experience in Integration of various Data Sources like MS-Access, Flat and CSV Files.
* Good experience in UNIX. ed with batch files and ran SAS programs using UNIX shell scripts. 
* Extensive experience in deployment of SAS programs/UNIX shell scripts and version controlling (PVCS)
* Used ODS to display outputs in HTML, CSV or other file formats.
* Extensive experience in writing reusable macros.
* Strong experience in testing methodologies. Involved in System Testing, Integration Testing & UAT.
* Excellent problem-solving  and having good knowledge in Finance and Credit Cards Business Domain.

 :
SAS 	SAS Base, SAS Macro, SAS SQL, SAS STAT, SAS Enterprise Miner, SAS HPA, SAS Access, SAS Graph, SAS Enterprise Business Intelligence, SAS HPF, SAS ECM, SAS Connect, SAS Enterprise Guide, SAS Data Integration Studio, SAS Web Report Studio, SAS Management Console, SAS AML, SAS ODS, SAS JMP
Statistical Software Packages	JMP, Design Expert, MINITAB, SPSS, R and Weka
Programming Languages	SQL, PL/SQL, HTML, CSS, PHP, AB INITIO, Visual Basic, Hive, Pig, Python, Java
Databases	Oracle, Teradata, SQL-Server, HADOOP, Sybase and DB2
Software Packages	MS Office, SSRS, SSMS, Tableau, DOMO, Informatica, UML, JIRA, MS Project
Operating Systems	Windows, UNIX, Salesforce application systems and Mainframe
 & Certification
Master of Science in Industrial Engineering & Statistics
SAS Certified Base Programmer V9 from SAS Institute.
Graduate Certificate in Statistics

Experience
Lead Data Scientist / SAS ECM AML Consultant				            		Oct 17  Present
Wells Fargo Bank (100% - Remote)
Environment: SAS Enterprise Guide, SAS ECM/AML, R, UNIX, SAS Macro, HADOOP, Machine Learning

Responsibilities:
* Performed advanced statistical analysis (univariate and multivariate analysis of variance, cluster and path analysis, principle component and factor analysis, analysis of covariance, survival & longitudinal analysis, logistic and linear regression modeling), created customized reports and presentation quality data  tables.
* Involved in the development of a net present value (NPV) model that was used to optimize the selection of prospects using SAS BASE, SAS/Enterprise Miner and SAS Enterprise Guide.
* Developed logistic regression models to predict subscription response rate based on customers variables like past transactions, response to prior mailings, promotions, demographics
* Utilized SAS management console for adding new user groups, LOBs for dashboards in SAS ECM portal.
* Statistically analyzed data using SAS and built Regression models for validation.
* Performed Multi-Variate Statistical Risk analysis, Eigen system Analysis, Canonical Correlation Analysis and logistic regression Analysis using PROC CANCORR, PROC MIXED, PROC MEANS and PROC UNIVARIATE.
* Used R statistical software for effective analysis by hypothesis testing to validate data and interpretations.
* Performed configuration and support of SAS ECM Metadata (Cases, Incidents, Parties, and Financial Items), configuration and support of ECM reference tables (All Lookup tables).
* Configured setting up User Accounts, Groups, Roles together with SAS Platform Administrator.
* Performed migration of UI Definitions, custom properties, flows, Scripts for UDF and Case Configurations.
* Developed Multiple Linear Regression model to determine how the factors like stock price, dividends, earnings, CPI and Interest rate affect.
* Implemented solutions for ingesting data from various sources and processing the Data-at-Rest utilizing Big Data  such as Hadoop, Map Reduce Frames, HBase, Hive.
* Implementation of Big Data ecosystem (Hive, Impala, Sqoop, Flume, Spark, Lambda) with Cloud Architecture.
* Designed and deployed full SDLC of AWS Hadoop cluster based on client's business need.
* Defined the process to build predictive model,  functions, variable selection and preparation and the statistical methodology and model validation.
* Developed and implemented logistic regression modeling for direct mail campaign by targeting responsive customers and minimizing risk.
* Implemented model validation and developed diagnostic tables and graphs that demonstrated how model can be used to improve the efficiency of the selection process for customer acquisition campaign.
* Verified program logic by overseeing the preparation of test data, testing and debugging of SAS programs.

Senior Data Scientist									             Aug 17  Oct 17
Mastercard, Purchase, NY (100% - Remote) 
Environment: Machine learning, Sqoop, SAS EG, SAS Forecast Studio, Python, DOMO, Tableau, R, Hive, Pig

Responsibilities:
* Developed SAS/Excel ad-hoc reports for Priceless management team.
* Generated comprehensive data files (MS Excel), identified trends, patterns and troubleshoot inconsistencies.
* ed in writing Hadoop Jobs for analyzing data using Hive, Pig accessing Text format files, sequence files, Parquet files.
* Performed data entry, cleansing and tagging (heavy volume of global assets)
* Generated DDL and DML scripts using Python and managed aspects of QA testing.
* Performed multiple regression modelling for load forecasting, and sequentially tested additional variables and combinations for model improvement using R programming.
* Developed predictive models and statistical analysis for external clients and internal business performance.
* Performed data acquisition, data mining, and analyzed data using valid analytical methods.
* Communicated findings using Tableau dashboard visualizations and results in a concise and actionable manner and ensure results are well understood by project sponsors.
* Experienced with batch processing of data sources using Apache Spark.
* Developing predictive analytic using Apache Spark Scala APIs.
* Involved in ing of big data analysis using Pig and User defined functions (UDF).
* Created Hive External tables and loaded the data into tables and query data using HQL.
* Used Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers.
* Designed, developed, maintained and evaluated statistical and predictive models to identify historical trends and forecast future patterns and other performance measures using SAS.
* Accessed and compiled large amounts of data, and applied advanced statistical techniques to analyze the data, forecast, interpret, and quantify trends on various aspects of information. 
* Responsible for statistical data gathering, sophisticated analysis, and development of recommendations.
* Performed advanced quantitative assessments of all aspects of stress test models including theoretical aspects, model design and implementation, data integrity, and reliability.
* Supported revenue management using statistical and quantitative analysis, developed several statistical approaches and optimization models to save more than half million $ in program cost and by price distribution.
* ed with various business units to scope and design the statistical frame for business experiments and socialize and help integrate results of analysis of experiments.
* Diagnosed patterns and relationships in business data and developing logical methodologies to answer complex business questions through analytics.

Data Scientist											    Jul 16  Jul 17
NBC Universal, NYC (100% - Remote) 
Project: SAS Forecast Models Generation for NBCU Nets     
Environment: SAS Enterprise Guide, SAS Forecast Studio, MySQL, Informatica Power Center, SQL Server, SAS Time Series Studio, MS Excel, Access, DOMO, Tableau, Spark, Hadoop, AWS

Responsibilities:
* Define and create SAS/Excel ad-hoc reports for NBC nets.
* Used the PL/SQL procedures for Informatica mappings for truncating the data in target tables at run time.
* Performed data cleansing and validation on ing with several millions of records and data points.
* Statistical validation of the forecast results using error estimation techniques.
* Developed SQL Server Stored Procedures, Tuned SQL Queries (using Indexes and Execution Plan).
* Installed and Configured MS Build Server, created build agents and Build Controllers.
* ed with AWS to implement the client-side encryption as Dynamo DB does not support at rest encryption. 
* Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN.
* Performance tuning daily for preventing issues & providing capacity planning using MySQL Enterprise Monitor.
* Developed DOMO dashboards with customer data trends and plots for visual analytics.
* Used Informatica Power Center 8.6 for extraction, transformation and load (ETL) of data in data warehouse.
* Developed stored procedures, triggers in MySQL for lowering traffic between servers & clients.
* I've performed correlation analysis using PROC HPCORR and generated pairwise Pearson correlation statistics and developed statistical predictive data models to predict the net impressions based on the past data provide business with forecast insights.
* Developed Tableau data visualization using Cross tabs, Heat maps, Box and Whisker charts, Scatter Plots, Geographic Map, Pie Charts and Bar Charts and Density Chart.
* Prepare detailed statistical reports in MS Access and MS Excel to track progress of weekly, monthly and quarterly customers and sales growth.
* Performance Tuning in SQL Server using SQL Profiler and Data Loading.
* ed with High Performance procedures like PROC HPSAMPLE, PROC HPREG, PROC HPF and HPBIN.
* Developed forecast models from scratch with SAS Forecast Studio for NBCs various nets E!, BRAVO, OXYG, SPROUT, USA, CHILLER, NBCSN, SYFY and MSNBC
* Developed ARIMA, Exponential Smoothing models (ESM), Intermittent Demand Models (IDM) and fine-tuned them by identifying outliers, creating pulse or level shift events and automated them.
* Performed data manipulations for data quality assurance/validation of table, listing and summaries
* Developed SAS programs for querying data from large datasets using SAS Enterprise Guide
* Summarized the datasets to generate daypart models with quarterly and monthly forecast estimates.

Data Scientist / SAS Data Integration Consultant					Feb 14  Jun 16
Nestle Purina, St Louis, MO (50% - Remote)							
Project: Demand Planning Migration to SAS module Demand driven Planning & Optimization (DDPO)
Environment: SAS Data Integration Studio, SAS Enterprise Guide, Informatica Power Center, SAS Enterprise Miner, SAS Macros, SAS ODS, UNIX, SAS Forecast Studio, SAS Forecast Server, Azure

Responsibilities:
* Primarily responsible for extracting, transforming and data loading (ETL) into SAS datasets and creating flat files, reports and excel files.
* Developed data structures, performed data manipulation and application development.
* Designing, developing and implementing the DI Studio job flows to load the data into SAS.
* Performed Double and Seasonal Exponential Smoothing using Time Series Exponential Smoothing (TS ESM) node, and data mining using SAS Enterprise Miner. 
* Utilized MS Azure services with focus on big Data Engineer /analytics / enterprise data warehouse and business intelligence solutions to ensure optimal architecture, scalability, flexibility, availability, performance, and to provide meaningful and valuable information for better decision-making.
* Gathering data requirements, designing and developing data marts using SAS Data Integration Studio.
* Generated Baseline and promoted demand planning forecasts using SAS Forecast Studio.
* Produced use cases and data flow diagrams as supporting documentation using MS Visio.
* Assisted conceptual, logical, and physical data modeling, and interface effectively with data modelers
* Involved in deployments, automation, scheduling of DI jobs and process documentation. 
* Creating Stored Processes to provide users of reporting DataMart easy access using SAS Enterprise Guide.
* Integrating and managing the data mappings from the clients data sources (mainly Oracle) to the SAS Analytical data mart using SAS DI Studio.
* Supported Deployment to all testing and production environments & automated test cases using MS Build.
* Designed flows with many sessions with decision, assignment task, event wait, and event raise tasks, used Informatica scheduler to schedule jobs. 
* Documented and setup files, SAS program, and log files for new production box.  
* Oversaw production jobs, production server, running daily/quarterly process jobs to support the business in reporting capability and day to day functions. 
* ed and collaborated with project team members and other relevant knowledge experts.
* Performing analytical and programming activities including analysis, design, development, unit & integration testing, implementation, and documentation of integrating the data management solutions. 

Data Scientist											Oct 12  Jan 14
CareSource, Dayton, OH (50% - Remote)
Environment: SAS Base/Macros, SAS Enterprise Guide, SQL Server, SAS ODS, Mainframe, R

Responsibilities:
* Develop complex automated reporting products using BASE SAS that meet the internal and external information needs of the organization. 
* Automated the manual exporting of data to complex Excel layouts and customized using SAS ODS Tagsets -EXCELXP utility.
* Assist in the design of Business Intelligence products for both internal and external use. Participate in testing activities as needed.
* Responsible for adhering to proper coding standards and IT change control process.
* Evaluate HEDIS requirements, obtain data for reporting and prepare HEDIS reports.
* Generate graphics that effectively describe, explore and summarize information for communication to appropriate parties.
* Used R to help with Turn-around Time Distribution Analysis and Forecast on shipped and cancelled referrals, and visualized the result using R Shiny.
* Used Machine Leaning techniques (e.g. Logistic Regression, Bayesian Linear regression) to build up predictive models on probability of cancellation of each new referral.
* Writing SAS and SQL code for querying data from large datasets of the SQL Server Database.
* Incorporate critical thinking  and judgment in the process to determine best course of action for each inquiry/problem. 
* Generated batch files for regulatory reporting via e-filing using SAS Enterprise Case Management (SAS ECM).
* Investigation of data that shows a pattern or indicates a significant variation from expected.
* Represent the department in project meetings and other meetings that require subject matter knowledge.
* Managing HEDIS and CAHPS reporting  for multiple markets with year-round monitoring and annual required reporting and submission using Data Transformation Services (DTS).
* Assist the manager in directing and coordinating  within the department.

Statistical Analyst / SAS Programmer								Aug 11  Aug 12
JP Morgan Chase Bank, Columbus, OH (75% remote)					            
Project: Consumer Model Governance - Score Monitoring System 
Environment: SAS Base/Macros, Oracle, SQL, DB2, Teradata, UNIX, Mainframe, SAS/EG, SAS ECM, SAS AML

Responsibilities:
* Writing SAS & SQL code for querying data from large datasets and extracted data from Oracle DB
* Involved in campaign auditing and generating water fall reports for management approvals 
* Used Dynamic LIBNAME, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC IMPORT, PROC APPEND, PROC REPORT AND PROC FORMAT
* Performed basic SAS analysis on existing customers data, and database segmentation using PROC MEANS, FREQ, PROC  AND TABULATE.
* Performed configuration and support of SAS ECM Metadata (Cases, Incidents, Parties, Financial Items), configuration and support of ECM reference tables (All Lookup tables).
* Imported existing records and historical information by custom ETL to prepopulate customer fields and prevent rekeying errors using SAS ECM.
* Developed forecasts for Cross sectional time series data using forecasting node in SAS Enterprise Miner.
* Design, development and implementation of a new model validation and tracking process
* Support of Business Intelligence validation reports used by other risk groups.
* Visually created customer segments for behavior monitoring using SAS Anti-Money Laundering (SAS AML).
* Design, produce and implement capabilities to execute file processing (ETL) required for production of SAS data sets for modeling and analytics that include data from credit records
*  with model developers and the model implementation teams to develop logical data models and ensure efficient flows of information through automated business processes
*  with the model reviewers to review business needs and strategies and assess the implications to the information/data architecture
* Utilized SAS Enterprise Guide for Data Management and increased data consistency that enabled data-driven decisions and streamlines administration & ran ad-hoc SAS queries.
* Performed Data de-duplication on ing with several millions of records and data points.
* Performed Extraction, transformation & loading (ETL) & Data Cleansing with SAS Data Integration Studio.
* Scheduling daily, weekly and monthly reports using SAS/MACROS.
* Formatted data using SAS defined and user defined formats using PROC FORMAT.
* Designed file directory built out structure for both production and development areas, so production job could be run in a controlled environment by AUTOSYS with users also being able to develop, code, test, and run ad-hoc reports against production data.
* Managed AUTOSYS performance and re-runs if needed in SAS version 9.2 UNIX  
* Set up log check program to review logs of production jobs to look for errors/warning and sent out emails if problems were found, eliminating manual checking of program logs.
* Involved in development and enhancement of SAS programs.

Statistical Analyst										 Jan 11  Jul 11
Office of Statewide Health Planning & Development (OSHPD), Sacramento, CA (75% - Remote)
Environment: SQL Server, SAS Base/EBI/EG/OLAP Cube Studio, SAS Management Console, Oracle, R

Responsibilities:
* Involved in requirement gathering from different groups by conducting JAD sessions. 
* Conducted regression analysis on impact of Medicaid expansion on population of Patient Assistance Program (PAP).
* Developed SAS programs for querying data from large datasets using SAS Enterprise Guide
* Extracting data from SQL Server, Oracle and created tables & populated them to centralize comprehensive data using SQL Server Management Studio
* Provided forecast analysis on monthly basis to operation manager on patients volume, referral volume to help with labor planning.
* Used multiple statistical methodologies with evaluation the impact of a nurse intervention project over time. (Propensity Score Matching Technique to generate control and treatment group population and did survival analysis and Cox Regression analysis).
* Involved in Design of  Requirements Documents and implementation plan of the Clearinghouse
* Used PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC MEANS, PROC FREQ, PROC TABULATE and PROC FORMAT in developing SAS programs.
* Performed basic SAS analysis on existing customers data using PROC MEANS
* Evaluate HEDIS requirements by monitoring and reporting on key performance indicators on a weekly and monthly basis, identifying trends, issues and potential solutions to those issues.
* Performed Extraction, transformation & loading (ETL) & Data Cleansing with SAS Data Integration Studio.
* Set up Log-Check and error-check program to review logs/error of production and development jobs.
* Created intermediate data marts for storage and loaded with the validated data.
* Build cubes using SAS OLAP Cube Studio & summarized datasets to generate region level reports.
* Involved in development and enhancement of SAS programs and formatted data using SAS defined and user defined formats using PROC FORMAT.
SAS Data Analyst									             Jun 10  Dec 10
Fannie Mae, Herndon, VA (50% remote)
Environment: SAS Base/Macros, SAS Forecast Studio, Oracle, UNIX, SQL, Ab Initio, Visio

Responsibilities:
* Involved in requirement gathering from different groups 
* Writing SAS and SQL code for querying data from large datasets of the SQL Server & Teradata Database
* Extracting data from Oracle (Hemisphere Database) 
* Generating excel reports, Visio diagrams and delivering to different groups
* Involved in design and implementation plan and Unit Testing of the SDLC
* Used Dynamic LIBNAME, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC MEANS, PROC FREQ, PROC TABULATE and PROC FORMAT
* Monitored production jobs, production server and running daily/quarterly/monthly process jobs.
* Developed large quantities of accurate monthly forecasts using SAS Forecast Studio.
* Involved in development and enhancement of SAS programs in Mainframe environment.
* Analyzed existing system logic (Ab Initio, SAS), and translating the logic into baseline functional requirements and managed the requirements using Rational Requisite Pro
* ed with business unit or other subject matter experts, in addition to IT staff to refine BRD and FSD.
* Communicating clearly and precisely in functional (vs. ) terms 
* Produced use cases and data flow diagrams as supporting documentation using MS Visio.
* Performed data step manipulations for data quality assurance/validation of table, listing and summaries
* Formatted data using SAS defined and user defined formats using PROC Format.

SAS Programmer / Analyst								            Aug 08  May 10
Comerica Bank, Phoenix, AZ (50% remote)
Project: Campaign List processing/auditing 
Environment: SAS Base/Macros, Oracle, Teradata, UNIX, Mainframe, SAS/EG SAS/EM, SQL, SAS AML

Responsibilities:
* Involved in requirement gathering from different groups, design, implementation plan and Unit Testing
* Writing SAS & SQL code for querying data from large datasets and extracted data from Oracle & Teradata
* Involved in campaign auditing and generating water fall reports for management approvals 
* Used Dynamic libname, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC IMPORT, PROC APPEND, PROC REPORT and PROC FORMAT
* Performed basic SAS analysis on existing customers data, and database segmentation using PROC MEANS, FREQ,  and TABULATE.
* Compared & analyzed entitys current behavior to its historical behavior and behavior of peers using SAS AML.
* Build Logistic Regression models and Multiple Linear Regression models to identify the control group cohorts using SAS/STAT and other SAS modules.
* Statistical validation of the forecast results using error estimation techniques
* Administered SAS system, SQL Server, Teradata & Oracle Database system and managed the system requirements, user access management & administration on UNIX and Windows server.
* Developed forecasting models from scratch with SAS and used them for consumer prediction and control. 
* Utilized SAS Enterprise Guide for Data Management and increased data consistency that enabled data-driven decisions and streamlines administration & ran adhoc SAS queries.
* Developed Multiple Linear Regression model to determine how the factors like stock price, dividends, earnings, CPI and Interest rate affect Price-Earnings Ratio in stock market.
* Doing Statistical Analysis with statistical procedures and univariate procedures from Base SAS, SAS/STAT
* Performed Data de-duplication on ing with several millions of records and data points.
* Performed Design of Experiments & Analysis (DOE) and optimized the combination of Model inputs.
* Performed Multi-Variate Statistical Risk analysis, Eigen system Analysis, Canonical Correlation Analysis and logistic regression Analysis using PROC CANCORR, PROC MIXED, PROC MEANS and PROC UNIVARIATE 
* Performed Data Mining and collected intricate details using SAS Enterprise Miner & Weka.
* Performed Extraction, Transformation & Loading (ETL) & Data Cleansing with SAS Data Integration Studio.
* Involved in development and enhancement of SAS programs.
* Statistically analyzed data using MINITAB, R and S-Plus statistical software packages and built Regression models for validation.
* Scheduling daily, weekly and monthly reports using SAS/MACROS.


	 


",Data Scientist,resume,"  * Senior Data Scientist with 10+ years of SAS & SQL Programming expertise, Statistical Analysis, Data Modeling, Risk Analysis and reporting experience in various SAS  emphasizing on analysis, design, development, testing and implementation of various  in Financial, Health care, and Pet care industries. * Experience ing remotely for clients such as Mastercard (100% remote), NBC Universal (100% remote), Nestle Purina (50% remote), CareSource (50% remote) and JPMorgan Chase (75% remote). * Experience using SAS in the analysis and reporting of clinical trials data in the pharmaceutical industry  * Experience in direct marketing processes, List Processing and marketing analytical methods. * Expertise in creating Packages using SQL Server Integration Services (SSIS). * Experienced in Database optimization and developing stored procedures, Triggers, Cursors, Joins, Views, Cursors and SQL on databases: MySQL, Oracle12g, OMWB tool. * Experience in HEDIS Analytics and Reports, as well as, Gaps in Care reports and Ad-hoc reporting requests. * Experience with Statistical Analysis & Data Modeling using JMP, MINITAB, Design Expert and Weka. * Expertise in Transact-SQL (DDL, DML, DCL) and in Design and Normalization of the database tables. * Experience in implementing business logic using Triggers, Indexes, Views and Stored procedures. * Extensive experience in SAS BASE, SAS MACROS, SAS SQL, SAS/STAT, SAS/GRAPH, SAS Enterprise Miner, SAS Forecast Studio, SAS BI : SAS Enterprise Guide, SAS DI Studio, SAS Web Report Studio. * ing knowledge of SAS High Performance Analytics (HPA) and SAS High Performance Forecasting (HPF) including PROC HP, PROC HPDMDB, PROC HPSAMPLE, PROC HPCORR and PROC HPREG. * Hands on experience in SAS Programming, Merging SAS Data Sets, Developing SAS procedures, Macros, Preparing Data, Producing Reports, SAS Formats, SAS Functions, SAS Informats and managing dataset. * Extensively ed on concatenating, interleaving, and merging large SAS datasets. * Extensive industry analytics experience using statistical analysis and predictive modeling. * Predictive modeling experience in linear and logistic regression analysis. * Had experience in data modeling using Erwin, Star Schema Modeling, and Snowflake modeling, FACT and Dimensions tables, physical and logical modeling. * Strong experience in Data Warehousing and ETL using Informatica Power Center and Salesforce systems. * Experience in PROC SQL joins and PROC SQL set operators to combine tables horizontally and vertically. * Experience in Integration of various Data Sources like MS-Access, Flat and CSV Files. * Good experience in UNIX. ed with batch files and ran SAS programs using UNIX shell scripts.  * Extensive experience in deployment of SAS programs/UNIX shell scripts and version controlling (PVCS) * Used ODS to display outputs in HTML, CSV or other file formats. * Extensive experience in writing reusable macros. * Strong experience in testing methodologies. Involved in System Testing, Integration Testing & UAT. * Excellent problem-solving  and having good knowledge in Finance and Credit Cards Business Domain.   : SAS 	SAS Base, SAS Macro, SAS SQL, SAS STAT, SAS Enterprise Miner, SAS HPA, SAS Access, SAS Graph, SAS Enterprise Business Intelligence, SAS HPF, SAS ECM, SAS Connect, SAS Enterprise Guide, SAS Data Integration Studio, SAS Web Report Studio, SAS Management Console, SAS AML, SAS ODS, SAS JMP Statistical Software Packages	JMP, Design Expert, MINITAB, SPSS, R and Weka Programming Languages	SQL, PL/SQL, HTML, CSS, PHP, AB INITIO, Visual Basic, Hive, Pig, Python, Java Databases	Oracle, Teradata, SQL-Server, HADOOP, Sybase and DB2 Software Packages	MS Office, SSRS, SSMS, Tableau, DOMO, Informatica, UML, JIRA, MS Project Operating Systems	Windows, UNIX, Salesforce application systems and Mainframe  & Certification Master of Science in Industrial Engineering & Statistics SAS Certified Base Programmer V9 from SAS Institute. Graduate Certificate in Statistics  Experience Lead Data Scientist / SAS ECM AML Consultant				            		Oct 17  Present Wells Fargo Bank (100% - Remote) Environment: SAS Enterprise Guide, SAS ECM/AML, R, UNIX, SAS Macro, HADOOP, Machine Learning  Responsibilities: * Performed advanced statistical analysis (univariate and multivariate analysis of variance, cluster and path analysis, principle component and factor analysis, analysis of covariance, survival & longitudinal analysis, logistic and linear regression modeling), created customized reports and presentation quality data  tables. * Involved in the development of a net present value (NPV) model that was used to optimize the selection of prospects using SAS BASE, SAS/Enterprise Miner and SAS Enterprise Guide. * Developed logistic regression models to predict subscription response rate based on customers variables like past transactions, response to prior mailings, promotions, demographics * Utilized SAS management console for adding new user groups, LOBs for dashboards in SAS ECM portal. * Statistically analyzed data using SAS and built Regression models for validation. * Performed Multi-Variate Statistical Risk analysis, Eigen system Analysis, Canonical Correlation Analysis and logistic regression Analysis using PROC CANCORR, PROC MIXED, PROC MEANS and PROC UNIVARIATE. * Used R statistical software for effective analysis by hypothesis testing to validate data and interpretations. * Performed configuration and support of SAS ECM Metadata (Cases, Incidents, Parties, and Financial Items), configuration and support of ECM reference tables (All Lookup tables). * Configured setting up User Accounts, Groups, Roles together with SAS Platform Administrator. * Performed migration of UI Definitions, custom properties, flows, Scripts for UDF and Case Configurations. * Developed Multiple Linear Regression model to determine how the factors like stock price, dividends, earnings, CPI and Interest rate affect. * Implemented solutions for ingesting data from various sources and processing the Data-at-Rest utilizing Big Data  such as Hadoop, Map Reduce Frames, HBase, Hive. * Implementation of Big Data ecosystem (Hive, Impala, Sqoop, Flume, Spark, Lambda) with Cloud Architecture. * Designed and deployed full SDLC of AWS Hadoop cluster based on client's business need. * Defined the process to build predictive model,  functions, variable selection and preparation and the statistical methodology and model validation. * Developed and implemented logistic regression modeling for direct mail campaign by targeting responsive customers and minimizing risk. * Implemented model validation and developed diagnostic tables and graphs that demonstrated how model can be used to improve the efficiency of the selection process for customer acquisition campaign. * Verified program logic by overseeing the preparation of test data, testing and debugging of SAS programs.  Senior Data Scientist									             Aug 17  Oct 17 Mastercard, Purchase, NY (100% - Remote)  Environment: Machine learning, Sqoop, SAS EG, SAS Forecast Studio, Python, DOMO, Tableau, R, Hive, Pig  Responsibilities: * Developed SAS/Excel ad-hoc reports for Priceless management team. * Generated comprehensive data files (MS Excel), identified trends, patterns and troubleshoot inconsistencies. * ed in writing Hadoop Jobs for analyzing data using Hive, Pig accessing Text format files, sequence files, Parquet files. * Performed data entry, cleansing and tagging (heavy volume of global assets) * Generated DDL and DML scripts using Python and managed aspects of QA testing. * Performed multiple regression modelling for load forecasting, and sequentially tested additional variables and combinations for model improvement using R programming. * Developed predictive models and statistical analysis for external clients and internal business performance. * Performed data acquisition, data mining, and analyzed data using valid analytical methods. * Communicated findings using Tableau dashboard visualizations and results in a concise and actionable manner and ensure results are well understood by project sponsors. * Experienced with batch processing of data sources using Apache Spark. * Developing predictive analytic using Apache Spark Scala APIs. * Involved in ing of big data analysis using Pig and User defined functions (UDF). * Created Hive External tables and loaded the data into tables and query data using HQL. * Used Sqoop to efficiently transfer data between databases and HDFS and used Flume to stream the log data from servers. * Designed, developed, maintained and evaluated statistical and predictive models to identify historical trends and forecast future patterns and other performance measures using SAS. * Accessed and compiled large amounts of data, and applied advanced statistical techniques to analyze the data, forecast, interpret, and quantify trends on various aspects of information.  * Responsible for statistical data gathering, sophisticated analysis, and development of recommendations. * Performed advanced quantitative assessments of all aspects of stress test models including theoretical aspects, model design and implementation, data integrity, and reliability. * Supported revenue management using statistical and quantitative analysis, developed several statistical approaches and optimization models to save more than half million $ in program cost and by price distribution. * ed with various business units to scope and design the statistical frame for business experiments and socialize and help integrate results of analysis of experiments. * Diagnosed patterns and relationships in business data and developing logical methodologies to answer complex business questions through analytics.  Data Scientist											    Jul 16  Jul 17 NBC Universal, NYC (100% - Remote)  Project: SAS Forecast Models Generation for NBCU Nets      Environment: SAS Enterprise Guide, SAS Forecast Studio, MySQL, Informatica Power Center, SQL Server, SAS Time Series Studio, MS Excel, Access, DOMO, Tableau, Spark, Hadoop, AWS  Responsibilities: * Define and create SAS/Excel ad-hoc reports for NBC nets. * Used the PL/SQL procedures for Informatica mappings for truncating the data in target tables at run time. * Performed data cleansing and validation on ing with several millions of records and data points. * Statistical validation of the forecast results using error estimation techniques. * Developed SQL Server Stored Procedures, Tuned SQL Queries (using Indexes and Execution Plan). * Installed and Configured MS Build Server, created build agents and Build Controllers. * ed with AWS to implement the client-side encryption as Dynamo DB does not support at rest encryption.  * Exploring with the Spark for improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark-SQL, Data Frame, Pair RDD's, Spark YARN. * Performance tuning daily for preventing issues & providing capacity planning using MySQL Enterprise Monitor. * Developed DOMO dashboards with customer data trends and plots for visual analytics. * Used Informatica Power Center 8.6 for extraction, transformation and load (ETL) of data in data warehouse. * Developed stored procedures, triggers in MySQL for lowering traffic between servers & clients. * I've performed correlation analysis using PROC HPCORR and generated pairwise Pearson correlation statistics and developed statistical predictive data models to predict the net impressions based on the past data provide business with forecast insights. * Developed Tableau data visualization using Cross tabs, Heat maps, Box and Whisker charts, Scatter Plots, Geographic Map, Pie Charts and Bar Charts and Density Chart. * Prepare detailed statistical reports in MS Access and MS Excel to track progress of weekly, monthly and quarterly customers and sales growth. * Performance Tuning in SQL Server using SQL Profiler and Data Loading. * ed with High Performance procedures like PROC HPSAMPLE, PROC HPREG, PROC HPF and HPBIN. * Developed forecast models from scratch with SAS Forecast Studio for NBCs various nets E!, BRAVO, OXYG, SPROUT, USA, CHILLER, NBCSN, SYFY and MSNBC * Developed ARIMA, Exponential Smoothing models (ESM), Intermittent Demand Models (IDM) and fine-tuned them by identifying outliers, creating pulse or level shift events and automated them. * Performed data manipulations for data quality assurance/validation of table, listing and summaries * Developed SAS programs for querying data from large datasets using SAS Enterprise Guide * Summarized the datasets to generate daypart models with quarterly and monthly forecast estimates.  Data Scientist / SAS Data Integration Consultant					Feb 14  Jun 16 Nestle Purina, St Louis, MO (50% - Remote)							 Project: Demand Planning Migration to SAS module Demand driven Planning & Optimization (DDPO) Environment: SAS Data Integration Studio, SAS Enterprise Guide, Informatica Power Center, SAS Enterprise Miner, SAS Macros, SAS ODS, UNIX, SAS Forecast Studio, SAS Forecast Server, Azure  Responsibilities: * Primarily responsible for extracting, transforming and data loading (ETL) into SAS datasets and creating flat files, reports and excel files. * Developed data structures, performed data manipulation and application development. * Designing, developing and implementing the DI Studio job flows to load the data into SAS. * Performed Double and Seasonal Exponential Smoothing using Time Series Exponential Smoothing (TS ESM) node, and data mining using SAS Enterprise Miner.  * Utilized MS Azure services with focus on big Data Engineer /analytics / enterprise data warehouse and business intelligence solutions to ensure optimal architecture, scalability, flexibility, availability, performance, and to provide meaningful and valuable information for better decision-making. * Gathering data requirements, designing and developing data marts using SAS Data Integration Studio. * Generated Baseline and promoted demand planning forecasts using SAS Forecast Studio. * Produced use cases and data flow diagrams as supporting documentation using MS Visio. * Assisted conceptual, logical, and physical data modeling, and interface effectively with data modelers * Involved in deployments, automation, scheduling of DI jobs and process documentation.  * Creating Stored Processes to provide users of reporting DataMart easy access using SAS Enterprise Guide. * Integrating and managing the data mappings from the clients data sources (mainly Oracle) to the SAS Analytical data mart using SAS DI Studio. * Supported Deployment to all testing and production environments & automated test cases using MS Build. * Designed flows with many sessions with decision, assignment task, event wait, and event raise tasks, used Informatica scheduler to schedule jobs.  * Documented and setup files, SAS program, and log files for new production box.   * Oversaw production jobs, production server, running daily/quarterly process jobs to support the business in reporting capability and day to day functions.  * ed and collaborated with project team members and other relevant knowledge experts. * Performing analytical and programming activities including analysis, design, development, unit & integration testing, implementation, and documentation of integrating the data management solutions.   Data Scientist											Oct 12  Jan 14 CareSource, Dayton, OH (50% - Remote) Environment: SAS Base/Macros, SAS Enterprise Guide, SQL Server, SAS ODS, Mainframe, R  Responsibilities: * Develop complex automated reporting products using BASE SAS that meet the internal and external information needs of the organization.  * Automated the manual exporting of data to complex Excel layouts and customized using SAS ODS Tagsets -EXCELXP utility. * Assist in the design of Business Intelligence products for both internal and external use. Participate in testing activities as needed. * Responsible for adhering to proper coding standards and IT change control process. * Evaluate HEDIS requirements, obtain data for reporting and prepare HEDIS reports. * Generate graphics that effectively describe, explore and summarize information for communication to appropriate parties. * Used R to help with Turn-around Time Distribution Analysis and Forecast on shipped and cancelled referrals, and visualized the result using R Shiny. * Used Machine Leaning techniques (e.g. Logistic Regression, Bayesian Linear regression) to build up predictive models on probability of cancellation of each new referral. * Writing SAS and SQL code for querying data from large datasets of the SQL Server Database. * Incorporate critical thinking  and judgment in the process to determine best course of action for each inquiry/problem.  * Generated batch files for regulatory reporting via e-filing using SAS Enterprise Case Management (SAS ECM). * Investigation of data that shows a pattern or indicates a significant variation from expected. * Represent the department in project meetings and other meetings that require subject matter knowledge. * Managing HEDIS and CAHPS reporting  for multiple markets with year-round monitoring and annual required reporting and submission using Data Transformation Services (DTS). * Assist the manager in directing and coordinating  within the department.  Statistical Analyst / SAS Programmer								Aug 11  Aug 12 JP Morgan Chase Bank, Columbus, OH (75% remote)					             Project: Consumer Model Governance - Score Monitoring System  Environment: SAS Base/Macros, Oracle, SQL, DB2, Teradata, UNIX, Mainframe, SAS/EG, SAS ECM, SAS AML  Responsibilities: * Writing SAS & SQL code for querying data from large datasets and extracted data from Oracle DB * Involved in campaign auditing and generating water fall reports for management approvals  * Used Dynamic LIBNAME, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC IMPORT, PROC APPEND, PROC REPORT AND PROC FORMAT * Performed basic SAS analysis on existing customers data, and database segmentation using PROC MEANS, FREQ, PROC  AND TABULATE. * Performed configuration and support of SAS ECM Metadata (Cases, Incidents, Parties, Financial Items), configuration and support of ECM reference tables (All Lookup tables). * Imported existing records and historical information by custom ETL to prepopulate customer fields and prevent rekeying errors using SAS ECM. * Developed forecasts for Cross sectional time series data using forecasting node in SAS Enterprise Miner. * Design, development and implementation of a new model validation and tracking process * Support of Business Intelligence validation reports used by other risk groups. * Visually created customer segments for behavior monitoring using SAS Anti-Money Laundering (SAS AML). * Design, produce and implement capabilities to execute file processing (ETL) required for production of SAS data sets for modeling and analytics that include data from credit records *  with model developers and the model implementation teams to develop logical data models and ensure efficient flows of information through automated business processes *  with the model reviewers to review business needs and strategies and assess the implications to the information/data architecture * Utilized SAS Enterprise Guide for Data Management and increased data consistency that enabled data-driven decisions and streamlines administration & ran ad-hoc SAS queries. * Performed Data de-duplication on ing with several millions of records and data points. * Performed Extraction, transformation & loading (ETL) & Data Cleansing with SAS Data Integration Studio. * Scheduling daily, weekly and monthly reports using SAS/MACROS. * Formatted data using SAS defined and user defined formats using PROC FORMAT. * Designed file directory built out structure for both production and development areas, so production job could be run in a controlled environment by AUTOSYS with users also being able to develop, code, test, and run ad-hoc reports against production data. * Managed AUTOSYS performance and re-runs if needed in SAS version 9.2 UNIX   * Set up log check program to review logs of production jobs to look for errors/warning and sent out emails if problems were found, eliminating manual checking of program logs. * Involved in development and enhancement of SAS programs.  Statistical Analyst										 Jan 11  Jul 11 Office of Statewide Health Planning & Development (OSHPD), Sacramento, CA (75% - Remote) Environment: SQL Server, SAS Base/EBI/EG/OLAP Cube Studio, SAS Management Console, Oracle, R  Responsibilities: * Involved in requirement gathering from different groups by conducting JAD sessions.  * Conducted regression analysis on impact of Medicaid expansion on population of Patient Assistance Program (PAP). * Developed SAS programs for querying data from large datasets using SAS Enterprise Guide * Extracting data from SQL Server, Oracle and created tables & populated them to centralize comprehensive data using SQL Server Management Studio * Provided forecast analysis on monthly basis to operation manager on patients volume, referral volume to help with labor planning. * Used multiple statistical methodologies with evaluation the impact of a nurse intervention project over time. (Propensity Score Matching Technique to generate control and treatment group population and did survival analysis and Cox Regression analysis). * Involved in Design of  Requirements Documents and implementation plan of the Clearinghouse * Used PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC MEANS, PROC FREQ, PROC TABULATE and PROC FORMAT in developing SAS programs. * Performed basic SAS analysis on existing customers data using PROC MEANS * Evaluate HEDIS requirements by monitoring and reporting on key performance indicators on a weekly and monthly basis, identifying trends, issues and potential solutions to those issues. * Performed Extraction, transformation & loading (ETL) & Data Cleansing with SAS Data Integration Studio. * Set up Log-Check and error-check program to review logs/error of production and development jobs. * Created intermediate data marts for storage and loaded with the validated data. * Build cubes using SAS OLAP Cube Studio & summarized datasets to generate region level reports. * Involved in development and enhancement of SAS programs and formatted data using SAS defined and user defined formats using PROC FORMAT. SAS Data Analyst									             Jun 10  Dec 10 Fannie Mae, Herndon, VA (50% remote) Environment: SAS Base/Macros, SAS Forecast Studio, Oracle, UNIX, SQL, Ab Initio, Visio  Responsibilities: * Involved in requirement gathering from different groups  * Writing SAS and SQL code for querying data from large datasets of the SQL Server & Teradata Database * Extracting data from Oracle (Hemisphere Database)  * Generating excel reports, Visio diagrams and delivering to different groups * Involved in design and implementation plan and Unit Testing of the SDLC * Used Dynamic LIBNAME, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC MEANS, PROC FREQ, PROC TABULATE and PROC FORMAT * Monitored production jobs, production server and running daily/quarterly/monthly process jobs. * Developed large quantities of accurate monthly forecasts using SAS Forecast Studio. * Involved in development and enhancement of SAS programs in Mainframe environment. * Analyzed existing system logic (Ab Initio, SAS), and translating the logic into baseline functional requirements and managed the requirements using Rational Requisite Pro * ed with business unit or other subject matter experts, in addition to IT staff to refine BRD and FSD. * Communicating clearly and precisely in functional (vs. ) terms  * Produced use cases and data flow diagrams as supporting documentation using MS Visio. * Performed data step manipulations for data quality assurance/validation of table, listing and summaries * Formatted data using SAS defined and user defined formats using PROC Format.  SAS Programmer / Analyst								            Aug 08  May 10 Comerica Bank, Phoenix, AZ (50% remote) Project: Campaign List processing/auditing  Environment: SAS Base/Macros, Oracle, Teradata, UNIX, Mainframe, SAS/EG SAS/EM, SQL, SAS AML  Responsibilities: * Involved in requirement gathering from different groups, design, implementation plan and Unit Testing * Writing SAS & SQL code for querying data from large datasets and extracted data from Oracle & Teradata * Involved in campaign auditing and generating water fall reports for management approvals  * Used Dynamic libname, PROC SQL, PROC SORT, PROC EXPORT, PROC COMPARE, PROC IMPORT, PROC APPEND, PROC REPORT and PROC FORMAT * Performed basic SAS analysis on existing customers data, and database segmentation using PROC MEANS, FREQ,  and TABULATE. * Compared & analyzed entitys current behavior to its historical behavior and behavior of peers using SAS AML. * Build Logistic Regression models and Multiple Linear Regression models to identify the control group cohorts using SAS/STAT and other SAS modules. * Statistical validation of the forecast results using error estimation techniques * Administered SAS system, SQL Server, Teradata & Oracle Database system and managed the system requirements, user access management & administration on UNIX and Windows server. * Developed forecasting models from scratch with SAS and used them for consumer prediction and control.  * Utilized SAS Enterprise Guide for Data Management and increased data consistency that enabled data-driven decisions and streamlines administration & ran adhoc SAS queries. * Developed Multiple Linear Regression model to determine how the factors like stock price, dividends, earnings, CPI and Interest rate affect Price-Earnings Ratio in stock market. * Doing Statistical Analysis with statistical procedures and univariate procedures from Base SAS, SAS/STAT * Performed Data de-duplication on ing with several millions of records and data points. * Performed Design of Experiments & Analysis (DOE) and optimized the combination of Model inputs. * Performed Multi-Variate Statistical Risk analysis, Eigen system Analysis, Canonical Correlation Analysis and logistic regression Analysis using PROC CANCORR, PROC MIXED, PROC MEANS and PROC UNIVARIATE  * Performed Data Mining and collected intricate details using SAS Enterprise Miner & Weka. * Performed Extraction, Transformation & Loading (ETL) & Data Cleansing with SAS Data Integration Studio. * Involved in development and enhancement of SAS programs. * Statistically analyzed data using MINITAB, R and S-Plus statistical software packages and built Regression models for validation. * Scheduling daily, weekly and monthly reports using SAS/MACROS.   	    "
"Willing to relocate: Anywhere
 Experience

Data Scientist
Cincinnati Children's Hospital Medical Center - Cincinnati, OH
April 2018 to Present
 used: Python, Keras, TensorFlow, Rstudio, MATLAB 
 Developed a deep learning model based on U-Net to segment cells from the biomedical images. Usedthe model to segment HEp-2 cells from the specimen images and achieved an F-Score of 0.8752.  Developed an algorithm to automatically segment regions of interest (ROI's) from the ultrasound images of the liver using deep neural nets. 
 Developed a classifier which uses clinical data of the patient and statistical data extracted fromtexture features of liver ultrasound images to classify the liver as healthy and unhealthy. 
 Implemented Random forest and state of the art deep learning techniques to predict whether thegiven antibiotic would  on people and achieved an overall accuracy of 91% and 95% respectively.
Data Scientist
University of Cincinnati - Cincinnati, OH
January 2017 to April 2017
 used: R, Python, SQL, Tableau, ggplot2 
 Analyzed conversations between recruiters and students and developed a large variety of variablesto predict the time of response from a recruiter using XGBoost. 
 Constructed a Latent Dirichlet Allocation model to segment a conversation between recruiters andstudents into different topics. Used Word2Vec to find similar words that are present in a given topic for further analysis. 
 Extracted, compiled and interpreted student data which is stored in various spreadsheets usingstatistical and data visualization techniques to generate ongoing campus placement reports and developed forecasting models to analyze and extrapolate financial resources and budget related to various divisions in college of law. 
 ed on developing a complete database of BAR examination reports of different students overthe years.
Data Analyst
Angryengine  - Visakhapatnam, Andhra Pradesh
October 2015 to April 2016
 Performed data mining, data cleaning & explored data visualization techniques on a variety of datastored in spreadsheets and text files using R and plotting the same using ggplot2 function 
 Developed a binary classification model with 83% accuracy to accept or reject future cash advanceapplications using regression in R based on 8 parameters and 8,000 data points processed from MySQL database  Created visually impactful interactive dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP


Master of Science in Computer Science in Artificial Intelligence
University of Cincinnati
July 2019
Software Engineering
Gitam University May 2016


SQL, Excel, Business Intelligence
Links

",Data Scientist,resume,"Willing to relocate: Anywhere  Experience  Data Scientist Cincinnati Children's Hospital Medical Center - Cincinnati, OH April 2018 to Present  used: Python, Keras, TensorFlow, Rstudio, MATLAB   Developed a deep learning model based on U-Net to segment cells from the biomedical images. Usedthe model to segment HEp-2 cells from the specimen images and achieved an F-Score of 0.8752.  Developed an algorithm to automatically segment regions of interest (ROI's) from the ultrasound images of the liver using deep neural nets.   Developed a classifier which uses clinical data of the patient and statistical data extracted fromtexture features of liver ultrasound images to classify the liver as healthy and unhealthy.   Implemented Random forest and state of the art deep learning techniques to predict whether thegiven antibiotic would  on people and achieved an overall accuracy of 91% and 95% respectively. Data Scientist University of Cincinnati - Cincinnati, OH January 2017 to April 2017  used: R, Python, SQL, Tableau, ggplot2   Analyzed conversations between recruiters and students and developed a large variety of variablesto predict the time of response from a recruiter using XGBoost.   Constructed a Latent Dirichlet Allocation model to segment a conversation between recruiters andstudents into different topics. Used Word2Vec to find similar words that are present in a given topic for further analysis.   Extracted, compiled and interpreted student data which is stored in various spreadsheets usingstatistical and data visualization techniques to generate ongoing campus placement reports and developed forecasting models to analyze and extrapolate financial resources and budget related to various divisions in college of law.   ed on developing a complete database of BAR examination reports of different students overthe years. Data Analyst Angryengine  - Visakhapatnam, Andhra Pradesh October 2015 to April 2016  Performed data mining, data cleaning & explored data visualization techniques on a variety of datastored in spreadsheets and text files using R and plotting the same using ggplot2 function   Developed a binary classification model with 83% accuracy to accept or reject future cash advanceapplications using regression in R based on 8 parameters and 8,000 data points processed from MySQL database  Created visually impactful interactive dashboards in Excel and Tableau for data reporting by using pivot tables and VLOOKUP   Master of Science in Computer Science in Artificial Intelligence University of Cincinnati July 2019 Software Engineering Gitam University May 2016   SQL, Excel, Business Intelligence Links  "
" 
 Looking for full time opportunities in Data Science or Data Analytics Domain. 
 2.5 years of experience in Java development and visualization and 8 months experience in Data Science and Machine Learning domain. 
 Expertise in Machine Learning, Fraud Detection, Deep Learning, Python, Java, SQL, Artificial Intelligence, Image and Text Processing.  
 

University of South Florida 	 
MS in Business Analytics and Information Systems GPA: 3.89/4 
                Tampa, FL Aug 2018 -  Expected Dec 2019 
University of Pune  	            Pune, India B. Tech in Electronics and Telecommunication GPA: 8.06/10      Aug 2011- May 2015 

	Programming Languages:- 	Python, R, SQL, MATLAB, JAVA, C, C# 	Database:- 	MS-SQL, Oracle, Cassandra 
	Data Mining :- 	WEKA, Microsoft Azure ML Studio, SAS 	Development :- 	Spyder, Eclipse, SQL Developer 
	Visualization :- 	Tableau, VISIO, Argo UML, Qlik 	 	Web :- 	HTML, CSS, Web Services, XML
 	Frame:- 	Keras, Tensor Flow, MVC 
DATA SCIENCE EXPERIENCE
 
Suncoast Credit Union                                                                                                                                                               Jan 2019  August 2019   
Data Science Intern 
 Building Statistical models to Automate the Car Loan process by predicting risky borrowers. Predicted default customers with 94% accuracy and 0.98 F1 Score. 
 Cost Based Analysis on highly skewed data to find the tradeoff between True Positive and False Positive values of Customer Default. 
 Developed a Genetic (Adaptive) algorithm for modeling User behavior which can be used for boosting a models forecasting accuracy. 
 PCA, Feature engineering using applied statics technique to determine the features influencing the loan application. 
 Implemented Hypothesis Testing, P value analysis, A/B testing, Bandit Testing, Cross Validation to find the best model for prediction. 
 Created end to end system to predict the individual probability of Loan going Delinquent, Possible Recovery of Loan and Loan Default. 
 Developed the Sun Logix Score that will aid in both Loan Decision Process and Portfolio Risk Analysis. 
 Designed Customer Segmentation and Loan Strategies using Sun Logix Score for profit maximization. 
 Similar analysis on Credit Card dataset to predict fraud, default and delinquent customers. 
 Models Used: Linear/Logistic Regression, XG Boost, Classification Algorithms, Deep Learning, Ensemble, ADASYN, SMOTE. 
 
Cognizant  Solutions                                                                                                                                                    Feb 2016  July 2018   
 Programmer Analyst 
 Achieved 45% reduction in time requirement by designing new tool for weekly tickets reporting. 
 Engineered complete website for Hospital Management System with Team Lead role. It included Java, Hibernate & MVC.  
 ed on HMH eCommerce domain with order processing, application development and code enhancement. 
 Data visualization with Tableau and Microsoft Visio on eCommerce Database. 
 Voluntary value adds like customer segmentation, predicting sales and customer buying pattern using machine learning. 
 

 
Loan Defaulter Analysis and Modeling 
 Analyzed data from lending club for predicting whether given variables and conditions a person would default loan or not. 
 Built a Cost Based matrix using Precision and Recall which will help business understand the Dollar value for actual profit after implementation of model. Models accuracy was 86%. 
 Models Used: Random Forest, Logistic Regression, PCA, SVM and XG Boost, Logit and Probit Modelling, Stacking Classifier. 
 
Sentiment Analysis on Bank of America  
 Extracted tweets from twitter using NLTK library of Python. Performed data cleaning by removing stop words, punctuations and stemming.  
 Converted data into TFIDF to determine most frequent words and classified the words in Positive and Negative Sentiments. 
 Developed interactive visualizations in Plotly and Matplotlib to determine the positive and negative tweets and the patterns of comments. 
 Models Used: Bayesian Classifiers, SVM, Decision Tree, KNN, Word Embedding, Glove. 
 
Image Classification on Yelp Dataset using Transfer Learning 
 Developed a model to classify the Food Images into different six food types using Transfer Leaning on Inception V3 model and CNN 
 Models Used: CNN, Auto Encoder, Transfer Learning, Machine Learning, Deep Learning. 
 
Employee Attrition Prediction on IBM Dataset  
 Created model to predict employee attrition on IBM Dataset, got 74% accuracy with ensemble of Neural Net and Decision Tree. 
 Feature selection done by Chi-Square, PCA via SAS Enterprise Miner.  
 Models Used: Neural Net, Decision Tree, MBR, KNN ",Data Scientist,resume,"   Looking for full time opportunities in Data Science or Data Analytics Domain.   2.5 years of experience in Java development and visualization and 8 months experience in Data Science and Machine Learning domain.   Expertise in Machine Learning, Fraud Detection, Deep Learning, Python, Java, SQL, Artificial Intelligence, Image and Text Processing.      University of South Florida 	  MS in Business Analytics and Information Systems GPA: 3.89/4                  Tampa, FL Aug 2018 -  Expected Dec 2019  University of Pune  	            Pune, India B. Tech in Electronics and Telecommunication GPA: 8.06/10      Aug 2011- May 2015   	Programming Languages:- 	Python, R, SQL, MATLAB, JAVA, C, C# 	Database:- 	MS-SQL, Oracle, Cassandra  	Data Mining :- 	WEKA, Microsoft Azure ML Studio, SAS 	Development :- 	Spyder, Eclipse, SQL Developer  	Visualization :- 	Tableau, VISIO, Argo UML, Qlik 	 	Web :- 	HTML, CSS, Web Services, XML  	Frame:- 	Keras, Tensor Flow, MVC  DATA SCIENCE EXPERIENCE   Suncoast Credit Union                                                                                                                                                               Jan 2019  August 2019    Data Science Intern   Building Statistical models to Automate the Car Loan process by predicting risky borrowers. Predicted default customers with 94% accuracy and 0.98 F1 Score.   Cost Based Analysis on highly skewed data to find the tradeoff between True Positive and False Positive values of Customer Default.   Developed a Genetic (Adaptive) algorithm for modeling User behavior which can be used for boosting a models forecasting accuracy.   PCA, Feature engineering using applied statics technique to determine the features influencing the loan application.   Implemented Hypothesis Testing, P value analysis, A/B testing, Bandit Testing, Cross Validation to find the best model for prediction.   Created end to end system to predict the individual probability of Loan going Delinquent, Possible Recovery of Loan and Loan Default.   Developed the Sun Logix Score that will aid in both Loan Decision Process and Portfolio Risk Analysis.   Designed Customer Segmentation and Loan Strategies using Sun Logix Score for profit maximization.   Similar analysis on Credit Card dataset to predict fraud, default and delinquent customers.   Models Used: Linear/Logistic Regression, XG Boost, Classification Algorithms, Deep Learning, Ensemble, ADASYN, SMOTE.    Cognizant  Solutions                                                                                                                                                    Feb 2016  July 2018     Programmer Analyst   Achieved 45% reduction in time requirement by designing new tool for weekly tickets reporting.   Engineered complete website for Hospital Management System with Team Lead role. It included Java, Hibernate & MVC.    ed on HMH eCommerce domain with order processing, application development and code enhancement.   Data visualization with Tableau and Microsoft Visio on eCommerce Database.   Voluntary value adds like customer segmentation, predicting sales and customer buying pattern using machine learning.       Loan Defaulter Analysis and Modeling   Analyzed data from lending club for predicting whether given variables and conditions a person would default loan or not.   Built a Cost Based matrix using Precision and Recall which will help business understand the Dollar value for actual profit after implementation of model. Models accuracy was 86%.   Models Used: Random Forest, Logistic Regression, PCA, SVM and XG Boost, Logit and Probit Modelling, Stacking Classifier.    Sentiment Analysis on Bank of America    Extracted tweets from twitter using NLTK library of Python. Performed data cleaning by removing stop words, punctuations and stemming.    Converted data into TFIDF to determine most frequent words and classified the words in Positive and Negative Sentiments.   Developed interactive visualizations in Plotly and Matplotlib to determine the positive and negative tweets and the patterns of comments.   Models Used: Bayesian Classifiers, SVM, Decision Tree, KNN, Word Embedding, Glove.    Image Classification on Yelp Dataset using Transfer Learning   Developed a model to classify the Food Images into different six food types using Transfer Leaning on Inception V3 model and CNN   Models Used: CNN, Auto Encoder, Transfer Learning, Machine Learning, Deep Learning.    Employee Attrition Prediction on IBM Dataset    Created model to predict employee attrition on IBM Dataset, got 74% accuracy with ensemble of Neural Net and Decision Tree.   Feature selection done by Chi-Square, PCA via SAS Enterprise Miner.    Models Used: Neural Net, Decision Tree, MBR, KNN "
"Master of Science in Information Management, Syracuse University          GPA 3.89/4.0 	                           August 2019 Certification of Advanced Study in Data Science 
Awarded Merit-based Diversity Scholarship for exemplary academic and extra-curricular performance 
Peer note-taker for a visually impaired student; Contributor to InfoSpace, the official blog of iSchool 
Bachelor of , Charotar University of Science and    GPA 3.41/4.0 	                         May 2015 
 Experience 	  	         
Data Analyst and Business Intelligence intern, Covanta Energy, Morristown, New Jersey               January 2019  August 2019 
 Improved operational efficiency of waste management plants by analyzing data, building dashboards on Power B.I. and presenting actionable insights to help stakeholders take data-driven decisions 
 Performed prescriptive analytics to identify KPIs of waste management plants by writing Python scripts and implemented association rules machine learning algorithm to provide strategic direction and improved the plants capability 
 Collaborated with the team and stakeholders by holding daily SCRUM meetings, weekly meetings for status update and built automated notification system to reduce carbon emission and ensured federal compliance 
Analyst & Customer Relationship Manager, Interactive Manpower Solution, Ahmedabad 	May 2016  June 2017 
 Assisted and collaborated on data visualization using R and Salesforce to perform digital transformation for an international client to improve market segmentation and online customer engagement 
 Programed, trained and tested different classification models (neural nets, Bayesian classifier, support vector machine, decision tree) in R language, to get the most accurate algorithm, predict customer pain point, and decreased customer support response time by 21% 
Relational Database developer and Analyst, ShipMyBox, Ahmedabad 	May 2015  April 2016 
 Designed coded, tested and supported database solutions for ShipMyBox in Microsoft SQL server, setup the technological backbone and built a ing product from scratch 
 Wrote python scripts for the extraction and analysis 19,100 Indian zipcodes to determine the best shipping companies based on geography using extensive data science techniques 
Relevant Course 	 
 Data Science: Applied Data Science, Data Analytics, Big Data Analytics, Text Analytics, System Analysis and Design, Database Management 
 Management: Project Management, Enterprise Risk Management, Management Principles for Information s, Strategic Management of Information Resources, Information Policy 
Academic and Real-world  	         
Database Management 
 Designed and built a database management system for public bus transportation system by creating entity relationship diagram, normalizing the database to 3rd normal form and satisfied business rules 
 Wrote complex SQL queries using joins, grouping aggregation, nested subqueries to extract relevant data on Microsoft SQL server, answer business questions and improved customer retention by 24% 
 Designed user-interface along with backend database to improve data extraction, transformation and loading 
Applied Data Science 
 Built a bag of words model to analyze restaurant reviews and determined factors that drive customers to give negative feedback and helped restaurants in identifying areas of improvement 
 Extracted relevant data using SQL on nation-wide zipcodes and wrote python scripts for the extraction and analysis of open data available on Indian shipping industry Data Analytics/Machine Learning 
 Built robust machine learning customer churn models to compare the behavior of customers who churn and customers who 
dont churn, predict the customers who will churn in future based on their behavior to improve customer attrition 
 Deployed KNN algorithm and built a model to classify user response thereby significantly improved and optimized a social media marketing campaign 
	Publications 	 
 Santani, S., (2019). The Government vs Citizens: The Fight for Personal Data. InfoSpace The Official Blog of the iSchool, SU 
 Santani, S., (2018). How to Protect Your Privacy on Social Media. InfoSpace The Official Blog of the Syracuse University 
 Deshpande, N., Santani, S., (2015). The End Where I Begin. Blackbuck Publications 
	 	 
 Programming languages: SQL, R, Python (Numpy, Matplotlib, Pandas, Sckit learn) 
 Data Analysis & Visualization: Power BI, Tableau, R, Gephi, Qlik, D3 Library, Machine Learning, Statistical Analysis, D3 
 Applications: Hadoop, Apache spark, AdWords, MS Project, MS Office, Microsoft SQL Server, Salesforce, Visio, Access, ggmap in R, Advanced Excel with Pivot Tables 
Leadership / Involvement 	 
 
 On-boarding for IMS: Accelerated on-boarding for IMS by training new hires & mentored them in orientation 
 Peers note-taker: Volunteered for office of disability to assist a visually impaired peer for class note-taking at the iSchool, Syracuse University ",Data Scientist,resume,"Master of Science in Information Management, Syracuse University          GPA 3.89/4.0 	                           August 2019 Certification of Advanced Study in Data Science  Awarded Merit-based Diversity Scholarship for exemplary academic and extra-curricular performance  Peer note-taker for a visually impaired student; Contributor to InfoSpace, the official blog of iSchool  Bachelor of , Charotar University of Science and    GPA 3.41/4.0 	                         May 2015   Experience 	  	          Data Analyst and Business Intelligence intern, Covanta Energy, Morristown, New Jersey               January 2019  August 2019   Improved operational efficiency of waste management plants by analyzing data, building dashboards on Power B.I. and presenting actionable insights to help stakeholders take data-driven decisions   Performed prescriptive analytics to identify KPIs of waste management plants by writing Python scripts and implemented association rules machine learning algorithm to provide strategic direction and improved the plants capability   Collaborated with the team and stakeholders by holding daily SCRUM meetings, weekly meetings for status update and built automated notification system to reduce carbon emission and ensured federal compliance  Analyst & Customer Relationship Manager, Interactive Manpower Solution, Ahmedabad 	May 2016  June 2017   Assisted and collaborated on data visualization using R and Salesforce to perform digital transformation for an international client to improve market segmentation and online customer engagement   Programed, trained and tested different classification models (neural nets, Bayesian classifier, support vector machine, decision tree) in R language, to get the most accurate algorithm, predict customer pain point, and decreased customer support response time by 21%  Relational Database developer and Analyst, ShipMyBox, Ahmedabad 	May 2015  April 2016   Designed coded, tested and supported database solutions for ShipMyBox in Microsoft SQL server, setup the technological backbone and built a ing product from scratch   Wrote python scripts for the extraction and analysis 19,100 Indian zipcodes to determine the best shipping companies based on geography using extensive data science techniques  Relevant Course 	   Data Science: Applied Data Science, Data Analytics, Big Data Analytics, Text Analytics, System Analysis and Design, Database Management   Management: Project Management, Enterprise Risk Management, Management Principles for Information s, Strategic Management of Information Resources, Information Policy  Academic and Real-world  	          Database Management   Designed and built a database management system for public bus transportation system by creating entity relationship diagram, normalizing the database to 3rd normal form and satisfied business rules   Wrote complex SQL queries using joins, grouping aggregation, nested subqueries to extract relevant data on Microsoft SQL server, answer business questions and improved customer retention by 24%   Designed user-interface along with backend database to improve data extraction, transformation and loading  Applied Data Science   Built a bag of words model to analyze restaurant reviews and determined factors that drive customers to give negative feedback and helped restaurants in identifying areas of improvement   Extracted relevant data using SQL on nation-wide zipcodes and wrote python scripts for the extraction and analysis of open data available on Indian shipping industry Data Analytics/Machine Learning   Built robust machine learning customer churn models to compare the behavior of customers who churn and customers who  dont churn, predict the customers who will churn in future based on their behavior to improve customer attrition   Deployed KNN algorithm and built a model to classify user response thereby significantly improved and optimized a social media marketing campaign  	Publications 	   Santani, S., (2019). The Government vs Citizens: The Fight for Personal Data. InfoSpace The Official Blog of the iSchool, SU   Santani, S., (2018). How to Protect Your Privacy on Social Media. InfoSpace The Official Blog of the Syracuse University   Deshpande, N., Santani, S., (2015). The End Where I Begin. Blackbuck Publications  	 	   Programming languages: SQL, R, Python (Numpy, Matplotlib, Pandas, Sckit learn)   Data Analysis & Visualization: Power BI, Tableau, R, Gephi, Qlik, D3 Library, Machine Learning, Statistical Analysis, D3   Applications: Hadoop, Apache spark, AdWords, MS Project, MS Office, Microsoft SQL Server, Salesforce, Visio, Access, ggmap in R, Advanced Excel with Pivot Tables  Leadership / Involvement 	     On-boarding for IMS: Accelerated on-boarding for IMS by training new hires & mentored them in orientation   Peers note-taker: Volunteered for office of disability to assist a visually impaired peer for class note-taking at the iSchool, Syracuse University "
"* Above 8+ years of experience in large datasets of Structured and Unstructured data, Data Visualization , Data Acquisition, Predictive modeling, NLP/NLU/NLG/AI/machine learning/Computer vision/Probabilistic Graphical Models/Inferential statistics/Graph,Data Validation. 
* Hands on experience indata mining algorithms and approach.
* Good at algorithm and design techniques
* Expert level understanding in ApplicationDesign, Development and testing in Mainframeenvironmentsusing PL/1, COBOL, EGL, Easytrieve, DB2, JCL, QC &VAG.
* Experience in developing different Statistical Machine Learning, Text Analytics, Data Mining solutions to various business generating and problems data visualizations using Python, R and Tableau. 
* Expertise in transforming business requirements into building models, designing algorithms, developing data mining and reporting solutions that scales across massive volume of unstructured data and structured. 
* Regressionanalysis, Statisticaltest analysis, Report and Dashboard generation, Datamanagement.
* Git, Java, MySQL, MongoDB, Neo4J, AngularJS, SPSS, Tableau.
* Python, Numpy, Scikit-Learn, genism, NLTK, Tensorflow, keras.
* Experience in MachineLearning, Statistics, Regression- Linear, Logistic, Poisson, Binomial.
* Experience in designing visualizations using Tableau software and Storyline on web and desktop platforms, publishing and presenting dashboards. 
* Single handed built a model to replace the job of doer in the pension sector. This model (Patent under progress) generates experience from structured data and learns through a bootstrapping mechanism new experience from unseen data.  
* Single handed Built and designed a whole Information extraction botPOC for KYC extraction. This bot is using adaptive learning techniques and uses some custom supervised classifiers for entity and relation extraction
* Proficient in Machine Learning techniques (Decision Trees, Linear, Logistics, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
* Hands on experience in implementing LDA, Naive Bayes and skilled in Decision Trees, Random Forests, Linear and Logistic Regression, SVM, Clustering, neural nets and good knowledge on Recommender Systems. 
* Develop Azure POC for Retailer Data Research tool. Develop and Data Factory Micro-service data movement from Azure SQL Server to Cosmos NoSQL Document database.
* Experience on advanced SASprogramming techniques, such as PROC APPEND, PROC DATASETS, and PROC TRANSPOSE. 
* Highly skilled in using visualization  like Tableau, Matplotlib for creating dashboards. 
* Extensive ing experience with Python including Scikit-learn, Pandas and Numpy. 
* Developed data variation analysis and data pair association analysis in the Bioinformatics field. 
* Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
* Having good domain knowledge on Retail and Airlines. 
* Experience in foundational machine learning models and concepts( Regression, boosting, GBM, NNs, HMMs, CRFs, MRFs, deep learning). 
* Well experienced in Normalization&De-Normalization techniques for optimum performance in relational and dimensional database environments. 
* Analyzed data using R, Perl, Hadoop and queried data using structured and unstructured databases 
* Strong programming expertise  Python and strong in Database SQL.
* Integration Architect & Data Scientist experience in Analytics, Big Data, SOA, ETL and Cloud . 
* ed and extracted data from various database sources like Oracle, SQL Server and Teradata. 
* Skilled in System Analysis, Dimensional Data Modeling, Database Design and implementing RDBMS specific features. 
* Facilitated and helped translate complex quantitative methods into simplified solutions for users. 
* Knowledge of ing with Proof of Concepts and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging. 
* Solid coding and engineering  in Machine Learning
* Experience with file systems, server architectures, databases, SQL, and data movement (ETL).
* Proficient in Python, experience building, and product ionizing end-to-end systems
* Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning

 

Bachelors of Engeneering.

 








Statistics/ML
Exploratory Data Analysis: Univariate/MultivariateOutlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization
Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods
Statistical Tests: T Test, Chi-Square tests, Stationarity tests,Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova
Sampling Methods: Bootstrap sampling methods and Stratified sampling
Model Tuning/Selection: Cross Validation, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series
Machine Learning /
Deep Learning
R:caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot
Python:pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib,tensorflow
SAS: Forecast server, SAS Procedures and Data Steps
Spark: MLlib, GraphX
SQL:Subqueries, joins, DDL/DML statements
Databases/ETL/Query
Teradata, SQL Server, Redshift, Postgres and Hadoop (MapReduce); SQL, Hive, Azure Data Factory ,Pig and Alteryx.
Visualization
Tableau, ggplot2 and RShiny
Prototyping
PowerPoint,RShiny and Tableau





Client: State of MA ,Boston,MA.				                                                                    Aug 2018- Till date
Role:Data Scientist

Description: Massachusetts, officially the Commonwealth of Massachusetts, is the most populous state in the New England region of the northeastern United States. It borders on the Atlantic Ocean to the east, the states of Connecticut and Rhode Island to the south, New Hampshire and Vermont to the north, and New York to the west.

Responsibilities:
* Involved in defining the source to target data mappings, business rules, and data definitions. 
* Performing data profiling on various source systems that are required for transferring data to ECH using 
* Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM). 
* Involved in data collection and induction to Teradata
* Conducted data cleaning, data preparation, and outlier detection
* Finding insights from millions of customer chat and calls records
* Gathering requirements from business.
* Involved in creating pipelines that move, transform, and analyze data from a wide variety of sources using multiple methods like the Azure Power shell utility that allows control of many other Azure resources,Python,  a REST API, and the Azure Portal UI.
* Reviewing business requirements and analyzing data sources 
* Developed predictive models for sales and Finance teams using various ML and DL algorithms 
* Utilizing Informatica et (InformaticaData Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling. 
* Created a Handler function in Python using AWS Lambda that can invoke when the service is executed.
* Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model.
* ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005 
* Involved in upgrading DTS packages to SSIS packages (ETL). 
* Involved in Training and Testing the ML Supervised and Unsupervised models
* Researching on Deep Learning to implement NLP
* Presented to the higher management the discovered trends and analysis, forecast data, recommendations, model results and risks identified.
* Design and Develop Data Warehouse for Tax Information Factory using Oracle PL/SQL.
* Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database. 
* Using HP Quality Center v 11 for defect tracking of issues.
* Involved in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.
* Created and presented executive dashboards to show the patterns & trends in the data using Tableau Desktop
* Developed NLP models for Topic extraction, Sentiment Analysis
* Developed Executive  KPI, Key value programs, NPI dashboards in Tableau
* Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reports and dashboards.
* Experience in Analysis, Design, Development, Implementation, Testing and Support of Data Warehousing and Data Integration Solutions using Informatica Power center.
* Was able to identify emerging issues using the models
* Developing & evaluating Machine Learning models
* Developed different visualizations using advanced features and deep analytics in Tableau
* Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed 
* Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through Reports, 100% stacked bar charts etc. in Tableau Desktop
* Involved in publishing, scheduling and subscriptions with Tableau Server andcreating and managing users, groups, sites in Tableau Server.
* Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively
* Tested dashboards to ensure data was matching as per the business requirements and if there were any changes in underlying data

Environment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.

Client: Enbridge,Houston,TX .		                                                                                                 May 2017- Jul2018
Role: Data Scientist

Description: Enbridge Inc. is a Canadian multinational energy transportation company based in Calgary, Alberta. It focuses on the transportation, distribution and generation of energy, primarily in North America.
Responsibilities:
* A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.
* Installed and used CaffeDeepLearningFrame
* ed on different data formats such as JSON, XML and performed machinelearningalgorithms in Python.
* Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis. 
* Developing Voice Bot  using AI (IVR ), improving the interaction between Human and the Virtual Assistant .
* Implemented Event Task for execute Application Automatically. 
* Involved in developing Patches & Updates Module. 
* Setup storage and dataanalysis  in AmazonWebServices cloud computing infrastructure. 
* Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms. 
* Development and Deployment using Google Dialogflow Enterprise.
* ed as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7 
* Data visualizationusingElasticsearch ,Kibana and Logstash in python.
* Used Kibana an open source plugin for Elasticsearch  in analytics and Data visualization.
* DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.
* ed on backing up and restoring the Azure Data Factory.
* Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frames.
* Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.
* Implemented Agile Methodology for building an internal application.
* Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets. 
* Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics 
* Broad knowledge of programming, and scripting (especially in R / Java / Python) 
* Developing and maintaining Data Dictionary to create metadata reports for  and business purpose. 
* Predictive modeling using state-of-the-art methods 
* Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS. 
* Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool. 
* Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators. 
* Proven experience building sustainable and trustful relationships with senior leaders
* Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems. 
* Development level experience in Microsoft Azure providing data movement and scheduling functionality to cloud-based  such as Azure Blob Storage and Azure SQL Database.
* Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors 
* Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. 
* Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management. 
* Extracted data from HDFS and prepared data for exploratory analysis using datamunging

Environment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM,  GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata,  OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.

Client: DAK Americas LLC,Moncks Corner,SC.					                       Jan 2016- Apr 2017
Role:DataAnalyst/Data Scientist

Description: DAK Americas LLC manufactures and supplies polyethylene terephthalate resins (PET), polyesters staple fibers (PSF), monomers/ingredients (TPA/PTA), and specialty polymers for customers in the United States and internationally.

Responsibilities:
* Assisting business by being able to deliver a machine learning project from beginning to end, aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization
* Created data modeling and data mapping document containing source, formulate transformational rules to populate target fields.
* Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics
* Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift, oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.
* Created impact & gap analysis documents specifying changes introduced as part of the program and lead the business process team
*  with big data consultants to analyze, extract, normalize and label relevant data using Statistical modeling techniques like Logistic regression, decision trees, Support vector machine, Random forest, Naive Bayes and neural nets
* Developed ETLs for data sources used in production reporting for marketing and operations teams.
* Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables
* Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billion acquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff.
* Review business data for trends, patterns or casual analysis to assist in identifying model drift and retraining models 
* Created customized reports and processes in SAS and Tableau Desktop
* Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria, conditions, business rules and data elements to be included into the report
* Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teams to implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers
* Performed SQL query for data analysis and integration
* Support PMO governance activities; defining and maintaining Project Management standards. 
* Responsible for generating ideas for product changes that improve key metrics
* Provided data analytics of the web-portal to the team for feedback and improvement. 

Environment:Python, HTML5, CSS3, AJAX,Teradata,  OLTP, random forest, OLAP, HDFS, ODS,  JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib,Hadoop, Pig Scripts.

Client:Emblem Health. Newyor,NY.							May 2014- Dec2015
Role: Data Analyst/Data Modeler

Description: Emblem Health is one of the United States' largest non-profit health plans. It is headquartered at 55 Water Street in Lower Manhattan, New York City. It is a $10 billion company with 3.1 million members.

Responsibilities:
* Involved in defining the source to target data mappings, business rules, data definitions. 
* Involved in defining the business/transformation rules applied for sales and service data. 
* ed with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines. 
* Define the list codes and code conversions between the source systems and the data mart. 
* Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality. 
* ed with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata. 
* Responsible for defining the key identifiers for each mapping/interface. 
* Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.
* Responsible for defining the key identifiers for each mapping/interface. 
* Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans 
* Performed data quality in TalendOpenStudio. 
* Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system. 
* Responsible for defining the functional requirement documents for each source to target interface. 
* Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements. 
* Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding. 
* Enterprise Metadata Library with any changes or updates. 
* Generate weekly and monthly asset inventory reports.

Environment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.

Client: GD Research centre Hyderabad,India.			                                              Dec2012- Apr2014
Role: Data Analyst

Description: GD Research Center is a leading provider of global business intelligence including competitor, market, product, and customer information. It provides in-depth research, analysis, data and forecasts through a range of interactive online databases.

Responsibilities:
* Analyze business information requirements and model class diagrams and/or conceptual domain models. 
* Managed the project requirements, documents and use cases by IBM Rational RequisitePro. 
* Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart. 
* Gather & Review Customer Information Requirements for OLAP and building the data mart. 
* Responsible for defining the key identifiers for each mapping/interface 
* Responsible for defining the functional requirement documents for each source to target interface. 
* Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system. 
* Enterprise Metadata Library with any changes or updates. 
* Document data quality and traceability documents for each source interface. 
* Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements. 
* Analyzed business process flows and assisted in the development of ETL procedures for mapping data from source to target systems. 
* ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. 
* Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL. 
* Establish standards of procedures. 
* Generate weekly and monthly asset inventory reports. 
* Document all data mapping and transformation processes in the Functional Design documents based on the business requirements

Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.

Client: ABP Engitech solutions PVT Ltd,Hyderabad				               Feb 2011- Nov 2012
Role: Data Analyst

Description: ABP is a global Engineering company firm that provides total engineering solutions for a variety of types and sized jobs to a substantial and diversified client base that includes Oil and Gas, Steel, Power plants, Rail, and Aerospace industries.The firm is in Hyderabad, India and maintains offices in UK, Kuwait,and Jamshedpur around the world.

Responsibilities:
* Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect. 
* Developed logical and Physical data models using Erwin to design OLTP system for different applications. 
* Facilitated transition of logical data models into the physical database design and recommended  approaches for good data management practices. 
* ed with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Model using Forward engineering. 
* ed with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes. 
* Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).
* Performed K-meansclustering, Multivariate analysis, and Support Vector Machines in R. 
* Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements. 
* Written complex Hive and SQL queries for data analysis to meet business requirements. 
* Written complex SQL queries for implementing business requirements 

Environment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.",Data Scientist,resume,"* Above 8+ years of experience in large datasets of Structured and Unstructured data, Data Visualization , Data Acquisition, Predictive modeling, NLP/NLU/NLG/AI/machine learning/Computer vision/Probabilistic Graphical Models/Inferential statistics/Graph,Data Validation.  * Hands on experience indata mining algorithms and approach. * Good at algorithm and design techniques * Expert level understanding in ApplicationDesign, Development and testing in Mainframeenvironmentsusing PL/1, COBOL, EGL, Easytrieve, DB2, JCL, QC &VAG. * Experience in developing different Statistical Machine Learning, Text Analytics, Data Mining solutions to various business generating and problems data visualizations using Python, R and Tableau.  * Expertise in transforming business requirements into building models, designing algorithms, developing data mining and reporting solutions that scales across massive volume of unstructured data and structured.  * Regressionanalysis, Statisticaltest analysis, Report and Dashboard generation, Datamanagement. * Git, Java, MySQL, MongoDB, Neo4J, AngularJS, SPSS, Tableau. * Python, Numpy, Scikit-Learn, genism, NLTK, Tensorflow, keras. * Experience in MachineLearning, Statistics, Regression- Linear, Logistic, Poisson, Binomial. * Experience in designing visualizations using Tableau software and Storyline on web and desktop platforms, publishing and presenting dashboards.  * Single handed built a model to replace the job of doer in the pension sector. This model (Patent under progress) generates experience from structured data and learns through a bootstrapping mechanism new experience from unseen data.   * Single handed Built and designed a whole Information extraction botPOC for KYC extraction. This bot is using adaptive learning techniques and uses some custom supervised classifiers for entity and relation extraction * Proficient in Machine Learning techniques (Decision Trees, Linear, Logistics, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.  * Hands on experience in implementing LDA, Naive Bayes and skilled in Decision Trees, Random Forests, Linear and Logistic Regression, SVM, Clustering, neural nets and good knowledge on Recommender Systems.  * Develop Azure POC for Retailer Data Research tool. Develop and Data Factory Micro-service data movement from Azure SQL Server to Cosmos NoSQL Document database. * Experience on advanced SASprogramming techniques, such as PROC APPEND, PROC DATASETS, and PROC TRANSPOSE.  * Highly skilled in using visualization  like Tableau, Matplotlib for creating dashboards.  * Extensive ing experience with Python including Scikit-learn, Pandas and Numpy.  * Developed data variation analysis and data pair association analysis in the Bioinformatics field.  * Regularly accessing JIRA tool and other internal issue trackers for the Project development.  * Having good domain knowledge on Retail and Airlines.  * Experience in foundational machine learning models and concepts( Regression, boosting, GBM, NNs, HMMs, CRFs, MRFs, deep learning).  * Well experienced in Normalization&De-Normalization techniques for optimum performance in relational and dimensional database environments.  * Analyzed data using R, Perl, Hadoop and queried data using structured and unstructured databases  * Strong programming expertise  Python and strong in Database SQL. * Integration Architect & Data Scientist experience in Analytics, Big Data, SOA, ETL and Cloud .  * ed and extracted data from various database sources like Oracle, SQL Server and Teradata.  * Skilled in System Analysis, Dimensional Data Modeling, Database Design and implementing RDBMS specific features.  * Facilitated and helped translate complex quantitative methods into simplified solutions for users.  * Knowledge of ing with Proof of Concepts and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.  * Solid coding and engineering  in Machine Learning * Experience with file systems, server architectures, databases, SQL, and data movement (ETL). * Proficient in Python, experience building, and product ionizing end-to-end systems * Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning     Bachelors of Engeneering.            Statistics/ML Exploratory Data Analysis: Univariate/MultivariateOutlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau  Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods Statistical Tests: T Test, Chi-Square tests, Stationarity tests,Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova Sampling Methods: Bootstrap sampling methods and Stratified sampling Model Tuning/Selection: Cross Validation, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series Machine Learning / Deep Learning R:caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot Python:pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib,tensorflow SAS: Forecast server, SAS Procedures and Data Steps Spark: MLlib, GraphX SQL:Subqueries, joins, DDL/DML statements Databases/ETL/Query Teradata, SQL Server, Redshift, Postgres and Hadoop (MapReduce); SQL, Hive, Azure Data Factory ,Pig and Alteryx. Visualization Tableau, ggplot2 and RShiny Prototyping PowerPoint,RShiny and Tableau      Client: State of MA ,Boston,MA.				                                                                    Aug 2018- Till date Role:Data Scientist  Description: Massachusetts, officially the Commonwealth of Massachusetts, is the most populous state in the New England region of the northeastern United States. It borders on the Atlantic Ocean to the east, the states of Connecticut and Rhode Island to the south, New Hampshire and Vermont to the north, and New York to the west.  Responsibilities: * Involved in defining the source to target data mappings, business rules, and data definitions.  * Performing data profiling on various source systems that are required for transferring data to ECH using  * Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).  * Involved in data collection and induction to Teradata * Conducted data cleaning, data preparation, and outlier detection * Finding insights from millions of customer chat and calls records * Gathering requirements from business. * Involved in creating pipelines that move, transform, and analyze data from a wide variety of sources using multiple methods like the Azure Power shell utility that allows control of many other Azure resources,Python,  a REST API, and the Azure Portal UI. * Reviewing business requirements and analyzing data sources  * Developed predictive models for sales and Finance teams using various ML and DL algorithms  * Utilizing Informatica et (InformaticaData Explorer, and Informatica Data Quality) to analyze legacy data for Data Profiling.  * Created a Handler function in Python using AWS Lambda that can invoke when the service is executed. * Implemented statistical modeling with XGBoost machine learning software package using Python to determine the predicted probabilities of each model. * ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005  * Involved in upgrading DTS packages to SSIS packages (ETL).  * Involved in Training and Testing the ML Supervised and Unsupervised models * Researching on Deep Learning to implement NLP * Presented to the higher management the discovered trends and analysis, forecast data, recommendations, model results and risks identified. * Design and Develop Data Warehouse for Tax Information Factory using Oracle PL/SQL. * Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.  * Using HP Quality Center v 11 for defect tracking of issues. * Involved in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling. * Created and presented executive dashboards to show the patterns & trends in the data using Tableau Desktop * Developed NLP models for Topic extraction, Sentiment Analysis * Developed Executive  KPI, Key value programs, NPI dashboards in Tableau * Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reports and dashboards. * Experience in Analysis, Design, Development, Implementation, Testing and Support of Data Warehousing and Data Integration Solutions using Informatica Power center. * Was able to identify emerging issues using the models * Developing & evaluating Machine Learning models * Developed different visualizations using advanced features and deep analytics in Tableau * Used algorithms and programming to efficiently go through large datasets and apply treatments, filters, and conditions as needed  * Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through Reports, 100% stacked bar charts etc. in Tableau Desktop * Involved in publishing, scheduling and subscriptions with Tableau Server andcreating and managing users, groups, sites in Tableau Server. * Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively * Tested dashboards to ensure data was matching as per the business requirements and if there were any changes in underlying data  Environment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.  Client: Enbridge,Houston,TX .		                                                                                                 May 2017- Jul2018 Role: Data Scientist  Description: Enbridge Inc. is a Canadian multinational energy transportation company based in Calgary, Alberta. It focuses on the transportation, distribution and generation of energy, primarily in North America. Responsibilities: * A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop. * Installed and used CaffeDeepLearningFrame * ed on different data formats such as JSON, XML and performed machinelearningalgorithms in Python. * Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation, visualization and performed Gapanalysis.  * Developing Voice Bot  using AI (IVR ), improving the interaction between Human and the Virtual Assistant . * Implemented Event Task for execute Application Automatically.  * Involved in developing Patches & Updates Module.  * Setup storage and dataanalysis  in AmazonWebServices cloud computing infrastructure.  * Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing various machinelearningalgorithms.  * Development and Deployment using Google Dialogflow Enterprise. * ed as Data Architects and IT Architects to understand the movement of data and its storage and ERStudio9.7  * Data visualizationusingElasticsearch ,Kibana and Logstash in python. * Used Kibana an open source plugin for Elasticsearch  in analytics and Data visualization. * DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView. * ed on backing up and restoring the Azure Data Factory. * Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using deep learning frames. * Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model. * Implemented Agile Methodology for building an internal application. * Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.  * Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics  * Broad knowledge of programming, and scripting (especially in R / Java / Python)  * Developing and maintaining Data Dictionary to create metadata reports for  and business purpose.  * Predictive modeling using state-of-the-art methods  * Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoop on AWS.  * Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.  * Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.  * Proven experience building sustainable and trustful relationships with senior leaders * Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.  * Development level experience in Microsoft Azure providing data movement and scheduling functionality to cloud-based  such as Azure Blob Storage and Azure SQL Database. * Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and data representation of the analysis and suggested solutions for investors  * Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization. These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.  * Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for client business data management.  * Extracted data from HDFS and prepared data for exploratory analysis using datamunging  Environment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM,  GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata,  OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.  Client: DAK Americas LLC,Moncks Corner,SC.					                       Jan 2016- Apr 2017 Role:DataAnalyst/Data Scientist  Description: DAK Americas LLC manufactures and supplies polyethylene terephthalate resins (PET), polyesters staple fibers (PSF), monomers/ingredients (TPA/PTA), and specialty polymers for customers in the United States and internationally.  Responsibilities: * Assisting business by being able to deliver a machine learning project from beginning to end, aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization * Created data modeling and data mapping document containing source, formulate transformational rules to populate target fields. * Performed Data Cleaning, features scaling, features engineering using pandas and numpy packages in python and build models using SAP Predictive Analytics * Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift, oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns. * Created impact & gap analysis documents specifying changes introduced as part of the program and lead the business process team *  with big data consultants to analyze, extract, normalize and label relevant data using Statistical modeling techniques like Logistic regression, decision trees, Support vector machine, Random forest, Naive Bayes and neural nets * Developed ETLs for data sources used in production reporting for marketing and operations teams. * Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables * Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billion acquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff. * Review business data for trends, patterns or casual analysis to assist in identifying model drift and retraining models  * Created customized reports and processes in SAS and Tableau Desktop * Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria, conditions, business rules and data elements to be included into the report * Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teams to implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers * Performed SQL query for data analysis and integration * Support PMO governance activities; defining and maintaining Project Management standards.  * Responsible for generating ideas for product changes that improve key metrics * Provided data analytics of the web-portal to the team for feedback and improvement.   Environment:Python, HTML5, CSS3, AJAX,Teradata,  OLTP, random forest, OLAP, HDFS, ODS,  JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib,Hadoop, Pig Scripts.  Client:Emblem Health. Newyor,NY.							May 2014- Dec2015 Role: Data Analyst/Data Modeler  Description: Emblem Health is one of the United States' largest non-profit health plans. It is headquartered at 55 Water Street in Lower Manhattan, New York City. It is a $10 billion company with 3.1 million members.  Responsibilities: * Involved in defining the source to target data mappings, business rules, data definitions.  * Involved in defining the business/transformation rules applied for sales and service data.  * ed with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.  * Define the list codes and code conversions between the source systems and the data mart.  * Coordinate with the business users in providing appropriate, effective and efficient way to design the new reporting needs based on the user with the existing functionality.  * ed with BTEQ to submit SQL statements, import and export data, and generate reports in Teradata.  * Responsible for defining the key identifiers for each mapping/interface.  * Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, Linear Regression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency. * Responsible for defining the key identifiers for each mapping/interface.  * Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures, Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans  * Performed data quality in TalendOpenStudio.  * Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.  * Responsible for defining the functional requirement documents for each source to target interface.  * Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.  * Document the complete process flow to describe program development, logic, testing, and implementation, application integration, coding.  * Enterprise Metadata Library with any changes or updates.  * Generate weekly and monthly asset inventory reports.  Environment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.  Client: GD Research centre Hyderabad,India.			                                              Dec2012- Apr2014 Role: Data Analyst  Description: GD Research Center is a leading provider of global business intelligence including competitor, market, product, and customer information. It provides in-depth research, analysis, data and forecasts through a range of interactive online databases.  Responsibilities: * Analyze business information requirements and model class diagrams and/or conceptual domain models.  * Managed the project requirements, documents and use cases by IBM Rational RequisitePro.  * Assisted in building an Integrated LogicalDataDesign, propose physical database design for building the data mart.  * Gather & Review Customer Information Requirements for OLAP and building the data mart.  * Responsible for defining the key identifiers for each mapping/interface  * Responsible for defining the functional requirement documents for each source to target interface.  * Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.  * Enterprise Metadata Library with any changes or updates.  * Document data quality and traceability documents for each source interface.  * Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.  * Analyzed business process flows and assisted in the development of ETL procedures for mapping data from source to target systems.  * ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.  * Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.  * Establish standards of procedures.  * Generate weekly and monthly asset inventory reports.  * Document all data mapping and transformation processes in the Functional Design documents based on the business requirements  Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.  Client: ABP Engitech solutions PVT Ltd,Hyderabad				               Feb 2011- Nov 2012 Role: Data Analyst  Description: ABP is a global Engineering company firm that provides total engineering solutions for a variety of types and sized jobs to a substantial and diversified client base that includes Oil and Gas, Steel, Power plants, Rail, and Aerospace industries.The firm is in Hyderabad, India and maintains offices in UK, Kuwait,and Jamshedpur around the world.  Responsibilities: * Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.  * Developed logical and Physical data models using Erwin to design OLTP system for different applications.  * Facilitated transition of logical data models into the physical database design and recommended  approaches for good data management practices.  * ed with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Model using Forward engineering.  * ed with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.  * Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW). * Performed K-meansclustering, Multivariate analysis, and Support Vector Machines in R.  * Extensive system study, design, development and testing were carried out in the Oracle environment to meet the customer requirements.  * Written complex Hive and SQL queries for data analysis to meet business requirements.  * Written complex SQL queries for implementing business requirements   Environment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access."
" Above 8+ years of experience in large datasets of Structured and Unstructured data, DataVisualization , Data Acquisition, Predictive modeling, NLP/NLU/NLG/AI/machine learning/Computer vision/Probabilistic Graphical Models/Inferential statistics/Graph,Data Validation. 
 Hands on experience indata mining algorithms and approach. 
 Good at algorithm and design techniques 
 Expert level understanding in ApplicationDesign, Development and testing in
Mainframeenvironmentsusing PL/1, COBOL, EGL, Easytrieve, DB2, JCL, QC &VAG. 
 Experience in developing different Statistical Machine Learning, Text Analytics, Data Mining solutionsto various business generating and problems data visualizations using Python, R and Tableau.  Expertise in transforming business requirements into building models, designing algorithms, developing data mining and reporting solutions that scales across massive volume of unstructured data and structured. 
 Regressionanalysis, Statisticaltest analysis, Report and Dashboard generation, Datamanagement. 
 Git, Java, MySQL, MongoDB, Neo4J, AngularJS, SPSS, Tableau. 
 Python, Numpy, Scikit-Learn, genism, NLTK, Tensorflow, keras. 
 Experience in MachineLearning, Statistics, Regression- Linear, Logistic, Poisson, Binomial. 
 Experience in designing visualizations using Tableau software and Storyline on web and desktopplatforms, publishing and presenting dashboards. 
 Single handed built a model to replace the job of doer in the pension sector. This model (Patentunder progress) generates experience from structured data and learns through a bootstrapping mechanism new experience from unseen data. 
 Single handed Built and designed a whole Information extraction botPOC for KYC extraction. Thisbot is using adaptive learning techniques and uses some custom supervised classifiers for entity and relation extraction 
 Proficient in Machine Learning techniques (Decision Trees, Linear, Logistics, Random Forest, SVM,
Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics,
Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles. 
 Hands on experience in implementing LDA, Naive Bayes and skilled in Decision Trees, Random
Forests, Linear and Logistic Regression, SVM, Clustering, neural nets and good knowledge on
Recommender Systems. 
 Develop Azure POC for Retailer Data Research tool. Develop and Data Factory Micro-service datamovement from Azure SQL Server to Cosmos NoSQL Document database. 
 Experience on advanced SASprogramming techniques, such as PROC APPEND, PROC DATASETS, and
PROC TRANSPOSE. 
 Highly skilled in using visualization  like Tableau, Matplotlib for creating dashboards. 
 Extensive ing experience with Python including Scikit-learn, Pandas and Numpy. 
 Developed data variation analysis and data pair association analysis in the Bioinformatics field. 
 Regularly accessing JIRA tool and other internal issue trackers for the Project development. 
 Having good domain knowledge on Retail and Airlines. 
 Experience in foundational machine learning models and concepts( Regression, boosting, GBM, NNs,
HMMs, CRFs, MRFs, deep learning). 
 Well experienced in Normalization&De-Normalization techniques for optimum performance inrelational and dimensional database environments. 
 Analyzed data using R, Perl, Hadoop and queried data using structured and unstructured databases 
 Strong programming expertise Python and strong in Database SQL. 
 Integration Architect & Data Scientist experience in Analytics, Big Data, SOA, ETL and Cloud. 
 ed and extracted data from various database sources like Oracle, SQL Server and Teradata.  Skilled in System Analysis, Dimensional Data Modeling, Database Design and implementing RDBMS specific features. 
 Facilitated and helped translate complex quantitative methods into simplified solutions for users.  Knowledge of ing with Proof of Concepts and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging. 
 Solid coding and engineering  in Machine Learning 
 Experience with file systems, server architectures, databases, SQL, and data movement (ETL). 
 Proficient in Python, experience building, and product ionizing end-to-end systems 
 Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning
Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Data Scientist
State of MA - Boston, MA
August 2018 to Present
Description: Massachusetts, officially the Commonwealth of Massachusetts, is the most populous state in the New England region of the northeastern United States. It borders on the Atlantic Ocean to the east, the states of Connecticut and Rhode Island to the south, New Hampshire and Vermont to the north, and New York to the west. 
 
Responsibilities: 
 Involved in defining the source to target data mappings, business rules, and data definitions.  Performing data profiling on various source systems that are required for transferring data to ECH using 
 Defining the list codes and code conversions between the source systems and the data mart using
Reference Data Management (RDM). 
 Involved in data collection and induction to Teradata 
 Conducted data cleaning, data preparation, and outlier detection 
 Finding insights from millions of customer chat and calls records 
 Gathering requirements from business. 
 Involved in creating pipelines that move, transform, and analyze data from a wide variety of sourcesusing multiple methods like the Azure Power shell utility that allows control of many other Azure resources,Python, a REST API, and the Azure Portal UI. 
 Reviewing business requirements and analyzing data sources 
 Developed predictive models for sales and Finance teams using various ML and DL algorithms 
 Utilizing Informatica et (InformaticaData Explorer, and Informatica Data Quality) to analyzelegacy data for Data Profiling. 
 Created a Handler function in Python using AWS Lambda that can invoke when the service isexecuted. 
 Implemented statistical modeling with XGBoost machine learning software package using Python todetermine the predicted probabilities of each model. 
 ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005
 
 Involved in upgrading DTS packages to SSIS packages (ETL). 
 Involved in Training and Testing the ML Supervised and Unsupervised models 
 Researching on Deep Learning to implement NLP 
 Presented to the higher management the discovered trends and analysis, forecast data,recommendations, model results and risks identified. 
 Design and Develop Data Warehouse for Tax Information Factory using Oracle PL/SQL. 
 Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL
Queries on the source database and comparing the results against the target database. 
 Using HP Quality Center v 11 for defect tracking of issues. 
 Involved in applying data mining techniques and optimization techniques in B2B and B2C industriesand proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling. 
 Created and presented executive dashboards to show the patterns & trends in the data usingTableau Desktop 
 Developed NLP models for Topic extraction, Sentiment Analysis 
 Developed Executive  KPI, Key value programs, NPI dashboards in Tableau 
 Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reportsand dashboards. 
 Experience in Analysis, Design, Development, Implementation, Testing and Support of Data
Warehousing and Data Integration Solutions using Informatica Power center. 
 Was able to identify emerging issues using the models 
 Developing & evaluating Machine Learning models 
 Developed different visualizations using advanced features and deep analytics in Tableau 
 Used algorithms and programming to efficiently go through large datasets and apply treatments,filters, and conditions as needed 
 Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through
Reports, 100% stacked bar charts etc. in Tableau Desktop 
 Involved in publishing, scheduling and subscriptions with Tableau Server andcreating and managingusers, groups, sites in Tableau Server. 
 Involved in developing and testing the SQL Scripts for report development, Tableau reports,
Dashboards and handled the performance issues effectively 
 Tested dashboards to ensure data was matching as per the business requirements and if there wereany changes in underlying data 
 
Environment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management.
Data Scientist
Enbridge,Houston,TX - Houston, TX
May 2017 to July 2018
Description: Enbridge Inc. is a Canadian multinational energy transportation company based in
Calgary, Alberta. It focuses on the transportation, distribution and generation of energy, primarily in
North America. 
Responsibilities: 
 A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping,
MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop. 
 Installed and used CaffeDeepLearningFrame 
 ed on different data formats such as JSON, XML and performed machinelearningalgorithms inPython. 
 Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation,visualization and performed Gapanalysis. 
 Developing Voice Bot using AI (IVR ), improving the interaction between Human and the VirtualAssistant 
 Implemented Event Task for execute Application Automatically. 
 Involved in developing Patches & Updates Module. 
 Setup storage and dataanalysis  in AmazonWebServices cloud computing infrastructure. 
 Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing variousmachinelearningalgorithms. 
 Development and Deployment using Google Dialogflow Enterprise. 
 ed as Data Architects and IT Architects to understand the movement of data and its storage andERStudio9.7 
 Data visualizationusingElasticsearch ,Kibana and Logstash in python. 
 Used Kibana an open source plugin for Elasticsearch in analytics and Data visualization. 
 DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects,
PowerBI and SmartView. 
 ed on backing up and restoring the Azure Data Factory. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python and build models using deep learning frames. 
 Implemented application of various machine learning algorithms and statistical modeling like
Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear
Regression using Python to determine the accuracy rate of each model. 
 Implemented Agile Methodology for building an internal application. 
 Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.  Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics 
 Broad knowledge of programming, and scripting (especially in R / Java / Python) 
 Developing and maintaining Data Dictionary to create metadata reports for technical and businesspurpose. 
 Predictive modeling using state-of-the-art methods 
 Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoopon AWS. 
 Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.  Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators. 
 Proven experience building sustainable and trustful relationships with senior leaders 
 Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of
Identity Systems. 
 Development level experience in Microsoft Azure providing data movement and schedulingfunctionality to cloud-based  such as Azure Blob Storage and Azure SQL Database. 
 Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and datarepresentation of the analysis and suggested solutions for investors 
 Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis. 
 Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for clientbusiness data management. 
 Extracted data from HDFS and prepared data for exploratory analysis using datamunging 
 
Environment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow.
DataAnalyst/Data Scientist
DAK Americas LLC - Moncks Corner, SC
January 2016 to April 2017
Description: DAK Americas LLC manufactures and supplies polyethylene terephthalate resins (PET), polyesters staple fibers (PSF), monomers/ingredients (TPA/PTA), and specialty polymers for customers in the United States and internationally. 
 
Responsibilities: 
 Assisting business by being able to deliver a machine learning project from beginning to end,aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization 
 Created data modeling and data mapping document containing source, formulate transformationalrules to populate target fields. 
 Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python and build models using SAP Predictive Analytics 
 Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift,oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns. 
 Created impact & gap analysis documents specifying changes introduced as part of the program andlead the business process team 
  with big data consultants to analyze, extract, normalize and label relevant data using Statisticalmodeling techniques like Logistic regression, decision trees, Support vector machine, Random forest,
Naive Bayes and neural nets 
 Developed ETLs for data sources used in production reporting for marketing and operations teams.  Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables 
 Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billionacquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff. 
 Review business data for trends, patterns or casual analysis to assist in identifying model drift andretraining models 
 Created customized reports and processes in SAS and Tableau Desktop 
 Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria,conditions, business rules and data elements to be included into the report 
 Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teamsto implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers 
 Performed SQL query for data analysis and integration 
 Support PMO governance activities; defining and maintaining Project Management standards. 
 Responsible for generating ideas for product changes that improve key metrics 
 Provided data analytics of the web-portal to the team for feedback and improvement. 
 
Environment:Python, HTML5, CSS3, AJAX,Teradata, OLTP, random forest, OLAP, HDFS, ODS, JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib,Hadoop, Pig Scripts.
Data Analyst/Data Modeler
Emblem Health. Newyor,NY
May 2014 to December 2015
Description: Emblem Health is one of the United States' largest non-profit health plans. It is headquartered at 55 Water Street in Lower Manhattan, New York City. It is a $10 billion company with 3.1 million members. 
 
Responsibilities: 
 Involved in defining the source to target data mappings, business rules, data definitions. 
 Involved in defining the business/transformation rules applied for sales and service data. 
 ed with project team representatives to ensure that logical and physical ER/Studio data modelswere developed in line with corporate standards and guidelines. 
 Define the list codes and code conversions between the source systems and the data mart. 
 Coordinate with the business users in providing appropriate, effective and efficient way to design thenew reporting needs based on the user with the existing functionality. 
 ed with BTEQ to submit SQL statements, import and export data, and generate reports inTeradata. 
 Responsible for defining the key identifiers for each mapping/interface. 
 Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, LinearRegression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency. 
 Responsible for defining the key identifiers for each mapping/interface. 
 Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures,Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans 
 Performed data quality in TalendOpenStudio. 
 Coordinated meetings with vendors to define requirements and system interaction agreementdocumentation between client and vendor system. 
 Responsible for defining the functional requirement documents for each source to target interface.  Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements. 
 Document the complete process flow to describe program development, logic, testing, andimplementation, application integration, coding. 
 Enterprise Metadata Library with any changes or updates. 
 Generate weekly and monthly asset inventory reports. 
 
Environment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica.
Data Analyst
GD Research centre - Hyderabad, Telangana
December 2012 to April 2014
Description: GD Research Center is a leading provider of global business intelligence including competitor, market, product, and customer information. It provides in-depth research, analysis, data and forecasts through a range of interactive online databases. 
 
Responsibilities: 
 Analyze business information requirements and model class diagrams and/or conceptual domainmodels. 
 Managed the project requirements, documents and use cases by IBM Rational RequisitePro. 
 Assisted in building an Integrated LogicalDataDesign, propose physical database design for buildingthe data mart. 
 Gather & Review Customer Information Requirements for OLAP and building the data mart. 
 Responsible for defining the key identifiers for each mapping/interface 
 Responsible for defining the functional requirement documents for each source to target interface.  Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system. 
 Enterprise Metadata Library with any changes or updates. 
 Document data quality and traceability documents for each source interface. 
 Performed document analysis involving creation of Use Cases and Use Case narrations using
Microsoft Visio, in order to present the efficiency of the gathered requirements. 
 Analyzed business process flows and assisted in the development of ETL procedures formapping data from source to target systems. 
 ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data. 
 Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using
Microsoft Access and Oracle SQL. 
 Establish standards of procedures. 
 Generate weekly and monthly asset inventory reports. 
 Document all data mapping and transformation processes in the Functional Design documents basedon the business requirements 
 
Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer.
Data Analyst
ABP Engitech solutions PVT Ltd - Hyderabad, Telangana
February 2011 to November 2012
Description: ABP is a global Engineering company firm that provides total engineering solutions for a variety of types and sized jobs to a substantial and diversified client base that includes Oil and Gas, Steel, Power plants, Rail, and Aerospace industries.The firm is in Hyderabad, India and maintains offices in UK, Kuwait,and Jamshedpur around the world. 
 
Responsibilities: 
 Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.  Developed logical and Physical data models using Erwin to design OLTP system for different applications. 
 Facilitated transition of logical data models into the physical database design and recommendedtechnical approaches for good data management practices. 
 ed with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Modelusing Forward engineering. 
 ed with the ETL team to document the transformation rules for data migration from OLTP to
Warehouse environment for reporting purposes. 
 Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW). 
 Performed K-meansclustering, Multivariate analysis, and Support Vector Machines in R. 
 Extensive system study, design, development and testing were carried out in the Oracle environmentto meet the customer requirements. 
 Written complex Hive and SQL queries for data analysis to meet business requirements.  Written complex SQL queries for implementing business requirements 
 
Environment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access.
Education

Bachelor's


Data analysis, Ddl, Sql server, Postgres, Sql, Mapreduce, Bayesian, Clustering, Etl, Hadoop, Machine learning, Teradata, Sas, Tableau, Hadoop, Hive, Bootstrap, Mapreduce, Pig, Python, Business Intelligence
Additional Information

TECHNICAL  
 
Statistics/ML 
Exploratory Data Analysis: Univariate/MultivariateOutlier detection, Missing value imputation,
Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble
Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning 
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means,
Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and
Low Rank Matrix Factorization 
Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods,
Wrapper Methods and Embedded Methods 
Statistical Tests: T Test, Chi-Square tests, Stationarity tests,Auto Correlation tests, Normality tests,
Residual diagnostics, Partial dependence plots and Anova 
Sampling Methods: Bootstrap sampling methods and Stratified sampling 
Model Tuning/Selection: Cross Validation, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization 
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series 
 
Machine Learning / 
Deep Learning 
 
R:caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot 
Python:pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib,tensorflow 
SAS: Forecast server, SAS Procedures and Data Steps 
Spark: MLlib, GraphX 
SQL:Subqueries, joins, DDL/DML statements 
 
Databases/ETL/Query Teradata, SQL Server, Redshift, Postgres and Hadoop (MapReduce); SQL, Hive,
Azure Data Factory ,Pig and Alteryx. 
Visualization Tableau, ggplot2 and RShiny 
Prototyping PowerPoint,RShiny and Tableau",Data Scientist,resume," Above 8+ years of experience in large datasets of Structured and Unstructured data, DataVisualization , Data Acquisition, Predictive modeling, NLP/NLU/NLG/AI/machine learning/Computer vision/Probabilistic Graphical Models/Inferential statistics/Graph,Data Validation.   Hands on experience indata mining algorithms and approach.   Good at algorithm and design techniques   Expert level understanding in ApplicationDesign, Development and testing in Mainframeenvironmentsusing PL/1, COBOL, EGL, Easytrieve, DB2, JCL, QC &VAG.   Experience in developing different Statistical Machine Learning, Text Analytics, Data Mining solutionsto various business generating and problems data visualizations using Python, R and Tableau.  Expertise in transforming business requirements into building models, designing algorithms, developing data mining and reporting solutions that scales across massive volume of unstructured data and structured.   Regressionanalysis, Statisticaltest analysis, Report and Dashboard generation, Datamanagement.   Git, Java, MySQL, MongoDB, Neo4J, AngularJS, SPSS, Tableau.   Python, Numpy, Scikit-Learn, genism, NLTK, Tensorflow, keras.   Experience in MachineLearning, Statistics, Regression- Linear, Logistic, Poisson, Binomial.   Experience in designing visualizations using Tableau software and Storyline on web and desktopplatforms, publishing and presenting dashboards.   Single handed built a model to replace the job of doer in the pension sector. This model (Patentunder progress) generates experience from structured data and learns through a bootstrapping mechanism new experience from unseen data.   Single handed Built and designed a whole Information extraction botPOC for KYC extraction. Thisbot is using adaptive learning techniques and uses some custom supervised classifiers for entity and relation extraction   Proficient in Machine Learning techniques (Decision Trees, Linear, Logistics, Random Forest, SVM, Bayesian, XG Boost, K-Nearest Neighbors) and Statistical Modeling in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, Factor analysis/ PCA, Ensembles.   Hands on experience in implementing LDA, Naive Bayes and skilled in Decision Trees, Random Forests, Linear and Logistic Regression, SVM, Clustering, neural nets and good knowledge on Recommender Systems.   Develop Azure POC for Retailer Data Research tool. Develop and Data Factory Micro-service datamovement from Azure SQL Server to Cosmos NoSQL Document database.   Experience on advanced SASprogramming techniques, such as PROC APPEND, PROC DATASETS, and PROC TRANSPOSE.   Highly skilled in using visualization  like Tableau, Matplotlib for creating dashboards.   Extensive ing experience with Python including Scikit-learn, Pandas and Numpy.   Developed data variation analysis and data pair association analysis in the Bioinformatics field.   Regularly accessing JIRA tool and other internal issue trackers for the Project development.   Having good domain knowledge on Retail and Airlines.   Experience in foundational machine learning models and concepts( Regression, boosting, GBM, NNs, HMMs, CRFs, MRFs, deep learning).   Well experienced in Normalization&De-Normalization techniques for optimum performance inrelational and dimensional database environments.   Analyzed data using R, Perl, Hadoop and queried data using structured and unstructured databases   Strong programming expertise Python and strong in Database SQL.   Integration Architect & Data Scientist experience in Analytics, Big Data, SOA, ETL and Cloud.   ed and extracted data from various database sources like Oracle, SQL Server and Teradata.  Skilled in System Analysis, Dimensional Data Modeling, Database Design and implementing RDBMS specific features.   Facilitated and helped translate complex quantitative methods into simplified solutions for users.  Knowledge of ing with Proof of Concepts and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data munging.   Solid coding and engineering  in Machine Learning   Experience with file systems, server architectures, databases, SQL, and data movement (ETL).   Proficient in Python, experience building, and product ionizing end-to-end systems   Knowledge of Information Extraction, NLP algorithms coupled with Deep Learning Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Data Scientist State of MA - Boston, MA August 2018 to Present Description: Massachusetts, officially the Commonwealth of Massachusetts, is the most populous state in the New England region of the northeastern United States. It borders on the Atlantic Ocean to the east, the states of Connecticut and Rhode Island to the south, New Hampshire and Vermont to the north, and New York to the west.    Responsibilities:   Involved in defining the source to target data mappings, business rules, and data definitions.  Performing data profiling on various source systems that are required for transferring data to ECH using   Defining the list codes and code conversions between the source systems and the data mart using Reference Data Management (RDM).   Involved in data collection and induction to Teradata   Conducted data cleaning, data preparation, and outlier detection   Finding insights from millions of customer chat and calls records   Gathering requirements from business.   Involved in creating pipelines that move, transform, and analyze data from a wide variety of sourcesusing multiple methods like the Azure Power shell utility that allows control of many other Azure resources,Python, a REST API, and the Azure Portal UI.   Reviewing business requirements and analyzing data sources   Developed predictive models for sales and Finance teams using various ML and DL algorithms   Utilizing Informatica et (InformaticaData Explorer, and Informatica Data Quality) to analyzelegacy data for Data Profiling.   Created a Handler function in Python using AWS Lambda that can invoke when the service isexecuted.   Implemented statistical modeling with XGBoost machine learning software package using Python todetermine the predicted probabilities of each model.   ed on DTS Packages, DTS Import/Export for transferring data between SQL Server 2000 to 2005    Involved in upgrading DTS packages to SSIS packages (ETL).   Involved in Training and Testing the ML Supervised and Unsupervised models   Researching on Deep Learning to implement NLP   Presented to the higher management the discovered trends and analysis, forecast data,recommendations, model results and risks identified.   Design and Develop Data Warehouse for Tax Information Factory using Oracle PL/SQL.   Performing an end to end InformaticaETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.   Using HP Quality Center v 11 for defect tracking of issues.   Involved in applying data mining techniques and optimization techniques in B2B and B2C industriesand proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.   Created and presented executive dashboards to show the patterns & trends in the data usingTableau Desktop   Developed NLP models for Topic extraction, Sentiment Analysis   Developed Executive  KPI, Key value programs, NPI dashboards in Tableau   Created customized Calculations, Conditions and Filters (Local, Global) for various analytical reportsand dashboards.   Experience in Analysis, Design, Development, Implementation, Testing and Support of Data Warehousing and Data Integration Solutions using Informatica Power center.   Was able to identify emerging issues using the models   Developing & evaluating Machine Learning models   Developed different visualizations using advanced features and deep analytics in Tableau   Used algorithms and programming to efficiently go through large datasets and apply treatments,filters, and conditions as needed   Developed Cross Tab, Chart, Funnel charts, Donut charts, Heat Maps, Tree Maps and Drill Through Reports, 100% stacked bar charts etc. in Tableau Desktop   Involved in publishing, scheduling and subscriptions with Tableau Server andcreating and managingusers, groups, sites in Tableau Server.   Involved in developing and testing the SQL Scripts for report development, Tableau reports, Dashboards and handled the performance issues effectively   Tested dashboards to ensure data was matching as per the business requirements and if there wereany changes in underlying data    Environment: Data Governance, SQL Server, ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, ETL, MS Office Suite - Excel(Pivot, VLOOKUP), DB2, R, Python, Visio, HP ALM, Agile, Azure, Data Quality, Tableau and Reference Data Management. Data Scientist Enbridge,Houston,TX - Houston, TX May 2017 to July 2018 Description: Enbridge Inc. is a Canadian multinational energy transportation company based in Calgary, Alberta. It focuses on the transportation, distribution and generation of energy, primarily in North America.  Responsibilities:   A highly immersive DataScience program involving DataManipulation&Visualization, Web Scraping, MachineLearning, Python programming, SQL, GIT, Unix Commands, NoSQL, MongoDB, Hadoop.   Installed and used CaffeDeepLearningFrame   ed on different data formats such as JSON, XML and performed machinelearningalgorithms inPython.   Participated in all phases of datamining; datacollection, datacleaning, developingmodels, validation,visualization and performed Gapanalysis.   Developing Voice Bot using AI (IVR ), improving the interaction between Human and the VirtualAssistant   Implemented Event Task for execute Application Automatically.   Involved in developing Patches & Updates Module.   Setup storage and dataanalysis  in AmazonWebServices cloud computing infrastructure.   Used pandas, numpy, seaborn, scipy, matplotlib, scikit-learn, NLTK in Python for developing variousmachinelearningalgorithms.   Development and Deployment using Google Dialogflow Enterprise.   ed as Data Architects and IT Architects to understand the movement of data and its storage andERStudio9.7   Data visualizationusingElasticsearch ,Kibana and Logstash in python.   Used Kibana an open source plugin for Elasticsearch in analytics and Data visualization.   DataManipulation and Aggregation from different source using Nexus, Toad, BusinessObjects, PowerBI and SmartView.   ed on backing up and restoring the Azure Data Factory.   Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python and build models using deep learning frames.   Implemented application of various machine learning algorithms and statistical modeling like Decision Tree, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python to determine the accuracy rate of each model.   Implemented Agile Methodology for building an internal application.   Extracting the source data from Oracle tables, MS SQL Server, sequential files and excel sheets.  Migrating Informatica mappings from SQL Server to Netezza Foster culture of continuous engineering improvement through mentoring, feedback, and metrics   Broad knowledge of programming, and scripting (especially in R / Java / Python)   Developing and maintaining Data Dictionary to create metadata reports for technical and businesspurpose.   Predictive modeling using state-of-the-art methods   Developed MapReduce/Spark Python modules for machine learning & predictive analytics in Hadoopon AWS.   Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.  Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.   Proven experience building sustainable and trustful relationships with senior leaders   Focus on integration overlap and Informatica newer commitment to MDM with the acquisition of Identity Systems.   Development level experience in Microsoft Azure providing data movement and schedulingfunctionality to cloud-based  such as Azure Blob Storage and Azure SQL Database.   Data analysis using regressions, data cleaning, excel v-look up, histograms and TOAD client and datarepresentation of the analysis and suggested solutions for investors   Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.These models are then implemented in SAS where they are interfaced with MSSQL databases and scheduled to update on a timely basis.   Attained good knowledge in Hadoop Data Lake Implementation and HADOOP Architecture for clientbusiness data management.   Extracted data from HDFS and prepared data for exploratory analysis using datamunging    Environment:ER Studio 9.7, Tableau 9.03, AWS, Teradata 15, MDM, GIT, Unix, Python 3.5.2, , MLLib, SAS, regression, logistic regression, Hadoop, NoSQL, Teradata, OLTP, random forest, OLAP, HDFS, ODS, NLTK, SVM, JSON, XML, MapReduce, Google Dialog Flow. DataAnalyst/Data Scientist DAK Americas LLC - Moncks Corner, SC January 2016 to April 2017 Description: DAK Americas LLC manufactures and supplies polyethylene terephthalate resins (PET), polyesters staple fibers (PSF), monomers/ingredients (TPA/PTA), and specialty polymers for customers in the United States and internationally.    Responsibilities:   Assisting business by being able to deliver a machine learning project from beginning to end,aggregating and exploring data, building and validating predictive models and deploying completed models to deliver business impacts to the organization   Created data modeling and data mapping document containing source, formulate transformationalrules to populate target fields.   Performed Data Cleaning, features scaling, features engineering using pandas and numpy packagesin python and build models using SAP Predictive Analytics   Used R and python for Exploratory Data Analysis, A/B testing, HQL, VQL, Data Lake, AWS Redshift,oozie, pySpark, Anova test and Hypothesis test to compare and identify the effectiveness of Creative Campaigns.   Created impact & gap analysis documents specifying changes introduced as part of the program andlead the business process team    with big data consultants to analyze, extract, normalize and label relevant data using Statisticalmodeling techniques like Logistic regression, decision trees, Support vector machine, Random forest, Naive Bayes and neural nets   Developed ETLs for data sources used in production reporting for marketing and operations teams.  Write SQL queries to perform data analysis, data modeling and prepare data mapping documents to explain the transformation rules from source to target tables   Led the Change Management stream of an HR/Payroll project resulting from a $16.5 billionacquisition and the formation of UTAS, created Change Management Plan, and ensured team was on target to deliver both communication and training to HR, finance and Payroll staff.   Review business data for trends, patterns or casual analysis to assist in identifying model drift andretraining models   Created customized reports and processes in SAS and Tableau Desktop   Performed data analysis to create reporting requirements by specifying inclusion & exclusion criteria,conditions, business rules and data elements to be included into the report   Scheduled and facilitated requirements gathering with HR, Payroll, finance and accounting teamsto implement ADP eTime and ADP Enterprise v5 and ADP General Ledger and drove requirements for data collection and data modeling with data engineers   Performed SQL query for data analysis and integration   Support PMO governance activities; defining and maintaining Project Management standards.   Responsible for generating ideas for product changes that improve key metrics   Provided data analytics of the web-portal to the team for feedback and improvement.    Environment:Python, HTML5, CSS3, AJAX,Teradata, OLTP, random forest, OLAP, HDFS, ODS, JSON, jQuery, MySQL, NumPy, SQL Alchemy, Matplotlib,Hadoop, Pig Scripts. Data Analyst/Data Modeler Emblem Health. Newyor,NY May 2014 to December 2015 Description: Emblem Health is one of the United States' largest non-profit health plans. It is headquartered at 55 Water Street in Lower Manhattan, New York City. It is a $10 billion company with 3.1 million members.    Responsibilities:   Involved in defining the source to target data mappings, business rules, data definitions.   Involved in defining the business/transformation rules applied for sales and service data.   ed with project team representatives to ensure that logical and physical ER/Studio data modelswere developed in line with corporate standards and guidelines.   Define the list codes and code conversions between the source systems and the data mart.   Coordinate with the business users in providing appropriate, effective and efficient way to design thenew reporting needs based on the user with the existing functionality.   ed with BTEQ to submit SQL statements, import and export data, and generate reports inTeradata.   Responsible for defining the key identifiers for each mapping/interface.   Used Python, R, SQL to create Statistical algorithms involving Multivariate Regression, LinearRegression, Logistic Regression, PCA, Random forest models, Decision trees, Support Vector Machine for estimating the risks of welfare dependency.   Responsible for defining the key identifiers for each mapping/interface.   Implementation of Metadata Repository, Maintaining Data Quality, Data Cleanup procedures,Transformations, Data Standards, Data Governance program, Scripts, Stored Procedures, triggers and execution of test plans   Performed data quality in TalendOpenStudio.   Coordinated meetings with vendors to define requirements and system interaction agreementdocumentation between client and vendor system.   Responsible for defining the functional requirement documents for each source to target interface.  Remain knowledgeable in all areas of business operations in order to identify systems needs and requirements.   Document the complete process flow to describe program development, logic, testing, andimplementation, application integration, coding.   Enterprise Metadata Library with any changes or updates.   Generate weekly and monthly asset inventory reports.    Environment: Erwin r7.0, SQL Server 2012/2008, Windows XP/NT/2000, Oracle 10g/9i, MS-DTS, UML, UAT, SQL Loader, OOD, OLTP, PL/SQL, MS Visio, Informatica. Data Analyst GD Research centre - Hyderabad, Telangana December 2012 to April 2014 Description: GD Research Center is a leading provider of global business intelligence including competitor, market, product, and customer information. It provides in-depth research, analysis, data and forecasts through a range of interactive online databases.    Responsibilities:   Analyze business information requirements and model class diagrams and/or conceptual domainmodels.   Managed the project requirements, documents and use cases by IBM Rational RequisitePro.   Assisted in building an Integrated LogicalDataDesign, propose physical database design for buildingthe data mart.   Gather & Review Customer Information Requirements for OLAP and building the data mart.   Responsible for defining the key identifiers for each mapping/interface   Responsible for defining the functional requirement documents for each source to target interface.  Coordinated meetings with vendors to define requirements and system interaction agreement documentation between client and vendor system.   Enterprise Metadata Library with any changes or updates.   Document data quality and traceability documents for each source interface.   Performed document analysis involving creation of Use Cases and Use Case narrations using Microsoft Visio, in order to present the efficiency of the gathered requirements.   Analyzed business process flows and assisted in the development of ETL procedures formapping data from source to target systems.   ed with BTEQ to submit SQL statements, import and export data, and generate reports in Terra-data.   Calculated and analyzed claims data for provider incentive and supplemental benefit analysis using Microsoft Access and Oracle SQL.   Establish standards of procedures.   Generate weekly and monthly asset inventory reports.   Document all data mapping and transformation processes in the Functional Design documents basedon the business requirements    Environment: SQL Server 2008R2/2005 Enterprise, SSRS, SSIS, Crystal Reports, Windows Enterprise Server 2000, DTS, SQL Profiler, and Query Analyzer. Data Analyst ABP Engitech solutions PVT Ltd - Hyderabad, Telangana February 2011 to November 2012 Description: ABP is a global Engineering company firm that provides total engineering solutions for a variety of types and sized jobs to a substantial and diversified client base that includes Oil and Gas, Steel, Power plants, Rail, and Aerospace industries.The firm is in Hyderabad, India and maintains offices in UK, Kuwait,and Jamshedpur around the world.    Responsibilities:   Developed an Object modeling in UML for Conceptual Data Model using Enterprise Architect.  Developed logical and Physical data models using Erwin to design OLTP system for different applications.   Facilitated transition of logical data models into the physical database design and recommendedtechnical approaches for good data management practices.   ed with DBA group to create Best-Fit Physical Data Model with DDL from the Logical Data Modelusing Forward engineering.   ed with the ETL team to document the transformation rules for data migration from OLTP to Warehouse environment for reporting purposes.   Developed Data Migration and Cleansing rules for the Integration Architecture (OLTP, ODS, DW).   Performed K-meansclustering, Multivariate analysis, and Support Vector Machines in R.   Extensive system study, design, development and testing were carried out in the Oracle environmentto meet the customer requirements.   Written complex Hive and SQL queries for data analysis to meet business requirements.  Written complex SQL queries for implementing business requirements    Environment: DB2, Teradata, SQL-Server 2008, Enterprise Architect, Power Designer, MS SSAS, Crystal Reports, SSRS, ER Studio, Lotus Notes, Windows XP, MS Excel, word and Access. Education  Bachelor's   Data analysis, Ddl, Sql server, Postgres, Sql, Mapreduce, Bayesian, Clustering, Etl, Hadoop, Machine learning, Teradata, Sas, Tableau, Hadoop, Hive, Bootstrap, Mapreduce, Pig, Python, Business Intelligence Additional Information  TECHNICAL     Statistics/ML  Exploratory Data Analysis: Univariate/MultivariateOutlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau  Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning  Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization  Feature Selection: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods  Statistical Tests: T Test, Chi-Square tests, Stationarity tests,Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova  Sampling Methods: Bootstrap sampling methods and Stratified sampling  Model Tuning/Selection: Cross Validation, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization  Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series    Machine Learning /  Deep Learning    R:caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot  Python:pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib,tensorflow  SAS: Forecast server, SAS Procedures and Data Steps  Spark: MLlib, GraphX  SQL:Subqueries, joins, DDL/DML statements    Databases/ETL/Query Teradata, SQL Server, Redshift, Postgres and Hadoop (MapReduce); SQL, Hive, Azure Data Factory ,Pig and Alteryx.  Visualization Tableau, ggplot2 and RShiny  Prototyping PowerPoint,RShiny and Tableau"
" 
CALIFORNIA STATE UNIVERSTY, FULLERTON	
MASTER OF SCIENCE IN STATISTICS  GPA: 3.51
- Advance Theory in Probability and Statistics	- Spatial Statistics
- Statistical Computation		              - Statistical Consulting
- Applied Biostatistics	   	                             - Bayesian Analysis
- Categorical Data Analysis		              - Multivariate Analysis
- Statistical/ Machine Learning	                             - Experiential Design

CALIFORNIA STATE POLYTECHNIC UNIVERSITY, POMONA
BACHELOR OF SCIENCE IN APPLIED MATH/STA  GPA:3.14

Passed SOA/CAS for exam P
Passed SOA/CAS for exam FM
Candidate for SOA/CAS for exam IFM

STATISTICAL CONSULTANT PANASONIC AVIONICS CORP	ORATION
* Manage team of seven to develope model to predict price to maximize profit
 for Panasonic for their on-flight WIFI.
* Design machine learning algorithm in python to predict profit at difference 
price point for on fight WIFI.
* Utilize SQL to organize more than 4M rows and more than 50 features of data.
* Performed data mining, Data visualization
* Assigned proper values for missing values and incorrect values.
* Provide high level recommendation on price to maximize profit base on
 difference regions.
MATH QUALITY CONTROL SPECIALIST ANSRSOURCE
* Part of team to create brand new online home platform for McGraw-Hill.
* ed closely with programmers to ensure quality for each algorithm.
* Implemented operational  into existing algorithms.
* Fostered and facilitated communications form QC team in Irvine to QC team and programmers team in India.
STUDENT ASSISTANT IV Mount San Antonio college
* Analyze data on students success and learning gain
* Specialized as personal tutor on Algebra, Calculus, Statistics, physics.

Languages: Vietnamese, English
Computer Science: 
Advance: R, Python, Microsoft suite
Intermediate: SQL, EXCEL, C++, MATLAB
Data Science: Machine learning, Statistical Computation, Experimental Design, Biostatistics, Classification, Regression.

VIETNAMESE STUDENT ASSIOCIATION TREASURER
Organize events, talks and meeting.
Effectively managed yearly budget.

		




08/2017  08/2019







09/2013  06/2016



11/2015
08/2016
11/2019

01/2019-06/2019







11/2016-12/2017





03/2013 - 06/2016
09/2018  06/2019








	
08/2012  08/2013


",Data Scientist,resume,"  CALIFORNIA STATE UNIVERSTY, FULLERTON	 MASTER OF SCIENCE IN STATISTICS  GPA: 3.51 - Advance Theory in Probability and Statistics	- Spatial Statistics - Statistical Computation		              - Statistical Consulting - Applied Biostatistics	   	                             - Bayesian Analysis - Categorical Data Analysis		              - Multivariate Analysis - Statistical/ Machine Learning	                             - Experiential Design  CALIFORNIA STATE POLYTECHNIC UNIVERSITY, POMONA BACHELOR OF SCIENCE IN APPLIED MATH/STA  GPA:3.14  Passed SOA/CAS for exam P Passed SOA/CAS for exam FM Candidate for SOA/CAS for exam IFM  STATISTICAL CONSULTANT PANASONIC AVIONICS CORP	ORATION * Manage team of seven to develope model to predict price to maximize profit  for Panasonic for their on-flight WIFI. * Design machine learning algorithm in python to predict profit at difference  price point for on fight WIFI. * Utilize SQL to organize more than 4M rows and more than 50 features of data. * Performed data mining, Data visualization * Assigned proper values for missing values and incorrect values. * Provide high level recommendation on price to maximize profit base on  difference regions. MATH QUALITY CONTROL SPECIALIST ANSRSOURCE * Part of team to create brand new online home platform for McGraw-Hill. * ed closely with programmers to ensure quality for each algorithm. * Implemented operational  into existing algorithms. * Fostered and facilitated communications form QC team in Irvine to QC team and programmers team in India. STUDENT ASSISTANT IV Mount San Antonio college * Analyze data on students success and learning gain * Specialized as personal tutor on Algebra, Calculus, Statistics, physics.  Languages: Vietnamese, English Computer Science:  Advance: R, Python, Microsoft suite Intermediate: SQL, EXCEL, C++, MATLAB Data Science: Machine learning, Statistical Computation, Experimental Design, Biostatistics, Classification, Regression.  VIETNAMESE STUDENT ASSIOCIATION TREASURER Organize events, talks and meeting. Effectively managed yearly budget.  		     08/2017  08/2019        09/2013  06/2016    11/2015 08/2016 11/2019  01/2019-06/2019        11/2016-12/2017      03/2013 - 06/2016 09/2018  06/2019         	 08/2012  08/2013   "
"______________________________________________________________________________  
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
California State University, Fullerton	           ?Cumulative GPA: 3.09 
Bachelor of Science in Computer Science	         Expected Graduation Date: May 2020 
______________________________________________________________________________ 
RELEVANT COURSE 
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
? Algorithm Engineering 	? Introduction to Data Science 
? Data Structure 	? Software Dev w/ Open Source 
? File Structure & Database 	? Software Engineering 
______________________________________________________________________________   
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
 
	? Developer Language:? Python, R, C++, 	? Operating Systems: Windows 7, 8, 10,?	 
	C 	Linux, Mac 
	? Software: ?RStudio, PyCharms, Visual 	? Visualization Tool:? Excel, Tableau 
	Studio, MySQL 	 
______________________________________________________________________________ RELATED EXPERIENCE 
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
Bytes and Bots	     Irvine, CA 
Lead Mentor 	       June 2018 - August 2019 
? Collaborate with other mentors and teachers to develop a ing environment that keeps primary school students engaged which allowed a team to win first place at a district competition 
? Educate primary school students about coding to promote careers in   
? Set up and test station equipment such as ultrasonic sensors and servo motors to be used with Raspberry Pi  
Quantum Automation	 	 Anaheim,CA 
Technician                                                                                            ?August 2015 - January 2016 
? Assembled solar power battery shells together to be shipped and transported to other manufacturers  
? Cooperate with software developers and engineers to produce solar powered devices  
? Operated with terminal shell that ran scripts which installed operating files and established a connection to a neting server 
______________________________________________________________________________   
__________________________________________________________________________________________________________________________________________________________________________________________________________________________________________ 
 
College vs High School Graduates Life-Time Revenue                      September 2019 - Present?	 
? Wrangle out College Scorecard database to pick up financial statistics from 1996 to 2017 to compare the revenue difference of High School Graduates to College Graduates 
? Transform the College Scoreboard database to a clustered bar graph to represent the changes in college tuition with Tableau  
Highway and Social Media Data Analysis?                                                 August 2019 - Present 
? Filter out PeMS databases to extract out valuable information such as traffic speed and number of active vehicles on a freeway to be analyzed with Twitter's API that tracks tweet activities 
Battleship Game Development                                                           ?July 2018 - December 2018 
? Develop an AI system that keeps track of its previous moves in order to establish a move selection foundation  
? Use Python and PyGames to implement a GUI that allows for the user and AI to interact with the field ",Data Scientist,resume,"______________________________________________________________________________   __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________  California State University, Fullerton	           ?Cumulative GPA: 3.09  Bachelor of Science in Computer Science	         Expected Graduation Date: May 2020  ______________________________________________________________________________  RELEVANT COURSE  __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________  ? Algorithm Engineering 	? Introduction to Data Science  ? Data Structure 	? Software Dev w/ Open Source  ? File Structure & Database 	? Software Engineering  ______________________________________________________________________________    __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________    	? Developer Language:? Python, R, C++, 	? Operating Systems: Windows 7, 8, 10,?	  	C 	Linux, Mac  	? Software: ?RStudio, PyCharms, Visual 	? Visualization Tool:? Excel, Tableau  	Studio, MySQL 	  ______________________________________________________________________________ RELATED EXPERIENCE  __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________  Bytes and Bots	     Irvine, CA  Lead Mentor 	       June 2018 - August 2019  ? Collaborate with other mentors and teachers to develop a ing environment that keeps primary school students engaged which allowed a team to win first place at a district competition  ? Educate primary school students about coding to promote careers in    ? Set up and test station equipment such as ultrasonic sensors and servo motors to be used with Raspberry Pi   Quantum Automation	 	 Anaheim,CA  Technician                                                                                            ?August 2015 - January 2016  ? Assembled solar power battery shells together to be shipped and transported to other manufacturers   ? Cooperate with software developers and engineers to produce solar powered devices   ? Operated with terminal shell that ran scripts which installed operating files and established a connection to a neting server  ______________________________________________________________________________    __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________    College vs High School Graduates Life-Time Revenue                      September 2019 - Present?	  ? Wrangle out College Scorecard database to pick up financial statistics from 1996 to 2017 to compare the revenue difference of High School Graduates to College Graduates  ? Transform the College Scoreboard database to a clustered bar graph to represent the changes in college tuition with Tableau   Highway and Social Media Data Analysis?                                                 August 2019 - Present  ? Filter out PeMS databases to extract out valuable information such as traffic speed and number of active vehicles on a freeway to be analyzed with Twitter's API that tracks tweet activities  Battleship Game Development                                                           ?July 2018 - December 2018  ? Develop an AI system that keeps track of its previous moves in order to establish a move selection foundation   ? Use Python and PyGames to implement a GUI that allows for the user and AI to interact with the field "
"A position in data analytics where I can utilize my al background, more than 5 years' experience, communication , and analytical solving  to contribute to the organization
 Experience

Analytics mentor at Thinkful
Data Science
January 2019 to Present
 support students by acting as an advisor and counselor as they complete the course and land theirfirst industry job 
 Host online video sessions on topics of my expertise: python, sql, statistics, etc 
 Review student checkpoints submissions and deliver written feedback, including on  andportfolios 
 Meet with students 1-on-1 in an online video session to provide  and  support asthe student progresses through the curriculum.
Business Analyst
Wells Fargo - Charlotte, NC
August 2018 to February 2019
 Act as a liaison between client area and  organization by planning, conducting, and directingthe analysis of complex business problems to be solved with automated systems 
 Provide  assistance in identifying, evaluating, and developing systems and procedures thatare cost effective and meet business requirements 
  with user groups to provide training, resolve questions, assess user needs, and recommendchanges 
 Prepare specifications for system changes 
 Develop systems test plan components and test scripts
Senior Analyst
Nissan - Nashville, TN
October 2017 to April 2018
 Prepared monthly internal management reports 
 Reviewed quarterly and annual financial information prepared by Senior Analysts 
 provided cash flow analytics for the reporting unit 
 performed account analysis and statistical analysis on various products 
 implemented and administer accounting systems in a variety of functional areas to produceappropriate management reports 
 analyzed financial metrics in Microsoft Excel and SAP 
 created dashboards and models on securitized products for the FPA team
Accounting Analyst
Callidus Construction - Atlanta, GA August 2015 to June 2017
 Managed complex Excel books utilizing macros and VBA scripts 
 Experienced reconciling report data between systems 
 Mapped data between different systems & validate when needed 
 Developed ad hoc reporting as needed 
 Assisted in the planning of major financial statements
Procurement Financial Analyst
ALCOA - Atlanta, GA
September 2013 to April 2015
 ed as a member of the Corporate analytics team by conducting analyses on spend data,business intelligence, designing modeling templates and analyzing data 
 Studied the world economic supply markets related to industry manufacturing products and makepredictions about exchange rates on the present financial condition 
 Performed Data mining and cleansing for large amounts of data across all procurement categoriesand financial systems (oracle and jde) 
 Provided a critical level of support on general and special , inventory, financial analysis,problem resolution and other business/financial analysis related needs that affect Procurement  Designed and present analytical models and supporting documentation to senior management 
 Managed saving multiple schedules and track monthly savings for sourced initiatives 
 Queried relevant spend data using SQL
Database Analyst
MCKESSON CORPORATION - Alpharetta, GA
March 2012 to June 2013
 Responsible for the development and administration of analytical data constructs/structures 
 Translated information into meaningful analysis using SQL within MS ACCESS and MS SharePoint
(certified SharePoint administrator at McKesson) 
 Visualized and maintained current organizational metrics and force data 
 Managed, manipulated, and analyzed data in Microsoft Excel 
 Effectively communicated the analysis and conclusions
Budget Analyst
US FISH AND WILDLIFE - Atlanta, GA December 2010 to December 2011
NACI GOVERNMENT CLEARANCE] 
 Sampled documentation and spending variances to determine compliance 
 Built quantitative financial models for the budget and finance team using MS Excel 
 Helped with the transition of the overall FFS system to an ERP system functionality(FBMS)-driven SAPsolution 
 Revised payment documentation related to the Deepwater Horizon Oil spill 
 Created and administrated spending targets for 10 programs based on the revisions of the federalbudget object codes


Master of Science in Business Analytics in MS-BA
Arizona State University - Tempe, AZ May 2019
Bachelor of Business Administration in Finance
Kennesaw State University - Kennesaw, GA May 2011


Excel (4 years), MICROSOFT SHAREPOINT (1 year), MS SharePoint (1 year), SAP (1 year), sql (3 years)
Additional Information

Management & Leadership 
 Member of the I school initiative at Kennesaw State University 
 Participated in SIFE (Students in free enterprise) 
 Member of the University of Hawaii football team 
 Member of football and basketball teams at Allen High School in Texas 
 Excellent written and verbal and all- encompassing communication  
 Mentored elementary school children as part of the Reading With The Eagle's high school community
initiative 
 
  
 Proficient in computers from: MS Office Suite, MS Visio, MS SharePoint, SAP financials, Tableau,
EazyBi, PostgreSQL, MySQL, SQL, SPSS, Essbase, Quickbooks, Minitab, Linear programing, PYTHON,
Text mining, Hadoop, AWS and Internet 
 Machine Learning : MS Azure ML, Python data science stack (Numpy, Pandas, Scikit, Matplotlib,etc.), A/B hypothesis testing, feature engineering and selection, dimensionality reduction, linear regression and regression classification algorithms, Naïve bayes models, prescriptive analytics, and expert knowledge of most common machine learning algorithms (KNN, SVM, logistic regression, decision trees, ensemble models, clustering, etc.) 
 Extensive use of Excel, including complex formulas such as VLOOKUP, ""if"" statements, VBA, Solver,macros and formatting, Palisade's Stat, @RISK, PrecisionTree, pivot tables to manipulate and analyze data",Data Scientist,resume,"A position in data analytics where I can utilize my al background, more than 5 years' experience, communication , and analytical solving  to contribute to the organization  Experience  Analytics mentor at Thinkful Data Science January 2019 to Present  support students by acting as an advisor and counselor as they complete the course and land theirfirst industry job   Host online video sessions on topics of my expertise: python, sql, statistics, etc   Review student checkpoints submissions and deliver written feedback, including on  andportfolios   Meet with students 1-on-1 in an online video session to provide  and  support asthe student progresses through the curriculum. Business Analyst Wells Fargo - Charlotte, NC August 2018 to February 2019  Act as a liaison between client area and  organization by planning, conducting, and directingthe analysis of complex business problems to be solved with automated systems   Provide  assistance in identifying, evaluating, and developing systems and procedures thatare cost effective and meet business requirements    with user groups to provide training, resolve questions, assess user needs, and recommendchanges   Prepare specifications for system changes   Develop systems test plan components and test scripts Senior Analyst Nissan - Nashville, TN October 2017 to April 2018  Prepared monthly internal management reports   Reviewed quarterly and annual financial information prepared by Senior Analysts   provided cash flow analytics for the reporting unit   performed account analysis and statistical analysis on various products   implemented and administer accounting systems in a variety of functional areas to produceappropriate management reports   analyzed financial metrics in Microsoft Excel and SAP   created dashboards and models on securitized products for the FPA team Accounting Analyst Callidus Construction - Atlanta, GA August 2015 to June 2017  Managed complex Excel books utilizing macros and VBA scripts   Experienced reconciling report data between systems   Mapped data between different systems & validate when needed   Developed ad hoc reporting as needed   Assisted in the planning of major financial statements Procurement Financial Analyst ALCOA - Atlanta, GA September 2013 to April 2015  ed as a member of the Corporate analytics team by conducting analyses on spend data,business intelligence, designing modeling templates and analyzing data   Studied the world economic supply markets related to industry manufacturing products and makepredictions about exchange rates on the present financial condition   Performed Data mining and cleansing for large amounts of data across all procurement categoriesand financial systems (oracle and jde)   Provided a critical level of support on general and special , inventory, financial analysis,problem resolution and other business/financial analysis related needs that affect Procurement  Designed and present analytical models and supporting documentation to senior management   Managed saving multiple schedules and track monthly savings for sourced initiatives   Queried relevant spend data using SQL Database Analyst MCKESSON CORPORATION - Alpharetta, GA March 2012 to June 2013  Responsible for the development and administration of analytical data constructs/structures   Translated information into meaningful analysis using SQL within MS ACCESS and MS SharePoint (certified SharePoint administrator at McKesson)   Visualized and maintained current organizational metrics and force data   Managed, manipulated, and analyzed data in Microsoft Excel   Effectively communicated the analysis and conclusions Budget Analyst US FISH AND WILDLIFE - Atlanta, GA December 2010 to December 2011 NACI GOVERNMENT CLEARANCE]   Sampled documentation and spending variances to determine compliance   Built quantitative financial models for the budget and finance team using MS Excel   Helped with the transition of the overall FFS system to an ERP system functionality(FBMS)-driven SAPsolution   Revised payment documentation related to the Deepwater Horizon Oil spill   Created and administrated spending targets for 10 programs based on the revisions of the federalbudget object codes   Master of Science in Business Analytics in MS-BA Arizona State University - Tempe, AZ May 2019 Bachelor of Business Administration in Finance Kennesaw State University - Kennesaw, GA May 2011   Excel (4 years), MICROSOFT SHAREPOINT (1 year), MS SharePoint (1 year), SAP (1 year), sql (3 years) Additional Information  Management & Leadership   Member of the I school initiative at Kennesaw State University   Participated in SIFE (Students in free enterprise)   Member of the University of Hawaii football team   Member of football and basketball teams at Allen High School in Texas   Excellent written and verbal and all- encompassing communication    Mentored elementary school children as part of the Reading With The Eagle's high school community initiative        Proficient in computers from: MS Office Suite, MS Visio, MS SharePoint, SAP financials, Tableau, EazyBi, PostgreSQL, MySQL, SQL, SPSS, Essbase, Quickbooks, Minitab, Linear programing, PYTHON, Text mining, Hadoop, AWS and Internet   Machine Learning : MS Azure ML, Python data science stack (Numpy, Pandas, Scikit, Matplotlib,etc.), A/B hypothesis testing, feature engineering and selection, dimensionality reduction, linear regression and regression classification algorithms, Naïve bayes models, prescriptive analytics, and expert knowledge of most common machine learning algorithms (KNN, SVM, logistic regression, decision trees, ensemble models, clustering, etc.)   Extensive use of Excel, including complex formulas such as VLOOKUP, ""if"" statements, VBA, Solver,macros and formatting, Palisade's Stat, @RISK, PrecisionTree, pivot tables to manipulate and analyze data"
"My life  experience has been in assistant manager and management positions. My past experiences have been in a business oriented atmosphere and always ed independantly. The  that mainly interest me is sales, manager, criminal justice field,or any business oriented atmosphere. I enjoy people and love helping people as much as I can.
Authorized to  in the US for any employer
 Experience

Massage Therapist
Thai massage - Ontario, CA
June 2019 to Present
Brand ambassador
Interactions marketing - Killeen, TX
June 2014 to Present
Responsibilities 
My Responsibilities are to promote and to demonstrate product, I demonstrate it to customers and let our customer know about the benefits of the product. To manage other brand ambassador about the product they are representing for the day. To make sure they know all their responsibilities as brand ambassadors and make sure they knew all the benefits of the products. My others responsibility was to manage, sales, count inventory, to set up and breakdown material.  
 
Accomplishments 
I met a lot of different people and whether they said no to the product I always tried my best to let them know how good it was and how they could save money, and how it could help them in the future. My main accomplishment was to always make sure the left as a satisfied customer. My main priority is always to make sure they knew the benefits of the product they purchased through the day. My main focus was also to make sure product sold out thru out the day. 
 
 Used 
Customer service, bilingual, lots of confidence and knowledge of product.
Massage Therapist - Independent Contractor
Intermission spa - Upland, CA
October 2017 to June 2019
Perform healing therapeutic massages, helped with little girls spa parties and was responsible for my own transaction fees. Also Performed couples massage.
Pool designer
Premier Pools & Spas - Ontario, CA
April 2016 to August 2016
Selling pools, doing home pool presentations, attending trade shows, and designing pools.
Skin care /sales manager
Infinite aloe/ skin care - Los Angeles, CA March 2015 to January 2016
Roadshow sales Manager: responsible for sales, and promoting skin care products at all Costcos locations. Responsible for assigning schedules, inventory, closing and opening store. Responsible for engaging with sales, and helping my peers with closing their sales, responsible with interacting with customers, and sellung the merchandise. Responsible for setting up and taking down booth.
Seasonal sales, engraver
Things remembered - Fort Hood, TX
January 2012 to November 2013
Sales, customer service, cashier. Responsible for meeting sales quote, and meeting requirements and always up selling product.
Sales Assistant Manager
More than Less - Fort Jackson, SC January 2010 to December 2012
Responsibilities 
Customer service,Merchandise sales such as hats, sunglasses, purses, and shirts . Checked and verified inventory, closed and open up kiosk store.
Assistant Manager
Lucky Wok Chinese Food - Mira Loma, CA
August 2001 to August 2003
Responsibilities 
Customer service, verified all food was done in a timely manner, prepared employees schedules, open and closed store.


Bachelors in Science in leadership
Trident University International - Anaheim, CA
January 2016 to October 2018
Associates in Criminal justice
CTC - Killeen, TX 2010 to 2014
Diploma in Massage Therapy
Bryman college - Ontario, CA
2004 to 2005
Certificate in Private investigator Health and Human Services - Riverside, CA 2001 to 2001


Marketing, Training, Receptionist, Swedish Massage, Deep Tissue, Microsoft Word, Customer Service
Certifications/Licenses

Citizens police academy
August 2014 to Present
Citizens police academy
January 2010 to Present
Adult and infant child CPR February 2012 to February 2015
120 hours of basic security and self defense tactics
January 2006 to Present
Massage Therapist
January 2018 to January 2021
Licensed Massage Therapist
Life and Health Insurance
Additional Information

I have done Volunteer  through military police, substance abuse program, youth ministry for summer vacation bible studies, and Red Cross.",Data Scientist,resume,"My life  experience has been in assistant manager and management positions. My past experiences have been in a business oriented atmosphere and always ed independantly. The  that mainly interest me is sales, manager, criminal justice field,or any business oriented atmosphere. I enjoy people and love helping people as much as I can. Authorized to  in the US for any employer  Experience  Massage Therapist Thai massage - Ontario, CA June 2019 to Present Brand ambassador Interactions marketing - Killeen, TX June 2014 to Present Responsibilities  My Responsibilities are to promote and to demonstrate product, I demonstrate it to customers and let our customer know about the benefits of the product. To manage other brand ambassador about the product they are representing for the day. To make sure they know all their responsibilities as brand ambassadors and make sure they knew all the benefits of the products. My others responsibility was to manage, sales, count inventory, to set up and breakdown material.     Accomplishments  I met a lot of different people and whether they said no to the product I always tried my best to let them know how good it was and how they could save money, and how it could help them in the future. My main accomplishment was to always make sure the left as a satisfied customer. My main priority is always to make sure they knew the benefits of the product they purchased through the day. My main focus was also to make sure product sold out thru out the day.     Used  Customer service, bilingual, lots of confidence and knowledge of product. Massage Therapist - Independent Contractor Intermission spa - Upland, CA October 2017 to June 2019 Perform healing therapeutic massages, helped with little girls spa parties and was responsible for my own transaction fees. Also Performed couples massage. Pool designer Premier Pools & Spas - Ontario, CA April 2016 to August 2016 Selling pools, doing home pool presentations, attending trade shows, and designing pools. Skin care /sales manager Infinite aloe/ skin care - Los Angeles, CA March 2015 to January 2016 Roadshow sales Manager: responsible for sales, and promoting skin care products at all Costcos locations. Responsible for assigning schedules, inventory, closing and opening store. Responsible for engaging with sales, and helping my peers with closing their sales, responsible with interacting with customers, and sellung the merchandise. Responsible for setting up and taking down booth. Seasonal sales, engraver Things remembered - Fort Hood, TX January 2012 to November 2013 Sales, customer service, cashier. Responsible for meeting sales quote, and meeting requirements and always up selling product. Sales Assistant Manager More than Less - Fort Jackson, SC January 2010 to December 2012 Responsibilities  Customer service,Merchandise sales such as hats, sunglasses, purses, and shirts . Checked and verified inventory, closed and open up kiosk store. Assistant Manager Lucky Wok Chinese Food - Mira Loma, CA August 2001 to August 2003 Responsibilities  Customer service, verified all food was done in a timely manner, prepared employees schedules, open and closed store.   Bachelors in Science in leadership Trident University International - Anaheim, CA January 2016 to October 2018 Associates in Criminal justice CTC - Killeen, TX 2010 to 2014 Diploma in Massage Therapy Bryman college - Ontario, CA 2004 to 2005 Certificate in Private investigator Health and Human Services - Riverside, CA 2001 to 2001   Marketing, Training, Receptionist, Swedish Massage, Deep Tissue, Microsoft Word, Customer Service Certifications/Licenses  Citizens police academy August 2014 to Present Citizens police academy January 2010 to Present Adult and infant child CPR February 2012 to February 2015 120 hours of basic security and self defense tactics January 2006 to Present Massage Therapist January 2018 to January 2021 Licensed Massage Therapist Life and Health Insurance Additional Information  I have done Volunteer  through military police, substance abuse program, youth ministry for summer vacation bible studies, and Red Cross."
"? Data Scientist with over 7+ years of experience in building data products powered by data scienceand machine learning algorithms. Developed data products using R, Python, SQL and Tableau by leveraging machine learning based algorithms, Statistical Modeling, Data Mining, Natural Language Processing (NLP) and Data Visualization techniques 
 
? Experienced doing hands on analytics in several industries like Retail, , Banking and
Airlines. Experienced building user centric analytics in several functions of the business such as Marketing, Operations and Supply Chain 
 
? Proactive participation in product roadmap discussions, data science initiatives and the optimalapproaches to the underlying business problems 
 
? Experience ing with large data and metadata sources; interpret and communicate insights andfindings from analysis and experiments to both  and non- audiences in across various functions in business
Willing to relocate: Anywhere
Sponsorship required to  in the US
 Experience

Data Scientist
ADOBE - San Jose, CA
October 2017 to Present


Bachelor's",Data Scientist,resume,"? Data Scientist with over 7+ years of experience in building data products powered by data scienceand machine learning algorithms. Developed data products using R, Python, SQL and Tableau by leveraging machine learning based algorithms, Statistical Modeling, Data Mining, Natural Language Processing (NLP) and Data Visualization techniques    ? Experienced doing hands on analytics in several industries like Retail, , Banking and Airlines. Experienced building user centric analytics in several functions of the business such as Marketing, Operations and Supply Chain    ? Proactive participation in product roadmap discussions, data science initiatives and the optimalapproaches to the underlying business problems    ? Experience ing with large data and metadata sources; interpret and communicate insights andfindings from analysis and experiments to both  and non- audiences in across various functions in business Willing to relocate: Anywhere Sponsorship required to  in the US  Experience  Data Scientist ADOBE - San Jose, CA October 2017 to Present   Bachelor's"
" Experience

Data Scientist
Computer Sciences Corporation - Tysons Corner, VA
December 2014 to Present


Masters of Science in Electronics and Computer Engineering
Temple University - Philadelphia, PA
Bachelors of Engineering in Electronics and Computer Engineering
RV College of Engineering - Bengaluru, Karnataka


Analysis of variance (Less than 1 year), Anova (Less than 1 year), Apache hadoop mapreduce (Less than 1 year), Association rules (Less than 1 year), Bayesian (Less than 1 year)",Data Scientist,resume," Experience  Data Scientist Computer Sciences Corporation - Tysons Corner, VA December 2014 to Present   Masters of Science in Electronics and Computer Engineering Temple University - Philadelphia, PA Bachelors of Engineering in Electronics and Computer Engineering RV College of Engineering - Bengaluru, Karnataka   Analysis of variance (Less than 1 year), Anova (Less than 1 year), Apache hadoop mapreduce (Less than 1 year), Association rules (Less than 1 year), Bayesian (Less than 1 year)"
"Willing to relocate: Anywhere
Authorized to  in the US for any employer
 Experience

Consultant, Data Scientist/Data Engineer
Cigna - Plano, TX
August 2018 to Present
Goal: Applying machine learning capabilities in order to improve business performance. To create a predictive model using the neural net algorithm for the customer's data to predict whether a customer will stay with the company in the future. 
 
Responsibilities: 
* Performed Data Collection, Data Cleaning, Data Clustering, Data Visualization using Python. 
* Preprocessed data by recognizing missing values, outliers, invalid values. 
* Used tableau desktop for creating data visualizations. 
* Captured data into pandas in-memory and fast data-frames to perform data wrangling and be utilizedby other libraries efficiently. 
* Used python scientific library stack. 
* Was involved in creating a neural net models using Python and Tensor-flow on cloud. 
* Performed hyper-parameter tuning for increasing efficiency and accuracy. 
* Implemented regularization techniques for reducing overfitting in the data. 
* Used Amazon S3 for storage and EC2 for training models. 
Environment: Python, Tableau Desktop, Tableau Prep, Amazon S3, Matplotlib, NumPy, Jupyter Notebooks
Consultant, Data Scientist/ Data Engineer
USAA - Dallas, TX
February 2018 to July 2018
Goal: To Integrate data from structured and unstructured sources (text) and enable correlated analytical solutions. Generating value from the data using advanced analytics. Creating various AI/ML models such as sentiment analysis and entity extraction. 
 
Responsibilities: 
* Captured data from databases and integrated required data into one data source. 
* Pre-processed text data using nltk library for tokenization, stop-word removal, stemming etc. 
* Performed feature extraction and reduction by using bigrams and trigrams. 
* Created word vectors using BOW, TF-IDF and word2vec algorithms. 
* Developed sentiment analysis and entity extraction model using RNN in Keras. 
* Reiterated multiple times to create automated end-to-end ML pipeline. 
* Deployed model which was giving results using web APIs. 
* Created model summaries to compare results of various models. 
* Built reports and visualization for easy analytics. 
Environment: Python, Jupyter Notebooks, Tensorflow, AWS EC2, R programming, nltk.
Consultant, Data Scientist
Amdocs - Pune, Maharashtra
June 2016 to July 2017
Goal: To find a click probability using machine learning models. Performed analysis on click through impressions. Implemented binary classification algorithm for prediction purpose. 
 
Responsibilities: 
* ed on around 20TB of click data. 
* Captured data from AWS S3 buckets using the Python and PySpark. 
* Used Spark SQL to query the data. 
* Performed data preprocessing by removing missing data, imputing required values in place ofmissing data. 
* Created visualizations and dashboard for the stakeholder in a tableau desktop. 
* Performed feature engineering by dropping irrelevant variables for better model generalization. 
* Created extra features using features from an original dataset. 
* Implemented random forest and logistic regression models using a sparks machine learning library. 
* Used mini-batch gradient descent for optimizing the algorithm. 
* Created general as well as campaign specific models. 
* Achieved 0.80 of F-score on the final logistic regression model. 
* Developed models in an anaconda environment using Jupyter notebooks. 
* Used lasso and ridge regularization using sparks regularization methods. 
Environment: Python, PySpark, SparkSQL, T-SQL, AWS S3, Jupyter Notebooks, Tableau Desktop.
Python Programmer
PTC - Pune, Maharashtra
June 2015 to June 2016
Goal: To perform web server log analysis using Python and Apache Spark. Performing transformation on data by using regular expressions. Creating visualizations using matplotlib. Monitoring servers using gathered information and insights. 
 
Responsibilities: 
* Cleaned data using a python regular expressions package. 
* Created sparks RDDs for distributing computation. 
* Performed analytics on logs to capture frequent hosts, top endpoints, unique daily hosts etc. 
* Involved in creating visualizations for analyses of data. 
* Used PyCharm IDE to create end-to-end analytics pipeline. 
* Involved in debugging python applications. 
Environment: Python, PyCharm, PySpark


Master of Science in Computer Science in Machine Learning and Artificial Intelligence
Texas A&M University - Commerce, TX


Microsoft visual studio, Visual studio, Algorithm, Apache spark, C++, Git, Natural, Python, Keras,
Matplotlib, Numpy, Pandas, Clustering, Data science, Machine learning, Natural language processing,
Nlp, Tableau desktop, Anomaly detection, Deep learning, SQL, testing, access, Business Intelligence, Excel
Additional Information

Computer Certifications 
* IBM Data Science Specialization (https://www.coursera.org/account/accomplishments/specialization/certificate/CZQR7HLB54TC) 
* Deeplearning.ai Specialization (https://www.coursera.org/account/accomplishments/certificate/
K5JHZ3EG667M) 
* SQL for Data Science by UCDAVIS (https://www.coursera.org/account/accomplishments/certificate/97P3XVW85J6D) 
* Neural Nets and Deep Learning by deeplearning.ai (https://www.coursera.org/account/accomplishments/certificate/K5JHZ3EG667M) 
* Improving Deep Neural Nets: Hyper-parameter tuning Regularization and Optimization bydeeplearning.ai (https://www.coursera.org/account/accomplishments/certificate/K2STL4FNKKKB) * Structuring Machine Learning  with deeplearning.ai (https://www.coursera.org/account/ accomplishments/certificate/GTDJZCMJDCMB * Hands-on Tableau for Data Science 
* AWS Cloud Practitioner on linux academy 
 
 
* Languages: Python, C, C++, SQL 
* Libraries and Frames: Keras, NumPy, Pandas, Matplotlib, SQLAlchemy, Scikit-Learn, Tensor-flow 
* Programing Software: Microsoft Visual Studio, Microsoft SSMS, Azure Data Studio, Jupyter Notebooks,
PyCharm, Tableau Desktop, Tableau Prep, AWS S3, EC2, AWS lambda, Databricks, Apache Spark, Scala, Git 
* Machine Learning Algorithms & Models: Convolutional Neural Net, Recurrent Neural Nets,
Linear Regression algorithms, Logistic Regression model, Anomaly Detection, K-Means Clustering,
Decision Tree algorithm, Random Forest, Time series analysis. 
* Research Papers: Classification of document in NLP (Natural Language Processing) using RNN(Performed text preprocessing to clean text data by removing stop words, using nltk for tokenization and lemmatization, using BOW, word2Vec, TF-IDF for vectorization, and creating natural language models using Recurrent Neural Model)",Data Scientist,resume,"Willing to relocate: Anywhere Authorized to  in the US for any employer  Experience  Consultant, Data Scientist/Data Engineer Cigna - Plano, TX August 2018 to Present Goal: Applying machine learning capabilities in order to improve business performance. To create a predictive model using the neural net algorithm for the customer's data to predict whether a customer will stay with the company in the future.    Responsibilities:  * Performed Data Collection, Data Cleaning, Data Clustering, Data Visualization using Python.  * Preprocessed data by recognizing missing values, outliers, invalid values.  * Used tableau desktop for creating data visualizations.  * Captured data into pandas in-memory and fast data-frames to perform data wrangling and be utilizedby other libraries efficiently.  * Used python scientific library stack.  * Was involved in creating a neural net models using Python and Tensor-flow on cloud.  * Performed hyper-parameter tuning for increasing efficiency and accuracy.  * Implemented regularization techniques for reducing overfitting in the data.  * Used Amazon S3 for storage and EC2 for training models.  Environment: Python, Tableau Desktop, Tableau Prep, Amazon S3, Matplotlib, NumPy, Jupyter Notebooks Consultant, Data Scientist/ Data Engineer USAA - Dallas, TX February 2018 to July 2018 Goal: To Integrate data from structured and unstructured sources (text) and enable correlated analytical solutions. Generating value from the data using advanced analytics. Creating various AI/ML models such as sentiment analysis and entity extraction.    Responsibilities:  * Captured data from databases and integrated required data into one data source.  * Pre-processed text data using nltk library for tokenization, stop-word removal, stemming etc.  * Performed feature extraction and reduction by using bigrams and trigrams.  * Created word vectors using BOW, TF-IDF and word2vec algorithms.  * Developed sentiment analysis and entity extraction model using RNN in Keras.  * Reiterated multiple times to create automated end-to-end ML pipeline.  * Deployed model which was giving results using web APIs.  * Created model summaries to compare results of various models.  * Built reports and visualization for easy analytics.  Environment: Python, Jupyter Notebooks, Tensorflow, AWS EC2, R programming, nltk. Consultant, Data Scientist Amdocs - Pune, Maharashtra June 2016 to July 2017 Goal: To find a click probability using machine learning models. Performed analysis on click through impressions. Implemented binary classification algorithm for prediction purpose.    Responsibilities:  * ed on around 20TB of click data.  * Captured data from AWS S3 buckets using the Python and PySpark.  * Used Spark SQL to query the data.  * Performed data preprocessing by removing missing data, imputing required values in place ofmissing data.  * Created visualizations and dashboard for the stakeholder in a tableau desktop.  * Performed feature engineering by dropping irrelevant variables for better model generalization.  * Created extra features using features from an original dataset.  * Implemented random forest and logistic regression models using a sparks machine learning library.  * Used mini-batch gradient descent for optimizing the algorithm.  * Created general as well as campaign specific models.  * Achieved 0.80 of F-score on the final logistic regression model.  * Developed models in an anaconda environment using Jupyter notebooks.  * Used lasso and ridge regularization using sparks regularization methods.  Environment: Python, PySpark, SparkSQL, T-SQL, AWS S3, Jupyter Notebooks, Tableau Desktop. Python Programmer PTC - Pune, Maharashtra June 2015 to June 2016 Goal: To perform web server log analysis using Python and Apache Spark. Performing transformation on data by using regular expressions. Creating visualizations using matplotlib. Monitoring servers using gathered information and insights.    Responsibilities:  * Cleaned data using a python regular expressions package.  * Created sparks RDDs for distributing computation.  * Performed analytics on logs to capture frequent hosts, top endpoints, unique daily hosts etc.  * Involved in creating visualizations for analyses of data.  * Used PyCharm IDE to create end-to-end analytics pipeline.  * Involved in debugging python applications.  Environment: Python, PyCharm, PySpark   Master of Science in Computer Science in Machine Learning and Artificial Intelligence Texas A&M University - Commerce, TX   Microsoft visual studio, Visual studio, Algorithm, Apache spark, C++, Git, Natural, Python, Keras, Matplotlib, Numpy, Pandas, Clustering, Data science, Machine learning, Natural language processing, Nlp, Tableau desktop, Anomaly detection, Deep learning, SQL, testing, access, Business Intelligence, Excel Additional Information  Computer Certifications  * IBM Data Science Specialization (https://www.coursera.org/account/accomplishments/specialization/certificate/CZQR7HLB54TC)  * Deeplearning.ai Specialization (https://www.coursera.org/account/accomplishments/certificate/ K5JHZ3EG667M)  * SQL for Data Science by UCDAVIS (https://www.coursera.org/account/accomplishments/certificate/97P3XVW85J6D)  * Neural Nets and Deep Learning by deeplearning.ai (https://www.coursera.org/account/accomplishments/certificate/K5JHZ3EG667M)  * Improving Deep Neural Nets: Hyper-parameter tuning Regularization and Optimization bydeeplearning.ai (https://www.coursera.org/account/accomplishments/certificate/K2STL4FNKKKB) * Structuring Machine Learning  with deeplearning.ai (https://www.coursera.org/account/ accomplishments/certificate/GTDJZCMJDCMB * Hands-on Tableau for Data Science  * AWS Cloud Practitioner on linux academy      * Languages: Python, C, C++, SQL  * Libraries and Frames: Keras, NumPy, Pandas, Matplotlib, SQLAlchemy, Scikit-Learn, Tensor-flow  * Programing Software: Microsoft Visual Studio, Microsoft SSMS, Azure Data Studio, Jupyter Notebooks, PyCharm, Tableau Desktop, Tableau Prep, AWS S3, EC2, AWS lambda, Databricks, Apache Spark, Scala, Git  * Machine Learning Algorithms & Models: Convolutional Neural Net, Recurrent Neural Nets, Linear Regression algorithms, Logistic Regression model, Anomaly Detection, K-Means Clustering, Decision Tree algorithm, Random Forest, Time series analysis.  * Research Papers: Classification of document in NLP (Natural Language Processing) using RNN(Performed text preprocessing to clean text data by removing stop words, using nltk for tokenization and lemmatization, using BOW, word2Vec, TF-IDF for vectorization, and creating natural language models using Recurrent Neural Model)"
" Data Scientist with an experience of around 8 years, ing through Telecom and Retail industries,holding master's degree in Computer Science and . 
 Extensive ing experience in data science  and  like R, Python, SQL, Tableau andPower BI. 
 Enough experience in agile methodology and ability to manage all phases of SDLC ranging fromrequirement analysis, design, development, testing to deployment. 
 Proficiency in developing story boards and advanced visualizations using Tableau, Python and Power
BI. 
 Maintained a key role in management of team by gathering requirements from clients and discusswith the team. Also, turnback the output with zero-defect. 
 Enough practical knowledge in performing Data Analysis process using Python like Importingdatasets, Data wrangling, Exploratory Data Analysis. 
 Ability to participate in long hour calls to understand and gather the business requirements.  ed on Adobe campaign tool to create and run the campaign flows and gather all the requirements for creating segments in campaign flows. 
 Proficient in creating the segments & suppressions to run the campaign flows and upload them backin production mode after encryption of files. 
 Skilled in implementing techniques like Regression, Classification, Clustering and Recommendersystems, Random forest, decision trees, K means clustering using packages of Python and R studio.  Analyzed pre-existing predictive model for predicting the conversion rate of customers from retail to mail developed by Advanced Analytics team and re-built predictive model using machine learning algorithms by considering factors that better influenced the conversion rate. Increase in the conversion rate is beneficial for both customers and company. 
 Designed and developed Big Data analytics platform for processing customer viewing preferencesand social media comments using Java, Hadoop, Hive and Pig. 
 Develop excellent quality software using agile techniques such as Test-Driven Development and PairProgramming 
 Skilled in implementing machine learning techniques like Regression, Classification, Clustering andRecommender systems including Random forest, decision trees, Support Vector Machines, K means clustering using packages of Python and R studio. 
 Strong experience in ETL data warehousing and implementing all phases of SDLC which includesrequirement gap analysis, design, Datawarehouse implementation, development, testing, deployment and production support maintenance. 
 Proficient with RDBMS like Oracle, and SQL developer. Possess hands on experience of creating UNIXshell scripts required to control the ETL flow and implementing complex ETL logic. 
 Hands on experience in Azure Development, ed on Azure web application, App services,Azure storage, Azure SQL Database, Virtual machines, Fabric controller, Azure AD, Azure search, and notification hub. 
 ed on IBM Watson NLP language to develop the mailbot grammerly which fixes the mail bodydepending on the type of mail. (Official / Personal / informal). 
 Skilled in pulling large datasets to run the manual flows and refresh the dashboards and setup theconnections for different databases. 
 Ability to multi-task the  with zero-defect. 
 Experience in ing with customers to determine their needs, gather, analyze and documentrequirements, communicate with customers throughout the development cycle, manage customer expectations, resolve issues and provide project status. 
 Good communication, interpersonal and quick learning  with proven ability to adapt to differentproject environments.
 Experience

Data Scientist
Comcast Center (HQ) - Philadelphia, PA
September 2017 to Present
Description: 
Comcast corporation is the largest cable TV company and largest home Internet service provider in the
United States, and the nation's third-largest home telephone service provider. ed with marketing & strategic clients to find the better opportunity and understanding the performance of segments in different circumstances. 
 
Responsibilities: 
 Key role in managing a team for a growing relationship through zero defect and aggressive delivery. 
 Responsible for Project Management, Coordination with offshore team and project delivery. 
 Developed solutions to enhance the DW/BI capabilities aligning with the steadily changing businessrequirements. 
 Developed offer calendar to track advertised offers across geography and marketing channels. Thedashboard includes calendar view of advertised offers and performance of the offers in individual marketing channel across time. 
 ed on gathering all the requirements fromUNICA and created the flowcharts for the transition.  Design and development of camping management tool as an additional feature to Segment Lab suite of dashboards. ed on segmentation and opportunity finding. 
 Participated in the transition of project and turnback the client requests with zero defect. 
 ed on pilots like Strawman PPT, KEB Express & DM dashboard requirements gathering, develop
& maintain dashboards deployment. 
 Used K-Means cluster analysis to identify the opportunity for upgrade and lower the churn rate.  Participate in the Adobe campaign development calls on creation of campaign flows and help clients understand the usage of tool. 
 Performed parameter tuning procedures to achieve optimal performance of the model. 
 ed on Machine learning algorithms like logistic regression, Decision trees, Support Vector
Machine and Random forest to achieve best accuracy for the propensity model 
 Extensive ing experience in RStudio packages and Python libraries like SciKit-Learn to improvethe model accuracy from 65% to 86%. 
 Strong practical experience in various Python libraries like Pandas, One dimensional NumPy and Twodimensional NumPy 
 Developed data visualizations in Tableau to display day to day accuracy of the model with newlyincoming data. 
 Identified factors to be considered for phase 2 development of the project and documented thosefindings with clear explanations 
 Implemented complete data science project involving data acquisition, data wrangling, exploratorydata analysis, model development and model evaluation. 
 
Environment: Teradata, Advanced SQL, RStudio (ggplot2, choroplethr, dplyr, caret), Python (Pandas,
NumPy), Machine Learning (Logistic Regression, Decision trees, SVM, Random forest), PyTorch, Keras, Knime Analytics tool, Tableau, Excel, SharePoint, Unix, Scala,
Data Scientist
Intelli InfoTech Inc - Plano, TX May 2016 to August 2017
Responsibilities: 
 Understanding business context and strategic plans and develop a data-driven business plan tosupport the attainment of business goals. 
 Designed and developed Power BI graphical and visualization solutions with business requirementdocuments and plans for creating interactive dashboards. 
 Gathered usage reports of Microsoft applications (Word, Excel, SharePoint online, Teams) of allemployees from Microsoft office portal in the form of excel sheets 
 Considered one-month period at a time and analyzed usage report data using RStudio packagesggplot2 to identify the patterns and trends of usage. 
 Extensively used PyTorch and Keras to build and train deep learning models. 
 ed with the data science team to build and deploy machine learning based models to predictcustomer churn and optimize customer acquisition using Teradata, Oracle, SQL, BTEQ, and UNIX.  Created story boards in Tableau and PowerBI for each application usage report categorized country, region and state wise. 
 Created Macros, to generate reports daily, monthly basis and moving files from Test to Production.  Analyzed feedbacks from employees regarding Microsoft applications usage in their day to day tasks and built predictive models using machine learning algorithms to understand the main issues those are hindering usage of these apps by the employees. 
 Documented results obtained and supplied Digital fluency reports of each individual team to theirrespective team leads. 
 Suggested individual teams' better practices of using these apps to improve their overall efficiency.  Responsible for ing with stakeholders to troubleshoot issues, communicate to team members, leadership and stakeholders on findings to ensure models are well understood and optimized. 
 
Environment: SQL*Plus, RStudio, Python (NumPy, Pandas), Machine learning algorithms (Logistic Regression, Decision trees), SharePoint Online, PyTorch, Tableau, PowerBI, Excel.
Data Analyst /Engineer
Raysoft Business Solution - Hyderabad, Telangana
January 2012 to December 2014
Responsibilities: 
 Responsible for reporting of findings that will use gathered metrics to infer and draw logicalconclusions from past and future behavior. 
 Implemented and managed several ETL  using Informatica PowerCenter by loading data froma variety of Data sources like flat files, JSON, XML files to Oracle for reporting. Source and target data were synced using Informatica and finally transformed data was stored in staging tables. 
 Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route. 
 Created reporting tables for comparing source and target data and report data discrepancies
(mismatch, missing scenarios) found in the data. 
 Performed validations not received in the requirement document from the customer end and learntthe SQL queries which helped to attend defect triage calls. 
 Results obtained from report mappings were displayed using MicroStrategy which is a better UserInterface tool. 
 Implemented rule-based expertise system from the results of exploratory analysis and informationgathered from the people from different departments. 
 Created test plans for conducting unit testing of developed code. 
 Created deployment groups in QA environment and deployed flows from DEV to QAenvironment. 
 Performed debugging of the code as per inputs given by IST (Integrated System Testing) team anddeployed code into PROD environment after receiving approval from IST team. 
 Created and maintained complete documentation of project from beginning till end. 
 Performed internal enhancements of the jobs running in PROD environment. 
 Successfully maintained and managed all the jobs running in production environment by offeringproduction support. 
 Extensive hands on experience of HP Quality Center tool used for performing production supportactivities. 
 
Environment: Informatica Power Center 9.1(Repository Manger, Designer, flow Monitor, flow Manager), Oracle 11g, Toad for Oracle, SQL, UNIX, Shell scripting, SQL*Plus, MS Visio, Erwin Data Modeler, MicroStrategy.
Data Analyst
LMSOFT Business Solutions - Hyderabad, Telangana
July 2010 to December 2011
Responsibilities: 
 Communicated and coordinated with other departments to gather business requirements. 
 Gathering all the data that is required from multiple data sources and creating datasets that will beused in analysis. 
 Performed Exploratory Data Analysis and Data Visualizations using R, and Python. 
 In Preprocessing phase, used Pandas and Scikit-Learn to remove or impute missing values, detectoutliers, scale features, and applied feature selection (filtering) to eliminate irrelevant features.  Conducted Exploratory Data Analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlation between features. 
 Used Python (NumPy, SciPy, Pandas, Scikit-Learn, Seaborn to develop variety of models andalgorithms for analytic purposes. 
 Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure.
Education

Bachelor of  in Computer Science in  & Science
Texas A&M University - Kingsville, TX


Decision trees, Linear regression, Logistic regression, Machine learning, Random forest, Support vector machines, Apache spark, Hadoop, Python, Scripting, Visio, Ms access, Oracle, Sql, Clustering, Hadoop, Informatica, Teradata, Microstrategy, Sas
Additional Information

: 
 Programming Languages: R, Python 
 RDBMS: Teradata, Oracle 11g, SQL*Plus, MS Access, SQL developer 
 Machine Learning algorithms: Linear Regression, Logistic Regression, Decision Trees, 
Support Vector Machines, Random Forest, K Means 
Clustering 
 Frames: Hadoop Ecosystem, Apache Spark, Scala 
 : RStudio, Jupyter notebooks, Knime Analytics Tool, Informatica Power Center, Teradata,SQL, Toad for Oracle, PowerBI, Tableau, UNIX, Shell scripting, SAS e-miner, Microsoft Azure ML,
MicroStrategy, HP quality center, MS Azure, MS Visio",Data Scientist,resume," Data Scientist with an experience of around 8 years, ing through Telecom and Retail industries,holding master's degree in Computer Science and .   Extensive ing experience in data science  and  like R, Python, SQL, Tableau andPower BI.   Enough experience in agile methodology and ability to manage all phases of SDLC ranging fromrequirement analysis, design, development, testing to deployment.   Proficiency in developing story boards and advanced visualizations using Tableau, Python and Power BI.   Maintained a key role in management of team by gathering requirements from clients and discusswith the team. Also, turnback the output with zero-defect.   Enough practical knowledge in performing Data Analysis process using Python like Importingdatasets, Data wrangling, Exploratory Data Analysis.   Ability to participate in long hour calls to understand and gather the business requirements.  ed on Adobe campaign tool to create and run the campaign flows and gather all the requirements for creating segments in campaign flows.   Proficient in creating the segments & suppressions to run the campaign flows and upload them backin production mode after encryption of files.   Skilled in implementing techniques like Regression, Classification, Clustering and Recommendersystems, Random forest, decision trees, K means clustering using packages of Python and R studio.  Analyzed pre-existing predictive model for predicting the conversion rate of customers from retail to mail developed by Advanced Analytics team and re-built predictive model using machine learning algorithms by considering factors that better influenced the conversion rate. Increase in the conversion rate is beneficial for both customers and company.   Designed and developed Big Data analytics platform for processing customer viewing preferencesand social media comments using Java, Hadoop, Hive and Pig.   Develop excellent quality software using agile techniques such as Test-Driven Development and PairProgramming   Skilled in implementing machine learning techniques like Regression, Classification, Clustering andRecommender systems including Random forest, decision trees, Support Vector Machines, K means clustering using packages of Python and R studio.   Strong experience in ETL data warehousing and implementing all phases of SDLC which includesrequirement gap analysis, design, Datawarehouse implementation, development, testing, deployment and production support maintenance.   Proficient with RDBMS like Oracle, and SQL developer. Possess hands on experience of creating UNIXshell scripts required to control the ETL flow and implementing complex ETL logic.   Hands on experience in Azure Development, ed on Azure web application, App services,Azure storage, Azure SQL Database, Virtual machines, Fabric controller, Azure AD, Azure search, and notification hub.   ed on IBM Watson NLP language to develop the mailbot grammerly which fixes the mail bodydepending on the type of mail. (Official / Personal / informal).   Skilled in pulling large datasets to run the manual flows and refresh the dashboards and setup theconnections for different databases.   Ability to multi-task the  with zero-defect.   Experience in ing with customers to determine their needs, gather, analyze and documentrequirements, communicate with customers throughout the development cycle, manage customer expectations, resolve issues and provide project status.   Good communication, interpersonal and quick learning  with proven ability to adapt to differentproject environments.  Experience  Data Scientist Comcast Center (HQ) - Philadelphia, PA September 2017 to Present Description:  Comcast corporation is the largest cable TV company and largest home Internet service provider in the United States, and the nation's third-largest home telephone service provider. ed with marketing & strategic clients to find the better opportunity and understanding the performance of segments in different circumstances.    Responsibilities:   Key role in managing a team for a growing relationship through zero defect and aggressive delivery.   Responsible for Project Management, Coordination with offshore team and project delivery.   Developed solutions to enhance the DW/BI capabilities aligning with the steadily changing businessrequirements.   Developed offer calendar to track advertised offers across geography and marketing channels. Thedashboard includes calendar view of advertised offers and performance of the offers in individual marketing channel across time.   ed on gathering all the requirements fromUNICA and created the flowcharts for the transition.  Design and development of camping management tool as an additional feature to Segment Lab suite of dashboards. ed on segmentation and opportunity finding.   Participated in the transition of project and turnback the client requests with zero defect.   ed on pilots like Strawman PPT, KEB Express & DM dashboard requirements gathering, develop & maintain dashboards deployment.   Used K-Means cluster analysis to identify the opportunity for upgrade and lower the churn rate.  Participate in the Adobe campaign development calls on creation of campaign flows and help clients understand the usage of tool.   Performed parameter tuning procedures to achieve optimal performance of the model.   ed on Machine learning algorithms like logistic regression, Decision trees, Support Vector Machine and Random forest to achieve best accuracy for the propensity model   Extensive ing experience in RStudio packages and Python libraries like SciKit-Learn to improvethe model accuracy from 65% to 86%.   Strong practical experience in various Python libraries like Pandas, One dimensional NumPy and Twodimensional NumPy   Developed data visualizations in Tableau to display day to day accuracy of the model with newlyincoming data.   Identified factors to be considered for phase 2 development of the project and documented thosefindings with clear explanations   Implemented complete data science project involving data acquisition, data wrangling, exploratorydata analysis, model development and model evaluation.    Environment: Teradata, Advanced SQL, RStudio (ggplot2, choroplethr, dplyr, caret), Python (Pandas, NumPy), Machine Learning (Logistic Regression, Decision trees, SVM, Random forest), PyTorch, Keras, Knime Analytics tool, Tableau, Excel, SharePoint, Unix, Scala, Data Scientist Intelli InfoTech Inc - Plano, TX May 2016 to August 2017 Responsibilities:   Understanding business context and strategic plans and develop a data-driven business plan tosupport the attainment of business goals.   Designed and developed Power BI graphical and visualization solutions with business requirementdocuments and plans for creating interactive dashboards.   Gathered usage reports of Microsoft applications (Word, Excel, SharePoint online, Teams) of allemployees from Microsoft office portal in the form of excel sheets   Considered one-month period at a time and analyzed usage report data using RStudio packagesggplot2 to identify the patterns and trends of usage.   Extensively used PyTorch and Keras to build and train deep learning models.   ed with the data science team to build and deploy machine learning based models to predictcustomer churn and optimize customer acquisition using Teradata, Oracle, SQL, BTEQ, and UNIX.  Created story boards in Tableau and PowerBI for each application usage report categorized country, region and state wise.   Created Macros, to generate reports daily, monthly basis and moving files from Test to Production.  Analyzed feedbacks from employees regarding Microsoft applications usage in their day to day tasks and built predictive models using machine learning algorithms to understand the main issues those are hindering usage of these apps by the employees.   Documented results obtained and supplied Digital fluency reports of each individual team to theirrespective team leads.   Suggested individual teams' better practices of using these apps to improve their overall efficiency.  Responsible for ing with stakeholders to troubleshoot issues, communicate to team members, leadership and stakeholders on findings to ensure models are well understood and optimized.    Environment: SQL*Plus, RStudio, Python (NumPy, Pandas), Machine learning algorithms (Logistic Regression, Decision trees), SharePoint Online, PyTorch, Tableau, PowerBI, Excel. Data Analyst /Engineer Raysoft Business Solution - Hyderabad, Telangana January 2012 to December 2014 Responsibilities:   Responsible for reporting of findings that will use gathered metrics to infer and draw logicalconclusions from past and future behavior.   Implemented and managed several ETL  using Informatica PowerCenter by loading data froma variety of Data sources like flat files, JSON, XML files to Oracle for reporting. Source and target data were synced using Informatica and finally transformed data was stored in staging tables.   Performed Multinomial Logistic Regression, Random Forest, Decision Tree, SVM to classify package isgoing to deliver on time for the new route.   Created reporting tables for comparing source and target data and report data discrepancies (mismatch, missing scenarios) found in the data.   Performed validations not received in the requirement document from the customer end and learntthe SQL queries which helped to attend defect triage calls.   Results obtained from report mappings were displayed using MicroStrategy which is a better UserInterface tool.   Implemented rule-based expertise system from the results of exploratory analysis and informationgathered from the people from different departments.   Created test plans for conducting unit testing of developed code.   Created deployment groups in QA environment and deployed flows from DEV to QAenvironment.   Performed debugging of the code as per inputs given by IST (Integrated System Testing) team anddeployed code into PROD environment after receiving approval from IST team.   Created and maintained complete documentation of project from beginning till end.   Performed internal enhancements of the jobs running in PROD environment.   Successfully maintained and managed all the jobs running in production environment by offeringproduction support.   Extensive hands on experience of HP Quality Center tool used for performing production supportactivities.    Environment: Informatica Power Center 9.1(Repository Manger, Designer, flow Monitor, flow Manager), Oracle 11g, Toad for Oracle, SQL, UNIX, Shell scripting, SQL*Plus, MS Visio, Erwin Data Modeler, MicroStrategy. Data Analyst LMSOFT Business Solutions - Hyderabad, Telangana July 2010 to December 2011 Responsibilities:   Communicated and coordinated with other departments to gather business requirements.   Gathering all the data that is required from multiple data sources and creating datasets that will beused in analysis.   Performed Exploratory Data Analysis and Data Visualizations using R, and Python.   In Preprocessing phase, used Pandas and Scikit-Learn to remove or impute missing values, detectoutliers, scale features, and applied feature selection (filtering) to eliminate irrelevant features.  Conducted Exploratory Data Analysis using Python Matplotlib and Seaborn to identify underlying patterns and correlation between features.   Used Python (NumPy, SciPy, Pandas, Scikit-Learn, Seaborn to develop variety of models andalgorithms for analytic purposes.   Setup storage and data analysis  in Amazon Web Services cloud computing infrastructure. Education  Bachelor of  in Computer Science in  & Science Texas A&M University - Kingsville, TX   Decision trees, Linear regression, Logistic regression, Machine learning, Random forest, Support vector machines, Apache spark, Hadoop, Python, Scripting, Visio, Ms access, Oracle, Sql, Clustering, Hadoop, Informatica, Teradata, Microstrategy, Sas Additional Information  :   Programming Languages: R, Python   RDBMS: Teradata, Oracle 11g, SQL*Plus, MS Access, SQL developer   Machine Learning algorithms: Linear Regression, Logistic Regression, Decision Trees,  Support Vector Machines, Random Forest, K Means  Clustering   Frames: Hadoop Ecosystem, Apache Spark, Scala   : RStudio, Jupyter notebooks, Knime Analytics Tool, Informatica Power Center, Teradata,SQL, Toad for Oracle, PowerBI, Tableau, UNIX, Shell scripting, SAS e-miner, Microsoft Azure ML, MicroStrategy, HP quality center, MS Azure, MS Visio"
"
Proactive data science candidate experienced at developing an end-to-end data ecosystem and producing data driven 
solutions to solve business problems. Specialized at predictive modelling and effective communicator of findings in data.
EXPERIENCE
Live in Bing, Binghamton, New York, U.S.
Data Scientist Intern	(Python, MySQL, GCP, BigQuery, Tableau, Dataflow)			May 2018  May 2019
* Live in Bing is a Real Estate domain tech startup, where, fashioned surveys and analyzed trends to gain insight into the Binghamton University student housing market behavior to increase leasing rates by 5%.
* Led a team of 3 on Predictive Maintenance project (predictive modeling), where developed an end-to-end IoT system.
* Defined goals and KPIs for product development and constructed data pipeline to ingest sensor data into GCP BigQuery. 
* Validated data and visualized by creating dashboard on Tableau for analysis and stakeholder presentation.
* Developed and deployed a predictive model by applying time series ARIMA technique to predict consumption and behavior of assets and to gain information about anomalies in the system.
Vikas Consultancy, Thane, India
Business Analyst	(Advanced Excel)								June 2014 - Aug 2016
* Vikas Consultancy being an Insurance consulting firm, managed client database on company portals and built on Excel.
* Handled execution of various client quality initiatives and marketing programs and supported ad-hoc requests.
* Ensured business process problems / requests are prioritized and communicated to the supporting organization.
* Increased referrals by 5%, revenue by 10% and secured a good efficiency in business.

Binghamton University, State University of New York
Master of Science in Industrial & Systems Engineering (GPA: 3.22)				   Aug 2017 - May 2019
Relevant courses: Operation Research, Probability & Statistics, Multivariate Data Analysis, Modelling & Simulation
University of Mumbai, Mumbai, India
Bachelor of Engineering in Instrumentation (Electrical) Engineering (GPA: 3.15) 			   Aug 2013 - May 2017	

Walmart Store Sales Forecasting using Advanced Regression techniques	(Python, pandas, sklearn)
* Forecasted sales of each department in each Walmart store by using Extremely Randomized Trees. Performed exploratory data analysis and used cross-validation techniques to improve model for best results.
Customer Segmentation using Cluster Analysis		(Python, pandas, sklearn, matplotlib)
* Segmented customers into categories by analyzing sales data of an online retailer using K-means algorithm. Performed data validation and feature engineering to transform unstructured data into a usable format for developing model.
Business Intelligence Analysis using Tableau	(Tableau)
* Assessed US Cities population dataset for Startup Expansion to see which of the regions are performing better.
* Identified which of the 10 new locations have better potentials for company to invest more funds into marketing.
Developing ETL pipeline using Apache Airflow (Apache Airflow, PostgreSQL, Python, API, json)
* Developed ETL pipeline using Airflow by setting up DAG to query the openweathermap.org API every day, process the json data and store it in a PostgreSQL database.
 
: Python, SQL, Tableau, R, Excel, Google Cloud Platform (GCP), BigQuery, Minitab
Packages and Libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, XGBoost, TensorFlow
Machine Learning: regression, classification, clustering, random forest, decision trees, SVM, KNN, boosting, PCA, regularization, time series analysis, gradient descent, inferential statistics
: data analysis, visualization, feature engineering, advanced statistics, hypothesis testing, BI, stakeholder management
Certification: Lean Six Sigma Green Belt Certified  KPMG India Pvt Ltd.
Visa Status: F-1 OPT EAD 36-months STEM extension (up to 07/2022 without sponsorship)

",Data Scientist,resume," Proactive data science candidate experienced at developing an end-to-end data ecosystem and producing data driven  solutions to solve business problems. Specialized at predictive modelling and effective communicator of findings in data. EXPERIENCE Live in Bing, Binghamton, New York, U.S. Data Scientist Intern	(Python, MySQL, GCP, BigQuery, Tableau, Dataflow)			May 2018  May 2019 * Live in Bing is a Real Estate domain tech startup, where, fashioned surveys and analyzed trends to gain insight into the Binghamton University student housing market behavior to increase leasing rates by 5%. * Led a team of 3 on Predictive Maintenance project (predictive modeling), where developed an end-to-end IoT system. * Defined goals and KPIs for product development and constructed data pipeline to ingest sensor data into GCP BigQuery.  * Validated data and visualized by creating dashboard on Tableau for analysis and stakeholder presentation. * Developed and deployed a predictive model by applying time series ARIMA technique to predict consumption and behavior of assets and to gain information about anomalies in the system. Vikas Consultancy, Thane, India Business Analyst	(Advanced Excel)								June 2014 - Aug 2016 * Vikas Consultancy being an Insurance consulting firm, managed client database on company portals and built on Excel. * Handled execution of various client quality initiatives and marketing programs and supported ad-hoc requests. * Ensured business process problems / requests are prioritized and communicated to the supporting organization. * Increased referrals by 5%, revenue by 10% and secured a good efficiency in business.  Binghamton University, State University of New York Master of Science in Industrial & Systems Engineering (GPA: 3.22)				   Aug 2017 - May 2019 Relevant courses: Operation Research, Probability & Statistics, Multivariate Data Analysis, Modelling & Simulation University of Mumbai, Mumbai, India Bachelor of Engineering in Instrumentation (Electrical) Engineering (GPA: 3.15) 			   Aug 2013 - May 2017	  Walmart Store Sales Forecasting using Advanced Regression techniques	(Python, pandas, sklearn) * Forecasted sales of each department in each Walmart store by using Extremely Randomized Trees. Performed exploratory data analysis and used cross-validation techniques to improve model for best results. Customer Segmentation using Cluster Analysis		(Python, pandas, sklearn, matplotlib) * Segmented customers into categories by analyzing sales data of an online retailer using K-means algorithm. Performed data validation and feature engineering to transform unstructured data into a usable format for developing model. Business Intelligence Analysis using Tableau	(Tableau) * Assessed US Cities population dataset for Startup Expansion to see which of the regions are performing better. * Identified which of the 10 new locations have better potentials for company to invest more funds into marketing. Developing ETL pipeline using Apache Airflow (Apache Airflow, PostgreSQL, Python, API, json) * Developed ETL pipeline using Airflow by setting up DAG to query the openweathermap.org API every day, process the json data and store it in a PostgreSQL database.   : Python, SQL, Tableau, R, Excel, Google Cloud Platform (GCP), BigQuery, Minitab Packages and Libraries: pandas, numpy, scikit-learn, matplotlib, seaborn, XGBoost, TensorFlow Machine Learning: regression, classification, clustering, random forest, decision trees, SVM, KNN, boosting, PCA, regularization, time series analysis, gradient descent, inferential statistics : data analysis, visualization, feature engineering, advanced statistics, hypothesis testing, BI, stakeholder management Certification: Lean Six Sigma Green Belt Certified  KPMG India Pvt Ltd. Visa Status: F-1 OPT EAD 36-months STEM extension (up to 07/2022 without sponsorship)  "
" 
 
Dynamic data  with proven academic and  in . Skilled in variety of functions inc 	luding data analysis, data engineering, and machine learning techniques, applied in the urban domain. Experience ing in cross 	-culture and functional teams who possesses a keen ability to think strategically and execute tactically.  
 
 	 
Sep 2018 	-	M.S. in Applied Urban Informatics, New York University - GPA 3.86 	Personal Info 
Sep 2019  	 	 Excelled in Data Science and Machine Learning course with focuses on Urban Science 	 
 	 	 Completed a capstone project to analyze the impact of Manhattan Congestion Surcharge 		on For-Hire vehicles using a Nested Multinomial Logistic Model 	 
 	 	 	 
Sep 2014 -  	B.S. in Informatics, University of California, Irvine - GPA 3.83 	 
Mar 2018  	 Completed courses in Computer Science and Software Design 	 
 Mastered 5 programming languages 	
 Completed a senior project building a doctor-scheduling online frame for 	 
	NeoGenomics Laboratories 	 
 Minored Business Accounting 	 
	 	 
Project Experience 	 
Jan 2019 - 	Impact of Manhattan Congestion Surcharge on For-Hire-Vehicles 	 
Present  	 Evaluated the impact of the congestion surcharge policy implemented in Manhattan and 
 	assessing whether the current price is appropriate or not. 	 Data Analysis 
 Built a nested multinomial logistic model to simulate peoples transportation choices 	 Data Engineering  	between public transportation, Taxi, private/shared For-Hire-Vehicles, and walking, which 	 Machine Learning  	will be used to predict the mode shift after the surcharge. 
 Statistical Modeling 
 
Feb 2019 - 	Felony Amount Prediction based on Street views and Demographics data                	 Anomaly Detection 
May 2019  Extracted Street view features from Google images using Pyramid Scene Parsing Net. 	 Data Visualization 
 Developed and optimized machine learning models (Decision Trees, Random Forest,  Human-Centered Design  Gradient Boost, SVM, Native Based) to predict the felony amount using the merged street 
 	view data and demographics data.  	 UI/UX Design 
 A/B Testing 
Feb 2019 - 	Text Analysis on Movie Reviews 	 Prototyping                          
May 2019   Explored the differences between Marvel and DC movies from the audiences perceptive 
  	that is, through text analysis of movie reviews. 	Programming/Software 
 Conducted sentiment analysis, term frequency analysis, machine learning models, and topic models on Marvels and DCs reviews to assess the differences between their movies 	 Python in terms of styles, themes, and audience experiences. 
 R 
 Experience 	 SQL 
 Spark 
Dec 2018 - 	Data and Web Development Intern - United Nations Development Programme 	 Hadoop Apr 2019    	(UNDP), New York 
                                                                                      JavaScript  	 Supported data collection, data cleaning, data analytics, and live data integrations in  
 	that analyze social neting behaviors and usefulness of knowledge products.  	 HTML, CSS 
                                                                                               Assisted the data specialists with training a machine learning system to understand and extract 	 Java 
 	the context of lessons learned from project reports. 	 Jupyter 
                                                                                               Leaded UI/UX design in data visualizations and web reports. 	 GitHub 
 	 
May 2018 - Web Usage Analysis & UI/UX Design Intern - Real Int'l SCM Corp., CA  	 Power Bi 
Jul 2018 	 Designed and built a  website and a commercial slogan for the company. 	 SharePoint 
 	 Collected weekly website usage data, summarized and presented the data in weekly reports.  	 Tableau  	 Analyzed the web data and conducted Search Engine Optimization (SEO) to improve online 	  	visibility for the website. 
 	 	 Languages 
Jun 2016 - 	Assistant Intern in User Research Group - Topway Video Communication Co. Ltd, 
Aug 2016 	China                                                                      	 English 
 Performed user interviews, user testing on companys new system and interface. 	 Chinese/Mandarin 
 Documented, analyzed and presented the collected user data using Microsoft Words, Excel 	 Cantonese and PowerPoint. 
 ",Data Scientist,resume,"    Dynamic data  with proven academic and  in . Skilled in variety of functions inc 	luding data analysis, data engineering, and machine learning techniques, applied in the urban domain. Experience ing in cross 	-culture and functional teams who possesses a keen ability to think strategically and execute tactically.      	  Sep 2018 	-	M.S. in Applied Urban Informatics, New York University - GPA 3.86 	Personal Info  Sep 2019  	 	 Excelled in Data Science and Machine Learning course with focuses on Urban Science 	   	 	 Completed a capstone project to analyze the impact of Manhattan Congestion Surcharge 		on For-Hire vehicles using a Nested Multinomial Logistic Model 	   	 	 	  Sep 2014 -  	B.S. in Informatics, University of California, Irvine - GPA 3.83 	  Mar 2018  	 Completed courses in Computer Science and Software Design 	   Mastered 5 programming languages 	  Completed a senior project building a doctor-scheduling online frame for 	  	NeoGenomics Laboratories 	   Minored Business Accounting 	  	 	  Project Experience 	  Jan 2019 - 	Impact of Manhattan Congestion Surcharge on For-Hire-Vehicles 	  Present  	 Evaluated the impact of the congestion surcharge policy implemented in Manhattan and   	assessing whether the current price is appropriate or not. 	 Data Analysis   Built a nested multinomial logistic model to simulate peoples transportation choices 	 Data Engineering  	between public transportation, Taxi, private/shared For-Hire-Vehicles, and walking, which 	 Machine Learning  	will be used to predict the mode shift after the surcharge.   Statistical Modeling    Feb 2019 - 	Felony Amount Prediction based on Street views and Demographics data                	 Anomaly Detection  May 2019  Extracted Street view features from Google images using Pyramid Scene Parsing Net. 	 Data Visualization   Developed and optimized machine learning models (Decision Trees, Random Forest,  Human-Centered Design  Gradient Boost, SVM, Native Based) to predict the felony amount using the merged street   	view data and demographics data.  	 UI/UX Design   A/B Testing  Feb 2019 - 	Text Analysis on Movie Reviews 	 Prototyping                           May 2019   Explored the differences between Marvel and DC movies from the audiences perceptive    	that is, through text analysis of movie reviews. 	Programming/Software   Conducted sentiment analysis, term frequency analysis, machine learning models, and topic models on Marvels and DCs reviews to assess the differences between their movies 	 Python in terms of styles, themes, and audience experiences.   R   Experience 	 SQL   Spark  Dec 2018 - 	Data and Web Development Intern - United Nations Development Programme 	 Hadoop Apr 2019    	(UNDP), New York                                                                                        JavaScript  	 Supported data collection, data cleaning, data analytics, and live data integrations in    	that analyze social neting behaviors and usefulness of knowledge products.  	 HTML, CSS                                                                                                 Assisted the data specialists with training a machine learning system to understand and extract 	 Java   	the context of lessons learned from project reports. 	 Jupyter                                                                                                 Leaded UI/UX design in data visualizations and web reports. 	 GitHub   	  May 2018 - Web Usage Analysis & UI/UX Design Intern - Real Int'l SCM Corp., CA  	 Power Bi  Jul 2018 	 Designed and built a  website and a commercial slogan for the company. 	 SharePoint   	 Collected weekly website usage data, summarized and presented the data in weekly reports.  	 Tableau  	 Analyzed the web data and conducted Search Engine Optimization (SEO) to improve online 	  	visibility for the website.   	 	 Languages  Jun 2016 - 	Assistant Intern in User Research Group - Topway Video Communication Co. Ltd,  Aug 2016 	China                                                                      	 English   Performed user interviews, user testing on companys new system and interface. 	 Chinese/Mandarin   Documented, analyzed and presented the collected user data using Microsoft Words, Excel 	 Cantonese and PowerPoint.   "
"EXPERIENCE 

	Niagara Bottling, LLC, Ontario, California  	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	01/2019  05/2019 
Practicum Data Analyst (Project Team Lead) 
 Ensured project milestones were met by managing a team of 5 to add business values and provide actionable insights.  
 Predicted missed pickup or transportation failures and identified the strongest indicators that a load will be late. 
 Implemented classification models including logistic regression, decision tree, and support vector machine using R and Python. 
 Performed data cleaning, exploratory analysis and scraped weather information as external factor to include in our model; leveraged Tableau to create data visualization dashboards; presented business issues and viable suggestions. 
Deloitte Touche Tohmatsu CPA LLP, Shanghai, China     	 	 	 	 	       	 	 	   10/2016  04/2018  Audit & Assurance, Associate 
 Analyzed financial statements of both listed and non-listed Chinese and international companies in , Media & Telecommunications industry(formulas, pivot tables).  
 Applied analytical thinking and  skepticism to surface possible audit issues and documented judgements. 
 Improved clients internal control environment after identifying risks by conducting interviews with clients key personnel from different departments. 
 Offered analysis report of significant audit issues as well as their impacts on the entity and the engagement. 
 Supervised tax memo delivery and ensured high-quality report to clients by collaborating with teams from other departments.  
	Nielsen, Guangzhou, China     	 	 	 	 	      	 	 	 	       	 	 	 	 	   11/2015  04/2016  
	CPG Vertical, Market Analyst Intern 	 	 
 Responsible for drafting monthly/quarterly insights reports for 4 local FMCG clients under the supervision of senior managers. 
 Extracted clients and competitors sales/marketing related data from database, Analyzed raw data using advanced Excel functions like conditional formatting, Pivot Tables and VLOOKUP. 
 Assisted in developing analytical solutions and utilized consumer insights to help clients grow their market.  
 Visualized marketing data to assist in the preparation of proposals and presentations.  
  

	Airbnb Homes Pricing Strategy Analysis  	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 2019 
 Analyzed Seattles Airbnb Homes listings information to explore how to remain competitive in the Airbnb Homes market. 
 Identified the key features that determine the listing price, as well as the attributes that can serve as determinants of customer influx, by applying regression and decision tree model (accuracy rate: 83%). 
 Performed data cleaning, data analysis, and data visualization using Pandas, NumPy, Seaborn, Matplotlib, Scikitlearn libraries in python and in Tableau. 
	Customer Insights at Sun Country Airlines Analysis  	 	 	 	 	 	 	 	 	 	      	 	 	 	 	 2018 
 Applied RMySQL, dplyr, ade4, and lubridate packages in R for data manipulation and data cleanup. 
 Used K-Means clustering algorithm to cluster customer data, summarized the key characteristics of each customer segment, and offer targeted advice to better achieve their business s. 
 Visualized our findings using Tableau and ggplot2 in R, and presented the insights derived from the analysis, aligned with the airlines business . 
 

Strengths: Data Analytics; Predictive analytics; Data mining; Data visualization; Financial analysis; Communication ; Team Collaboration ;  under tight timelines and pressure; Time management; Fast learning; Organization  
Computer: Advanced Excel(formulas, pivot tables, macros, etc.); Microsoft PowerPoint; Microsoft Word; SQL; 
Alteryx; Tableau; R; Python; NLP(TFIDF, N-gram, sentiment analysis, etc); Power BI; Arena; Weka; MegaStat; ETL 
 

	University of California, Irvine | Irvine, CA | Master of Science in Business Analytics, GPA 3.76  	 	 06/2019 
 	Beta Gamma Sigma member; UCI 2018 Faculty Fellowship Recipient; Most Inspirational Award 
   Jinan University | Guangzhou, China | Bachelor of Management, Accounting, GPA 3.79 	 	 	 	 	 06/2016   	Jinan University 2014 Scholarship Recipient ",Data Scientist,resume,"EXPERIENCE   	Niagara Bottling, LLC, Ontario, California  	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	01/2019  05/2019  Practicum Data Analyst (Project Team Lead)   Ensured project milestones were met by managing a team of 5 to add business values and provide actionable insights.    Predicted missed pickup or transportation failures and identified the strongest indicators that a load will be late.   Implemented classification models including logistic regression, decision tree, and support vector machine using R and Python.   Performed data cleaning, exploratory analysis and scraped weather information as external factor to include in our model; leveraged Tableau to create data visualization dashboards; presented business issues and viable suggestions.  Deloitte Touche Tohmatsu CPA LLP, Shanghai, China     	 	 	 	 	       	 	 	   10/2016  04/2018  Audit & Assurance, Associate   Analyzed financial statements of both listed and non-listed Chinese and international companies in , Media & Telecommunications industry(formulas, pivot tables).    Applied analytical thinking and  skepticism to surface possible audit issues and documented judgements.   Improved clients internal control environment after identifying risks by conducting interviews with clients key personnel from different departments.   Offered analysis report of significant audit issues as well as their impacts on the entity and the engagement.   Supervised tax memo delivery and ensured high-quality report to clients by collaborating with teams from other departments.   	Nielsen, Guangzhou, China     	 	 	 	 	      	 	 	 	       	 	 	 	 	   11/2015  04/2016   	CPG Vertical, Market Analyst Intern 	 	   Responsible for drafting monthly/quarterly insights reports for 4 local FMCG clients under the supervision of senior managers.   Extracted clients and competitors sales/marketing related data from database, Analyzed raw data using advanced Excel functions like conditional formatting, Pivot Tables and VLOOKUP.   Assisted in developing analytical solutions and utilized consumer insights to help clients grow their market.    Visualized marketing data to assist in the preparation of proposals and presentations.       	Airbnb Homes Pricing Strategy Analysis  	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 	 2019   Analyzed Seattles Airbnb Homes listings information to explore how to remain competitive in the Airbnb Homes market.   Identified the key features that determine the listing price, as well as the attributes that can serve as determinants of customer influx, by applying regression and decision tree model (accuracy rate: 83%).   Performed data cleaning, data analysis, and data visualization using Pandas, NumPy, Seaborn, Matplotlib, Scikitlearn libraries in python and in Tableau.  	Customer Insights at Sun Country Airlines Analysis  	 	 	 	 	 	 	 	 	 	      	 	 	 	 	 2018   Applied RMySQL, dplyr, ade4, and lubridate packages in R for data manipulation and data cleanup.   Used K-Means clustering algorithm to cluster customer data, summarized the key characteristics of each customer segment, and offer targeted advice to better achieve their business s.   Visualized our findings using Tableau and ggplot2 in R, and presented the insights derived from the analysis, aligned with the airlines business .     Strengths: Data Analytics; Predictive analytics; Data mining; Data visualization; Financial analysis; Communication ; Team Collaboration ;  under tight timelines and pressure; Time management; Fast learning; Organization   Computer: Advanced Excel(formulas, pivot tables, macros, etc.); Microsoft PowerPoint; Microsoft Word; SQL;  Alteryx; Tableau; R; Python; NLP(TFIDF, N-gram, sentiment analysis, etc); Power BI; Arena; Weka; MegaStat; ETL     	University of California, Irvine | Irvine, CA | Master of Science in Business Analytics, GPA 3.76  	 	 06/2019   	Beta Gamma Sigma member; UCI 2018 Faculty Fellowship Recipient; Most Inspirational Award     Jinan University | Guangzhou, China | Bachelor of Management, Accounting, GPA 3.79 	 	 	 	 	 06/2016   	Jinan University 2014 Scholarship Recipient "
" 	 	 	 	 	 	 	 	  	 	 	 	 	 	 
The University of Texas at Austin 	Master of Science in Information  and Management 	May 2019     
 	Course Includes: Big Data & Distributed Programming, 	 
Business Data Science, Advanced Data Mining & Web Analytics, 
Cognitive Computing, User Generated Content Analytics  Overall GPA: 3.6/4.0 
 	 	 
Beijing University of Posts and 	Bachelor of Engineering in Telecommunications Engineering with 	June 2017    
Telecommunications  	Management 
Dual-degree program with Queen Mary University of London Overall GPA: 3.5/4.0 
 
  	 	 	 	 	 	 	 	 	 	 
* Programming Languages: R, Python, SQL, PL/SQL, C, JAVA, Kotlin, Html, CSS, JavaScript 
* Computer Softwares/Platforms: MS Office, Git, Jupyter Notebook, Amazon AWS, Azure, Google Cloud Platform 
* Data Science/Analysis : Machine Learning, Natural Language Processing (NLP), NumPy, Pandas, Scikit-Learn, Apache Spark, Hadoop Map/Reduce, Tensorflow, Excel Pivot table, Matplotlib, Seaborn, Power BI, Tableau 
 
EXPERIENCE 	  	 	 	 	 	 	 	 	 	 	 	 	 	 UTIMCO  Data Analyst Intern, IT Capstone Project; Austin, Texas, United States                                  Jan. 2019  May 2019 
* Collected time series data of financial factors from Bloomberg and refine the raw data into datasets for analysis ? 	Conducted data cleaning and transformation including linking split time series and getting percentage change 
* Applied machine learning techniques PCA and K-means clustering to find key factors among financial categories ? 	Created data visualizations, wrote reports and presented findings from data analysis to clients  
 
Baidu, Inc.  Product Manager Intern, Baidu Map Client Division; Beijing, China                                 Nov. 2017  Apr. 2018 ? 	Conducted whole software development process and created business cases to improve employees  efficiency 
* Collaborated with software engineers, business development and operating teams to create product positioning ? 	Designed the product architecture and website prototype for content management system (CMS) with Axure 
* Collected and summarized daily page view data from A/B test with Excel to evaluate products and get insights  
 
ACADEMIC  	  	 	 	 	 	 	 	 	 	 	 	 	 Stock Price Data Analysis  University of Texas at Austin, Austin, Texas, United States                        Apr. 2019  May 2019 ? 	ed on historical stock data of 10 tech companies from 2010 to 2015 and predicted the stock price trends  
* Made visualization for the historical stock data and the fundamental financial data of different companies in Python 
* Conducted fundamental analysis with Linear Regression and applied Arima, LSTM models for time series prediction  
 
User Generated Content Data Analysis on Twitter University of Texas at Austin, United States                            Dec. 2018 
* Wrote Python script to scrape from Twitter and collected 4k tweets involving content of 2018 Texas Senate Race ? 	Conducted sentiment analysis on the tweets regarding to the candidates collected from large versus small cities 
* Visualized sentiment data of candidates with plot in Python to show how people feel about different candidates ? 	Provided findings and recommendations to different candidates based on topic modelling and NLP techniques 
 
Database Design and SQL Programming Project  University of Texas at Austin, Austin, Texas, United States     Nov. 2018 
* Created a web management system with Python and Django frame that allows all employees to manage their points and exchange rewards as well as administrator to view all the transaction records and data reports 
* Designed and created an Oracle database with SQL commands to store user information and transaction records ? 	Wrote SQL queries to fetch data from the database and SQL commands to create triggers, views and reports 
 
Kaggle Competition for Binary Classification  University of Texas at Austin, Austin, Texas, United States            Oct. 2018 ? 	Researched individually to predict binary outcome based on the given 3.6MB dataset with unknown features 
* Described the dataset with Python and Pandas package and visualized data with matplotlib and seaborn libraries 
* Conducted feature engineering then implemented models such as Logistic Regression, Random Forest, XGBoost, etc  ? 	Tuned hyperparameters to improve models performance and achieved AUC score of 89.39 % 
 
Movie Recommender using Map-Reduce  University of Texas at Austin, Austin, Texas, United States                   Sep.2018 
* Developed a movie recommendation system with Hadoop Map/Reduce frame and Spark RDDs in Python ? 	Generated and provided users customized recommendations based on users ratings and genres 
 
HONORS 	 	 	 	 	 	 	 	 	 	 	 	 	 	 ? 	Award for excellent final year capstone project                         	 	 	                                Summer 2017 
* Scholarships to Beijing University of Posts and Telecommunications                                   Fall 2014, Fall 2015, Fall 2016 ",Data Scientist,resume," 	 	 	 	 	 	 	 	  	 	 	 	 	 	  The University of Texas at Austin 	Master of Science in Information  and Management 	May 2019       	Course Includes: Big Data & Distributed Programming, 	  Business Data Science, Advanced Data Mining & Web Analytics,  Cognitive Computing, User Generated Content Analytics  Overall GPA: 3.6/4.0   	 	  Beijing University of Posts and 	Bachelor of Engineering in Telecommunications Engineering with 	June 2017     Telecommunications  	Management  Dual-degree program with Queen Mary University of London Overall GPA: 3.5/4.0      	 	 	 	 	 	 	 	 	 	  * Programming Languages: R, Python, SQL, PL/SQL, C, JAVA, Kotlin, Html, CSS, JavaScript  * Computer Softwares/Platforms: MS Office, Git, Jupyter Notebook, Amazon AWS, Azure, Google Cloud Platform  * Data Science/Analysis : Machine Learning, Natural Language Processing (NLP), NumPy, Pandas, Scikit-Learn, Apache Spark, Hadoop Map/Reduce, Tensorflow, Excel Pivot table, Matplotlib, Seaborn, Power BI, Tableau    EXPERIENCE 	  	 	 	 	 	 	 	 	 	 	 	 	 	 UTIMCO  Data Analyst Intern, IT Capstone Project; Austin, Texas, United States                                  Jan. 2019  May 2019  * Collected time series data of financial factors from Bloomberg and refine the raw data into datasets for analysis ? 	Conducted data cleaning and transformation including linking split time series and getting percentage change  * Applied machine learning techniques PCA and K-means clustering to find key factors among financial categories ? 	Created data visualizations, wrote reports and presented findings from data analysis to clients     Baidu, Inc.  Product Manager Intern, Baidu Map Client Division; Beijing, China                                 Nov. 2017  Apr. 2018 ? 	Conducted whole software development process and created business cases to improve employees  efficiency  * Collaborated with software engineers, business development and operating teams to create product positioning ? 	Designed the product architecture and website prototype for content management system (CMS) with Axure  * Collected and summarized daily page view data from A/B test with Excel to evaluate products and get insights     ACADEMIC  	  	 	 	 	 	 	 	 	 	 	 	 	 Stock Price Data Analysis  University of Texas at Austin, Austin, Texas, United States                        Apr. 2019  May 2019 ? 	ed on historical stock data of 10 tech companies from 2010 to 2015 and predicted the stock price trends   * Made visualization for the historical stock data and the fundamental financial data of different companies in Python  * Conducted fundamental analysis with Linear Regression and applied Arima, LSTM models for time series prediction     User Generated Content Data Analysis on Twitter University of Texas at Austin, United States                            Dec. 2018  * Wrote Python script to scrape from Twitter and collected 4k tweets involving content of 2018 Texas Senate Race ? 	Conducted sentiment analysis on the tweets regarding to the candidates collected from large versus small cities  * Visualized sentiment data of candidates with plot in Python to show how people feel about different candidates ? 	Provided findings and recommendations to different candidates based on topic modelling and NLP techniques    Database Design and SQL Programming Project  University of Texas at Austin, Austin, Texas, United States     Nov. 2018  * Created a web management system with Python and Django frame that allows all employees to manage their points and exchange rewards as well as administrator to view all the transaction records and data reports  * Designed and created an Oracle database with SQL commands to store user information and transaction records ? 	Wrote SQL queries to fetch data from the database and SQL commands to create triggers, views and reports    Kaggle Competition for Binary Classification  University of Texas at Austin, Austin, Texas, United States            Oct. 2018 ? 	Researched individually to predict binary outcome based on the given 3.6MB dataset with unknown features  * Described the dataset with Python and Pandas package and visualized data with matplotlib and seaborn libraries  * Conducted feature engineering then implemented models such as Logistic Regression, Random Forest, XGBoost, etc  ? 	Tuned hyperparameters to improve models performance and achieved AUC score of 89.39 %    Movie Recommender using Map-Reduce  University of Texas at Austin, Austin, Texas, United States                   Sep.2018  * Developed a movie recommendation system with Hadoop Map/Reduce frame and Spark RDDs in Python ? 	Generated and provided users customized recommendations based on users ratings and genres    HONORS 	 	 	 	 	 	 	 	 	 	 	 	 	 	 ? 	Award for excellent final year capstone project                         	 	 	                                Summer 2017  * Scholarships to Beijing University of Posts and Telecommunications                                   Fall 2014, Fall 2015, Fall 2016 "
"

 and Experience:
* Ten years of experience in developing different Statistical Machine Learning, Text Analytics, and Data Mining solutions across various business functions: providing BI, Insights and Reporting frame to optimize business outcomes through data analysis.
* Experience in Machine Learning, Deep Learning, Data Mining with large datasets of structured and unstructured data, Data Validation, Data acquisition, DataVisualization, Predictive Modeling and developed predictive models that help to provide intelligent solutions.
* Strong mathematical knowledge and hands on experience in implementing Machine Learning algorithms like K-Nearest Neighbors, Logistic Regression, Linear regression, Naïve Bayes, Support Vector Machines, Decision Trees, Random Forests, Gradient Boosted Decision Trees, Stacking Models.
* Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, MapReduce concepts, and ecosystems including Hive and Pig.
* Experience with data visualization using  like GGplot, Matplotlib, Seaborn, Tableau, R Shiny and using Tableau software to publish and presenting dashboards, storyline on web and desktop platforms. 
* Experienced in python data manipulation for loading and extraction as well as with python libraries such as NumPy, SciPy and Pandas and Spark 2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes.
* Well experienced in Normalization and Standardization techniques for optimal performance in relational and dimensional database environments.
* Thorough grounding in all phases of data analysis (both structured and unstructured), including definition and analysis of questions with respect to available data and resources, overview of data and assessment of data quality, selection of appropriate predictive models, statistical tests and presentation of results.
* Proficient in Natural Language Processing (NLP), Text Analytics etc. using R and Python (NLTK, genism, TextBlob etc.). Used NLP- Bag of words / N-gram algorithms, Term-document matrices, Text categorization and text routing,
* Processing unstructured text, Processing speech / using speech-to-text algorithms and, data mining and big data algorithms and methods.
* Developed machine learning solutions in Natural Language Processing (NLP), document classification, Named Entity Recognition (NER), topic modelling, document summarization, computational linguistics, advanced and semantic information search, extraction, induction, classification and exploration.
* Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system.  
* Excellent knowledge and experience in using open source NLP packages such as NLTK, Word2Vec, SpaCy, Gensim, Standford CoreNLP.
* Experience in understanding of NLP/ML & algorithms and models (GLMs, SVM, PCA, NB, Clustering, DTs) and their underlying computational and probabilistic statistics.
* Experience building Machine Learning & NLP solutions over open source platforms such as SciKit-Learn,Tensorflow,TensorBoard, Keras, PyTorch, SparkML, Torch, Caffe, H2O
* Understanding of different components of Hadoop ecosystem such as Hue, Pig, Hive, HBase, HDFS, Map-reduce, Flume etc.
* Hands on Experience on Customer Churn, Sales Forecasting, Market Mix Modeling, Customer Classification, Survival Analysis, Sentiment Analysis, Text Mining, Recommendation Systems.
* Experience in using Statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regression and Time Series Analysis to analyze data for further Model Building.
* Installed and configured Kubernetes and Docker on google cloud platform (GCP) in Dev, Test, Stage and Production Environments  
* Configured Microservices on GitHub with Kubernetes deploy file, service file and config maps.
* Hands on experience in implementing Dimensionality Reduction Techniques like Truncated SVD, Principal Component Analysis, t-Stochastics Neighborhood Embedding (t-SNE).
* Hands on experience on Deep Learning Techniques such as Back Propagation, Choosing Activation Functions, Weight Initialization based on Optimizer, Avoiding Vanishing Gradient and Exploding Gradient Problems, Using Dropout, Regularization and Batch Normalization, Gradient Monitoring and Clipping Padding and Striding, Max pooling, LSTM.
* Expertise writing production quality code in SQL, R, Python and Spark. Hands on experience building regression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments, along with that used MongoDB for extraction data.
* Experience in Data Modeling retaining concepts of RDBMS, Logical and Physical Data Modeling until 3NormalForm (3NF) and Multidimensional Data Modeling Schema (Star schema, Snow-Flake Modeling, Facts and dimensions). Hands on experience in optimizing the SQL Queries and database performance tuning in Oracle, SQL Server and Teradata databases.
* Experience using multiple ETL  in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export such as Ab Initio and Alteryx.



Temple University, Philadelphia, PA 
Masters of Science, Electronics and Computer Engineering

RV College of Engineering, Bangalore, India 
Bachelors of Engineering, Electronics and Computer Engineering

 
Statistics/ML
Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau 
Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning
Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization
Feature Engineering: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods
Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova
Sampling Methods: Bootstrap sampling methods and Stratified sampling
Model Tuning/Selection: Cross Validation, AUC, Precision/Recall, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization
Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series

Machine Learning /
Deep Learning
R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot
Python: pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib, tensorflow
SAS: Forecast server, SAS Procedures and Data Steps
Spark: MLlib, GraphX
SQL: Subqueries, joins, DDL/DML statements
Databases/ETL/Query
Teradata, SQL Server, Postgres and Hadoop (MapReduce),Cloudera stack; SQL, Hive, Pig and Alteryx,NOSQL,Source Code Management
Visualization
Tableau, ggplot2 and RShiny
Prototyping
PowerPoint, RShiny and Tableau EXPERIENCE
Computer Sciences Corporation (CSC), Tysons Corner, VA                	              June 2015  Till date 
Lead Data Scientist/ SME  Artificial Intelligence + Machine Learning

KEY 
App Recommender System:
> Developed a personalized recommender system using recommender algorithms (collaborative filtering, low rank matrix factorization) that recommended best apps to a user based on similar user s. The recommendations enabled users to engage better and helped improving the overall user retention rates at CSC
Sales Forecasting:
> Forecasted sales and improved accuracy by 10-20% by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates. Increased accuracy helped business plan better with respect to budgeting and sales and operations planning

Anomaly Detection:
> Created interactive dashboard suite that illustrated outlier characteristics across several sales-related dimensions and overall impact of outlier imputation in R (Shiny).Used iterative outlier detection and imputation algorithm using multiple density-based clustering techniques (DBSCAN, kernel density estimation)

Cross Sell and Upsell Opportunity Analysis:
> Implemented market basket algorithms from transactional data, which helped identify items used/purchased together frequently. Discovering frequent item sets helped unearth cross sell and upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams

Customer Churn Prediction:
> Predicted the likelihood of customer churn based on customer attributes like customer size, RFM loyalty metrics, revenue, type of industry, competitor products and growth rates etc. The models deployed in production environment helped detect churn in advance and aided sales/marketing teams plan for various retention strategies in advance like price discounts, custom licensing plans. 

Customer Purchase Propensity Modeling:
> Built machine learning based regression models using scikit-learn python frames to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in, revenue, historic purchases, frequency and recency behaviors. These predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients.

Price Elasticity Analysis:
> Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, CSC made selective and cautious price cuts for certain licensing categories.

Customer Segmentation:
> Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, CSC made selective and cautious price cuts for certain licensing categories.
> Customer segmentation based on their behavior or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behavior patterns.
> The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.
> Used Principal Component Analysis and t-SNE in feature engineering to analyze high dimensional data.

BB & T Bank, Greensboro, NC                                                                                                                                                        
Data Scientist  						                                       July 2013  May 2015

Played a key role in developing and maintaining statistical and machine learning models that mine, analyze and turn BB & T customer and sales data into insights that helped BB & T make strategic decisions that led to growth in their user base and revenue

Customer Participation Analysis:
> Built relational databases in SQL server of several flat files of partner information from several large flat files in Python. Used logistics regression and random forests models in R/Python to predict the likelihood of customer participation in various marketing programs. 
> Designed and developed visualizations and dashboards in R /Tableau that surfaced the primary factors that drove program participation and identified the best targets for future targeted marketing efforts.
> Performed Data Profiling to learn about behaviour with various features such as traffic pattern, location, time, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends.
> Involved in various pre-processing phases of text data like Tokenizing, Stemming, Lemmatization and converting the raw text data to structured data.
> Personalization, Target Marketing, Customer Segmentation and profiling.
Customer Life Time Value Analysis:
> Projected customer lifetime values based on historic customer usage and churn rates using survival models. Understanding customer lifetime values helped business to establish strategies to selectively attract customers who tend to be more profitable for BB & T. 
> It also helped business to establish appropriate marketing strategies based on customer values.
> Performed Data Cleaning, features scaling, featurization, features engineering.
> Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN.
> Implemented number of Natural Language process mechanism for chatbot.
> Customer segmentation based on their behaviour or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behaviour patterns.
Customer Segmentation:
> Developed 11 customer segments using unsupervised learning techniques like KMeans.
> The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention, acquisition and spend 
Price Optimization and Revenue management:
> Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, BB&T made selective and cautious price cuts for certain licensing categories
> The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.
> Performed Clustering with historical, demographic and behavioural data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device.
> Analyzed high volume, high dimensional client and survey data from different sources using SAS and R.
> Used Principal Component Analysis and t-SNE in feature engineering to analyse high dimensional data.

Bank of America, Jersey City, NJ 
Data Scientist 		         				                                        Aug 2012 - June 2013

Played key role in developing and deploying DFAST Stress Test models across several bank portfolios. Provided architectural leadership on several high priority initiatives including account prioritization, account prospecting, and opportunity scoring. Drove the creation of comprehensive datasets encompassing user s and behaviors, and incorporating a wide variety of signals and data types.

Forecasting Loan balance:
> Forecasted bank-wide loan balances under normal and stressed macroeconomic scenarios using R. Performed variable reduction using the stepwise, lasso, and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques.

Top down Models (Commercial Real Estate):
> Automated the scraping and cleaning of data from various data sources in R and Python. Developed Bankss loss forecasting process using relevant forecasting and regression algorithms in R. 
> The projected losses under stress conditions helped bank reserve enough funds per DFAST policies.

Fraud Detection:
> This project includes creation of statistical machine learning models which implements Customer Segmentation, Customer Lifetime Value, Fraud detection, Sales Forecasting, Anomaly Detection, Customer churn prediction, Elasticity Analysis, propensity Modeling.
> Gathered requirements from Business and reviewed business requirements and analyzing data sources.
> Performed Data collection, Data Cleaning, features scaling, features engineering, validation, Visualize, interpret, report findings, and develop strategic uses of data by python libraries like NumPy, Pandas, SciPy, Scikit-Learn.
> Involved with Recommendation Systems such as Collaborative filtering and content-based filtering.
> Studied and implemented Fraud detection models to monitor the unconventional purchases from customer bases and alert them with updates.
> Analyzed and implemented few research proofs of concept models for Real time fraud detection over credit card and online banking purchases.
> ed with Credit Analysis, Risk modeling algorithms to implement in customer acquisition strategies into the real time business.      
Nationwide Insurance,	Columbus, OH
Data Scientist 						                                       Jan 2012  July 2012 

Policy Payment Default Prediction:
> Built classification models using several features related to customer demographics, macroeconomic dynamics, historic payment behavior, type and size of insurance policy, credit scores and loan to value ratios and with the model predicted the likelihood of default under various stressed conditions.
> Data required extensive cleaning and preparation for machine learning modeling, as some observations were censored without any clear notification
> Carried out segmentation, building predictive models and integrating secondary and primary data using R, python, SPSS and SQL
> Built classification models that predict the probability of a customer's response to a cross-sell campaign using python.
Customer Trial Repeat Analysis:
> Designed and deployed real time Tableau dashboards that identified policies which are most/least liked by the customers using key performance metrics that aided the company for better rationalization of their product offerings.
> Analyzed large data sets for reporting and visualization using R.
> Tracked campaigns and communicate campaign performance and ROI analysis.
> We used modifiers, including L1 regularization, dropout, and Nesterov momentum to enhance the neural net and optimize generalization.
> Provided reporting dashboard for stakeholders and project owners to rapidly provide information via plotly, bokeh, matplotlib, and seaborn.
> We required internally generated data on policies, premiums, and payouts from customer databases, to model policy payout as a function of survival model outputs.
> Retrieved data from devices, which was streamed to our company database using AWS-kinesis (real-time data streaming).
Customer Segmentation:
> Clustered the customers based on demographics, health attributes, policy inclinations using hierarchical clustering models and identified strategies for each of the clusters to better optimize retention, marketing and product offering strategies
> Responsible for managing monitoring and coordinating claims fraud risk management .
> Performed Sentiment Analysis using social media and survey data to address customer grievances and brand awareness using Natural Language Processing (NLP).
> Solved a binary classification problem (transferring to lower risk group or not with given financial incentive) with a logistic regression. 
> An artificial neural net was utilized with Keras/TensorFlow, PyTorch in python to solve binary classification problem for premiums and their intersection with the discriminant.
Social Media Campaign Application:
> For this largest property and casualty insurer in the United States, with over 120 offices located in 54 locations, and offers commercial, property, casualty, specialty and personal insurance services- developed, executed, tracked and analyzed targeted marketing campaigns, Utilized the social media campaign management application to develop and report on complex, multi-step campaigns. Analyze campaign performance, report on key business metrics and develop insights through customer analysis.
> Used unsupervised learning techniques such as K-means clustering and Gaussian Mixture Models to cluster customers into different risk groups based on health parameters provided through wearable technology regarding their activities and health goals
> Multiple statistical modeling approaches were applied to determining the usefulness of the wearable technology data for various insurance products.
> Survival modeling techniques, such as Poisson regression, hidden Markov models, and Cox proportional hazards, were used to model time to different events utilizing wearable data (time to death for life insurance, time to next hospital visit, time to next accident, time to critical illness, etc.)
> Documented methodology, data reports and model results and communicated with the Project Team / Manager and other data scientists to share the knowledge on retention analytics
> Took End-to-end ownership of designing, developing and deploying machine learning models (data preparation => variable selection => model building and evaluation => deployment)

JBS Swift, Greely, CO					          		                                                            Data Modeler/Data Analyst                                                                                   Jun 2010 to Dec 2011

Marketing Campaign Measurement:
> Built executive dashboards in Tableau that measured changes in customer behavior post campaign launch; the ROI measurements helped company to strategically select the effective campaigns 
Credit Risk Scorecards:
> Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex  analysis into easily digestible reports for top executives in the company.
> Developed several interactive dashboards in Tableau to visualize nearly 2 Terabytes of credit data by designing a scalable data cube structure.
Others
> Analyzed large datasets to provide strategic direction to the company. Performed quantitative analysis of ad sales trends to recommend pricing decisions.
> Conducted cost and benefit analysis on new ideas. Scrutinized and tracked customer behavior to identify trends and unmet needs.
> Developed statistical models to forecast inventory and procurement cycles. Assisted in developing internal  for data analysis.
> Achieved a broad spectrum of end results putting into action the ability to find, and interpret rich data sources, merge data sources together, ensure consistency of data-sets, create visualizations to aid in understanding data, build mathematical models using the data, present and communicate the data insights/findings to specialists and scientists in their team
> Implemented full lifecycle in Data Modeler/Data Analyst, Data warehouses and DataMarts with Star Schemas, Snowflake Schemas, and SCD& Dimensional Modeling Erwin.
>  Performed data mining on data using very complex SQL queries and discovered pattern and used extensive SQL for data profiling/analysis to provide guidance in building the data model

HSBC, New York, NY                          
Data Analyst /Data Modeler                                                                                 Oct 2005 to May 2010

> ed with end users and analysts to provide analysis of credit card customer base on demographical basis and forecasting risk. 
> The permissions to grant a credit card to an individual were based on the analysis done. 
> The information of the person applying for the credit card is gathered and processed for further approval or rejection.
> Involved in Analysis & Marketing Team to make business decisions
> Involved with key departments to analyze areas and discuss the primary model requirements for the project
> Documented methodology, data reports and machine learning model results and communicated with the Project Team / Manager to share the knowledge.
> Performed competitor and customer analysis, risk and pricing analysis and forecasted results for credit card holders on demographical basis
> Used Machine Learning algorithms and Natural Language Processing (NLP) for response modeling and fraud detection efforts for Credit cards.
> Developed needs-based segmentation that aided management in gaining a deeper understanding of consumer behavior. These segments assisted management in development and marketing of credit cards.
> Performed machine learning to estimate the probability of a new customer being classified as a good or bad customer.
> Utilized graph clustering algorithms, such as ClusterONE (Clustering with Overlapping Neighborhood Expansion) and MbiRW (measure and Bi-Random walk) which is a k-means-based net cluster algorithm.
> Utilized Natural Language Processing (NLP) techniques, such as term frequency-inverse document frequency (TF-IDF) to measure the importance of terms within the literature when constructing the document net for use with above mentioned graphical clustering algorithms.
> Used machine learning with Theano/Keras to further perform risk assessment of the potential adverse effects for drugs and their motifs. 
> Collaborated with molecular simulation experts to combine literature-based discovery with computer simulations for the purpose of predicting the risk of certain drugs 
> Utilized multiple cross validation frames (k-folds and train-validate-test, where appropriate) to optimize, and ensure generalizability for drug risk assessment.
> Since there is a vast number of features/variables we use principal component analysis (PCA) for dimensionality reduction on large text datasets.
> Design, develop and produce reports that connect quantitative data to insights that drive and change business.
> Supported client by developing Machine Learning Algorithms on big data using PySpark to analyze transaction fraud, Cluster Analysis etc.
> Perform ad hoc custom analysis as needed using SQL and R.
> Designed and published visually rich and intuitively interactive Tableau books and dashboards for executive decision making.
> Maintain and enhance data model with changes and furnish with definitions, notes, reference values and check lists.
> Participated in JAD sessions, gathered information from Business Analysts, end users and other stakeholders to determine the requirements
> Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models
> Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP and OLAP systems. Generated DDL scripts using Forward Engineering technique to create objects and deploy them into the database
> ed with SME's and other stakeholders to determine the requirements to identify Entities and Attributes to build Conceptual, Logical and Physical data Models. 
> Used Star Schema methodologies in building and designing the logical data model into Dimensional Models extensively.


 (Not willing to relocate from NJ, can travel up to 60%, 1-2 days remote for non- NJ jobs, I have a Green Card)

",Data Scientist,resume,"   and Experience: * Ten years of experience in developing different Statistical Machine Learning, Text Analytics, and Data Mining solutions across various business functions: providing BI, Insights and Reporting frame to optimize business outcomes through data analysis. * Experience in Machine Learning, Deep Learning, Data Mining with large datasets of structured and unstructured data, Data Validation, Data acquisition, DataVisualization, Predictive Modeling and developed predictive models that help to provide intelligent solutions. * Strong mathematical knowledge and hands on experience in implementing Machine Learning algorithms like K-Nearest Neighbors, Logistic Regression, Linear regression, Naïve Bayes, Support Vector Machines, Decision Trees, Random Forests, Gradient Boosted Decision Trees, Stacking Models. * Good knowledge of Hadoop Architecture and various components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, Secondary Name Node, MapReduce concepts, and ecosystems including Hive and Pig. * Experience with data visualization using  like GGplot, Matplotlib, Seaborn, Tableau, R Shiny and using Tableau software to publish and presenting dashboards, storyline on web and desktop platforms.  * Experienced in python data manipulation for loading and extraction as well as with python libraries such as NumPy, SciPy and Pandas and Spark 2.0 (PySpark, MLlib) to develop variety of models and algorithms for analytic purposes. * Well experienced in Normalization and Standardization techniques for optimal performance in relational and dimensional database environments. * Thorough grounding in all phases of data analysis (both structured and unstructured), including definition and analysis of questions with respect to available data and resources, overview of data and assessment of data quality, selection of appropriate predictive models, statistical tests and presentation of results. * Proficient in Natural Language Processing (NLP), Text Analytics etc. using R and Python (NLTK, genism, TextBlob etc.). Used NLP- Bag of words / N-gram algorithms, Term-document matrices, Text categorization and text routing, * Processing unstructured text, Processing speech / using speech-to-text algorithms and, data mining and big data algorithms and methods. * Developed machine learning solutions in Natural Language Processing (NLP), document classification, Named Entity Recognition (NER), topic modelling, document summarization, computational linguistics, advanced and semantic information search, extraction, induction, classification and exploration. * Excellent knowledge of Machine Learning, Mathematical Modeling and Operations Research. Comfortable with R, Python, SAS and Weka, MATLAB, Relational databases. Deep understanding & exposure of Big Data Eco-system.   * Excellent knowledge and experience in using open source NLP packages such as NLTK, Word2Vec, SpaCy, Gensim, Standford CoreNLP. * Experience in understanding of NLP/ML & algorithms and models (GLMs, SVM, PCA, NB, Clustering, DTs) and their underlying computational and probabilistic statistics. * Experience building Machine Learning & NLP solutions over open source platforms such as SciKit-Learn,Tensorflow,TensorBoard, Keras, PyTorch, SparkML, Torch, Caffe, H2O * Understanding of different components of Hadoop ecosystem such as Hue, Pig, Hive, HBase, HDFS, Map-reduce, Flume etc. * Hands on Experience on Customer Churn, Sales Forecasting, Market Mix Modeling, Customer Classification, Survival Analysis, Sentiment Analysis, Text Mining, Recommendation Systems. * Experience in using Statistical procedures and Machine Learning algorithms such as ANOVA, Clustering, Regression and Time Series Analysis to analyze data for further Model Building. * Installed and configured Kubernetes and Docker on google cloud platform (GCP) in Dev, Test, Stage and Production Environments   * Configured Microservices on GitHub with Kubernetes deploy file, service file and config maps. * Hands on experience in implementing Dimensionality Reduction Techniques like Truncated SVD, Principal Component Analysis, t-Stochastics Neighborhood Embedding (t-SNE). * Hands on experience on Deep Learning Techniques such as Back Propagation, Choosing Activation Functions, Weight Initialization based on Optimizer, Avoiding Vanishing Gradient and Exploding Gradient Problems, Using Dropout, Regularization and Batch Normalization, Gradient Monitoring and Clipping Padding and Striding, Max pooling, LSTM. * Expertise writing production quality code in SQL, R, Python and Spark. Hands on experience building regression and classification models and other unsupervised learning algorithms with large datasets in distributed systems and resource constrained environments, along with that used MongoDB for extraction data. * Experience in Data Modeling retaining concepts of RDBMS, Logical and Physical Data Modeling until 3NormalForm (3NF) and Multidimensional Data Modeling Schema (Star schema, Snow-Flake Modeling, Facts and dimensions). Hands on experience in optimizing the SQL Queries and database performance tuning in Oracle, SQL Server and Teradata databases. * Experience using multiple ETL  in Data Analysis, Data Migration, Data Cleansing, Transformation, Integration, Data Import, and Data Export such as Ab Initio and Alteryx.    Temple University, Philadelphia, PA  Masters of Science, Electronics and Computer Engineering  RV College of Engineering, Bangalore, India  Bachelors of Engineering, Electronics and Computer Engineering    Statistics/ML Exploratory Data Analysis: Univariate/Multivariate Outlier detection, Missing value imputation, Histograms/Density estimation, EDA in Tableau  Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Elastic Nets, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Deep Neural Nets, Bayesian Learning Unsupervised Learning: Principal Component Analysis, Association Rules, Factor Analysis, K-Means, Hierarchical Clustering, Gaussian Mixture Models, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization Feature Engineering: Stepwise, Recursive Feature Elimination, Relative Importance, Filter Methods, Wrapper Methods and Embedded Methods Statistical Tests: T Test, Chi-Square tests, Stationarity tests, Auto Correlation tests, Normality tests, Residual diagnostics, Partial dependence plots and Anova Sampling Methods: Bootstrap sampling methods and Stratified sampling Model Tuning/Selection: Cross Validation, AUC, Precision/Recall, Walk Forward Estimation, AIC/BIC Criterions, Grid Search and Regularization Time Series: ARIMA, Holt winters, Exponential smoothing, Bayesian structural time series  Machine Learning / Deep Learning R: caret, glmnet, forecast, xgboost, rpart, survival, arules, sqldf, dplyr, nloptr, lpSolve, ggplot Python: pandas, numpy, scikit-learn, scipy, statsmodels, matplotlib, tensorflow SAS: Forecast server, SAS Procedures and Data Steps Spark: MLlib, GraphX SQL: Subqueries, joins, DDL/DML statements Databases/ETL/Query Teradata, SQL Server, Postgres and Hadoop (MapReduce),Cloudera stack; SQL, Hive, Pig and Alteryx,NOSQL,Source Code Management Visualization Tableau, ggplot2 and RShiny Prototyping PowerPoint, RShiny and Tableau EXPERIENCE Computer Sciences Corporation (CSC), Tysons Corner, VA                	              June 2015  Till date  Lead Data Scientist/ SME  Artificial Intelligence + Machine Learning  KEY  App Recommender System: > Developed a personalized recommender system using recommender algorithms (collaborative filtering, low rank matrix factorization) that recommended best apps to a user based on similar user s. The recommendations enabled users to engage better and helped improving the overall user retention rates at CSC Sales Forecasting: > Forecasted sales and improved accuracy by 10-20% by implementing advanced forecasting algorithms that were effective in detecting seasonality and trends in the patterns in addition to incorporating exogenous covariates. Increased accuracy helped business plan better with respect to budgeting and sales and operations planning  Anomaly Detection: > Created interactive dashboard suite that illustrated outlier characteristics across several sales-related dimensions and overall impact of outlier imputation in R (Shiny).Used iterative outlier detection and imputation algorithm using multiple density-based clustering techniques (DBSCAN, kernel density estimation)  Cross Sell and Upsell Opportunity Analysis: > Implemented market basket algorithms from transactional data, which helped identify items used/purchased together frequently. Discovering frequent item sets helped unearth cross sell and upselling opportunities and led to better pricing, bundling and promotion strategies for sales and marketing teams  Customer Churn Prediction: > Predicted the likelihood of customer churn based on customer attributes like customer size, RFM loyalty metrics, revenue, type of industry, competitor products and growth rates etc. The models deployed in production environment helped detect churn in advance and aided sales/marketing teams plan for various retention strategies in advance like price discounts, custom licensing plans.   Customer Purchase Propensity Modeling: > Built machine learning based regression models using scikit-learn python frames to estimate the customer propensity to purchase based on attributes such as customer verticals they operate in, revenue, historic purchases, frequency and recency behaviors. These predictions helped estimate propensities with higher accuracy improving the overall productivity of sales teams by accurately targeting the prospective clients.  Price Elasticity Analysis: > Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, CSC made selective and cautious price cuts for certain licensing categories.  Customer Segmentation: > Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, CSC made selective and cautious price cuts for certain licensing categories. > Customer segmentation based on their behavior or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behavior patterns. > The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers. > Used Principal Component Analysis and t-SNE in feature engineering to analyze high dimensional data.  BB & T Bank, Greensboro, NC                                                                                                                                                         Data Scientist  						                                       July 2013  May 2015  Played a key role in developing and maintaining statistical and machine learning models that mine, analyze and turn BB & T customer and sales data into insights that helped BB & T make strategic decisions that led to growth in their user base and revenue  Customer Participation Analysis: > Built relational databases in SQL server of several flat files of partner information from several large flat files in Python. Used logistics regression and random forests models in R/Python to predict the likelihood of customer participation in various marketing programs.  > Designed and developed visualizations and dashboards in R /Tableau that surfaced the primary factors that drove program participation and identified the best targets for future targeted marketing efforts. > Performed Data Profiling to learn about behaviour with various features such as traffic pattern, location, time, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends. > Involved in various pre-processing phases of text data like Tokenizing, Stemming, Lemmatization and converting the raw text data to structured data. > Personalization, Target Marketing, Customer Segmentation and profiling. Customer Life Time Value Analysis: > Projected customer lifetime values based on historic customer usage and churn rates using survival models. Understanding customer lifetime values helped business to establish strategies to selectively attract customers who tend to be more profitable for BB & T.  > It also helped business to establish appropriate marketing strategies based on customer values. > Performed Data Cleaning, features scaling, featurization, features engineering. > Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN. > Implemented number of Natural Language process mechanism for chatbot. > Customer segmentation based on their behaviour or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behaviour patterns. Customer Segmentation: > Developed 11 customer segments using unsupervised learning techniques like KMeans. > The clusters helped business simplify complex patterns to manageable set of 11 patterns that helped set strategic and tactical objectives pertaining to customer retention, acquisition and spend  Price Optimization and Revenue management: > Measured the price elasticity for products that experienced price cuts and promotions using regression methods; based on the elasticity, BB&T made selective and cautious price cuts for certain licensing categories > The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers. > Performed Clustering with historical, demographic and behavioural data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device. > Analyzed high volume, high dimensional client and survey data from different sources using SAS and R. > Used Principal Component Analysis and t-SNE in feature engineering to analyse high dimensional data.  Bank of America, Jersey City, NJ  Data Scientist 		         				                                        Aug 2012 - June 2013  Played key role in developing and deploying DFAST Stress Test models across several bank portfolios. Provided architectural leadership on several high priority initiatives including account prioritization, account prospecting, and opportunity scoring. Drove the creation of comprehensive datasets encompassing user s and behaviors, and incorporating a wide variety of signals and data types.  Forecasting Loan balance: > Forecasted bank-wide loan balances under normal and stressed macroeconomic scenarios using R. Performed variable reduction using the stepwise, lasso, and elastic net algorithms and tuned the models for accuracy using cross validation and grid search techniques.  Top down Models (Commercial Real Estate): > Automated the scraping and cleaning of data from various data sources in R and Python. Developed Bankss loss forecasting process using relevant forecasting and regression algorithms in R.  > The projected losses under stress conditions helped bank reserve enough funds per DFAST policies.  Fraud Detection: > This project includes creation of statistical machine learning models which implements Customer Segmentation, Customer Lifetime Value, Fraud detection, Sales Forecasting, Anomaly Detection, Customer churn prediction, Elasticity Analysis, propensity Modeling. > Gathered requirements from Business and reviewed business requirements and analyzing data sources. > Performed Data collection, Data Cleaning, features scaling, features engineering, validation, Visualize, interpret, report findings, and develop strategic uses of data by python libraries like NumPy, Pandas, SciPy, Scikit-Learn. > Involved with Recommendation Systems such as Collaborative filtering and content-based filtering. > Studied and implemented Fraud detection models to monitor the unconventional purchases from customer bases and alert them with updates. > Analyzed and implemented few research proofs of concept models for Real time fraud detection over credit card and online banking purchases. > ed with Credit Analysis, Risk modeling algorithms to implement in customer acquisition strategies into the real time business.       Nationwide Insurance,	Columbus, OH Data Scientist 						                                       Jan 2012  July 2012   Policy Payment Default Prediction: > Built classification models using several features related to customer demographics, macroeconomic dynamics, historic payment behavior, type and size of insurance policy, credit scores and loan to value ratios and with the model predicted the likelihood of default under various stressed conditions. > Data required extensive cleaning and preparation for machine learning modeling, as some observations were censored without any clear notification > Carried out segmentation, building predictive models and integrating secondary and primary data using R, python, SPSS and SQL > Built classification models that predict the probability of a customer's response to a cross-sell campaign using python. Customer Trial Repeat Analysis: > Designed and deployed real time Tableau dashboards that identified policies which are most/least liked by the customers using key performance metrics that aided the company for better rationalization of their product offerings. > Analyzed large data sets for reporting and visualization using R. > Tracked campaigns and communicate campaign performance and ROI analysis. > We used modifiers, including L1 regularization, dropout, and Nesterov momentum to enhance the neural net and optimize generalization. > Provided reporting dashboard for stakeholders and project owners to rapidly provide information via plotly, bokeh, matplotlib, and seaborn. > We required internally generated data on policies, premiums, and payouts from customer databases, to model policy payout as a function of survival model outputs. > Retrieved data from devices, which was streamed to our company database using AWS-kinesis (real-time data streaming). Customer Segmentation: > Clustered the customers based on demographics, health attributes, policy inclinations using hierarchical clustering models and identified strategies for each of the clusters to better optimize retention, marketing and product offering strategies > Responsible for managing monitoring and coordinating claims fraud risk management . > Performed Sentiment Analysis using social media and survey data to address customer grievances and brand awareness using Natural Language Processing (NLP). > Solved a binary classification problem (transferring to lower risk group or not with given financial incentive) with a logistic regression.  > An artificial neural net was utilized with Keras/TensorFlow, PyTorch in python to solve binary classification problem for premiums and their intersection with the discriminant. Social Media Campaign Application: > For this largest property and casualty insurer in the United States, with over 120 offices located in 54 locations, and offers commercial, property, casualty, specialty and personal insurance services- developed, executed, tracked and analyzed targeted marketing campaigns, Utilized the social media campaign management application to develop and report on complex, multi-step campaigns. Analyze campaign performance, report on key business metrics and develop insights through customer analysis. > Used unsupervised learning techniques such as K-means clustering and Gaussian Mixture Models to cluster customers into different risk groups based on health parameters provided through wearable technology regarding their activities and health goals > Multiple statistical modeling approaches were applied to determining the usefulness of the wearable technology data for various insurance products. > Survival modeling techniques, such as Poisson regression, hidden Markov models, and Cox proportional hazards, were used to model time to different events utilizing wearable data (time to death for life insurance, time to next hospital visit, time to next accident, time to critical illness, etc.) > Documented methodology, data reports and model results and communicated with the Project Team / Manager and other data scientists to share the knowledge on retention analytics > Took End-to-end ownership of designing, developing and deploying machine learning models (data preparation => variable selection => model building and evaluation => deployment)  JBS Swift, Greely, CO					          		                                                            Data Modeler/Data Analyst                                                                                   Jun 2010 to Dec 2011  Marketing Campaign Measurement: > Built executive dashboards in Tableau that measured changes in customer behavior post campaign launch; the ROI measurements helped company to strategically select the effective campaigns  Credit Risk Scorecards: > Built credit risk scorecards and marketing response models using SQL and SAS. Evangelized the complex  analysis into easily digestible reports for top executives in the company. > Developed several interactive dashboards in Tableau to visualize nearly 2 Terabytes of credit data by designing a scalable data cube structure. Others > Analyzed large datasets to provide strategic direction to the company. Performed quantitative analysis of ad sales trends to recommend pricing decisions. > Conducted cost and benefit analysis on new ideas. Scrutinized and tracked customer behavior to identify trends and unmet needs. > Developed statistical models to forecast inventory and procurement cycles. Assisted in developing internal  for data analysis. > Achieved a broad spectrum of end results putting into action the ability to find, and interpret rich data sources, merge data sources together, ensure consistency of data-sets, create visualizations to aid in understanding data, build mathematical models using the data, present and communicate the data insights/findings to specialists and scientists in their team > Implemented full lifecycle in Data Modeler/Data Analyst, Data warehouses and DataMarts with Star Schemas, Snowflake Schemas, and SCD& Dimensional Modeling Erwin. >  Performed data mining on data using very complex SQL queries and discovered pattern and used extensive SQL for data profiling/analysis to provide guidance in building the data model  HSBC, New York, NY                           Data Analyst /Data Modeler                                                                                 Oct 2005 to May 2010  > ed with end users and analysts to provide analysis of credit card customer base on demographical basis and forecasting risk.  > The permissions to grant a credit card to an individual were based on the analysis done.  > The information of the person applying for the credit card is gathered and processed for further approval or rejection. > Involved in Analysis & Marketing Team to make business decisions > Involved with key departments to analyze areas and discuss the primary model requirements for the project > Documented methodology, data reports and machine learning model results and communicated with the Project Team / Manager to share the knowledge. > Performed competitor and customer analysis, risk and pricing analysis and forecasted results for credit card holders on demographical basis > Used Machine Learning algorithms and Natural Language Processing (NLP) for response modeling and fraud detection efforts for Credit cards. > Developed needs-based segmentation that aided management in gaining a deeper understanding of consumer behavior. These segments assisted management in development and marketing of credit cards. > Performed machine learning to estimate the probability of a new customer being classified as a good or bad customer. > Utilized graph clustering algorithms, such as ClusterONE (Clustering with Overlapping Neighborhood Expansion) and MbiRW (measure and Bi-Random walk) which is a k-means-based net cluster algorithm. > Utilized Natural Language Processing (NLP) techniques, such as term frequency-inverse document frequency (TF-IDF) to measure the importance of terms within the literature when constructing the document net for use with above mentioned graphical clustering algorithms. > Used machine learning with Theano/Keras to further perform risk assessment of the potential adverse effects for drugs and their motifs.  > Collaborated with molecular simulation experts to combine literature-based discovery with computer simulations for the purpose of predicting the risk of certain drugs  > Utilized multiple cross validation frames (k-folds and train-validate-test, where appropriate) to optimize, and ensure generalizability for drug risk assessment. > Since there is a vast number of features/variables we use principal component analysis (PCA) for dimensionality reduction on large text datasets. > Design, develop and produce reports that connect quantitative data to insights that drive and change business. > Supported client by developing Machine Learning Algorithms on big data using PySpark to analyze transaction fraud, Cluster Analysis etc. > Perform ad hoc custom analysis as needed using SQL and R. > Designed and published visually rich and intuitively interactive Tableau books and dashboards for executive decision making. > Maintain and enhance data model with changes and furnish with definitions, notes, reference values and check lists. > Participated in JAD sessions, gathered information from Business Analysts, end users and other stakeholders to determine the requirements > Designed the Data Warehouse and MDM hub Conceptual, Logical and Physical data models > Used Normalization methods up to 3NF and De-normalization techniques for effective performance in OLTP and OLAP systems. Generated DDL scripts using Forward Engineering technique to create objects and deploy them into the database > ed with SME's and other stakeholders to determine the requirements to identify Entities and Attributes to build Conceptual, Logical and Physical data Models.  > Used Star Schema methodologies in building and designing the logical data model into Dimensional Models extensively.    (Not willing to relocate from NJ, can travel up to 60%, 1-2 days remote for non- NJ jobs, I have a Green Card)  "
" 
 
~  and  ~
 	 
- Python 	- 	Molecular Dynamics Simulation 
- R 	- 	Machine Learning 
- RNA-Seq Analysis 	- 	MATLAB 
- SQL 	- 	Affinity Chromatography 
- Linux 	- 	Electroporation 
- Cheminformatics 	- 	FRET 
- Chimera 	- 	Gel Electrophoresis 
- COMSOL 	- 	Next Generation Sequencing 
- Solids 	- 	Polymerase Chain Reaction (PCR) 
- Swift Programming 	- 	Protein Purification 
 	 
~  ~ 

 
University of California, Riverside | Bachelor of Science in Bioengineering,  Riverside, CA                                                                  2015  2019 
GPA: 3.64 (Graduated Cum Laude)  
 
Completed Coursera Classes: Python for Genomic Data Science, Genomic Data Science with Galaxy, Algorithms for DNA Sequencing, Command Line  for Genomic Data Science, Bioconductor for Genomic Data Science, and Statistics for Genomic Data Science.  
                                                              
 ~ s ~ 

   Girke Bioinformatics Research Lab | University of California, Riverside                                                             August 2018  June 2019 
 Utilized ChemmineR package to parse through sdf files from ChEMBL database.  
 Implemented RSQLite and dplyr libraries in R to develop DrugBankR database.  
 Utilized Linux to submit and manage R scripts to High-Performance Computing Center (HPCC) at UCR.  
 Developed SQL functions to query the DrugBankR database for various assays and drugs.  
 Used RNA-Seq flow and systemPipeR library to analyze 18 paired-end read sets from Arabidposis thaliana. 
 
PillTrac: Capstone Project | University of California, Riverside                                                                            August 2018  June 2019 
 Collaborated with team members to develop blister pack with integrated NFC microcontroller to monitor patient compliance. 
 Utilized Eagle to create circuit designs for Printed Circuit Board (PCB) Manufacturing.  
 Developed LED matrix circuit with Arduino NFC module. 
 Utilized Swift to program iOS 12 smartphone application for monitoring medication regimen. 
 Developed business plan to determine financial viability of product.  
 
	Undergraduate  Research Fellow | University of California, Riverside 	        July 2017  June 2018 
 Obtained $5000 in funding as a stipend to support financial expenses of research project. 
 Computationally analyzed C-Reactive Protein (CRP) and Phosphocholine (PC) to quantify effectiveness in immune system. 
 Utilized Chimera to visualize physiological properties of CRP and PC. 
 Performed Molecular Dynamics simulations to visualize molecular fluctuations between CRP and PC with Linux commands. 
 Developed Python scripts to analyze trajectories of Molecular Dynamics Simulations.  
 Presented research at the 2018 UCR Undergraduate Research Symposium in Riverside, CA. 
 
BioMoDeL Research Lab | University of California, Riverside 	               July 2016  July 2018 
 Computationally analyzed the hormone resistin to evaluate compatibility as a biotherapeutic drug. 
 Applied the PDBePISA software to define stabilizing intermolecular interactions in resistin. 
 Utilized the bioinformatics tool AESOP to investigate electrostatic interactions between amino acids in resistin. 
 Published paper in the 11th edition of the UCR Undergraduate Research Journal.  
 Presented research at the 2017 Southern California Conferences for Undergraduate Research in Pomona, CA. 
 
Bio 155 Lab | University of California, Riverside                                                                       October 2018  December 2018 
 Transformed the CyPet-SUMO1 gene into Top-10 E.Coli cells via electroporation. 
 Verified CyPet-SUMO1 gene sequence through PCR and gel electrophoresis 
 Analyzed coding regions of pET28(b)-CyPet-SUMO1 using CLC Viewer and Chromas. 
 Performed protein expression and purification through centrifugation and affinity chromatography. 
 Measured energy transfer from CyPet-SUMO1 to YPet-Ubc9 through FRET assay.    ",Data Scientist,resume,"    ~  and  ~  	  - Python 	- 	Molecular Dynamics Simulation  - R 	- 	Machine Learning  - RNA-Seq Analysis 	- 	MATLAB  - SQL 	- 	Affinity Chromatography  - Linux 	- 	Electroporation  - Cheminformatics 	- 	FRET  - Chimera 	- 	Gel Electrophoresis  - COMSOL 	- 	Next Generation Sequencing  - Solids 	- 	Polymerase Chain Reaction (PCR)  - Swift Programming 	- 	Protein Purification   	  ~  ~     University of California, Riverside | Bachelor of Science in Bioengineering,  Riverside, CA                                                                  2015  2019  GPA: 3.64 (Graduated Cum Laude)     Completed Coursera Classes: Python for Genomic Data Science, Genomic Data Science with Galaxy, Algorithms for DNA Sequencing, Command Line  for Genomic Data Science, Bioconductor for Genomic Data Science, and Statistics for Genomic Data Science.                                                                   ~ s ~      Girke Bioinformatics Research Lab | University of California, Riverside                                                             August 2018  June 2019   Utilized ChemmineR package to parse through sdf files from ChEMBL database.    Implemented RSQLite and dplyr libraries in R to develop DrugBankR database.    Utilized Linux to submit and manage R scripts to High-Performance Computing Center (HPCC) at UCR.    Developed SQL functions to query the DrugBankR database for various assays and drugs.    Used RNA-Seq flow and systemPipeR library to analyze 18 paired-end read sets from Arabidposis thaliana.    PillTrac: Capstone Project | University of California, Riverside                                                                            August 2018  June 2019   Collaborated with team members to develop blister pack with integrated NFC microcontroller to monitor patient compliance.   Utilized Eagle to create circuit designs for Printed Circuit Board (PCB) Manufacturing.    Developed LED matrix circuit with Arduino NFC module.   Utilized Swift to program iOS 12 smartphone application for monitoring medication regimen.   Developed business plan to determine financial viability of product.     	Undergraduate  Research Fellow | University of California, Riverside 	        July 2017  June 2018   Obtained $5000 in funding as a stipend to support financial expenses of research project.   Computationally analyzed C-Reactive Protein (CRP) and Phosphocholine (PC) to quantify effectiveness in immune system.   Utilized Chimera to visualize physiological properties of CRP and PC.   Performed Molecular Dynamics simulations to visualize molecular fluctuations between CRP and PC with Linux commands.   Developed Python scripts to analyze trajectories of Molecular Dynamics Simulations.    Presented research at the 2018 UCR Undergraduate Research Symposium in Riverside, CA.    BioMoDeL Research Lab | University of California, Riverside 	               July 2016  July 2018   Computationally analyzed the hormone resistin to evaluate compatibility as a biotherapeutic drug.   Applied the PDBePISA software to define stabilizing intermolecular interactions in resistin.   Utilized the bioinformatics tool AESOP to investigate electrostatic interactions between amino acids in resistin.   Published paper in the 11th edition of the UCR Undergraduate Research Journal.    Presented research at the 2017 Southern California Conferences for Undergraduate Research in Pomona, CA.    Bio 155 Lab | University of California, Riverside                                                                       October 2018  December 2018   Transformed the CyPet-SUMO1 gene into Top-10 E.Coli cells via electroporation.   Verified CyPet-SUMO1 gene sequence through PCR and gel electrophoresis   Analyzed coding regions of pET28(b)-CyPet-SUMO1 using CLC Viewer and Chromas.   Performed protein expression and purification through centrifugation and affinity chromatography.   Measured energy transfer from CyPet-SUMO1 to YPet-Ubc9 through FRET assay.    "
"M.S. Statistics graduate looking for a full-time job in data science, machine learning, statistics or quantitative analysis. I am only considering the Los Angeles Metropolitan Area.
Authorized to  in the US for any employer
 Experience

Statistics Intern
Penumbra, Inc - Alameda, CA
May 2019 to Present
ing closely with the statistics team to automate repetitive SAS processes. 
Building the global and local SAS macro library for the statistics team. 
Creating, editing, testing and debugging SAS macros. 
Writing documentation detailing description, purpose, parameter definitions, execution requirements and expected output for each macro. 
Version-controlling SAS macro programs using Git with GitLab
Economic Policy Intern
Environmental Defense Fund - San Francisco, CA
February 2019 to May 2019
Created complex structured datasets from raw unstructured datasets.  
Data wrangling, cleaning and analyzing using R.  
Created raw unstructured datasets from raster and shape GIS files.  
Resampling, spatial joining and calculating zonal statistics using ArcGIS.  
Performed test runs to ensure no defects in the columns, rows or values in the datasets. Maintained version-controlled and thoroughly documented R notebooks.
Data Science Student Intern
State Compensation Insurance Fund - Pleasanton, CA
June 2018 to August 2018
Helped the predicative analytics and modeling team with the implementation of a new actuarial data mart through user acceptance testing. 
Performed UAT test runs on the claims payment aging by date of injury table. 
Debugged source scripts and target scripts to verify data matching test case between two different database schemas for all claims. 
Used pass through queries in SAS to use proc compare to compare the source script and target script tables. 
Solved all defects with exact match between the tables. 
Performed sanity checks to verify valid mathematical relationships between the facts in the claims payment aging table for all claims. 
Documented test runs of test cases with version controlled SAS programs in HP Application Lifecycle Management for possible review by the internal audit team.
SAT Rater
al - Los Angeles, CA
August 2016 to August 2017
Reading and scoring student SAT essays by rubric Completed 16 hours of holistic scoring training in scoring reading, writing, and analysis
Student Records Technician
University of Southern California - Los Angeles, CA October 2016 to February 2017
Processed undergraduate and graduate application materials for admissions review for national and international students. 
Sorted, filed, labeled, indexed and barcoded applications, transcripts, degrees, letters of recommendation, test scores, passports, correspondences, resumes, financial statements and statement of purposes. 
Troubleshooted application materials for potential mistakes in sorting, labeling, indexing and barcoding.
Accounting Specialist
Breakdown Services, Ltd. - Los Angeles, CA December 2015 to May 2016
Accounted for payments, invoices, accounts receivable, and accounts payable using Quickbooks. 
Received payments for services from customers and paid bills to vendors. 
Completed monthly bank reconciliations to accounts. 
Created daily, monthly, and yearly sales reports on Microsoft Excel. Created sales receipts for items purchased by customers.
Engagement Management Coordinator Support
KPMG - Los Angeles, CA
April 2014 to July 2014
Provided accounting, analytical, administrative, and informative support for the tax, audit, and advisory engagements of the firm. 
Accounted for the payments, invoices, accounts receivable, accounts payable of the firms engagements. 
Financial analysis with the different financial data of engagements. 
Set up, approved, and closed out engagements for the engagement team. 
Served as an information reference for firm related accounting and risk management systems.


Master's in Statistics
California State University, East Bay - Hayward, CA
August 2017 to June 2019
Bachelor of Arts in Philosophy and Psychology
California State University, East Bay - Hayward, CA
2012 to 2014


Data Analysis (2 years), Microsoft Office (8 years), Statistical Analysis (3 years), SAS (2 years), Python
(2 years), SQL (2 years), Big Data (2 years), Data Visualization (2 years), Hadoop (2 years), Bayesian Statistics (2 years)
Links

",Data Scientist,resume,"M.S. Statistics graduate looking for a full-time job in data science, machine learning, statistics or quantitative analysis. I am only considering the Los Angeles Metropolitan Area. Authorized to  in the US for any employer  Experience  Statistics Intern Penumbra, Inc - Alameda, CA May 2019 to Present ing closely with the statistics team to automate repetitive SAS processes.  Building the global and local SAS macro library for the statistics team.  Creating, editing, testing and debugging SAS macros.  Writing documentation detailing description, purpose, parameter definitions, execution requirements and expected output for each macro.  Version-controlling SAS macro programs using Git with GitLab Economic Policy Intern Environmental Defense Fund - San Francisco, CA February 2019 to May 2019 Created complex structured datasets from raw unstructured datasets.   Data wrangling, cleaning and analyzing using R.   Created raw unstructured datasets from raster and shape GIS files.   Resampling, spatial joining and calculating zonal statistics using ArcGIS.   Performed test runs to ensure no defects in the columns, rows or values in the datasets. Maintained version-controlled and thoroughly documented R notebooks. Data Science Student Intern State Compensation Insurance Fund - Pleasanton, CA June 2018 to August 2018 Helped the predicative analytics and modeling team with the implementation of a new actuarial data mart through user acceptance testing.  Performed UAT test runs on the claims payment aging by date of injury table.  Debugged source scripts and target scripts to verify data matching test case between two different database schemas for all claims.  Used pass through queries in SAS to use proc compare to compare the source script and target script tables.  Solved all defects with exact match between the tables.  Performed sanity checks to verify valid mathematical relationships between the facts in the claims payment aging table for all claims.  Documented test runs of test cases with version controlled SAS programs in HP Application Lifecycle Management for possible review by the internal audit team. SAT Rater al - Los Angeles, CA August 2016 to August 2017 Reading and scoring student SAT essays by rubric Completed 16 hours of holistic scoring training in scoring reading, writing, and analysis Student Records Technician University of Southern California - Los Angeles, CA October 2016 to February 2017 Processed undergraduate and graduate application materials for admissions review for national and international students.  Sorted, filed, labeled, indexed and barcoded applications, transcripts, degrees, letters of recommendation, test scores, passports, correspondences, resumes, financial statements and statement of purposes.  Troubleshooted application materials for potential mistakes in sorting, labeling, indexing and barcoding. Accounting Specialist Breakdown Services, Ltd. - Los Angeles, CA December 2015 to May 2016 Accounted for payments, invoices, accounts receivable, and accounts payable using Quickbooks.  Received payments for services from customers and paid bills to vendors.  Completed monthly bank reconciliations to accounts.  Created daily, monthly, and yearly sales reports on Microsoft Excel. Created sales receipts for items purchased by customers. Engagement Management Coordinator Support KPMG - Los Angeles, CA April 2014 to July 2014 Provided accounting, analytical, administrative, and informative support for the tax, audit, and advisory engagements of the firm.  Accounted for the payments, invoices, accounts receivable, accounts payable of the firms engagements.  Financial analysis with the different financial data of engagements.  Set up, approved, and closed out engagements for the engagement team.  Served as an information reference for firm related accounting and risk management systems.   Master's in Statistics California State University, East Bay - Hayward, CA August 2017 to June 2019 Bachelor of Arts in Philosophy and Psychology California State University, East Bay - Hayward, CA 2012 to 2014   Data Analysis (2 years), Microsoft Office (8 years), Statistical Analysis (3 years), SAS (2 years), Python (2 years), SQL (2 years), Big Data (2 years), Data Visualization (2 years), Hadoop (2 years), Bayesian Statistics (2 years) Links  "
"
Im a data scientist with a background in neural nets, ML models, and cloud computing. I have experience in analytic data frames using Python, SQL, and Excel. With an aptitude for math, my  is to build insightful yet pragmatic machine learning algorithms with a collaborative and driven team. 

 | 
Analysis Stack: Python 3.7 (Pandas, Numpy, Scikit- Learn, Keras, Tensorflow), SQL, Excel
Data Visualization: Python (Matplotlib, Seaborn), Excel, Tableau
Machine Learning: Regression Models, Classifications Models, Ensembles, Dimensionality Reduction (PCA), Clustering, Recommender Systems, Time Series Models, Parameter Tuning, Pipeline
Deep Learning: Keras Neural Nets (RNN, CNN, LSTM)
Natural Language Processing: Python (NLTK Stemming, TfidfVectorizer, VaderSentiment)
Data Science flow: Jupyter Notebook, GitHub, Databricks Cluster
Cloud Computing: AWS EC2, GCP Compute
Customer Relationship Management: Hubspot


Image Classification of Fruits and Vegetables 
 : Classify grocery items using convolutional neural nets 
 Executed and parameter tuned CNN modeling on processed images on Google Cloud Platform Compute. 
 Process: Flickr API, PIL, cv2, Google Cloud Platform, Keras CNN, ImageNet, Flask
 Results: Validation Loss score of .3988, Validation Accuracy score of .9183
Customer Data Metrics and Predictions  
 : Target key metrics to visualize and predict consumer behavior
 Executed feature engineering to implement several customer segmentations to predict customer lifetime value, churn probability, and next day purchase.
 Process: KMeans Clustering, Elbow Curve, Plotly, XGB Classifier, Cross Val Score, Classification Reports

Sales Forecast 
 : Predict future quantity sales
 Conducted times-series feature engineering to perform LSTM modeling to forecast sales quantity for the next six months 
 Process: Keras, EarlyStopping, Adler Test, MinMaxScaler, Inverse Transformation, Plotly 
 Results: Validation Loss score (MSE) of 0.0116, RMSE of 29,665


LLove Inc. 	    August 2018 - May 2019
Fashion wholesale company 
Sales Analyst 
 Analyzed sales report for each trade show with prior years performance, showcased customer retention rate, new customers, and average sales per order 
 Initiated market segmentation to target specific clients and personalize email campaigns 
 Responsible for creating sales dashboards in Excel for upper management
 Taught e-commerce associates Excel  and functions to avoid repetitive tasks (Ku add-on, vlookup, concat, pivot)- saving estimated 1.5 hrs of  per day per employee 
 Created and launched email campaigns by implementing merge tag customization- improving opening rate from 5% to 35% on Mailchimp

SupplyShift 	September 2017  January 2018
Software company focusing on supply chain management traceability, and decision-making
Finance and Operations Intern
 Evaluated quarterly net income to calculate Profits & Loss and Budget variance analysis
 Performed data scrubbing, and created data visualizations of Key Factors  of 2017 for the Board of Directors 
 Tracked and computed average sales cycle (in months) for each stage in SupplyShifts software supply chain (using Hubspot CRM) to elongate customer use 


General Assembly	  Los Angeles, CA
Data Science Immersive                                                                                                                           	2019                     
	  
University of California, Santa Cruz	 Santa Cruz, CA
Bachelors of Arts, Business Economics,  Information Management minor	2018

",Data Scientist,resume," Im a data scientist with a background in neural nets, ML models, and cloud computing. I have experience in analytic data frames using Python, SQL, and Excel. With an aptitude for math, my  is to build insightful yet pragmatic machine learning algorithms with a collaborative and driven team.    |  Analysis Stack: Python 3.7 (Pandas, Numpy, Scikit- Learn, Keras, Tensorflow), SQL, Excel Data Visualization: Python (Matplotlib, Seaborn), Excel, Tableau Machine Learning: Regression Models, Classifications Models, Ensembles, Dimensionality Reduction (PCA), Clustering, Recommender Systems, Time Series Models, Parameter Tuning, Pipeline Deep Learning: Keras Neural Nets (RNN, CNN, LSTM) Natural Language Processing: Python (NLTK Stemming, TfidfVectorizer, VaderSentiment) Data Science flow: Jupyter Notebook, GitHub, Databricks Cluster Cloud Computing: AWS EC2, GCP Compute Customer Relationship Management: Hubspot   Image Classification of Fruits and Vegetables   : Classify grocery items using convolutional neural nets   Executed and parameter tuned CNN modeling on processed images on Google Cloud Platform Compute.   Process: Flickr API, PIL, cv2, Google Cloud Platform, Keras CNN, ImageNet, Flask  Results: Validation Loss score of .3988, Validation Accuracy score of .9183 Customer Data Metrics and Predictions    : Target key metrics to visualize and predict consumer behavior  Executed feature engineering to implement several customer segmentations to predict customer lifetime value, churn probability, and next day purchase.  Process: KMeans Clustering, Elbow Curve, Plotly, XGB Classifier, Cross Val Score, Classification Reports  Sales Forecast   : Predict future quantity sales  Conducted times-series feature engineering to perform LSTM modeling to forecast sales quantity for the next six months   Process: Keras, EarlyStopping, Adler Test, MinMaxScaler, Inverse Transformation, Plotly   Results: Validation Loss score (MSE) of 0.0116, RMSE of 29,665   LLove Inc. 	    August 2018 - May 2019 Fashion wholesale company  Sales Analyst   Analyzed sales report for each trade show with prior years performance, showcased customer retention rate, new customers, and average sales per order   Initiated market segmentation to target specific clients and personalize email campaigns   Responsible for creating sales dashboards in Excel for upper management  Taught e-commerce associates Excel  and functions to avoid repetitive tasks (Ku add-on, vlookup, concat, pivot)- saving estimated 1.5 hrs of  per day per employee   Created and launched email campaigns by implementing merge tag customization- improving opening rate from 5% to 35% on Mailchimp  SupplyShift 	September 2017  January 2018 Software company focusing on supply chain management traceability, and decision-making Finance and Operations Intern  Evaluated quarterly net income to calculate Profits & Loss and Budget variance analysis  Performed data scrubbing, and created data visualizations of Key Factors  of 2017 for the Board of Directors   Tracked and computed average sales cycle (in months) for each stage in SupplyShifts software supply chain (using Hubspot CRM) to elongate customer use    General Assembly	  Los Angeles, CA Data Science Immersive                                                                                                                           	2019                      	   University of California, Santa Cruz	 Santa Cruz, CA Bachelors of Arts, Business Economics,  Information Management minor	2018  "
"
:
* Machine learning in R, and other software
* Python and R programming
* Bioinformatics research, crime database research, real estate research
* Able to extract data from large, free, and available public data bases
* Clean data, organize data, create new fields to help analyze data
* Spreadsheets and database navigator
* filtering and merging of datasets for building relevant reports
* formula building in software to run programs to report credit accurately
* Tableau, Excel Advanced features
* Experience with Virtual Machines with VirtualBox, Cloudera, Hortons Ambari, Microsoft Azure, Apache Spark, Amazon Web Services (AWS) EC2, Linux CLI, SSH
* Experience developing application programs in Hadoop, R, and Python   
* Windows OS, Linux OS, Microsoft Office, Excel, Access, Python, R-Studio, SQL, HDFS, Stata, Html, Google Docs

:
       Master of Science 
       Data Science: Bioinformatics			Expected Graduation:  October 2019
       Lewis University, Romeoville, IL			
       Concentration:  Data Science
       
       Bachelor of Science					Graduated: December 2015
       University of California, San Diego, CA		
       Majors:  Economics and Math

 :  
* Used Linux in VMs to upload using SSH in HDFS, then ran HQL queries to analyze database information on airline arrivals and departures: https://themassagenegotiator.com/blog/f/example-codes-of-python-3-hql-t-sql-and-r 
* Set up different user account and permissions in AWS S3, also using AWS EC2 for cloud computing: https://themassagenegotiator.com/blog/f/aws-amazon-web-services-ec2-elastic-cloud-computing-vm 
* Analyzed crime statistics from 12 policing metropolitan cities and real estate statistics from Zillow using Tableau, Python, Excel, and R: https://themassagenegotiator.com/blog/f/cash-flow-wage-gap-and-theft-crimes-per-top-14-metros 
* Online computational research using GEO, BLAST, ENCODE, PFAM and the like: https://github.com/JanJanJan2018/UL_research_files_Final_Week_Due 
* After ing for a credit reporting agency, I developed a fictitious client database to help with formatting of credit reporting information using the Metro2 standards using random number generators and extracted data over the web of popular names of people and streets using Python and Excel: https://themassagenegotiator.com/blog/f/building-a-fictitious-database-for-credit-reporting-program
* 

* Exploratory analysis on word mining from text documents using R and Orange Biolab software for machine learning on document classification based on text mining: https://themassagenegotiator.com/blog/f/the-bio-lab-orange-word-cloud-software and https://github.com/JanJanJan2018/ufcNFLjloABkylieHaterComments/blob/master/FileRenamePasteName.R 

 EXPERIENCE:

* Crediauto Financial: Database Analyst and Credit Reporting Analyst | | April 2018-July 2018 employee | 2150 Palomar Airport Rd.Suite 209, Carlsbad, CA 92011 | supervisor: Michael Fattahi | 858-382-4077
* Massage Therapist: themassagenegotiator.com | 12+ years independent and employed. Various places employed or contracted with wages from minimum plus tips to only commission plus tips, or minimum plus service, commission, and tips. 
* Massage Heights | Chino Hills 91709 | 13925 City Center Dr. | May 2019  Present | Job Title: LMT | Supervisor: Tamera | 909-536-1477
* Massage Heights | Rancho Cucamonga 91739 | 12188 Foothill Blvd. | Mar 2019  June 2019 | Job Title: LMT | Supervisor: Phil | 909-922-2013
* Heller Chiropractic | Costa Mesa 92626 | 2900 Bristol St. | Feb 2019  Mar 2019 | Job Title: Independent LMT | Supervisor: Nastassia | 714-557-9454
* OC Wellness Center | Westminster  92683 | 14120 Beach Blvd. #214 | Sep 2018  Nov 2018 | Job Title: Independent LMT | Supervisor: Anna | 866-303-9355
* Massage Heights | Newport Beach 92660 | 1334 Bison Ave. | July 2017  Present | Job Title: LMT | Supervisor: Michelle or Kristin | 949-644-7352
* Massage Green | Corona 92881| 1312 E. Ontario Ave. | January 11, 2016  Present | Job Title: Licensed Massage Therapist CA | Supervisor: Libni | (951) 371-7918
* Hand and Stone Massage | Costa Mesa 92627| September 2014-January 2015 | Job Title: Licensed Massage Therapist | Supervisor: Tasha | (949) 645-4823 
* Massage Envy | Yorba Linda 92886 | November 2013-July 2014 | Job Title: Licensed Massage Therapist | Supervisor: Colleen | (714) 701-0200
* Massage Envy | Tustin 92782 | June 2012-October 2013 | Job Title: Licensed Massage Therapist | Supervisor: Brittany | (714) 617-8900 

",Data Scientist,resume," : * Machine learning in R, and other software * Python and R programming * Bioinformatics research, crime database research, real estate research * Able to extract data from large, free, and available public data bases * Clean data, organize data, create new fields to help analyze data * Spreadsheets and database navigator * filtering and merging of datasets for building relevant reports * formula building in software to run programs to report credit accurately * Tableau, Excel Advanced features * Experience with Virtual Machines with VirtualBox, Cloudera, Hortons Ambari, Microsoft Azure, Apache Spark, Amazon Web Services (AWS) EC2, Linux CLI, SSH * Experience developing application programs in Hadoop, R, and Python    * Windows OS, Linux OS, Microsoft Office, Excel, Access, Python, R-Studio, SQL, HDFS, Stata, Html, Google Docs  :        Master of Science         Data Science: Bioinformatics			Expected Graduation:  October 2019        Lewis University, Romeoville, IL			        Concentration:  Data Science                Bachelor of Science					Graduated: December 2015        University of California, San Diego, CA		        Majors:  Economics and Math   :   * Used Linux in VMs to upload using SSH in HDFS, then ran HQL queries to analyze database information on airline arrivals and departures: https://themassagenegotiator.com/blog/f/example-codes-of-python-3-hql-t-sql-and-r  * Set up different user account and permissions in AWS S3, also using AWS EC2 for cloud computing: https://themassagenegotiator.com/blog/f/aws-amazon-web-services-ec2-elastic-cloud-computing-vm  * Analyzed crime statistics from 12 policing metropolitan cities and real estate statistics from Zillow using Tableau, Python, Excel, and R: https://themassagenegotiator.com/blog/f/cash-flow-wage-gap-and-theft-crimes-per-top-14-metros  * Online computational research using GEO, BLAST, ENCODE, PFAM and the like: https://github.com/JanJanJan2018/UL_research_files_Final_Week_Due  * After ing for a credit reporting agency, I developed a fictitious client database to help with formatting of credit reporting information using the Metro2 standards using random number generators and extracted data over the web of popular names of people and streets using Python and Excel: https://themassagenegotiator.com/blog/f/building-a-fictitious-database-for-credit-reporting-program *   * Exploratory analysis on word mining from text documents using R and Orange Biolab software for machine learning on document classification based on text mining: https://themassagenegotiator.com/blog/f/the-bio-lab-orange-word-cloud-software and https://github.com/JanJanJan2018/ufcNFLjloABkylieHaterComments/blob/master/FileRenamePasteName.R    EXPERIENCE:  * Crediauto Financial: Database Analyst and Credit Reporting Analyst | | April 2018-July 2018 employee | 2150 Palomar Airport Rd.Suite 209, Carlsbad, CA 92011 | supervisor: Michael Fattahi | 858-382-4077 * Massage Therapist: themassagenegotiator.com | 12+ years independent and employed. Various places employed or contracted with wages from minimum plus tips to only commission plus tips, or minimum plus service, commission, and tips.  * Massage Heights | Chino Hills 91709 | 13925 City Center Dr. | May 2019  Present | Job Title: LMT | Supervisor: Tamera | 909-536-1477 * Massage Heights | Rancho Cucamonga 91739 | 12188 Foothill Blvd. | Mar 2019  June 2019 | Job Title: LMT | Supervisor: Phil | 909-922-2013 * Heller Chiropractic | Costa Mesa 92626 | 2900 Bristol St. | Feb 2019  Mar 2019 | Job Title: Independent LMT | Supervisor: Nastassia | 714-557-9454 * OC Wellness Center | Westminster  92683 | 14120 Beach Blvd. #214 | Sep 2018  Nov 2018 | Job Title: Independent LMT | Supervisor: Anna | 866-303-9355 * Massage Heights | Newport Beach 92660 | 1334 Bison Ave. | July 2017  Present | Job Title: LMT | Supervisor: Michelle or Kristin | 949-644-7352 * Massage Green | Corona 92881| 1312 E. Ontario Ave. | January 11, 2016  Present | Job Title: Licensed Massage Therapist CA | Supervisor: Libni | (951) 371-7918 * Hand and Stone Massage | Costa Mesa 92627| September 2014-January 2015 | Job Title: Licensed Massage Therapist | Supervisor: Tasha | (949) 645-4823  * Massage Envy | Yorba Linda 92886 | November 2013-July 2014 | Job Title: Licensed Massage Therapist | Supervisor: Colleen | (714) 701-0200 * Massage Envy | Tustin 92782 | June 2012-October 2013 | Job Title: Licensed Massage Therapist | Supervisor: Brittany | (714) 617-8900   "
