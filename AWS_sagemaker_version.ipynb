{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94PhLbg4tV_r"
   },
   "source": [
    "## Overview of project - this is the AWS sagemaker version\n",
    "This notebook is steps in bold.\n",
    "1. **Extract text from PDF Resumes**\n",
    "2. **Use Cosine similarity to match resumes to job postings**\n",
    "3. **Review Scoring with Data Visualizations and comparisons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket and prefix\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "prefix = 'sagemaker/hireone' # place to upload training files within the bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eKbvy9nA3z-1"
   },
   "source": [
    "## Step 1. Convert pdf resume to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqYFf-x3w8NB"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.summarization import keywords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVrknXC68y4w"
   },
   "source": [
    "Download the test pdf resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q78mSBSP47ho"
   },
   "outputs": [],
   "source": [
    "#location link\n",
    "#'https://raw.githubusercontent.com/AVJdataminer/HireOne/master/data/Binoy_Dutt_Resume.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install pdfminer\n",
    "#! pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSMbae635Zzg"
   },
   "source": [
    "Convert the pdf resume to text strings and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "id": "EsKQWFuc5YKb",
    "outputId": "5cb286ee-dd75-4ce6-cd31-fee2ddd85c2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Binoy Dutt - https://www.linkedin.com/in/binoydutt \\nEmail id: binoydutt@yahoo.com Mobile No.: +1-682-252-8637 https://github.com/binoydutt \\n\\nEDUCATIONAL  QUALIFICATION: \\nMS - Information  Systems (4.0/4.0) \\nUniversity of Texas at Arlington, Arlington, TX \\n(Business Statistics, OOPS, Data Mining, Data Science, DBMS, Project Management, Systems Analysis and Design, Big Data Analytics, Data Warehouse) \\nBS - Chemical Engineering  (3.69/4.0) \\n June - 2012 \\nNirma University, Gujarat, India \\n\\nMay  – 2018 \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nTECHNICAL SKILLS: \\nTools:  \\n\\nLanguages:  \\nSkills: \\n\\nTableau,  Hadoop  Api,  Spark,  Sqoop,  Flume,  Stata,  Weka ,  Hive,  Eclipse  Ide,  SAS,  MS  Project,  MySQL,  Mongodb,  QTP, \\nSelenium IDE, Jmeter,  Ms Office, Power BI \\nJava, SQL, Python, R \\nBusiness Statistical Analysis,  Big Data - Distributed Computing, Data Analysis, Data Visualization, Data Mining, Model \\nBuilding using Sklearn, Machine Learning, Deep Learning, Natural Language Processing, Data Cleaning/Pre-Processing, \\nBusiness Analytics, Systems Analysis and Design, Quality Assurance, Team Leading \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nJuly- 2018 – Present  \\n\\nPROFESSIONAL  EXPERIENCE: \\nCapital  One, Plano,  TX \\nRole: Data  Analyst – Actively participated in Data Base migration validating fields and creating Analytical Reports on Snowflake Cloud DB \\nusing SQL and Python. Performed Data analysis using Python and created automated validation scripts using NumPy, Pandas and S QL. \\nUniversity of Texas at Arlington,  Arlington, TX \\n             June -2017 – May - 2018 \\nRole: Data Assistant - Experience on dealing with Prospective student’s data for UTA along with the Data pre-processing and cleaning and \\nupdating the People Soft SQL Oracle DB portal. Creating and maintaining reports in Excel, SQL and Python.  \\n \\nIntellimedia Networks, Ahmedabad,  India    \\nRole: Lead Quality Analyst - Experience in leading QA team and requirement gathering. Worked upon improving processes and suggesting \\ninputs upon key areas of challenges and implementing effective sol utions. Introduced automation testing using Selenium. \\nVibrant Enterprise, Ahmedabad,  India   \\n \\nRole: Marketing  Analyst -Understanding client’s requirement and providing the suitable analytics bridging the gap between manufacturer \\nand customers. Creating dashboards/report on sales and customer data for targeting marketing using SQL and Python.  \\nCognizant  Technology  Solutions,  Pune, India   \\nRole:  User  Acceptance Lead and Data Analyst- Experience in developing insights based on customer behavior data. Acquired Customer \\nbehavior  using  web  logs  using  Python  and  SQL.  Playing  a  key  role  in  the  implementation  of  the  project/initiative  from  a  business \\nprospective and commenting on the Go/No-Go  decisions. \\n\\n             Sep-2015 – May  - 2016 \\n\\n              Dec-2012 – Dec-2014 \\n\\n              Jan-2015 – Aug-2015 \\n\\n            \\n\\n           \\n\\n        \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\nPROJECTS: \\n\\uf0b7 \\n\\n\\uf0b7 \\n\\n\\uf0b7 \\n\\nStatistical  Analysis/Model  Building  -  On  Players  data  using  SAS  using  linear  Regression  approach coming up with the best model \\nbased on various assumptions and criteria’s  like R2 in predicting the Player Salary and performing Hypothesis testing (Annova, Chi- \\nSquare, Two way Annova). \\nSocial  Media  Analytics  –  Collected  tweets  using  twython,  analyzed  and developed insights  implementing sentiment analysis using \\nTextblob,  WordCloud,  Topic  Modelling,  summarization  and  keyword  extraction  using  Genism  and optimizing the result  to gather \\nlocation-based insights – Python based \\nPokemon GO Analytics  – Data Collection of 11 variable usi ng web scrapping on Google Play/App Store for 3 months at an interval of \\n10  mins.  Analyzed  the  collected  Dataframe  using  Scatter  Plots  and  Co-relation  coefficient.  Compared  best  prediction  model \\ncomparing Linear, Ridge, Lasso and Elastic Net regressions to predict future date values. Performed Deep  Learning  analysis to identify \\nimage tags using Tensor Flow – Python based \\n\\n\\uf0b7  Resume Job Description Matching  – Collecting data from Glass Door using selenium and performing resume parsing. Pre-processing \\nthe data using stemming, generating a similarity score using 300-dimensional vector space modelled using Deep Learning - Word2Vec \\nand their TFIDF scores – Python based \\n\\uf0b7 \\nSystems Analysis  and Design –Took the project through planning, analysis, and design phases of SDLC with relevant diagrams. \\n\\uf0b7  Big Data Project – Performed  Predictive Machine learning project on Titanic Data set using Distributed Computing  using Spark ML. \\n\\nACHIEVEMENTS: \\n\\uf0b7  Awards:  High  Flyer  of  the  Month  -  2011  Cognizant  Technology  Solutions ,  Certification  of  Appreciation  –  2012  Lloyds  Banking \\n\\n\\uf0b7 \\n\\uf0b7 \\n\\nGroup/Cognizant Technology Solutions. \\nProcess Automation – At  Intellimedia Networks which saved 8 hours of efforts per bug fix/release. \\nStreamed lined the project delivery - At Intellimedia Networks with increased documentation and processes allowing 30% increase in \\nquality and overall productivity. \\n\\n\\uf0b7  Machine  Learning by Stanford University  on Coursera. Certificate earned at Friday, May 25, 2018 8:15 AM GMT \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n \\n\\n\\x0c'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "#test out reading one pdf file\n",
    "text = extract_text('data/Binoy_Dutt_Resume.pdf')\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X2AL3e-z5fAG"
   },
   "source": [
    "## Step 2. Matching job listings to resumes with cosine similarity\n",
    "### load the job description data and save to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "zxf5mc9Jw8OA",
    "outputId": "4dcd79e3-a19c-48ce-f5a7-e6f314e73bd1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobOrResumeDescription</th>\n",
       "      <th>role</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>: Artificial Intelligence / Machine Learning D...</td>\n",
       "      <td>Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: Data Scientist/Architect\\n: 6+ months + Hig...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: Data Analyst\\n: Davidson, NC\\n: 04+ Months\\...</td>\n",
       "      <td>Data Analyst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: Big Data Architect or Data Scientist\\n: New...</td>\n",
       "      <td>Data Scientist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>: Data Engineer\\n: Woonsocket, RI\\n: 6+ Months...</td>\n",
       "      <td>Data Engineer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              jobOrResumeDescription            role\n",
       "0  : Artificial Intelligence / Machine Learning D...       Developer\n",
       "1   : Data Scientist/Architect\\n: 6+ months + Hig...  Data Scientist\n",
       "2   : Data Analyst\\n: Davidson, NC\\n: 04+ Months\\...    Data Analyst\n",
       "3   : Big Data Architect or Data Scientist\\n: New...  Data Scientist\n",
       "4  : Data Engineer\\n: Woonsocket, RI\\n: 6+ Months...   Data Engineer"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/AVJdataminer/HireOne/master/data/job_descriptions.csv', encoding = 'unicode_escape')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XzYIKHX8HMXC"
   },
   "source": [
    "Clean up job description column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1UK0cxBZw8NJ"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.replace('\\n', ' ')                # remove newline\n",
    "    text = text.replace(':', ' ')\n",
    "    return text\n",
    "df['description'] = df.apply(lambda x: clean_text(x['jobOrResumeDescription']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3zxu0UuHqG4"
   },
   "source": [
    "Print first job desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "d6NG3nbDKvOE",
    "outputId": "c9063081-4ac7-4acc-f889-9129ca8dfc29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  Artificial Intelligence / Machine Learning Developer     Irving TX  Terms  Contract   Details             Bachelor's degree or 7-10 or more years of relevant  experience.     7+ years of server app development (design/develop/deploy).     3+ years of Python 3.x, experience in ML algorithms/data analytics.     5+ years of advanced SQL development (ER modeling, SQL scripts, stored procedures, functions, s) with RDBMS such as PostgreSQL/MS SQL Server.     3+ years on AWS S3, EC2, Serverless computing (Lambda).     3+ years of experience/familiarity with DevOps using Stash/Jenkins/Chef and Puppet.     Excellent communication  in interfacing with different cross-functional teams.         5+ years of experience in designing, building applications using .NET platform using C#, .NET Core, ORM, SQL, MS SQL Server, Visual Studio.     1+ years' experience in developing containerized Docker .net core apps.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yom1LRGaIc9J"
   },
   "source": [
    "Create a list from the cleaned job description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YE6II7vhIhxg"
   },
   "outputs": [],
   "source": [
    "jd = df['description'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q3Q8puCuIQEu"
   },
   "source": [
    "Build model to tag each job description as a seperate document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "FhMYG7L6Md3z",
    "outputId": "53e0834d-9a4f-4888-fce8-1b7e4c494dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=\"  Artificial Intelligence / Machine Learning Developer     Irving TX  Terms  Contract   Details             Bachelor's degree or 7-10 or more years of relevant  experience.     7+ years of server app development (design/develop/deploy).     3+ years of Python 3.x, experience in ML algorithms/data analytics.     5+ years of advanced SQL development (ER modeling, SQL scripts, stored procedures, functions, s) with RDBMS such as PostgreSQL/MS SQL Server.     3+ years on AWS S3, EC2, Serverless computing (Lambda).     3+ years of experience/familiarity with DevOps using Stash/Jenkins/Chef and Puppet.     Excellent communication  in interfacing with different cross-functional teams.         5+ years of experience in designing, building applications using .NET platform using C#, .NET Core, ORM, SQL, MS SQL Server, Visual Studio.     1+ years' experience in developing containerized Docker .net core apps.\", tags=[0])]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim import models\n",
    "# Create the tagged document needed for Doc2Vec\n",
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "train_data = list(create_tagged_document(jd))\n",
    "\n",
    "print(train_data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NsndaLppIhJv"
   },
   "source": [
    "Train the model on the job descriptions for matching later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIri67T3IlaI"
   },
   "outputs": [],
   "source": [
    "# Init the Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# Build the Volabulary\n",
    "model.build_vocab(train_data)\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"doc2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to s3 bucket\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('doc2vec.model').upload_file('doc2vec.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3gXtAilyI07r"
   },
   "source": [
    "Let's look at an example of how it converts a list of words to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "8lbisgLtIyIX",
    "outputId": "77209aaa-f0a9-4890-bdb8-bf0d06a75efd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00780979 -0.00644648 -0.00306589  0.00989605  0.00984437  0.00326291\n",
      "  0.00484538  0.00164365  0.00119862 -0.00875538 -0.00070328  0.00609018\n",
      "  0.00615306  0.00185711  0.00123987  0.00951959 -0.00200013  0.00508516\n",
      " -0.00913142 -0.0087736  -0.00365493  0.00469759 -0.0097369  -0.00736931\n",
      "  0.00567346  0.00839816  0.00244525  0.00335604  0.00847165 -0.00898291\n",
      " -0.00957979 -0.00881275  0.00113318  0.00835414 -0.00266704  0.00292283\n",
      "  0.00416033  0.00068854  0.00710566  0.00578847  0.00117751 -0.00391402\n",
      " -0.00928959  0.00558384 -0.00700451 -0.00558073  0.00560036 -0.00403199\n",
      " -0.0016452   0.00123571]\n"
     ]
    }
   ],
   "source": [
    "print(model.infer_vector(['data', 'science','python']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FBp0Z2fMmd-"
   },
   "source": [
    "Here we apply the model to each job description in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "83wjxiCoMl46"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range(len(jd)):\n",
    "    data.append(model.docvecs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.42520192, -1.3092649 , -0.04263038,  0.4279058 , -0.10579854,\n",
       "        -0.97848916,  0.39704782, -0.8654084 ,  1.3791399 ,  0.6703254 ,\n",
       "        -0.3810882 ,  0.92027277, -0.2372605 , -0.41007313, -1.0953057 ,\n",
       "        -0.3717445 , -0.18925144,  0.1201918 , -0.37750494, -0.2879344 ,\n",
       "        -0.32020578,  0.70193315, -1.0198233 , -0.15192962, -0.13900192,\n",
       "        -1.4699234 ,  0.05974163, -0.9802179 ,  0.1741539 ,  0.26403722,\n",
       "        -0.01968134, -0.45910412,  0.56507534,  0.6179015 , -0.32542878,\n",
       "        -0.26430875, -0.16219409, -0.7055974 ,  0.3019703 , -0.6720942 ,\n",
       "        -0.4591857 , -0.29650146,  0.11393345, -0.46844593,  0.06476261,\n",
       "         0.33086056, -0.11803669,  0.64546335,  0.220103  , -0.77934384],\n",
       "       dtype=float32),\n",
       " array([ 0.5347606 , -0.29791248, -0.09209407,  0.28679615,  0.3883696 ,\n",
       "        -0.2507692 ,  0.74912447, -0.2986683 ,  0.8155025 , -0.40040928,\n",
       "        -0.78248364, -0.04564193, -0.4029797 ,  0.25449017,  0.09985797,\n",
       "        -0.6713737 ,  0.41936225,  0.30315915, -0.6891875 , -0.19841647,\n",
       "        -0.01287439,  0.5203129 , -0.8838748 ,  0.27552608,  0.01398395,\n",
       "        -0.852794  , -0.37011454, -0.25388756, -0.30843565, -0.5622332 ,\n",
       "         0.7288489 ,  0.04874227, -0.01369846, -0.15434276, -1.0532786 ,\n",
       "         0.38011295, -0.13583408, -0.15544812,  0.27954724, -0.29514298,\n",
       "        -0.20664842, -0.536685  , -0.10930972,  0.8713408 , -0.69692355,\n",
       "         0.08044355,  0.1872217 ,  0.39199954,  0.60468537, -0.04903914],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_resume(new_doc):\n",
    "\n",
    "#loading the model\n",
    "    model=\"paper.model\"\n",
    "    m=gsm.Doc2Vec.load(model)\n",
    "print(\"model is loaded\")\n",
    "\n",
    "\treturn json.dumps(ret_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BeFuMMLJAB6"
   },
   "source": [
    "## Now let's load the text from our resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "-vD2ko0AKL-q",
    "outputId": "143fe088-77b0-4ddc-ffcd-27964375911b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobOrResumeDescription</th>\n",
       "      <th>role</th>\n",
       "      <th>sourceType</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Â  with around 5 years of experience in all p...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>resume</td>\n",
       "      <td>Â  with around 5 years of experience in all p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n \\nData scientist with a strong math backgro...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>resume</td>\n",
       "      <td>Data scientist with a strong math backgroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n* Around 4+  years of experience in Data...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>resume</td>\n",
       "      <td>* Around 4+  years of experience in Data An...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n\\nExpert in logical and problem-solving  wit...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>resume</td>\n",
       "      <td>Expert in logical and problem-solving  with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Experienced  with 2+ years of hands-on experie...</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>resume</td>\n",
       "      <td>Experienced  with 2+ years of hands-on experie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              jobOrResumeDescription            role  \\\n",
       "0  Â  with around 5 years of experience in all p...  Data Scientist   \n",
       "1  \\n \\nData scientist with a strong math backgro...  Data Scientist   \n",
       "2  \\n\\n\\n* Around 4+  years of experience in Data...  Data Scientist   \n",
       "3  \\n\\nExpert in logical and problem-solving  wit...  Data Scientist   \n",
       "4  Experienced  with 2+ years of hands-on experie...  Data Scientist   \n",
       "\n",
       "  sourceType                                        description  \n",
       "0     resume  Â  with around 5 years of experience in all p...  \n",
       "1     resume     Data scientist with a strong math backgroun...  \n",
       "2     resume     * Around 4+  years of experience in Data An...  \n",
       "3     resume    Expert in logical and problem-solving  with ...  \n",
       "4     resume  Experienced  with 2+ years of hands-on experie...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = pd.read_csv('https://raw.githubusercontent.com/AVJdataminer/HireOne/master/data/resumes.csv', encoding = 'unicode_escape')\n",
    "resume.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzdqFARmLUW0"
   },
   "source": [
    "We only need one resume to start with so let's select the first one and split into words to push into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PeZUSh18P_Ye"
   },
   "outputs": [],
   "source": [
    "#select one row for one resume to input\n",
    "r1 = resume['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blLQ9AqBLblu"
   },
   "outputs": [],
   "source": [
    "#spit the resume to be put in the model\n",
    "resume = resume['description'].iloc[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXdIQ6qzMNxv"
   },
   "source": [
    "Review the resulting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "r3ciaV5xMH-n",
    "outputId": "d314c6af-4fb6-4715-f0b3-b991e0b136a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27107853 -0.7261962   0.99032944  1.1882536   0.03354041  0.06974223\n",
      " -1.094732   -0.60880846  0.68811274 -0.19271101 -0.4033713   0.43284371\n",
      " -2.2821734   0.05450503  0.679879    0.09024543 -0.5191957   0.0099173\n",
      " -0.7345251   0.73208183 -0.5478404  -0.02232062 -0.09744713 -1.4524012\n",
      "  0.04702066  0.31641507  0.06645563  1.0455699   0.16331407  0.49770996\n",
      " -1.9085443  -1.136299   -0.90063626 -0.02473042 -0.4723798   0.48916912\n",
      "  0.14631593 -1.0307761  -1.3113008   0.3127503  -0.6040659   0.23199959\n",
      " -1.3394972   0.06328759  0.19346857  0.43667534  0.3575999   0.3960119\n",
      "  0.71312     0.09871811]\n"
     ]
    }
   ],
   "source": [
    "#apply and view model scores\n",
    "print(model.infer_vector(resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdG-iOPhMTJF"
   },
   "outputs": [],
   "source": [
    "#apply models scores\n",
    "resume_vect = model.infer_vector(resume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYvrFWw9M4dh"
   },
   "source": [
    "## Compare our resume to the job descriptions using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "colab_type": "code",
    "id": "vY7SGrOwMbgB",
    "outputId": "50405b4c-ee60-48e2-db15-27d0d7ce4540"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_pca(data):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2) #, whiten=True\n",
    "    X = pca.fit_transform(data)\n",
    "    xs,ys =X[:,0], X[:,1]\n",
    "    plt.scatter(X[:,0], X[:,1])\n",
    "    plt.scatter(xs[-1], ys[-1], c='Red', marker='+')\n",
    "    plt.text(xs[-1], ys[-1],'resume')\n",
    "    plt.grid()\n",
    "    plt.suptitle('PCA')\n",
    "    #plt.savefig('distance_PCA_improved.png')\n",
    "    plt.show()\n",
    "plot_pca(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyMDvR4-NG3d"
   },
   "source": [
    "## Calculate the cosine distances between our resume and each of the job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wZYbYjokM-KO"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "cos_dist =[]\n",
    "for i in range(len(data)):\n",
    "    cos_dist.append(float(cosine_distances(resume_vect[0:].reshape(1,-1),data[i].reshape(1,-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mf_bGHdyNja6"
   },
   "source": [
    "create a key words list for each job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVYzteudNgqZ"
   },
   "outputs": [],
   "source": [
    "key_list =[]\n",
    "\n",
    "for j in jd:\n",
    "    key =''\n",
    "    for word in keywords(j).split('\\n'):\n",
    "        key += '{} '.format(word)\n",
    "    key_list.append(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "whc0xp1xNa4y"
   },
   "source": [
    "Create a nice data frame to put the scores and keywords together. Print out the first 10 lowest scores. Those jobs will the most similar to the resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "id": "HM85b1XbNW_o",
    "outputId": "4b0136da-7d70-474c-885d-0e5cece648ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Role Title</th>\n",
       "      <th>Cosine Distances</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>0.411758</td>\n",
       "      <td>knowledge engineer experience learning data go...</td>\n",
       "      <td>- Data Scientist / Data Engineer    - Chica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>0.525758</td>\n",
       "      <td>knowledge experience scientist learning google</td>\n",
       "      <td>Data Scientist    Chicago, IL    Contract &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>0.571648</td>\n",
       "      <td>computational high computing numerical methods...</td>\n",
       "      <td>Applied Computational Mathematician / Engineer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>0.583079</td>\n",
       "      <td>experience data lead leading bdm time field di...</td>\n",
       "      <td>Big Data Lead    Raritan- NJ    -12 months  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>0.641342</td>\n",
       "      <td>experience experiments data models model model...</td>\n",
       "      <td>Data Scientist     Richardson, TX    Full T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>0.653861</td>\n",
       "      <td>solutions like perform alternative solution pr...</td>\n",
       "      <td>Sr. Business Analyst UC Innovation - Irvine, C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Developer</td>\n",
       "      <td>0.662683</td>\n",
       "      <td>years developer development developing sql ser...</td>\n",
       "      <td>Artificial Intelligence / Machine Learning D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>0.674135</td>\n",
       "      <td>data experience hive technical terms term cont...</td>\n",
       "      <td>Data  Engineer     Plano TX  Terms  Long Te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>0.693346</td>\n",
       "      <td>data biomarker clinical patient good multivari...</td>\n",
       "      <td>Clinical Data Analyst ll   7+ months Contra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>0.711819</td>\n",
       "      <td>data analytics analytical experience software ...</td>\n",
       "      <td>Data Analytics Consultant   Basking Ridge, N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Role Title  Cosine Distances  \\\n",
       "34  Data Scientist          0.411758   \n",
       "35  Data Scientist          0.525758   \n",
       "98   Data Engineer          0.571648   \n",
       "20  Data Scientist          0.583079   \n",
       "30  Data Scientist          0.641342   \n",
       "76    Data Analyst          0.653861   \n",
       "0        Developer          0.662683   \n",
       "13   Data Engineer          0.674135   \n",
       "23    Data Analyst          0.693346   \n",
       "15    Data Analyst          0.711819   \n",
       "\n",
       "                                             Keywords  \\\n",
       "34  knowledge engineer experience learning data go...   \n",
       "35    knowledge experience scientist learning google    \n",
       "98  computational high computing numerical methods...   \n",
       "20  experience data lead leading bdm time field di...   \n",
       "30  experience experiments data models model model...   \n",
       "76  solutions like perform alternative solution pr...   \n",
       "0   years developer development developing sql ser...   \n",
       "13  data experience hive technical terms term cont...   \n",
       "23  data biomarker clinical patient good multivari...   \n",
       "15  data analytics analytical experience software ...   \n",
       "\n",
       "                                      Job Description  \n",
       "34     - Data Scientist / Data Engineer    - Chica...  \n",
       "35     Data Scientist    Chicago, IL    Contract &...  \n",
       "98  Applied Computational Mathematician / Engineer...  \n",
       "20    Big Data Lead    Raritan- NJ    -12 months  ...  \n",
       "30     Data Scientist     Richardson, TX    Full T...  \n",
       "76  Sr. Business Analyst UC Innovation - Irvine, C...  \n",
       "0     Artificial Intelligence / Machine Learning D...  \n",
       "13     Data  Engineer     Plano TX  Terms  Long Te...  \n",
       "23     Clinical Data Analyst ll   7+ months Contra...  \n",
       "15    Data Analytics Consultant   Basking Ridge, N...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = df['role'].tolist()\n",
    "summary = pd.DataFrame({\n",
    "        'Role Title': role,\n",
    "        'Cosine Distances': cos_dist,\n",
    "        'Keywords': key_list,\n",
    "        'Job Description': jd\n",
    "    })\n",
    "z = summary.sort_values(by ='Cosine Distances', ascending=True)\n",
    "z.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Y2zTt1XPpFE"
   },
   "source": [
    "Let's print the first job description and our resume text to visually compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "PGRBAfmPPnZl",
    "outputId": "6207ec44-3a5e-4414-ef5a-fbacf1de7f1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   - Data Scientist / Data Engineer    - Chicago, IL    - Long Term  Exp Req  - 8+ Years        -      Senior data scientist / engineer      Financial Domain Knowledge & experience     Strong Experience in AI related      Knowledge & exposure in rendering ML functionality     Understanding  AI/Deep Learning algorithm such as CNN, RNN, LSTM     Experience in building AI based NLP and OCR solution using Keras, Google Tensorflow, Theano, Caffe 2 etc? '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z['Job Description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "1M8mMowlP5OB",
    "outputId": "d13b3aff-57a3-47e9-9944-697a9be86afc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Â\\x95  with around 5 years of experience in all phases of diverse   specializing in Data Science, Big Data, Azure Machine Learning, Google Cloud and Tableau, using Cloud based infrastructure.   Â\\x95 ed on analyzing large datasets on distributed databases and developing Machine Learning algorithms to gain operational insights and present them to the leadership.   Â\\x95 Extensively ed on Data preparation, exploratory analysis, Feature engineering using supervised and unsupervised modeling.   Â\\x95 Experienced the full software life cycle in SDLC, Agile and Scrum methodologies.   Â\\x95 Expert in using of statistical  and programming languages (R, Python, C, C++, Java, SQL, UNIX)   Â\\x95 Adapted statistical programming languages like R and Python   Â\\x95 Well-versed with Linear/non-linear, regression and classification modeling predictive algorithms.   Â\\x95 Actively involved in model selection, statistical analysis using SAS and Gretl statistical tool.   Â\\x95 Created dashboards as part of Data Visualization using Tableau.   Â\\x95 Proficiency in using Spark for Bigdata processing in the Hadoop/DataProc/ EMRE ecosystem.   Â\\x95 Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values using Talend tool.   Â\\x95 Performed Dimensionality reduction using principal component analysis, auto encoders, and t-SNE.   Â\\x95 Validate the consolidated data and develop the model that best fits the data. Interpret data from multiple sources, consolidate it, and perform data cleansing using R/Python/Spark.   Â\\x95 Performed multiple Data Mining techniques and derive new insights from the data.   Â\\x95 Skilled in Machine Learning, Statistical Modeling, and Big Data.   Â\\x95 Creative problem-solver with strong analytical, leadership, and communication    Â\\x95 Proficient in Python, R, Scala, Java, SQL, and C   Â\\x95 Experienced in Machine Learning, Data mining with large datasets of Structured and Unstructured Data, Data Acquisition, Data Validation, and Predictive Modeling   Â\\x95 Data Science Specialties include: Machine Learning, Sequential Modeling, Natural Language Processing (NLP)   Â\\x95 Use of Analytical : Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Part of Speech Tagging, and Predictive Analytics   Â\\x95 Experienced in stochastic optimization and regression with machine learning algorithms   Â\\x95 Experienced in formulating and solving discrete and continuous optimization problems   Â\\x95 Able to research statistical machine learning, supervised learning, and classification methods   Â\\x95 Strong mathematical and statistical modeling and computer programming  in an innovative manner   Â\\x95 Use of Various Analytics : Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, NaÃ¯ve Bayes, Support Vector Machines   Â\\x95 Experienced in AWS cloud computing, Spark, and capable of ing with large datasets   Â\\x95 Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Nets, TensorFlow, Keras   Â\\x95 Delivered presentations and highly  reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights   Â\\x95 Development of clear analytical reports which directly address strategic goals   Â\\x95 Identified and learn applicable new techniques independently as needed   Â\\x95 Able to  comfortably and effectively within an interdisciplinary research environment   Â\\x95 Experienced with validation of machine learning ensemble classifiers   Â\\x95 Utilized the online datasets to implement machine learning models using Spark ML for building prototypes  Willing to relocate: Anywhere  Authorized to  in the US for any employer   Experience  Data Science Engineer  Jefferies LLC - Jersey City, NJ  February 2019 to Present  Responsibilities:   Â\\x95 Coded in Python with selenium and automated website data scraping   Â\\x95 Scripted using R for cleaning, merging and extraction of relevant data   Â\\x95 Created interactive visualization using Tableau and performed data analysis to report findings and trends   Â\\x95 Analyzed massive data models with tables having over 100s of millions of records to draw insights and useful information.   Â\\x95 Architect complex database systems on Hadoop to scale out data development processes.   Â\\x95 Explore  and gather insights from Amfam's operational data stores and data warehouses (stored in Oracle, Greenplum, HDFS and S3) by querying them and creating data visualizations (in Tableau).   Â\\x95 Designed and developed scripts to test data and find data defects.   Â\\x95 Determined the quality of data, verify accuracy of information and ensure that the data is fit for modeling purposes.   Â\\x95 Transformed data elements and attributes into usable form based on business requirements.   Â\\x95 Blend data sets at different granularity levels using analytical queries, window functions and SQL joins.   Â\\x95 Identified data duplicates and develop means to remove them.   Â\\x95 Analyzed tabular data to determine or alter their grain (drill down or roll up) using analytical queries, MapReduce or python.   Â\\x95 Designed and develop data pipelines to preprocess modeling data such as handle null values and clean up defective data attributes.   Â\\x95 Developed Spark code to parse out and transform semi structured data such as XMLs, JSONs and CSVs into hive tables or data frames.   Â\\x95 Explored and determined ways to organize data in Hive tables for fast read and writes through hive table partitions and buckets for optimized performance.   Â\\x95 Developed programs to store data in appropriate file formats and logical grouping in tables.   Â\\x95 Optimized code and queries to run faster and efficiently. Optimize ETL processes for distributed data marts on Hadoop   Â\\x95 Maintained development activities in version control and create updated documentation.      Environment: HADOOP (HDFS) Horton s, AWS, SPARK, Python, Java, Hive, Beeline, Apache pig, Tableau, SAS, Oracle, DB2, MySQL  Data Scientist  Anthem - Atlanta, GA  June 2018 to February 2019  Responsibilities:   Â\\x95 Translated business questions into research s, design and conduct analyses, develop findings and synthesize recommendations to deliver valuable, relevant, and actionable insights   Â\\x95 Strong track record of contributing to successful end-to-end analytic solutions (clarifying business s and hypotheses, communicating project deliverables and timelines, and informing action based on findings)   Â\\x95 Used Pandas, NumPy, Scikit-Learn in Python for performing exploratory analysis and developing various machine learning models such Random forest   Â\\x95 The missing data in the dataset is handled using Imputer method in SkLearn library   Â\\x95 Performed categorical variable analysis using python Label Encoder, fit transform, One Hot Encoder methods in sklearn library   Â\\x95 Responsible for design and development of advanced R/Python programs to prepare to transform and harmonize data sets in preparation for modeling   Â\\x95 Defined a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores   Â\\x95 Advanced SQL ability to efficiently  with very large datasets. Ability to deal with non-standard machine learning datasets   Â\\x95 Built forecasting models in Python using Gradient Boost Regression Trees. Forecasted the revenue for future   Â\\x95 ed with applied statistics and applied mathematics  for performance optimization   Â\\x95 ed with K-Means clustering and Hierarchical clustering algorithm to do segmentation of stores   Â\\x95 Collected various store attributes and added them into our segmentation model in order to better classify different segments using clustering algorithms   Â\\x95 Used cross-validation to test the models with different batches of data to optimize the models and prevent over fitting   Â\\x95 Analyzed the SQL scripts and designed the solution to implement using PySpark and developed scripts as per the requirement   Â\\x95 ed with Tableau in order to represent the data in visual format and better describe the problem with solutions.      Environments: Python, PyCharm, Jupyter, Notebook, Spyder, R, Tableau, MySQL  Data Scientist  US Bank - Brookfield, WI  September 2017 to June 2018  Responsibilities:   Â\\x95 Used SQL alongside a variety or reporting  - BusinessObjects, Power BI, Tableau - to develop operational and visual reports for KPI monitoring   Â\\x95 Data analysis and visualization (Python, R)   Â\\x95 Designed, implemented and automated modeling and analysis procedures on existing and experimentally created data   Â\\x95 Increased pace & confidence of learning algorithm by combining state of the art  and statistical methods; provided expertise and assistance in integrating advanced analytics into ongoing business processes   Â\\x95 Parsed data, producing concise conclusions from raw data in a clean, well-structured and easily maintainable format   Â\\x95 Implemented Topic Modelling, linear classifier models   Â\\x95 Collaborate with UI engineers, project managers, and designers to develop web portal that aggregates reports from various sources and    Â\\x95 Member of Data Science team tasked with helping clients turn data into a strategic asset   Â\\x95 Focused on front end features, browser manipulation, and cross-browser compatibility.   Â\\x95 Utilize  expertise including HTML5, CSS3, JavaScript, JQUERY, HTML, Node.js, Angular.js, and DOM to develop reporting portal.   Â\\x95 Used Agile Scrum for BI  across different clients, which allowed for production prototyping, rapid deployment and transparency.      Environment: Tableau, SQL, Java, HTML, Oracle, Agile, Hadoop.  Junior Data Scientist  Ordnance Factory Board - IN  January 2016 to July 2017  Description: Built a new team and managed in designing cost effective A/B tests to determine high performance marketing campaigns and contributed to increase in sales by 20% and reduced the promotional cost by 35%.      Responsibilities:   Â\\x95 ed on various phases of data mining- data collection, data cleaning, developing models, validation, visualization.   Â\\x95 Captured Modelling requirements from Senior Stakeholders to fetch functional requirements for SAS/R, Python.   Â\\x95 Performed Data Manipulation and Aggregation from various sources including HDFS and created various Predictive and Descriptive analytics using R and Tableau.   Â\\x95 Used various libraries and developed various matching learning algorithms using Pandas, NumPy, Seaborn, Scipy, matplotlib, Scikit-learn in python.   Â\\x95 Designed Predictive analysis algorithms using Historical Data.   Â\\x95 Utilized machine learning algorithms as well as implemented algorithms such as Decision Tree, linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, & KNN.   Â\\x95 ed on Map Reduce/Spark Python modules for machine learning and predictive analytics in Hadoop on AWS.   Â\\x95 ed on Reporting tool (Tableau) Test, Validate Data Integrity of Reports.      Environment: Python, I Python, Scikit-Learn, MySQL, SQL, NoSQL, Data Modelling, Data Warehouse, Hadoop (MapReduce, HBase, Hive), Gradient Boost, Random Forest, Neural Nets, Sklearn etc.  Data Science Intern  iPrism Technologies - Hyderabad, Telangana  April 2015 to December 2015  Description: The project was to build a classification model predicting the probability of a customer who will not subscribe to paid membership, to help the marketing team to focus on improving the subscription rate.   Responsibilities:   Â\\x95 Collected data from end client, performed ETL and defined the uniform standard format   Â\\x95 Wrote queries to retrieve data from SQL Server database to get the sample dataset containing basic fields   Â\\x95 Performed string formatting on the dataset converting hours from date format to a numerical integer   Â\\x95 Used Python libraries like Matplotlib and Seaborn to visualize the numerical columns of the dataset such as day of week, age, hour and number of screens.   Â\\x95 Developed and implemented predictive models like Logistic Regression, Decision Tree, Support Vector Machine (SVM) to predict the probability of enrollment   Â\\x95 Used Ensemble learning methods like Random Forest, Bagging & Gradient Boosting& picked the final model based on confusion matrix, ROC & AUC & predicted the probability of customer enrollment   Â\\x95 Tuned the hyper parameters of the above models using Grid Search to find the optimum models   Â\\x95 Designed and implemented K-Fold Cross-validation to test and verify the model's significance   Â\\x95 Developed a dashboard and story in Tableau showing the benchmarks and summary of model's measure.      Environment: SQL Server 2012/2014, Python 3.x (Scikit-Learn, NumPy, Pandas, Matplotlib, Dateutil, Seaborn), Tableau, Hadoop  Education  Master's in Computer Engineering in Data Science and Analytics  Arizona State University - Tempe, AZ    Apache spark, Hadoop, Hive, Javascript, D3.js, Mapreduce, Natural, Pig, Python, Mapreduce, Data science, Hadoop, Machine learning, Natural language processing, Nosql, Ms sql server, Sql server, Mysql, Pl/sql, Sql  Additional Information   :   Programming languages Python, Java, R. C   Data  and Frames s Hadoop (Apache Spark, Pig, Hive, MapReduce), D3.js, Tableau, MATLAB, R-Studio   Data Science Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Neural Nets, Machine Perception   Operating systems Windows, Linux.   Databases MySQL, MS SQL Server, NoSQL   Web and Cloud Technologies AWS, HTML5   Languages Python 3.7, SQL, R 3.6, Java, JavaScript, PL/SQL\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iiuCjeStOYUB"
   },
   "source": [
    "\n",
    "Let's test it out on another resume or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1EG7ZhrOJyW"
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('https://raw.githubusercontent.com/JimKing100/techsearch/master/data/techsearch_p1.csv')\n",
    "df1 = df1.drop(df1.columns[0], axis=1)\n",
    "df2 = pd.read_csv('https://raw.githubusercontent.com/JimKing100/techsearch/master/data/techsearch_p2.csv')\n",
    "df2 = df2.drop(df2.columns[0], axis=1)\n",
    "both_df = pd.concat([df1, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "FMVm-XbrQZjQ",
    "outputId": "bb92079c-21b7-4afe-b9b5-09d18f046034"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>counts</th>\n",
       "      <th>city</th>\n",
       "      <th>job</th>\n",
       "      <th>low_salary</th>\n",
       "      <th>high_salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist (All Levels) - Santa Clara</td>\n",
       "      <td>LeanTaaS</td>\n",
       "      <td>Santa Clara, CA 95050</td>\n",
       "      <td>Help build technology that saves lives!\\n\\nWe'...</td>\n",
       "      <td>1259</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist (Intern) - United States</td>\n",
       "      <td>Cisco Careers</td>\n",
       "      <td>San Jose, CA</td>\n",
       "      <td>What You‚Äôll DoAcquire, clean and structure d...</td>\n",
       "      <td>1259</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>Stanford, CA</td>\n",
       "      <td>Data Scientist (Data Analyst 2)\\nJob Family: I...</td>\n",
       "      <td>1259</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist in Santa Clara, CA (corp-corp c...</td>\n",
       "      <td>Advantine Technologies</td>\n",
       "      <td>Santa Clara, CA</td>\n",
       "      <td>Job Description\\n\\nTitle : Data Scientist\\nLoc...</td>\n",
       "      <td>1259</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Palo Verde Consulting</td>\n",
       "      <td>Campbell, CA 95008</td>\n",
       "      <td>Job Title: Data ScientistLocation: Campbell, C...</td>\n",
       "      <td>1259</td>\n",
       "      <td>San Jose</td>\n",
       "      <td>data scientist</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>210000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job_title                 company  \\\n",
       "0          Data Scientist (All Levels) - Santa Clara                LeanTaaS   \n",
       "1            Data Scientist (Intern) - United States           Cisco Careers   \n",
       "2                                     Data Scientist     Stanford University   \n",
       "3  Data Scientist in Santa Clara, CA (corp-corp c...  Advantine Technologies   \n",
       "4                                     Data Scientist   Palo Verde Consulting   \n",
       "\n",
       "                location                                        description  \\\n",
       "0  Santa Clara, CA 95050  Help build technology that saves lives!\\n\\nWe'...   \n",
       "1           San Jose, CA  What You‚Äôll DoAcquire, clean and structure d...   \n",
       "2           Stanford, CA  Data Scientist (Data Analyst 2)\\nJob Family: I...   \n",
       "3        Santa Clara, CA  Job Description\\n\\nTitle : Data Scientist\\nLoc...   \n",
       "4     Campbell, CA 95008  Job Title: Data ScientistLocation: Campbell, C...   \n",
       "\n",
       "   counts      city             job  low_salary  high_salary  \n",
       "0    1259  San Jose  data scientist         NaN          NaN  \n",
       "1    1259  San Jose  data scientist         NaN          NaN  \n",
       "2    1259  San Jose  data scientist         NaN          NaN  \n",
       "3    1259  San Jose  data scientist         NaN          NaN  \n",
       "4    1259  San Jose  data scientist    150000.0     210000.0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLcPJxjoQ8pL"
   },
   "source": [
    "If you feel like you've got it, go ahead apply the steps we used up above on the job descriptions in this new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G_IwTIVUQbtE",
    "outputId": "05798f38-bda3-4222-ac9e-40e441cccbc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7827, 9)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "both_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMLKbLDdCoO6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BnTE8fJ35rXW"
   ],
   "include_colab_link": true,
   "name": "HiringApp_NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
